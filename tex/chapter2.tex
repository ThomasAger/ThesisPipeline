\chapter{Background}
% What text representations actually are
% Why text representations are useful, what they are used for, how do they relate to our work, how do they relate to interpretability
% Classification, what classification is, example with Decision Tree
% What semantic spaces actually are
% Examples of classification with SVM's 
% What interpretable representations actually are, desiradata/features of interpretable representations
% Examples with Decision trees, examples of classifiers that produce an interpretable result automatically
\section{Text Representations}
Need to write about the concept of salient features of a domain here.
\subsection{Bag-of-words}\label{background:BOW}
We begin by processing an unstructured text corpus, composed of documents $C_D$. We then remove all punctuation, convert any accented characters to non-accented characters, and lowercase the documents to obtain word tokens for each document $D_W$. From here, we can assume that any $W \approx W$ will now $W = W$, if a word varied in format but not alphanumeric characters. 

Then, we count the occurrences of each word
\begin{itemize}
	\item Frequency
	\item Tf-idf
	\item PPMI \label{bg:ppmi}
\end{itemize}
\section{Text classification}
\subsection{Decision Trees}\label{bg:trees}
\begin{itemize}
	\item Explanation of what decision trees are
	\item Explanation that they may not perform well on sparse information
	\item Max features
	\item Criterion
	\item CART decision trees versus others
\end{itemize}
\subsection{Support Vector Machines}\label{bg:SVM}
\begin{itemize}
	\item Performance increase for support vector machines on sparse data, balancing, etc
	\item C parameters, gamma parameters
\end{itemize}
\subsection{Neural Networks}
\begin{itemize}
	\item Difference between SVM and Nnet
\end{itemize}
\subsection{Semantic Spaces}\label{bg:SemanticSpaces}
Bag-Of-Words representations of text result in large sparse vectors for each document, 


% What representations are semantic spaces? What is not a semantic space?
\subsubsection{How do vector spaces represent semantics? Why do we use them to represent semantics?}\label{background:WhySpace}
Distributional representations of semantics, known as 'semantic spaces' are well-recognized for their ability to represent semantic information spatially. These representations have been widely adopted for Natural Language Processing (NLP) tasks %Tasks here
thanks to their ability to represent complex information in a dense representation. In particular, entity-embeddings have been applied  to represent items in recommender systems \cite{Vasile:2016:MPE:2959100.2959160,liang2016factorization,van2016learning}, to represent entities in semantic search engines \cite{DBLP:conf/sigir/JameelBS17,van2017structural}, or to represent examples in classification tasks \cite{DBLP:conf/iccv/DemirelCI17}. %Copied from CONLL paper. Shift this to talk more about applications and tasks rather than specific stuff related to our ideas.

Vector spaces are a popular way to represent unstructured text data, and have been broadly applied to and transformed by supervised approaches. They vary in method, producing structure from Cosine Similarity, Matrix Factorization, Word-Vectors/Doc2Vec, etc. %More refs
They also vary in how they linearly separate entities. %How?
However, their commonality is that they are able to represent semantic relationships spatially. %ref
See Section \ref{background:WhySpace}
This brings up an essential point: When using a semantic space, are we taking advantage of relationships that are discriminative or incorrect? The danger of relying on these spaces and the models that use them has greatly affected their adoption in critical application areas like medicine, %Citation needed
and has raised legal concerns about their application in e.g. determining if someone is suitable for a loan. 

See Section \ref{background:WhySpace}\begin{itemize}
	\item Word-vectors
\end{itemize}
\subsection{Document Representations}
\subsubsection{LSA}
Principal Component Analysis is a dimensionality reduction method that results in dimensions ordered by importance. Starting with a large data matrix, e.g. our TF-IDF values from before, we first find the covariance matrix for these values. Then, from this covariance matrix we obtain the eigenvalues. We can then linearly transform the old data in-terms of this covariance matrix to obtain a new space of size equal to an arbitrary value smaller than our matrix.

\subsubsection{Dimensionality Reduction Methods}\label{ch2:reps}
\begin{itemize} 
	\item PCA %Maybe explain directions a bit here
	\item MDS
\end{itemize}
\section{Interpretable Representations}\label{ch2:Interpretability}
a. NNSE
b. compositional
c. 2007 paper as wikipedia similarities
d. Topic models\label{bg:TopicModel}
e. Infogan, etc

\cite{Zhang2012} Sparse PCA (Why not compare lol)

Vector space models typically use a form of matrix factorization to obtain low-dimensional document representations. By far the most common approach is to use Singular Value Decomposition \cite{ASI:ASI1}, although other approaches have been advocated as well. 
Instead of matrix factorization, another possible strategy is to use a neural network or least squares optimization approach. This is commonly used for generating word embeddings \cite{DBLP:conf/nips/MikolovSCCD13,glove2014}, but can similarly be used to learn representations of (entities that are described using) text documents \cite{DBLP:journals/corr/DaiOL15,van2016learning,DBLP:conf/sigir/JameelBS17}. Compared to topic models, such approaches have the advantage that various forms of domain-specific structured knowledge can easily be taken into account. Some authors have also proposed hybrid models, which combine topic models and vector space models. For example, the Gaussian LDA model represents topics as multivariate Gaussian distributions over a word embedding \cite{DBLP:conf/acl/DasZD15}. Beyond document representation, topic models have also been used to improve word embedding models, by learning a different vector for each topic-word combination \cite{DBLP:conf/aaai/LiuLCS15}. %copy pasted

The most commonly used representations for text classification are bag-of-words representations, topic models, and vector space models. Bag-of-words representations are interpretable in principle, but because the considered vocabularies typically contain tens (or hundreds) of thousands of words, the resulting learned models are nonetheless difficult to inspect and understand. Topic models and vector space models are two alternative approaches for generating low-dimensional document representations. %copy pasted

\subsection{Word Vectors}
