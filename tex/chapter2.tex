\chapter{Background}
\section{Text}
\subsection{Bag-of-words}\label{background:BOW}
We begin by processing an unstructured text corpus, composed of documents $C_D$. We then remove all punctuation, convert any accented characters to non-accented characters, and lowercase the documents to obtain word tokens for each document $D_W$. From here, we can assume that any $W \approx W$ will now $W = W$, if a word varied in format but not alphanumeric characters. 

Then, we count the occurrences of each word
\begin{itemize}
	\item Frequency
	\item Tf-idf
	\item PPMI
\end{itemize}
\section{Text classification}
\subsection{Decision Trees}
\begin{itemize}
	\item Explanation of what decision trees are
	\item Explanation that they may not perform well on sparse information
\end{itemize}
\subsection{Support Vector Machines}
\begin{itemize}
	\item Performance increase for support vector machines on sparse data, balancing, etc
\end{itemize}
\subsection{Neural Networks}
\begin{itemize}
	\item Difference between SVM and Nnet
\end{itemize}
\subsection{Vector Spaces}
\subsubsection{How do vector spaces represent semantics? Why do we use them to represent semantics?}\label{background:WhySpace}
\subsection{Word Representations}
\begin{itemize}
	\item Word-vectors
\end{itemize}
\subsection{Document Representations}
\subsubsection{Conceptual spaces}
\begin{itemize}
	\item PCA %Maybe explain directions a bit here
	\item MDS
\end{itemize}
\section{Interpretable Representations}
a. NNSE
b. compositional
c. 2007 paper as wikipedia similarities
d. Topic models
e. Infogan, etc

\cite{Zhang2012} Sparse PCA (Why not compare lol)

Vector space models typically use a form of matrix factorization to obtain low-dimensional document representations. By far the most common approach is to use Singular Value Decomposition \cite{ASI:ASI1}, although other approaches have been advocated as well. 
Instead of matrix factorization, another possible strategy is to use a neural network or least squares optimization approach. This is commonly used for generating word embeddings \cite{DBLP:conf/nips/MikolovSCCD13,glove2014}, but can similarly be used to learn representations of (entities that are described using) text documents \cite{DBLP:journals/corr/DaiOL15,van2016learning,DBLP:conf/sigir/JameelBS17}. Compared to topic models, such approaches have the advantage that various forms of domain-specific structured knowledge can easily be taken into account. Some authors have also proposed hybrid models, which combine topic models and vector space models. For example, the Gaussian LDA model represents topics as multivariate Gaussian distributions over a word embedding \cite{DBLP:conf/acl/DasZD15}. Beyond document representation, topic models have also been used to improve word embedding models, by learning a different vector for each topic-word combination \cite{DBLP:conf/aaai/LiuLCS15}. %copy pasted

The most commonly used representations for text classification are bag-of-words representations, topic models, and vector space models. Bag-of-words representations are interpretable in principle, but because the considered vocabularies typically contain tens (or hundreds) of thousands of words, the resulting learned models are nonetheless difficult to inspect and understand. Topic models and vector space models are two alternative approaches for generating low-dimensional document representations. %copy pasted


