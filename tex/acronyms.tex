\chapter*{List of Acronyms} 
\begin{acronym}
\acro{ML}{Machine Learning}
\acro{NLP}{Natural Language Processing}
\acro{NDCG}{Normalized Discounted Cumulative Gain}


\subsection{Definitions}

\textbf{Domain} Where the data was originally sourced from $DOM^IMDB$, e.g. IMDB movie reviews. 

\textbf{Word} A string of alphanumeric characters that originated from text in the domain $DOM_w$, e.g. the $w = "Horror"$ from a domain of IMDB movie reviews $DOM^IMDB$.

$w$

\textbf{Corpus of Documents} A unique group of words, e.g. a review from a domain of IMDB movie reviews $DOM_IMDB$.

$C_dw$

\textbf{Document} A document of words

$d_w$

\textbf{Vector Space} A representation composed of vectors.

$S_v$

\textbf{Semantic Space} A representation where spatial relationships between vectors correspond to semantic relationships.

$S_v$

\textbf{Word frequency} The frequency of a word $wf$ for its document $D_wf$.

$wf$


\textbf{Bag-Of-Words} a matrix BOW of documents $BOW_D$ where each document is composed of unordered frequencies of words $D = [wf_1, ..., wf_n]$. and Conceptual Space we obtain a representation of entities composed of properties. Then, we cover the additional methods we propose to improve this process.

$BOW_d$ 

\textbf{Bag-Of-Words PPMI}



\textbf{Feature} A feature is a distinct useful aspect of the domain, corresponding to a numerical value. 

$R_f$

\textbf{Hyper-plane} The hyper-plane for a word

$H_w$

\textbf{Direction vector} The orthogonal direction to a hyper plane that separates a word in a vector space. 

$D_w$

\textbf{Cluster label} A cluster of words that describe a property.

$C_w$

\textbf{Cluster direction} The averaged directions of all words in the label.
% What does a vector space have that other representations do not?

$D_C$

\textbf{Feature rankings} The rankings induced from a feature direction.

$R_DC$

% to use in the thesis, use: \ac{GPS}
% more goes here.
\end{acronym}


