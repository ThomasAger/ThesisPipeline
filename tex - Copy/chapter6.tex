\chapter{Conclusion}\label{ch6}


%Introduction+conclusion are shorter chapters

%Introduction
%Hypothesis
%research questions
%context
%Summary of the contributions
%Good to introduce gently but research questions can be fine
%Doesn't have to be accessible to the man on the street, need computer science background, vector spaces are included in that, so are decision trees and neural networks, if it's borderline reference background

%%Conclusion revisits research questions, provides the answer not just a summary

\section{Introduction}


%%%% WHAT IS THIS SECTION ABOUT? WHY AM I READING IT? WHAT IS IT?
This \hmark{c}hapter provides conclusions on how the work in this thesis contribute\hmark{s} to the hypothesis and research questions. 

\section{Thesis Summary and Contributions}

In Chapter \ref{ch1} the hypothesis was introduced. It was as follows:  

\begin{quote}
\hmark{\textit{Semantically meaningful features can be obtained from vector space representations of documents. These features are sufficiently predictive to be useful for simple interpretable classifiers, of which low-depth decision trees are a prototypical example, allowing for a performance that is close to the performance of an unbounded decision tree using the same features as input. Further,  these feature can be obtained from neural network hidden layers, and can provide valuable insights into  aspects of the domain that the neural network has learned. }}
\end{quote}

In Chapter \ref{ch3}, disentangled feature representations obtained using the methods in this work performed better than the original unsupervised representations when used as input to low-depth decision trees. These results showed their potential \hmark{as semantic features for interpretable classifiers}, \hmark{especially} as the classifiers in some cases performed better than linear \hmark{SVMs} trained on all features of the original unsupervised embedding.  The qualitative investigation found that the features are clear and meaningful, and these features were used to determine the difference between document embeddings and parameters of the method. The NDCG scoring metric introduced for use as a scoring metric for directions in this work was found to generally be a good choice, in particular because it had the most consistent performance when the resulting features were used as input to low-depth decision trees. \hmark{Varying the clustering method was found to be successful in some cases. }

In Chapter \ref{ch4} feed-forward and auto-encoder neural networks were \hmark{quantitatively and qualitatively} investigated. Characteristics of feed-forward networks that used BOW as input were identified and compared with unsupervised representations\hmark{. For} feed-forward networks, we found that higher-quality feature directions could be found, for those features that were relevant to the classification task on which the network was trained. Furthermore, we noticed that terms that were less relevant to the classification task scored less. Auto-encoders with increasingly low-dimensional embeddings were qualitatively and quantitatively investigated, and \hmark{despite not being effective at learning natural categories, they seemed to capture} increasingly abstract features. \hmark{This was validated quantitatively.} Finally,  a rule-based classifier that connected the features from layers together was used to further investigate what the neural network was doing. %Although the results were preliminary for the auto-encoder, they lay a methodological foundation for connecting together the layers of neural networks. 

In Chapter \ref{ch5} a methodology was introduced to fine-tune the original document embedding that the feature representation was obtained from. This was done in order to improve the directions, and in turn improve the associated rankings. It was found to improve the scores of the feature representations when used as input to low-depth decision trees. Additionally, a qualitative investigation showed that the entities were more intuitive and specific than before, and that noise had been eliminated.



% These low-depth decision trees when the disentangled feature representation was used as input, obtained from unsupervised representations were disentangled the, and a qualitative investigation was conducted to further verify that the dimensions correspond to clear semantic features. The disentangled feature-representation had strong performance on low-depth decision trees, sometimes matching the performance of the original representation when classified with a linear SVM. 

\section{Research Questions}

In the introduction, three research questions were asked. This section repeats those research questions and discusses how they were answered in the thesis.

\hmark{\textit{\textbf{Question 1:} Can meaningful semantic features be characterised as directions across a wide range of vector space encodings and domains, and do these semantic features allow us to learn effective low-depth decision trees?}}

In Chapter \ref{ch3} it was found that feature representations derived from unsupervised vector spaces contained clear semantic features, resulting in good disentangled representations. Chapter \ref{ch4} also provided some quantitative analysis of  disentangled representations obtained from the hidden layers of neural networks, finding that their performance when used as input to low-depth decision \hmark{trees} was close to, matched or exceeded the performance of the original network on the task. In Chapter \ref{ch5} the associated rankings of these features was made more clear and their performance on low-depth decision trees was improved, resulting in an overall better disentangled representation using an entirely unsupervised method.

\hmark{\textit{\textbf{Question 2:} Can semantic features be obtained from the hidden layers of neural networks, and to what extent can these features be used to  investigate  the characteristics of   different neural networks?}}

\hmark{From the qualitative analysis of the hidden layer representations in Chapter 5, it was clear that the} properties derived from these representations was related to how well they classified the task. \hmarkn{Quantitatively, properties derived from feed-forward networks when used as input to low-depth decision trees allowed them to achieve good performance classifying key domain tasks.} Some characteristics of feed-forward networks were identified, in particular that they are representing terms related to the task better than the unsupervised representations, and that \hmark{that they reduced how well terms unrelated to the task are captured in the space.} Additionally, increasingly low-dimensional auto-encoders were found to capture increasingly abstract properties.

\hmark{\textit{\textbf{Question 3:} Is it possible to obtain higher-quality semantic features in an unsupervised way by fine-tuning the initial vector space?}}

In Chapter \ref{ch5} an unsupervised methodology was introduced that fine-tuned the initial vector space, resulting in higher performance of low-depth decision trees  when the improved disentangled representations were used as input. Additionally, qualitative results showed that the entities had improved. 


\section{Future Work}

% Verifying the interpretability of the disentangled feature representations normal and fine-tuned
%		How will you verify the interpretability?thi
%		What can you compare it to?

This thesis has investigated the disentanglement of  vector spaces, but it has not experimentally validated how interpretable the  labels of the features  are. To do so, the intruder task from topic models could be used. Each group of words in a label (either the cluster words or the single-term word\hmark{s} and the words associated with its most similar directions) is given an intruder word from another cluster. The feature representation performs well if users are able to identify the intruder word. Another interesting test would be to use a domain, e.g.\ text in medicine, where some text classification task is required and verify that the decision trees make sense to expert users like doctors. 


% What form can explanations take

Although decision trees are interpretable, they may not make sense to end-users. One avenue for future work that will benefit many other applications of this method is converting low-depth decision trees that use disentangled features into human-readable explanations that can be understood by users in the domain. Although this might seem straightforward (e.g.\ the explanation could just be of the form IF Movie has "Horror", and it is "Romantic", then it is a Romantic Horror movie) it can be a bit more complicated when using features that do not have such a natural intuition or classifying things that cannot be put into such a simple format (e.g.\ Compare "IF Blood AND Gun THEN Mobsters" to IF there is "Blood" in the movie, and there are many "Gun" scenes, then the movie is likely about "Mobsters"). These kind of natural language explanations could result in better adoption of the method in real-world domains.

% Explaining neural networks with a proxy model low-depth decision tree
%		How will you ensure the model matches the predictions of the orignal neural network?
%		What form will the explanations take

This work also has potential applications  to eXplainable AI (XAI), methods that explain existing learned neural network models. Essentially, the approach outlined in \hmark{Section} \ref{ch4} would be used to create a "proxy model" that approximates the  behaviour that the neural network does in a simple way. Then, the quality of this proxy model is determined by how well it matches the predictions that the neural network made. Following the previous idea about transforming decision trees into natural language explanations, the method could be applied in a variety of domains to explain existing models to end-users.

% Creating alternative interpretable models that match or outperform state-of-the-art models
%	 	Which interpretable classifiers?
%	 	Which models? How will you do it?

Models learned from disentangled feature representations derived from neural network hidden layers could also act as interpretable alternatives to them. This brings up a primary question that follow-up work could address: can disentangled feature represenations be derived from neural networks that achieve state-of-the-art e.g.\ BERT \cite{Kenton1953}? BERT is a neural network that achieves strong results on a variety of tasks and has entered mainstream application usage,  but can the hidden layers of BERT be disentangled into interpretable features, and can an interpretable classifier be learned? This is but one of a variety of potential  state-of-the-art models that could be disentangled  to produce an alternative interpretable model.

% Using Rank SVM to learn the rankings rather than taking the orthogonal direction

In Chapter \ref{ch3} the linear SVMs were used to obtain hyperplanes that classify words on a binary occurrence task, i.e.\ if the word occurred or if it did not occur in the document. Following this, the orthogonal direction was taken and \hmark{documents} were ranked on it using the dot product. It would be interesting to instead try RankSVM \cite{Lee2012} to directly learn rankings of documents on words, where the "true ranking" is determined by the Positive Pointwise Mutual Information (PPMI) score. This would potentially result in a more accurate ranking derived from the spatial representation for the word.


% Improving the clusters by ensuring that they are not disrupted by noisy words w/ post-processing step?
%		What is the problem?
%		What is your idea for solving that problem?
%		How will it work?

In Chapter \ref{ch3} a potential problem with cluster features was identified, in particular that their  meaning may be disrupted due to terms unrelated to the task being clustered with them. It would be interesting for a qualitative investigation into this phenomenon, seeing if manual removal of words from clusters would indeed benefit their performance in low-depth decision trees. If this is the case, there is potential for a new clustering method that performs this process automatically. Theoretically this could be done by taking a bag of PPMI scores as in Chapter \ref{ch5, and if the similarity between the ranking of \hmark{documents} on the cluster direction and the bag of PPMI scores drops too far with addition of the new word then it could not be added. 


% Fine-tuning state-of-the-art neural networks such that their internal representations are disentangled
%		How will you do this? 
%		What will be different from preliminary tests?
%		What is the goal of this? How will it achieve that goal?


The fine-tuning process outlined in Chapter \ref{ch5} originally followed the results demonstrating  direction scores went down in when obtained from neural network hidden layers as they are  stacked. Given this, the idea was formed that it would be possible to fine-tune these spaces such that they can retain the semantic features in the first layer while also adjusting the representation according to the objective. However, in preliminary tests this did not yield such a result, instead of fine-tuning the representation it stopped learning information related to the task. This preliminary investigation was done by taking each resulting representation, fine-tuning it, and then using it as input to the next auto-encoder. There is potential future work in pursuing this idea in application to explainability and interpretability. In particular, work in investigating if neural networks can be forced to retain disentanglement during learning. If this is the case, it may be possible to make neural networks interpretable.




\section{Summary}

In summary, this thesis demonstrated that disentangled feature representations can be obtained from a variety of document embeddings, and a qualitative analysis was conducted on these representations. It was found that the features were clear and meaningful. Additionally, a method was introduced that fine-tunes vector spaces to improve these disentangled features. The methods seem promising in application to interpretability and explainability and offer much potential for future work.

%This thesis experimentally validates and improves on a methodology for obtaining disentangled feature representations for text documents. It is validated using document classification tasks, and disentangled features from neural networks are  qualitatively  analysed. The method is unsupervised, acts as a post-processing step on vector space representations, and  disentangles important semantic relationships in the space.  

