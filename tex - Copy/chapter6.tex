\chapter{Conclusion}\label{ch6}


%Introduction+conclusion are shorter chapters

%Introduction
%Hypothesis
%research questions
%context
%Summary of the contributions
%Good to introduce gently but research questions can be fine
%Doesn't have to be accessible to the man on the street, need computer science background, vector spaces are included in that, so are decision trees and neural networks, if it's borderline reference background

%%Conclusion revisits research questions, provides the answer not just a summary

\section{Introduction}


%%%% WHAT IS THIS SECTION ABOUT? WHY AM I READING IT? WHAT IS IT?
This Chapter provides conclusions on how the work in this thesis contribute to the hypothesis and research questions. 

\section{Thesis Summary and Contributions}

In Chapter \ref{ch1} the hypothesis was introduced. It was as follows:  

\begin{quote}
\textit{Vector space representations of documents can be disentangled  in an unsupervised way such that their dimensions correspond to clear semantic features. Furthermore, these disentangled representations provide a useful inductive bias to classifiers, and can be used to  investigate the hidden layers of neural networks to gain valuable insights into how they represent documents. }
\end{quote}

In Chapter \ref{ch3}, disentangled feature representations obtained using the methods in this work performed better than the original unsupervised representations when used as input to low-depth decision trees. These results showed their potential for providing an inductive bias, as the classifiers in some cases performed better than linear SVM's trained on all features of the original unsupervised embedding. This shows that the feature representation obtained by the method resulted in disentangled and semantic features. The qualitative investigation found that the features are clear and meaningful, and these features were used to determine the difference between document embeddings and parameters of the method. The NDCG scoring metric introduced for use as a scoring metric for directions in this work was found to generally be a good choice, in particular because it had the most consistent performance when the resulting features were used as input to low-depth decision trees. Using features obtained from cluster-directions that k-means produced also resulted in better performance than the Derrac variation on low-depth decision trees.

In Chapter \ref{ch4} feed-forward and auto-encoder neural networks were qualitatively investigated. Characteristics of feed-forward networks that used BOW as input were identified and compared with unsupervised representations.For feedforward networks, we found that higher-quality feature directions could be found, for those features that were relevant to the classification task on which the network was trained. Furthermore, we noticed that terms that were less relevant to the classification task scored less. It was found that neural networks that used the initial embeddings as input retained the noise that was present in that representation in addition to retaining features that were relevant to the task, however they had few new features. Auto-encoders with increasingly low-dimensional embeddings were qualitatively investigated, and seemed to capture increasingly abstract properties. Finally,  a rule-based classifier that connected the features from layers together was used to further investigate what the neural network was doing. %Although the results were preliminary for the auto-encoder, they lay a methodological foundation for connecting together the layers of neural networks. 

In Chapter \ref{ch5} a methodology was introduced to fine-tune the original document embedding that the feature representation was obtained from. This was done in order to improve the directions, and in turn improve the associated rankings. It was found to improve the scores of the feature representations when used as input to low-depth decision trees. Additionally, a qualitative investigation showed that the entities were more intuitive and specific than before, and that noise had been eliminated.



% These low-depth decision trees when the disentangled feature representation was used as input, obtained from unsupervised representations were disentangled the, and a qualitative investigation was conducted to further verify that the dimensions correspond to clear semantic features. The disentangled feature-representation had strong performance on low-depth decision trees, sometimes matching the performance of the original representation when classified with a linear SVM. 



\section{Research Questions}

In the introduction, three research questions were asked. This section repeats those research questions and discusses how they were answered in the thesis.

\textit{\textbf{Question 1:} Can directions be identified that correspond to clear semantic features in an unsupervised way, and how can they be used to achieve good disentangled representations and across a wide range of different types of document embeddings,  of documents across a range of domains and from a variety of vector space embeddings?}

In Chapter \ref{ch3} it was found that feature representations derived from unsupervised vector spaces contained clear semantic features, resulting in good disentangled representations. Chapter \ref{ch4} also provided some quantitative analysis of  disentangled representations obtained from the hidden layers of neural networks, finding that their performance when used as input to low-depth decision treees was close to, matched or exceeded the performance of the original network on the task. In Chapter \ref{ch5} the associated rankings of these features was made more clear and their performance on low-depth decision trees was improved, resulting in an overall better disentangled representation using an entirely unsupervised method.

\textit{\textbf{Question 2:} To what extent can these directions and associated disentangled representations be used to gain qualitative insights into the characteristics of different neural networks?}

Although the investigation of neural network hidden layer representations in Chapter \ref{ch4} was qualitative, it was clear that the properties derived from these representations was related to how well they classified the task. Some characteristics of feed-forward networks were identified, in particular that they are representing terms related to the task better than the unsupervised representations, and that they reduced noise. Additionally, increasingly low-dimensional auto-encoders were found to capture increasingly abstract properties.

\textit{\textbf{Question 3:} Is it possible to obtain higher-quality disentangled representations by fine-tuning the initial vector space, while remaining in an unsupervised setting?}

In Chapter \ref{ch5} an unsupervised methodology was introduced that fine-tuned the initial vector space, resulting in higher performance of low-depth decision trees  when the improved disentangled representations were used as input. Additionally, qualitative results showed that the entities had improved. 


\section{Future Work}

% Verifying the interpretability of the disentangled feature representations normal and fine-tuned
%		How will you verify the interpretability?thi
%		What can you compare it to?

This thesis has investigated the disentanglement of  vector spaces, but it has not experimentally validated how interpretable the  labels of the features  are. To do so, the intruder task from topic models could be used. Each group of words in a label (either the cluster words or the single term word and the words associated with its most similar directions) is given an intruder word from another cluster. The feature representation performs well if users are able to identify the intruder word. Another interesting test would be to use a domain, e.g. text in medicine, where some text classification task is required and verify that the decision trees make sense to expert users like doctors. 


% What form can explanations take

Although decision trees are interpretable, they may not make sense to end-users. One avenue for future work that will benefit many other applications of this method is converting low-depth decision trees that use disentangled features into human-readable explanations that can be understood by users in the domain. Although this might seem straightforward (e.g.\ the explanation could just be of the form IF Movie has "Horror", and it is "Romantic", then it is a Romantic Horror movie) it can be a bit more complicated when using features that do not have such a natural intuition or classifying things that cannot be put into such a simple format (e.g.\ Compare "IF Blood AND Gun THEN Mobsters" to IF there is "Blood" in the movie, and there are many "Gun" scenes, then the movie is likely about "Mobsters"). These kind of natural language explanations could result in better adoption of the method in real-world domains.

% Explaining neural networks with a proxy model low-depth decision tree
%		How will you ensure the model matches the predictions of the orignal neural network?
%		What form will the explanations take

This work also has potential applications  to eXplainable AI (XAI), methods that explain existing learned neural network models. Essentially, the approach outlined in \ref{ch4} would be used to create a "proxy model" that approximates the  behaviour that the neural network does in a simple way. Then, the quality of this proxy model is determined by how well it matches the predictions that the neural network made. Following the previous idea about transforming decision trees into natural language explanations, the method could be applied in a variety of domains to explain existing models to end-users.

% Creating alternative interpretable models that match or outperform state-of-the-art models
%	 	Which interpretable classifiers?
%	 	Which models? How will you do it?

Models learned from disentangled feature representations derived from neural network hidden layers could also act as interpretable alternatives to them. This brings up a primary question that follow-up work could address: can disentangled feature represenations be derived from neural networks that achieve state-of-the-art e.g. BERT \cite{Kenton1953}? BERT is a neural network that achieves strong results on a variety of tasks and has entered mainstream application usage,  but can the hidden layers of BERT be disentangled into interpretable features, and can an interpretable classifier be learned? This is but one of a variety of potential  state-of-the-art models that could be disentangled  to produce an alternative interpretable model.

% Using Rank SVM to learn the rankings rather than taking the orthogonal direction

In Chapter \ref{ch3} the linear SVMs were used to obtain hyper-planes that classify words on a binary occurrence task, i.e.\ if the word occurred or if it did not occur in the document. Following this, the orthogonal direction was taken and entities were ranked on it using the dot product. It would be interesting to instead try RankSVM \cite{Lee2012} to directly learn rankings of documents on words, where the "true ranking" is determined by the Positive Pointwise Mutual Information (PPMI) score. This would potentially result in a more accurate ranking derived from the spatial representation for the word.


% Improving the clusters by ensuring that they are not disrupted by noisy words w/ post-processing step?
%		What is the problem?
%		What is your idea for solving that problem?
%		How will it work?

In Chapter \ref{ch3} a potential problem with cluster features was identified, in particular that their  meaning may be disrupted due to terms unrelated to the task being clustered with them. It would be interesting for a qualitative investigation into this phenomenon, seeing if manual removal of words from clusters would indeed benefit their performance in low-depth decision trees. If this is the case, there is potential for a new clustering method that performs this process automatically. Theoretically this could be done by taking a bag of PPMI scores as in Chapter \ref{ch5, and if the similarity between the ranking of entities on the cluster direction and the bag of PPMI scores drops too far with addition of the new word then it could not be added. 


% Fine-tuning state-of-the-art neural networks such that their internal representations are disentangled
%		How will you do this? 
%		What will be different from preliminary tests?
%		What is the goal of this? How will it achieve that goal?


The fine-tuning process outlined in Chapter \ref{ch5} originally followed the results demonstrating  direction scores went down in when obtained from neural network hidden layers as they are  stacked. Given this, the idea was formed that it would be possible to fine-tune these spaces such that they can retain the semantic features in the first layer while also adjusting the representation according to the objective. However, in preliminary tests this did not yield such a result, instead of fine-tuning the representation it stopped learning information related to the task. This preliminary investigation was done by taking each resulting representation, fine-tuning it, and then using it as input to the next auto-encoder. There is potential future work in pursuing this idea in application to explainability and interpretability. In particular, work in investigating if neural networks can be forced to retain disentanglement during learning. If this is the case, it may be possible to make neural networks interpretable.




\section{Summary}

In summary, this thesis demonstrated that disentangled feature representations can be obtained from a variety of document embeddings, and a qualitative analysis was conducted on these representations. It was found that the features were clear and meaningful. Additionally, a method was introduced that fine-tunes vector spaces to improve these disentangled features. The methods seem promising in application to interpretability and explainability and offer much potential for future work.

%This thesis experimentally validates and improves on a methodology for obtaining disentangled feature representations for text documents. It is validated using document classification tasks, and disentangled features from neural networks are  qualitatively  analysed. The method is unsupervised, acts as a post-processing step on vector space representations, and  disentangles important semantic relationships in the space.  

