\chapter{Disentangling neural network layers}\label{ch4}

% Using semantic features to investigate neural networks, NOT disentangled feature representations
% Disentangled feature representations are composed of semantic features

\section{Introduction}

The previous chapter showed how in a vector space of entities (e.g.\ different movies) fine-grained semantic relationships can be identified with directions (e.g.\ more violent than) and used as features in a document classification task. 

\hmark{This chapter addresses two research questions, the first is if good disentangled representations  can be obtained from neural networks. This question concerns the quality of the representation, which is evaluated in the same way as the previous chapter. The second is if neural networks can be better understood through the lens of  semantic features. This question is about the insight into how these networks achieve their results, and is obtained by examining these networks quantitatively and qualitatively. }

Two kinds of neural networks are \hmark{used}, feed-forward networks and auto-encoders. The hidden layers of these networks are viewed as vector spaces, and \hmark{disentangled feature representations can be obtained from them using the method} in Chapter \ref{ch3}. \hmark{These disentangled feature representations are quantitatively evaluated, and the labelled outputs are qualitatively investigated. In the case of the feed-forward networks, it is found that good disentangled representations can be obtained from them using low-depth decision trees, and  qualitative examples further validate this claim.}  In particular, it is found that a low-depth decision tree that uses a disentangled feature representation obtained from a feed-forward neural network hidden layer can outperform the original neural network\hmark{, in this case, in the 20 newsgroups domain. This is likely due to finding features through the unsupervised process that correspond closely to the task, essentially serving as an inductive bias for the decision tree.} It is also found that feed-forward networks do not necessarily introduce new properties in-order to perform well, but rather improve relevant properties to the classification task.


Auto-encoders are also investigated by obtaining semantic features from their  layers. \hmark{First, a negative result is found using a quantitative experiment that shows that there is a significant drop in accuracy and f1-score between properties from the original representation and those obtained from the hidden-layers of a stacked denoising auto-encoder. However, it is observed that the properties obtained in the representation derived from the stacked denoising auto-encoder layers are indeed meaningful. In particular, it is found that earlier in the stacked denoising auto-encoder less frequent (i.e. specific to fewer documents) features are obtained, and in later layers more frequent (i.e. more general to more  documents) features are obtained.  Following this insight, symbolic rules are induced that relate less frequent properties to more frequent ones.} Illustrative examples are provided of the hidden layers and these rules. As an example, below is one of the rules derived with this method, where the first two terms are from one entity embedding, and the final term is from a more abstract entity embedding.
\begin{align}\label{rule1}\texttt{IF Emotions  AND  Journey THEN Adventure}\end{align} 


One potential application of this work in recommendations is in the robustness of explanations using these features. In the domain of movies, we may have a situation where the synopsis or reviews mention the words "Emotions" and "Journey", from which the system could derive that it is probably an "Adventure" movie and use that term as a part of its supporting explanation e.g.\ "You liked how this movie was an emotional and a journey, so you may like this adventure movie." Of-course if these features are indeed salient and can be linked in this way, these features are also be useful for investigating how  neural networks represent information.

%To investigate the denoising auto-encoders, the problem of how to learn symbolic rules from unstructured text documents that describe entities of interest in terms of increasingly abstract properties is considered, e.g.\ how the property of "Blood" relates to the property of "Violence" for movies. A straightforward approach to obtain a similar result might be to directly learn rules from bag-of-words representations of documents. However, such an approach would typically lead to a large number of rules of little interest, e.g.\ rules pertaining more to which words are used together rather than capturing capturing meaningful semantic relationships.  %Obtaining meaningful and interpretable symbolic rules is important in fields like exploratory data analysis, or explaining classifier decisions, as they can be interpreted easily by human users.

%The approach in this chapter builds on the method from Chapter \ref{ch3}, finding directions which correspond to interpretable properties in  neural network vector spaces, labelled using adjectives and nouns that appear in the text collection.  

In conclusion, feed-forward networks and auto-encoders are investigated using the methods described in Chapter \ref{ch3}. In Section \ref{ch5:bg} some neural-network specific methods for interpretability are discussed. Following that, in Section \ref{ch5:method} the detail of how the method from Chapter \ref{ch3} is used to investigate neural networks is made clear, as well as how rules are induced that explain how semantic features relate to each other across the different layers of an auto-encoder. Next in Section \ref{ch5:results} the results of the qualitative and quantitative investigation of the feed-forward networks is discussed followed by the qualitative results for the auto-encoders. Specifically, it is found that the semantic features derived from neural network embeddings are meaningful, low-depth decision trees perform well when using these semantic features as input, and various characteristics of neural network embeddings and their associated semantic features are observed in a qualitative analysis.  Finally in \ref{ch5:conclusion} conclusions are drawn as to the usefulness of the method, the results, their limitations and how the work in Chapter \ref{ch4} solves  these limitations. %In particular, we find that the properties are meaningful for neural networks and provide some insight but it would be useful to improve the feature representation such that it remains semantic even in stacked denoising auto-encoders, which is what is covered in Chapter \ref{ch5}.

% Section \ref{InducingRulesFromEntityEmbeddings}  In Section \ref{QualitativeEvaluation} these properties and rules are qualitatively examined, and in Section \ref{RelatedWork} we place our work in the context of related work. Finally, in Section \ref{Conclusions} we provide our conclusions.


% Neural networks are not interpretable

% However, they clearly contain a lot of useful information, as they perform well at tasks

% There is usually a theory that these networks contain more complex relations the deeper they go, or more 'abstract' dimensions.

% Write things that cannot be challenged, everything can and will be used against you

%%%% Aim is to do a comprehensive qualitative analysis of using these interpretable features obtained from neural networks

 %First: Can we verify that the  neural networks are finding meaningful directions for the task by investigating the directions that we will use as features in the interpretable represenation obtained from the network? 

% Three questions, first: Is it possible to obtain these more complex, accurate to domain knowledge relations and use them to produce interpretable representations that achieve similar results to neural networks on tasks even when using a limited number of features in a simple interpretable classifier e..g a low-depth decision tree, not replacing neural network, just using it. NOT transfer learning, cant really justify that (good space that can be used for anything)

% Finally: In networks which obtain successively more abstract representations, can we create a mapping of specific to abstract concepts?

%Conclusion: The rankings are sometimes weird/absurd, would be nice if it was more interpretable, thats where fine-tuning comes in



% The contribution of this chapter is as follows:

% We look at feed-forward networks and auto-encoders. The idea with auto-encoders is that we will be able to map specific terms to more complex ones by reducing the size of the vector space. With feed-forward networks, we want to see how the neural network achieves strong performance on a text classification task, aka how it transforms the representation such that it can achieve strong results on a supervised task.

% The results for auto-encoders was this, feed-forward networks was that

% Summary/whats coming next

\section{Background}\label{ch5:bg}

Neural network rule extraction algorithms can be categorized as either decompositional, pedagogical or eclectic \cite{Andrews1995a}. Decompositional approaches derive rules by analysing the units of the network, while pedagogical approaches treat the network as a black box, and examine the global relationships between inputs and outputs. Eclectic approaches use elements of both decompositional and pedagogical approaches. Our method could be classified as decompositional, as we re-organize the hidden layer of the neural network. We will now describe some similar approaches and explain how our methods differs.

The algorithm in \cite{Kim2000} is a decompositional approach that applies to a neural network with two hidden layers. It uses hyperplanes based on the weight parameters of the first layer, and then combines them into a decision tree. NeuroLinear \cite{Setiono1997c} is a decompositional approach applied to a neural network with a single hidden layer that discretizes hidden unit activation values and uses a hyperplane rule to represent the relationship between the discretized values and the first layer's weights. HYPINV \cite{Saad2007b} is a pedagogical approach that calculates changes to the input of the network to find hyperplane rules that explain how the network functions. 

\hmark{In Section \ref{InducingRulesFromEntityEmbeddings} a disentangled representation is derived from the hidden layers of a neural network, and rules are learned from that disentangled representation, rather than learning rules that describe the relationships between units in the network itself.} The focus \hmark{of this section} is on a qualitative investigation of the network and its potential to learn increasingly general entity embeddings from hidden representations rather than tuning network parameters such that weights directly relate to good rules. 

There are many methods for obtaining an explanation of a black-box classifier after it has been learned. One typical approach is to learn a 'proxy model', a more interpretable classifier that approximates the decisions made by the network. This is a pedagogical approach, that focuses on mapping inputs to outputs. The most popular approach is  LIME which produces a linear model to explain a single prediction in terms of the input features  \cite{Ribeiro2016}. By approximating many difference instances, it can thus give users an impression of how the model is behaving, despite these local predictions sometimes being contradictory in how features are used \cite{Ribeiro2016a}.  LIME differs from the work in this thesis as it focuses on the relationship between input and output, rather than explaining the internal layers of the network. Essentially, the work on this thesis focuses on layers, while their work focuses on explaining a black-box in terms of its input. 

Another method that can be used to explain a neural network is DeepRed. DeepRed explains a neural network by creating a proxy model decision tree that is faithful to the original network \cite{Zilke2016}. However, in practice when producing a decision tree that approximates a neural network with multiple layers, the tree can be large, despite pruning methods. Although this method may seem similar on the surface, as this work uses decision trees, it differs from the work here as the goal of this work is not to be faithful to the predictions of the neural network, but rather to investigate the representation in a neural network's hidden layer by re-organizing it into a disentangled feature representation. The decision tree in this work is used as a simple  model that can verify the interpretable feature representaiton is disentangled. Hypothetically, a variety of other simple  models could be used.

Another recent topic that relates to our work is improving neural networks and entity embeddings using symbolic rules \cite{Hu2016}. In \cite{Rocktaschel2015} a combination of first-order logic formulae and matrix factorization is used to capture semantic relationships between concepts that were not in the original text. This results in relations that are able to generalize well from input data.  

%Section \ref{auto-encoder-qual} considers the opposite task: using semantic features derived from neural network embeddings to learn better rules. The rules that are derived are intended to  clarify the nature of the transformations that are learned by the neural network.%not intended to explain how the network functions but rather to attempt to describe the semantic relationships that hold in the considered domain. In other words, the aim of the work on auto-encoders to learn rules in this Chapter is to investigate the use of neural network representations in the hidden layer as a tool for learning logical domain theories, where the focus is on producing rules that capture meaningful semantic relationships.

%There are two typical methods for making neural networks interpretable, explaining the neural network after it has been learned and modifying the way the neural network is learned such that it is interpretable. However, our method does not easily fit into either of these categories, as we do not explain the neural network in terms of its predictions or making it so that it learns an additional interpretable objective.

%The method that we introduce is a post-processing method on the hidden layer of a neural network to re-organize it's internal semantic structure into interpretable features. This satisfies a number of important criteria in the literature. It provides interpretability of the models representation, completeness of that representation, doesn't rely on input vectors instead using information directly from the model and is not limited to a model or a classifier. 

%One relevant work in the image domain is TCAV which separates user-specified entities using linear regression and obtains a direction using the weights for that concept. 

%As the method can be used to post-process vector spaces that encode semantic directions, it can be applied to many existing models without the need for re-training or adopting a new architecture. 

%Interpretability and completeness

%Interpretability of information inside the network versus predictions


%Post-hoc versus baking it into the model

%Creating networks that are easier to explain:  Disentangled representations, GAN, PCA< Independent Component Analytsis, Nonnegative matrix factorization, Variational autoencoding, Beta-VAE, InfoGAN, Construction of graphs, decision trees

%Deep networks that are trained to make their own explanations 

%Linear proxy models

%Decision trees from neural networks  (super deep and bad)

%Rule extraction from neural networks

%Concept Activation Vectors "are a framework for interpretation of a neural nets representations by identifying and probing driections that align with human interpretable concepts"

%Persuasive versus transparent

%Saliency maps "One of the most popular approaches in interpreting NN is saliency methods (24; 22; 25; 8; 5).
%These techniques seek to identify regions of the input most relevant to the classification output by
%the network. Qualitatively, these methods often successfully label regions of the input which seem
%semantically relevant to the classification.
%" - copy pasted MIGHT BE WRONG

%"Even a truthful explanation may be misleading if it is only locally truthful (20). For example, since
%the importance of features only needs to be truthful in the vicinity of the data point of interest, there
%is no guarantee that the method will not generate two completely conflicting explanations. These
%inconsistencies may result in decreased user trust at a minimum. On the other hand, making a
%globally truthful explanation may be difficult as the networks decision boundaries may be complex.
%TCAV produces globally explanations, and uses model’s output to generate explanations to maintain
%consistency between explanations and the model’s reasoning.
%" - copy pasted

%" there is evidence that they work by gradually disentangling concepts of interest, layer by
%layer (2; 3)."

%"although feedforward networks learn highly nonlinear
%functions there is evidence that they work by gradually disentangling concepts of interest, layer by
%layer (2; 3). It has also been shown that representations are not necessarily contained in individual
%%neurons but more generally in linear combinations of neurons (19; 27). Thus the space of neuron
%activations in layers of a neural network may have a meaningful global linear structure. Furthermore,
%if such a structure exists, we can uncover it by training a linear classifier mapping the representation
%in a single layer to a human selected set of concepts."

%[2] Understanding intermediate layers using linear classifier
%probes.

%[3] Network dissection: Quantifying interpretability of deep visual representations

%[19] Svcca: Singular
%vector canonical correlation analysis for deep understanding and improvement

%[27] Intriguing properties of neural networks.

%\subsection{Explanations}

% Explanation is a huge field

% The different kinds of explanations

% Where our directions fit-into this paradigm


%\subsection{Rules and Neural Networks}

% We plan to use rules in-order to create a kind of mapping of domain knowledge from specific to general using auto-encoders

% There is a history of using rules to understand neural networks, pedagogical etc

% Where our method fits into this


%\subsection{Investigation methods}

% There are many visualization/investigation tools

% Where our directions fit-into this paradigm

%\subsection{Feed-forward networks}

% Feed-forward networks 

% Basic layout, input/hidden/output, weights

% Activation functions (Relu, Tanh) refer to paper

% Loss (binary_crossentropy), trainer, Adagrad

% Dropout

% Thresholding


%\subsection{Auto-encoders}

% Input/output is the same

% Hidden layer constrained in some way so that the representation is more general

% Denoising auto-encoders

% Variations of auto-encoders in recent years. We just use the basic one.

\section{Method}\label{ch5:method}



%An unlimited depth Decision-tree classifier that uses a bag-of-words as input typically results in a tree that uses a large variety of related granular features. For example, when classifying a movie as a "comedy",   features used to classify would be words like "laughed" "witty" "charming" and so on. Generally, this results in a larger tree. 

%Generally, the representation used in a neural network is more granular the larger that it is, and more general and abstract the smaller that it is. 

%Generally, there is a parallel to neural networks in the sense that if a hidden layer has a large number of nodes, there is less need for the network to generalize. This can result in more granular concepts being modelled. When deriving a feature representation from high-dimensional neural networks, there may be directions like "gags, slapstick, gag", "witty, charming, wit", "comedies, comedy" "eccentric", simple concepts that directly relate to the class.

%As the layers get smaller, the concepts become more condensed and general. To see an example of this, see \ref{ch5:dtng} where example clustered features from the best performing decision trees are shown. The method in this section for the feedforward network is essentially to use a reasonably sized hidden layer that will result in good properties, and then take that hidden layer representation and investigate it qualitatively. In the case of the auto-encoders, instead successively smaller, more abstract, less disentangled representations are obtained and they and the relationships between them are investigated.




% Generally, we treat the hidden layers of neural networks as vector spaces.

% We use the same methods as described in chapter 3, obtaining directions and rankings

% Using these directions and rankings, we obtain interpretable representations

\subsection{Stacked Denoising Auto-encoders}

\hmark{In this section, the method to obtain a series of  entity embeddings from stacked denoising auto-encoders explained. The intention is to evaluate what is represented in the hidden-layers of stacked denoising auto-encoders, and to understand the relationship between the layers of the representation. }

The MDS embedding on the movies dataset is used, as it performed the best on low-depth Decision Trees in Chapter \ref{ch3} and its features were used in previous qualitative analysis and found to be intuitive. To construct embeddings that are different from the initial embedding provided by the MDS method, stacked denoising auto-encoders \cite{Vincent2008a} are used. Standard auto-encoders are composed of an "encoder" that maps the input representation into a hidden layer, and a "decoder" that aims to recreate the input from the hidden layer. Auto-encoders are normally trained using an objective function that minimizes information loss (e.g.\ Mean Squared Error) between the input and output layer~\cite{Bengio2009}. The task of recreating the input is made non-trivial by constraining the dimensionality of the hidden layer to be smaller than the input layer, forcing the information to be represented using fewer dimensions, or in denoising auto-encoders by corrupting the input with random noise, \hmark{forcing the auto-encoder to generalize specific information into abstract information, as it must "fill in the gaps".} By repeatedly  using the hidden layer as input to another auto-encoder,\hmark{ we can repeatedly transform the representation.} To obtain the entity representations from our auto-encoders, we use the activations of the neurons in a hidden layer as the coordinates of entities in a new vector space. 

\subsection{Feed-forward networks}

We train two neural networks with non-linear activation functions. The first is NNET-U, a neural network with a single hidden layer that uses a pre-trained document embedding as input. The second is NNET-B, a neural network that starts with the PPMI BOW as input. The output layer has node \hmark{ equal to the} number of classes. Each network is trained on one task from each domain. As the goal is a qualitative analysis, these tasks were chosen as they have a clear semantic meaning in the domain. In particular, for the movies the "Genres" task was chosen, and for the place-types the "Foursquare" task was chosen as its classes are the easier to understand without expert knowledge. The newsgroups domain is also included in the results for comparison. To obtain a representation from this neural network, the activation values from a layer of the trained neural network are treated as the coordinates of a vector space.

The representations derived from these networks should differ from the  neural network as it is trained on a multi-label classification problem (See Section \ref{bg:multi-label}), meaning that all of the classification objectives are predicted in the output layer simultaneously. In the case of a binary task like sentiment (where only one class is classified and it is either 1 or 0),  the neural network may filter out a large amount of information if it is performing well, as a large portion of it is irrelevant to the task. However, as multi-label objectives like those of the newsgroups or genres tasks are used, it can be expected that a large portion of information will be retained as much of the data is highly relevant to these classes. 

% that are not overly specific like sentiment, and so should require a variety of information to classify. For example, in the case of the newsgroups, there are twenty distinct being learned at once in the representation with little crossover. These objectives were chosen because of the expectation that in the representations derived from neural network layers a qualitative analysis  will discover new and interesting features relevant to the task without discarding a large amount of domain knowledge. Second, a non-linear activation function is used, sometimes in multiple layers. Finally, the supervised objective should shape the space more than an unsupervised objective. 

Following results showing that simple neural networks can still perform well on text classification without needing a complex architecture  \cite{Lakhotia2018} \cite{Nam2014}, in this work, the following parameters were recommended for the feed-forward network: Cross-entropy loss-function, Adagrad trainer with default parameters, Dropout, Sigmoid activation on the output layer, and Relu hidden activation units. However, in this work the $tanh$ activation function is used instead of Relu, as the binary cut-off  did not result in high-quality directions, and did not improve performance remarkably in early tests. The reason $tanh$ was chosen is because in initial tests the directions were meaningful. Alternative activation functions  like sigmoid were not tested. %
 %The assumption behind this is that if the network is able to achieve stronger results on the task than a baseline representation, it must be doing something more than representing the information. In other words, the representation must be better for the task than an unsupervised representation. The goal of this section is to discover why neural networks are able to perform so well, and explain it in terms of directions. 


%The  differences between the supervised representation and the unsupervised representation are examined here, with the main question being how exactly does a neural network achieve such strong results on the task? Some general ideas or explanations that are given are that through non-linear transformations, the internal representation of the neural network is able to find a more complex or accurate representation of domain knowledge relevant to the task.

\hmark{In preliminary experiments, a network with two layers outperformed a single layer network consistently.   As such, NNET-B uses a   bag-of-words as input and has  two hidden-layers, the first has  1,000 nodes and the second has 100.} \hmark{In the case of newsgroups, this network outperformed the top-performance linear SVM  that used an unsupervised document embedding as input.} The representation that the interpretable feature representation is derived from is the output of the hidden layer of dimensionality 100 in the learned network. For the qualitative examples derived from this network,  the final hidden layer of dimensionality 100 was used.

To learn NNET-U, the input is the unsupervised document embedding associated with the single-term disentangled feature representation that performed highest in F1-score when used as input to a  decision tree limited to a depth of 3,  found in Table \ref{bg:repsresults}. For example, in the newsgroups domain, this is the Doc2Vec embedding with 100 dimensions. The reason this space is used is because it is assumed that a representation that performs well on  decision trees limited to a depth of 3 contains semantic features that can be meaningfully adjusted by the network. Only a single hidden layer is used, as there was not a significant performance difference in initial experiments when using multiple hidden layers. The number of nodes in the hidden-layer is set to be the same dimensionality as the input representation. This enables easy comparison between the initial  embedding and the embedding derived from the neural network.



%%%%%%% NOTE

% Explain NNET-U here?

%%%%%%%%% NOTE

% We simply take the hidden-layer representation and look at what's going on. 



% Using the interpretable representation, we create a mapping from layer-to-layer using a rule-based classifier

\section{Investigating Feed-forward Neural Networks}\label{ch5:results}

\subsection{Parameters}

Three different domains are investigated: Newsgroups, Placetypes and Movie reviews. These domains are chosen as their  associated classification tasks are relevant to a large number of documents, meaning that the network must learn general domain concepts, e.g.\ in order to classify the 20-newsgroups a variety  of information is required. Although the place-types domain is included, due to the low number of entities a high accuracy score is not expected. This domain is included  because their entities are easy to understand in qualitative examples. In fact, as the positive instances for the Foursquare task, the task chosen, has such a low number of positive instances it would be unreasonable to expect good results with neural networks which typically perform well with a large number of entities and a more balanced task.

As in Chapter 3 \ref{ch3:LearningInterpretableDirections} the words used when obtaining candidate directions are limited by a  frequency cut-off. In prior experiments, this was used as a  hyper-parameter. However, in this chapter the frequency threshold is set for all experiments so that only the   top 10,000 most frequent words are considered. Additionally, the cut-off to decide how many top-scoring directions are used as input to the clustering algorithm and decision trees is set to the top 1,000 highest-scoring directions. In the previous experiment, both of these cut-offs were tuned for the specific space-type and task in order to achieve stronger results. The reason that these parameters are not tuned for this experiment is because the main focus of these experiments is a qualitative investigation of the directions, rather than attempting to maximize \hmark{disentanglement} or  performance on the text classification task. Essentially, standardizing these cut-offs makes it easier to come to conclusions when comparing between them qualitatively, despite potential performance losses. Similarly, as Normalized Discounted Cumulative Gain (See Section \ref{ch3:NDCG}) was shown to perform well for a variety of embedding types and domains, it was the only scoring method used.

%The reason behind this is twofold: First, , and second that this allows for easy comparison between the original representation and the representation taken from the hidden layer of the network, as the highest performing directions can be compared.

%We investigate the task using the newsgroups as it gives a good example of a representation that contains many different concepts and aspects of domain knowledge, as well as a well defined classification task that if achieved well on clearly demonstrates the representation contains a good amount of information in the domain. The place-types task does not contain too many entities, making it unreliable for neural network training and the reuters task has similar problems. One alternative would be the movies domain, but as identifying the genres of movies is functionally similar to the newsgroups task, we assume that any results found for the newsgroups can easily generalize to any classification task.


The network is hyper-parameter tuned with the following values: 

\begin{itemize}
	\item $\textit{epoch} = [100, 200, 300]$
	\item $\textit{dropout} = [0.1, 0.25, 0.5, 0.75]$
	\item $\textit{hidden layer size} =[1,  2, 3, 4]$
\end{itemize}

The batch size is 100 for all experiments, excluding the place-types which used a batch-size of 10 as there were so few entities. For the linear SVM and the Decision Trees of depth-3, hyper-parameter optimization for these models was used with the same parameters as described in Section \ref{ch3:hyperparam}.


\subsection{Quantitative Evaluation Of Feedforward Networks}\label{ch5:quanevalffnnet}

The  \hmark{intention of this experiment} is to validate that \hmark{the disentangled feature representations derived from feedforward networks} are composed of semantic features, and this is measured as in Chapter \ref{ch3} by the performance of low-depth decision trees when they are used as input.  The second but more primary intention is to try and determine what can be expected qualitatively from these disentangled feature representations, and this is determined by comparison between different models. The main comparison, informed by the results and investigation in Chapter \ref{ch3}, is by comparing the results for the depth-3 limited decision trees that use  disentangled feature representations derived from neural networks as input to the performance of the decision trees that used disentangled feature representations derived from unsupervised document embeddings as input. %The intention of this section is to give preliminary insights that inform what can be expected from the qualitative investigation of directions in Section \ref{ch5:qual}, and to investigate if  depth-3 decision trees that use disentangled feature representations derived from the hidden layers of neural networks  perform well on the associated domain tasks. 
As a general overview of the results,  the disentangled feature embeddings derived from neural networks perform as well as or better than the unsupervised representations on the task. 

\hmark{The results in Table \ref{ch5:quantresults} first shows the performance in F1-score  of the neural networks NNET-U and NNET-B. Alongside this,  the best performing unsupervised document embedding used as input to a linear SVM is shown}. These results serve as a reference point for the performance of the properties. The results for properties derived from these networks and this unsupervised document embedding are then shown when used as input to decision trees limited to a depth of three.  If the decision tree results do indeed outperform or match the neural networks, then they are performing well.  }
%The results for place-types were obtained as their entities can be easily understood without expert knowledge (e.g.\ "tree" and "cliff" rather than "The Shining" in the case of the movies). It is not unexpected that they performed poorly in the neural networks, as they have a very low number of entities (391) and it is also an unbalanced task. 

\begin{table}[]
	\centering
\begin{tabular}{llll}
		& \textbf{Movies}      & \textbf{Placetypes}  & \textbf{Newsgroups}  \\
	\toprule
	U-Embedding (Linear SVM)   & 0.532082959 & 0.630040432 & 0.628171051 \\
	NNET-U                      & 0.559090895 & 0.563313632 & 0.627577055 \\
	NNET-B                      & 0.435345945 & 0.597008771 & 0.673618458 \\
	Term-features U-Embedding DT3 & 0.493072458 & 0.508163669 & 0.536686468 \\
	Term-features NNET-U DT3       & 0.505155338 & 0.460648739 & 0.50988517  \\
	Term-features NNET-B DT3       & 0.501545907 & 0.521526974 & 0.664793407 \\
	Cluster-features U-Embedding DT3   & 0.506096231 & 0.544001448 & 0.513367341 \\
	Cluster-features NNET-U DT3         & 0.472297312 & 0.541887565 & 0.478077988 \\
	Cluster-features NNET-B DT3         & 0.501510396 & 0.5967867   & 0.682928611
\end{tabular}\caption{F1-scores for each embedding type and domain}\label{ch5:quantresults}
\end{table}

In particular, the quantitative results in Table \ref{ch5:quantresults} are, where "U-Embedding" is the initial embedding. The various methods used to obtain the results are as follows:

\begin{itemize}
	\item \textbf{U-Embedding Linear SVM:} The best performing linear SVM for the task that used an unsupervised document embedding is used as input  from Chapter \ref{ch3}.
	\item \textbf{DT3} This refers to a decision tree limited to a depth of 3.
	\item \textbf{NNET-U and NNET-B:} These are the scores when the output layer of the neural networks are used to predict the class of documents. 
	\item \textbf{Term-features:} This refers to the results for decision trees limited to a depth of 3 when using  disentangled feature representations as input, composed of rankings of entities on single-term directions.
	\item \textbf{Cluster-features:} This refers to the results for decision trees limited to a depth of 3 when using  disentangled feature representations as input, composed of rankings of entities on cluster directions.
\end{itemize}

The assumption that good  features can be obtained from the neural networks using the method in Chapter \ref{ch3} is validated, as not much predictive power is lost over the original neural network in most cases. In particular, for the domain of place-types and the domain of newsgroups the Decision Trees of depth 3 that used  single-term features and cluster-features derived from NNET-B outperformed the other approaches by a significant margin. In the newsgroups domain, the low-depth decision trees that used the disentangled feature representation as input outperformed the neural network that  it was derived from. This \hmark{gives credence to}  the idea that decision trees of depth 3 that use disentangled feature representations as input can outperform more complex classifiers. Interestingly however, in the  movies domain the neural network that used an unsupervised document embedding as input (NNET-U) performed the best, and NNET-B had a relatively low performance.  This is likely because the vocabulary for the movies is large (limited to 100,000 unique terms for movies from the original vocabulary size of 551,080 versus 51,064 for newsgroups, see Section \ref{ch2.5:technical}), and this resulted in overfitting. 

These results give an  expectation that well-represented (i.e. have a high NDCG score) features of the disentangled representation derived from NNET-U  will be relevant to the task. Similarly, as in the movies domain the neural network that used a  bag-of-words as input (NNET-B) performed poorly, it can be expected that properties that are well represented in this representation are less relevant to the task. However, all of these representations performed similarly in the depth-3 decision trees. This likely means that they  have all been able to identify  features that are particularly relevant to the class, e.g.\ a feature that corresponds to "Horror" when classifying the genres. NNET-U performs better than the others, and NNET-B significantly worse, but the low-depth decision trees that use disentangled feature representations derived from NNET-U and NNET-B perform similarly to each other. This can be understood by seeing that the features that are well-represented in NNET-U and NNET-B are not used in the decision trees, as features that are common to all representations are sufficient for good performance. This idea is \hmark{expanded on} in the qualitative analysis (Section \ref{ch5:qual}).

%In particular, it can be assumed that the reason that NNET-U performed well, but the disentangled feature representation derived from it did not retain that performance is likely because it did not disentangle features such that those relevant to the class were grouped together. This means that although properties like "comedies" are more separable in the space, as they are not similar to the cluster-direction for "comedy", instead forming their own cluster-direction, the "comedies" cluster-feature is not used in the depth-3 decision tree, making it meaningless for the interpretable classifier but meaningful for the neural network. We find evidence to support this idea in Section .

%This section  provides insight into the directions contained within the neural network hidden layers.  If the neural network performs better than a linear svm with the unsupervised representation as input, it gives the insight that it must contain more relevant or refined directions than the unsupervised representation. Similarly if it performs poorly, it must be arranging the entities poorly. 

%\subsubsection{Neural Network Results}

%For the newsgroups, the neural network with bow input performed significantly better than the one with a vector space as input. 

%Compared to newsgroups, the neural network that used a vector space as input performed significantly better than the linear SVM on the unsupervised representation. It obtained an F1-score of 0.574, when the linear SVM's score was 0.532. The assumption follows that if it actually has found more meaningful properties in the space, then the directions should also perform well on a depth-3 decision tree.

%For the place-types domain, the neural network that used a vector space as input performed significantly worse than the SVM, as it scored 0.53 in F1 score compared to 0.622. However, this is not particularly meaningful as there are so few entities.

\subsubsection{Direction results}


Generally, the score of a depth-3  decision tree classifier when using a disentangled feature representation obtained from a neural network embedding is  close to the performance of the neural network it was derived from. In the following, potential reasons why disentangled feature representations derived from neural network embeddings might perform well are described:

\begin{itemize}
	\item Metadata that is not relevant to the task is not well-represented, meaning they are not disentangled from semantic features relevant to the task. (e.g.\ metadata like e-mail list names "listinfo" "robomod" are no longer highly separable, meaning that the only features found are meaningful)
	\item Features that are beneficial to classifying the task that were previously entangled are disentangled  (e.g.\ "Blood" may be a feature that is linearly separable in the unsupervised representation, and that was beneficial for classifying the "Horror task, but in the neural network representation a feature for "Zombie" is also linearly separable which enables the classifier to better identify if a movie is classified as being in the "Horror" genre)
	\item The features more faithfully capture the corresponding semantic property (e.g.\ The "Comedy" direction more correctly ranks movies based on how funny they are) If the directions perform well and use similar features it is reasonable to assume that the rankings are better than in the unsupervised representation.
\end{itemize}

In conclusion, disentangled features that can be used to classify entities can be identified in the document embeddings derived from supervised neural networks, and in the newsgroups domain  low-depth decision trees that use the disentangled feature representations derived from these neural network embeddings perform  better than the original neural network. Assumptions are made about what kind-of features these disentangled feature representations will contain, in particular that disentangled feature representations derived from neural networks that perform well on the task will likely contain unique features and potentially improved rankings for features that are particularly relevant to the task.  The next section qualitatively investigates  disentangled feature representations derived from neural network  embeddings.

\subsection{Qualitative Investigation of Feedforward Networks}\label{ch5:qual}

%As low-depth decision trees  performed well with disentangled feature representations as input, they are likely meaningful qualitatively, so they are included in the qualitative investigation in Section \ref{ch5:qual}. 

This  section  focuses on qualitatively investigating the characteristics of feed-forward neural network embeddings, as well as the disentangled feature representations derived from them. In particular, there are three main goals of this qualitative investigation section:

\begin{itemize}
	\item Verify that semantic features derived from the feed-forward neural network embeddings are meaningful.
	\item Gain a better understanding of the disentangled feature representations derived from feed-forward neural network embeddings.
	\item Identify distinguishing characteristics of feed-forward neural networks. 
\end{itemize}

If the semantic features obtained from the neural network makes intuitive sense, then they are verified  to be meaningful. Disentangled feature representations derived from feed-forward neural network embeddings are explained using examples, and  distinguishing characteristics of feed-forward neural networks are determined by comparisons between the semantic features derived from neural network embeddings and those derived from initial embeddings. %The second goal is achieved by examining examples from the feed-forward neural network embeddings. suitable comparisons and conclusions  are made between the semantic features obtained from the embeddings and the semantic features obtained from the unsupervised document embedding. Finally, the distinguishing characteristics of feed-forward neural networks are identified by 

When showing results for single directions, the words that these directions are labelled with e.g.\ "Blood" will be accompanied by the two terms with the most similar directions e.g.\ "Blood (Gore, Horror)". This is so the meaning of the direction can be understood more easily. Unlike the clustered features, the direction is not changed. The initial embedding, NNET-U and NNET-B are used in this section for each domain.  

Terms are scored using NDCG \ref{ch3:NDCG} as it was found to perform well on a variety of tasks and spaces in Chapter \ref{ch3}. The frequency threshold was set to the top 10,000 terms and the score threshold was set to the top 2,000 scoring terms in NDCG. 

\subsubsection{Top Scoring Terms}\label{ch5:topscore}

In this section, we look at the top-scoring terms for three different domains. These initial results are intended to give a general impression of what these terms and their associated similar directions are like, as well as to gain some initial insight into what the embeddings are representing.

In Table \ref{ch5:topscores} the top NDCG scoring terms are listed for each domain and embedding type. One immediate observation is that the initial embedding and the NNET with the initial embedding as input ("NNET-U") have similar top scoring terms, e.g.\ "listinfo" "mailing". Extremely separable metadata directions that are not relevant to the task retained this separability despite the neural network being trained to optimize the task. This seems to indicate that directions that are separable in an input embedding will not be made less separable in the neural network embedding. In the case of the neural network using BOW as input (NNET-B), however, these  terms are not present. This brings up the question, what exactly has the neural network learned such that it performs higher in F1-score in the case of the initial embedding as input? 


\begin{table}[]
	\scriptsize
	\begin{tabular}{lll}
		\textbf{Movies}                                   &                                                   &                                                          \\
		Initial Embedding                                  & NNET BOW Input                                       & NNET Embedding Input                                             \\
		\toprule
		listinfo (mailman, rec)                  & horror (pacing, dialog)                           & listinfo (mailman, rec)                                  \\
		robomod (mailman, rec)                   & westerns (russian, digitally)                     & robomod (mailman, rec)                                   \\
		mailing (mailman, listinfo)              & documentary (joke, ultimate)                      & mailing (mailman, listinfo)                              \\
		noir (fatale, femme)                     & comedies (actress, entertain)                     & noir (fatale, femme)                                     \\
		martial (kung, fight)                    & hilarious (disappointment, dozen)                 & horror (scary, horrific)                                 \\
		gay (homosexual, homosexuality)          & laughs (woods, rights)                            & martial (kung, arts)                                     \\
		horror (scary, scares)                   & sci (equivalent, intent)                          & gay (homosexual, homosexuality)                          \\
		prison (jail, prisoners)                 & adults (grown, rushed)                            & prison (jail, convicted)                                 \\
		arts (rec, listinfo)                     & songs (battle, speak)                             & animation (animated, cartoon)                            \\
		musicals (musical, singing)              & war (military, james)                             & arts (rec, listinfo)                                     \\
		mailman (listinfo, robomod)              & western (don, stick)                              & musicals (musical, numbers)                              \\
		Placetypes                               &                                                   &                                                          \\
		southcoast (filters, reala)              & leafs (botanic, f100)                             & interchange (midday, elevated)                           \\
		interchange (underpass, hk)              & aerialphotography (beaver, kiteaerialphotography) & canonrebel (controluce, pigeons)                         \\
		canonrebel (1855mm, nikond300s)          & irvine (jay, meet)                                & rave (dj, erin)                                          \\
		statua (stern, 1st)                      & centralcoast (published, aloha)                   & winnipeg (konicaminolta, twincities)                     \\
		municipal (citizen, farbe)               & swell (fin, polar)                                & windmills (goldenhour, puffy)                            \\
		madrid (noir, df)                        & pacificocean (puerto, waves)                      & statua (palacio, estatua)                                \\
		reizen (seventies, canonef2470mmf28lusm) & trunks (birch, pair)                              & reizen (1022mm, t3)                                      \\
		commuter (underpass, muni)               & southflorida (fla, hawaiian)                      & f456 (300mm, a550)                                       \\
		crime (illegal, violence)                & lapland (alberi, topv333)                         & gibraltar (iow, upon)                                    \\
		leafs (iris, cyan)                       & sunbathing (underwater, relaxed)                  & song (singing, juni)                                     \\

		\textbf{Newsgroups}                               &                                                   &                                                          \\
		\toprule
		solid (design, single)                   & vol (vast, foot)                                  & temperature (discoveries, surveys)                       \\
		struggle (grew, landed)                  & volt (amplifier, soldered)                        & testify (ali, concentrated)                              \\
		spreading (pursued, tolerant)            & voltage (cautious, scanned)                       & solid (company, sides)                                   \\
		salt (drinking, combinations)            & volume (fairly, distinguish)                      & tea (tech, buck)                                         \\
		random (attacks, described)              & volumes (expressed, scenario)                     & widely (recently, 1982)                                  \\
		widely (developed, tend)                 & voluntary (demonstrating, explore)                & denial (judaism, kurds)                                  \\
		temperature (layers, consumption)        & volvo (aftermarket, horsepower)                   & detecting (skeptical, signals)                           \\
		viable (motivation, emphasized)          & tigers (carter, brady)                            & dick (quoted, seen)                                      \\
		vol (published, journal)                 & postage (straightforward, engaged)                & pitches (screw, headers)                                 \\
		volt (garage, voltage)                   & povray (animations, pbmplus)                      & random (dropped, atheism)                                \\
		voltage (amps, resistor)                 & starters (worthwhile, fame)                       & salt (stations, alike)                                   \\
		volume (hundred, frequently)             & secular (descendants, fundamentalists)            & vol (contents, students)                                 \\
		volumes (historians, turkish)            & ring (increasing, behind)                         & volt (disagreement, clouds)                              \\
		voluntary (heterosexual, posed)          & utterly (des, retain)                             & voltage (criteria, disclosed)                            \\
		volvo (saab, chrysler)                   & occurring (communicate, contacted)                & volume (although, quickly)                               \\
		tea (nagornokarabakh, mothers)           & single (entire, prove)                            & volumes (raster, josephus)                               \\
		quick (careful, usual)                   & oxygen (speeding, extraordinary)                  & voluntary (transmitted, satan)                           \\
		utterly (violates, sinners)              & widgets (shells, experimentation)                 & volvo (outlet, 302)                                     
	\end{tabular}\caption{Top-scoring directions measured using NDCG score}\label{ch5:topscores}
\end{table}

\subsubsection{Common and Unique Terms to Each Embedding}\label{ch5:commonunique}

In order to further investigate the question posed in the previous section, this section evaluates whether terms from the movies domain are unique to the embedding or common to all three embeddings. The intention is to see if NNET-U and NNET-B embeddings have introduced any new features to the top 2,000. To this end,  a basic procedure is used  where if terms did not occur in either of the top 2,000 highest scoring terms for the other embeddings then they were considered not unique, and if a term  occurs in the 2,000 top scoring terms of both the other two embeddings it is considered common. 

Interestingly, there were only 14 unique terms for the initial embedding, and only 19 for the neural network that used the initial embedding as input. Meanwhile, there were 770 unique terms for the neural network with bag-of-words as input. This shows that representing new directions was not the reason for superior performance for NNET-U. Rather, it is likely that terms that are relevant to the genre that were already in the top 2,000 top-scoring terms are more separable. In particular, we see that terms that are highly relevant to the task of genres are common to all embeddings, e.g.\ "horror (scary, scares)", "hilarious (funniest, laughing)", "jokes (laughs, joke)". This gives some indication of why in the Quantitative Investigation it was seen in Table \ref{ch5:quantresults} that although NNET-U outperformed NNET-B and a linear SVM that used the initial embedding as input, the disentangled feature representations derived from these embeddings performed similarly. The explanation for this is that each embedding contains the same features that are most suitable to use in a low-depth decision tree for the task, but they differ in those that are less relevant to the task. This idea is further validated in Section \ref{ch5:commonterms}.

%In order to qualitatively investigate further what has changed between the embeddings, the differences between the scores of the terms was obtained.

% Most difference
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\begin{table}[]
	\scriptsize
	\begin{tabular}{L{3.8cm}L{3.8cm}L{3.8cm}L{3.8cm}}
\textbf{Movies}                               &                                             &                                        &                                            \\
Unique Initial                  & Unique NNET-B                           & Unique NNET-U                & Common                                     \\
\toprule
immigrants (immigrant, america)      & unlike (terrific, efforts)                  & carry (recent, hoping)                 & noir (fatale, femme)                       \\
flashback (flashbacks, present)      & efforts (detail, directors)                 & federal (fbi, agents)                  & martial (kung, fight)                      \\
bloated (spectacle, overlong)        & impression (throw, truth)                   & possessed (demonic, forces)            & horror (scary, scares)                     \\
chapter (previous, installment)      & viewed (catch, escape)                      & faustus (geocities, html)              & arts (rec, robomod)                        \\
rebel (rebellious, freedom)          & suggest (wondering, trip)                   & spike (african, lees)                  & musicals (musical, singing)                \\
assault (attack, violent)            & focuses (sadly, thoughts)                   & dashing (handsome, excitement)         & hilarious (funniest, laughing)             \\
client (lawyer, attorney)            & terms (theatre, marvelous)                  & wartime (wwii, bombing)                & westerns (western, cowboy)                 \\
predecessor (sequel, sequels)        & suppose (heres, greatly)                    & phantom (sees, opera)                  & jokes (laughs, joke)                       \\
competitive (competition, sport)     & credit (clever, sequence)                   & theories (theory, conspiracy)          & romantic (romance, romances)               \\
jealous (attraction, crush)          & exception (beat, passed)                    & robots (sci, princess)                 & animation (animated, cartoon)              \\
betrayal (loyalty, affair)           & heres (suppose, negative)                   & abused (abuse, abusive)                & western (westerns, west)                   \\
artsy (pretentious, artistic)        & fare (twenty, concerned)                    & fiance (fianc, engaged)                & songs (song, lyrics)                       \\
hotel (manager, vacation)            & unable (convey, accept)                     & hatred (hate, racism)                  & comedies (comedic, laughs)                 \\
stereotypical (stereotypes, clich)   & deliver (depth, limited)                    & mysteries (mystery, clues)             & war (soldiers, military)                   \\                                   
	\end{tabular}\caption{Terms from three different document embeddings, the initial embedding, the neural network that used a bag-of-words as input and the neural network that used the unsupervised vector space as input. Arranged by NDCG, from highest to lowest}
\end{table}


\subsubsection{The Difference Between the Term Scores in the Embeddings}\label{ch5:diffsection}

In this section, the differences between the scores of terms in the movies domain are qualitatively analysed. The intention is to determine what some differences are between the initial embeddings and the neural network embeddings, and evaluate previous assumptions regarding the behaviour of the network. These results are for all 10,000 terms term directions obtained from each embedding. 

In the first column of  Table \ref{ch5:diff} terms that have a higher associated NDCG score in the case of the initial embedding \hmark{than in the} case of NNET-U are not clearly related to genres. For example, "yup", "wright", "wimpy" and "zoom", which are the top four highest score differences, do not seem related to genres at all. Meanwhile, the terms that have gained in associated NDCG score derived from in NNET-U are clearly relevant to genres, e.g.\ "animation", "adventure", "comedies". Although, it has also increased the separability of some metadata e.g.\ the term "listinfo", which is a problem already identified when using the initial embedding as input to the neural network. This gives us some indication of what the neural network is doing to solve the task. This behaviour of the neural network decreasing separability of noisy terms and increasing separability of terms relevant to the task could contribute to the difference in score where the neural network outperformed a linear SVM on the initial embedding. 

Meanwhile NNET-B seems to have elevated terms that are not relevant to the task. As the neural network seems to have overfit due to the large vocabulary, it has made terms separable that are not immediately meaningful, e.g.\ "attends" "adds" "foray". However, as previously mentioned it did  not have some of the same metadata as the initial embedding. However, it seems to have made metadata unique to it separable. 
%Common terms: relevant to the class
%Unique terms: unique noise, vector space nnet performed better while bow nnet performed worse. should see something that reflects that
%Common terms btwn unsupervised and vector: common noise
%Most distance: 

%Generally, we see a similar trend in other domains. If noise is in the original space, using it as input to a neural network will not remove that noise. In that case, what changes between the original vector space and the neural network? To answer this question, the terms arranged by the difference between them is listed in table 

\begin{landscape}
\begin{table}[]
	\scriptsize
	\centering
	\setlength\extrarowheight{-0.5pt}
	\begin{tabular}{llll}
		Unsupervised - NNET-U          & Unsupervised - NNET-B              & NNET-U - Unsupervised                 & NNET-B - Unsupervised                \\
		\toprule
		zoom (cuts, editing)           & zoom (cuts, editing)               & allens (woody, allen)                 & atmosphere (mood, atmospheric)       \\
		wimpy (villian, instance)      & zombies (zombie, undead)           & animation (animated, cartoon)         & biased (agree, showed)               \\
		wright (forrest, buyer)        & williams (jim, includes)           & adults (children, adult)              & abomination (insult, horrible)       \\
		yards (shoot, yard)            & willingly (explicitly, risk)       & arts (rec, listinfo)                  & absent (absence, previous)           \\
		yup (junk, downright)          & wrecked (crashes, crashed)         & allen (woody, allens)                 & car (cars, driving)                  \\
		watchers (underlying, holding) & welles (orson, kane)               & adaptation (adaptations, adapted)     & abandons (leaving, decides)          \\
		valiant (merit, admirable)     & writers (write, finish)            & animated (animation, cartoon)         & attends (attending, attend)          \\
		wee (pee, pick)                & willingness (conventional, goal)   & aboard (ship, board)                  & adds (added, addition)               \\
		winding (road, wheel)          & wars (enemy, war)                  & adventure (adventures, adventurous)   & broader (greater, balanced)          \\
		yea (hey, wanna)               & winner (winning, won)              & australian (australia, sydney)        & artwork (painting, animation)        \\
		wax (inclined, usage)          & walt (disneys, disney)             & addiction (addict, addicted)          & cheerfully (gleefully, bursts)       \\
		woke (yeah, wake)              & unstable (psychotic, mentally)     & berardinelli (employers, distributor) & foray (marks, venture)               \\
		wrecked (crashes, crashed)     & walking (walk, walks)              & abuse (abused, abusive)               & aboard (ship, board)                 \\
		winded (significant, deliver)  & wrath (gods, angry)                & actress (actresses, lady)             & absurd (ridiculous, absurdity)       \\
		wine (bottle, drink)           & widely (recently, equally)         & apes (monkey, monkeys)                & civilization (civilized, survival)   \\
		wrapping (product, needed)     & wondering (figured, wanting)       & comedies (comedic, laughs)            & abrupt (lacked, ended)               \\
		yahoo (faustus, faust)         & yawn (dull, tedious)               & listinfo (mailman, rec)               & active (helped, allowed)             \\
		whip (beat, kick)              & zeal (enthusiasm, celluloid)       & africa (african, countries)           & admiration (respect, remarkable)     \\
		zeal (enthusiasm, celluloid)   & wolf (wolves, pack)                & bogart (casablanca, noir)             & consciousness (profound, meaning)    \\
		zone (twilight, concept)       & wire (wires, walls)                & animals (animal, humans)              & abandoned (deserted, forced)         \\
		wayward (cross, join)          & undead (zombie, zombies)           & anime (animation, japan)              & attitude (respect, reasons)          \\
		visitor (arrival, visiting)    & winter (snow, frozen)              & african (racial, racist)              & graphically (graphic, violent)       \\
		weaving (weaves, crafted)      & yesteryear (throwback, nostalgic)  & band (bands, rock)                    & acceptable (rhodes, internetreviews) \\
		yearns (desires, affection)    & voted (vote, awards)               & accurate (accuracy, accurately)       & advances (developed, attractive)     \\
		youngster (kid, childhood)     & unquestionably (arguably, closing) & accuracy (accurate, inaccuracies)     & depths (beneath, surface)            \\
		yell (angry, yelling)          & wifes (husband, marriage)          & artist (artists, artistic)            & advice (decided, spent)              \\
		walker (damage, bill)          & worthwhile (skip, include)         & catholic (priest, church)             & explanation (explained, explain)     \\
		\end{tabular}\caption{The largest differences in term scores between embeddings for the movies. The first two columns are the terms that were most reduced in score, while the third and fourth columns are the terms that increased most in score}\label{ch5:diff}
\end{table}
\end{landscape}

\subsubsection{Newsgroups Decision Tree Cluster-Features}\label{ch5:newsgroupsquant}

In the newsgroups domain, low-depth decision trees when disentangled cluster features were used as input outperformed the neural network they were derived from. In this section we qualitatively investigate the clusters for the newsgroups, with particular interest in why the NNET-B clusters performed as well as the neural network and better than the unsupervised representation by a wide margin.

In Table \ref{ch5:dtng}, we examine the clustered features used in the decision trees for the newsgroups. In particular, 
we obtain the top decision tree node for each class. This is to give an overview of the different kinds of clusters and how they perform for each task. When examining the difference between the cluster-features for the initial embedding and NNET-B, we can see that the features chosen as the top nodes of the decision tree seem more relevant to the task. Rather than refining the rankings of the previous features, new cluster-directions have been found that more closely correspond to the task. For example, the cluster-feature used for comp.graphics seems to capture a more relevant and complex property  "(pbmplus, sgis, sunview, phigs, pex, colormap, pixmap)" compared to the initial embedding "(povray, raytracing)" as does the cluster-feature used for rec.autos "(sedan, camry, saab, ..., diesel, coupe)" versus "(sedan, camry)". Following this the conclusion is that having access to the BOW representation allows the neural network to better arrange the entities such that these natural cluster directions are meaningful to the class. However, the reason this did not occur for the movies domain was because the neural network overfit. 

The best-performing cluster parameters for  NNET-U resulted in directions that are clusters of terms not relevant to the task, and single-term features that are relevant to the class. In the examples in \ref{ch5:dtng}, only those single-term features that are relevant to the class are shown, as these  cluster features were not used in the depth-3 decision tree. This means that the clustering process did not capture features that represent more abstract but relevant features despite separating relevant term directions, e.g.\  "rsa" is particularly relevant to the class sci.crypt, but the NNET-B cluster-feature of  "(authentication, diffiehellman, publickey, 80bit, vesselin, skipjack)" captures a more abstract but relevant feature.  %This is likely due to the hyper-parameter for the clusters, but it is interesting as that means these single-directions performed better than clustering terms together.  %However, this did not lead to good quantitative results. For example, "bikes" is particularly relevant to the class rec.motorcycles but the accompanying directions seen in the unsupervised and NNET-B embeddings are not present. 




%%% Unique dirs

%\begin{table}[]\label{Unique word directions identified in the bow representation}
%	Entities, terms that have gone down the most from the original space, terms that have gone up the most
%\end{table}

%However, this only covers those terms which were also in the original space. There are also unique word directions identified in the bow representation. %Noisy entities have gone down, prototypical entities have gone up?

%The clusters obtained from the bag-of-words as input neural network directions seemed to have a general trend as well, they grouped noisy terms that were not related to the genres together, and kept the genre clusters tight without excess terms. The hypothesis that follows is that the neural network focused on optimizing the rankings for these directions, moving anything not directly related to the objective away. 

%To demonstrate this, we show a comparison of the neural network with bow input representation and the original representation each projected using PCA into two dimensions.

%Additionally, we investigate the decision trees associated with these clusters. In these decision trees, we can see that the same features were used repeatedly with more refinement, speaking to the fact that rather than neural networks finding new concepts in the domain, they instead refine those that are relevant to the task. This makes explanation of why an entity is classified a certain way easy, as it can simply be put in terms of relative scale. For example, you could say that this movie was classified as a horror, as it was more {horror, scary, bloody} than {example movie}.

%The difference between the rankings of common single word directions is also investigated. Hypothetically, the rankings should make more intuitive sense, and noise should be removed, if the neural network does indeed prioritize the rankings of this important domain knowledge.


%%% Common dirs
 
\begin{landscape}
	\begin{table}[]
		\scriptsize
		\centering
\begin{tabular}{llll}
	\textbf{Newsgroups}               & Initial Embedding                                     & NNET-B                                                               & NNET-U  \\
	\toprule
	alt.atheism              & (celestial, creationist, psalm, ..., fallacy, baptism)     & (messenger, faiths)                                                    & (homosexuality) \\
	comp.graphics            & (povray, raytracing)                                       & (pbmplus, sgis, sunview, phigs, pex, colormap, pixmap)                 & (tiff)          \\
	comp.os.ms-windows.misc  & (winini, systemini, cica)                                  & (cirrus, workgroups, hicolor, ..., keystrokes, sdk)                    & (xlib)          \\
	comp.sys.ibm.pc.hardware & (lpt1, irq, chipset, mfm, logitech)                        & (qemm, emm386, hernandez, ..., com2, brewers)                          & (motherboards)  \\
	comp.sys.mac.hardware    & (centris, lciii)                                           & (democrats, emissions, helicopters, enforced, tribes, reign, clintons) & (quadra)        \\
	comp.windows.x           & (xdm, makefile, r4, ..., rainer, ow)                       & (xmotif, tvtwm, xfree86, ..., xsun, polygon)                           & (xlib)          \\
	misc.forsale             & (diffiehellman, exterior, strips, ..., uucp, slots)        & (267, saga, warriors, ..., pov, carnage)                               & (obo)           \\
	rec.autos                & (sedan, camry)                                             & (sedan, camry, saab, ..., diesel, coupe)                               & (toyota)        \\
	rec.motorcycles          & (porsche, countersteering, msf)                            & (carb, bikes, reed)                                                    & (bikes)         \\
	rec.sport.baseball       & (gant, duke, padres, ..., marlins, slg)                    & (larkin, marlins, pennant, ..., platoon, coaching)                     & (pitching)      \\
	rec.sport.hockey         & (providence, keenan, hawks, ..., champs, ahl)              & (nyr, messier, motto, ..., goaltending, keenan)                        & (stanley)       \\
	sci.crypt                & (playback, ciphertext, cryptanalysis, ..., escrow, nist)   & (authentication, diffiehellman, publickey, 80bit, vesselin, skipjack)  & (rsa)           \\
	sci.electronics          & (ham, amp, reactor, watts, amplifier, amps)                & (omega, diode, charger, ..., antennas, joystick)                       & (voltage)       \\
	sci.med                  & (intake, calcium, antibiotic, ..., kidney, quack)          & (bethesda, sensation, defects, ..., therapies, tablets)                & (patients)      \\
	sci.space                & (reboost, fusion, astronomers, ..., pasadena, galaxy)      & (shafer, reusable, pluto, ..., orbiter, billboards)                    & (lunar)         \\
	soc.religion.christian   & (divorce, sinless, moses, ..., corinthians, baptized)      & (pastor, baptized, congregation, corinthians, repent, pagan)           & (scripture)     \\
	talk.politics.guns       & (federalist, unregistered, tyranny, ..., wright, shotguns) & (tpg, progun, 9mm, ..., rkba, shotguns)                                & (batf)          \\
	talk.politics.mideast    & (arab, rebellion, syrian, ..., zionist, gaza)              & (asalasdpaarf, revisionism, zionism, ..., hamas, grandparents)         & (palestinian)   \\
	talk.politics.misc       & (croats, redraw, shelling, ..., bosnians, yugoslavia)      & (wiretapping, safeguards, vlsi)                                        & (homosexual)    \\
	talk.religion.misc       & (divorce, sinless, moses, ..., corinthians, baptized)      & (thy, forgiveness, churchs, ..., protestant, ephesians)                & (scripture)    
\end{tabular}
\caption{The top node of the Decision Tree for newsgroups}
\label{ch5:dtng}
\end{table}
\end{landscape}


\subsubsection{Movies Decision Tree Cluster-Features}\label{ch5:cluster-features}

In this section, we qualitatively investigate the cluster-features used in the top decision tree nodes for the movies domain, with a particular interest in why all embeddings have similar overall predictive performance despite containing very different features (as talked about in Section \ref{ch5:diffsection}).

One observation that follows the discussion in Section \ref{ch5:diffsection} is that the Adventure genre is classified using a cluster-feature that contains the word "adventure" in the case of NNET-U (adventure, spectacle, exciting, ..., seat, boyfriend), rather than the "(animation, animated, anime, voiced, cartoon)" cluster-feature that is re-used in the initial embedding. This follows the previous observation as the "adventure" direction has a higher associated NDCG score in the NNET-U embedding versus the initial embedding, as seen in \ref{ch5:diff}. Despite this, the performance overall is the same. 

The reason for the similar performance is likely that at the cost of re-organizing the representation such that the adventure cluster-direction existed, other cluster-directions were disrupted. The most prominent example of this is that the a good cluster for classifying animation is no longer present, instead this  cluster feature "(voice, voiced, recording, voices, vocal, listening)". Another example is that in the case of NNET-U the history class is classified in the unsupervised representation by the seemingly relevant "(events, accuracy, accurate, facts, confusing)", but in the case of NNET-U this cluster is disrupted by seemingly irrelevant terms "(western, historical, musicians, ..., biography, propaganda)". Note that we do not really see this in the newsgroups Table \ref{ch5:dtng} for NNET-B, where instead new and meaningful cluster-features are found that are relevant to the task.

\begin{landscape}
	
	\begin{table}[]
		\scriptsize
		\centering
		\setlength\extrarowheight{-0.5pt}
		
		\begin{tabular}{lp{7cm}p{7cm}p{7cm}}
			\textbf{Movies}      & Initial Embedding                                                       & NNET-B                                                           & NNET-U                                                           \\
			\toprule
			Action      & (fight, fighting, epic, ..., battle, weapons)                                & (martial)                                                          & (martial, fight, fighting, fights, choreography, choreographed, fighter) \\
			Adventure   & (animation, animated, anime, voiced, cartoon)                                & (adventure, adventures)                                            & (adventure, spectacle, exciting, ..., seat, boyfriend)                   \\
			Animation   & (animation, animated, anime, voiced, cartoon)                                & (voiced)                                                           & (voice, voiced, recording, voices, vocal, listening)                     \\
			Biography   & (biography, biopic)                                                          & (biopic)                                                           & (gritty, historically, fiction, ..., accurate, accuracy)                 \\
			Comedy      & (hilarious, jokes, comedies, ..., funniest, funnier)                         & (comedies, funniest, funnier, ..., laughed, slapstick)             & (hilarious, jokes, comedies, ..., funniest, funnier)                     \\
			Crime       & (noir, crime, caper, criminal, criminals, crimes)                            & (crime, police, criminal, cop, cops)                               & (gangster, gangsters)                                                    \\
			Documentary & (documentary, footage, documentaries, interviews, interviewed, informative)  & (documentary, footage, documentaries)                              & (documentary, footage, documentaries, ..., extras, interviewed)          \\
			Drama       & (emotional, magical, waves, ..., silent, evidence)                           & (emotional, emotions, emotionally, ..., relationships, study)      & (bond, emotional, families, ..., powerful, gripping)                     \\
			Family      & (adults, children, childrens, ..., parents, daughter)                        & (adult, ages, children, parents, teenager)                         & (disneys, walt)                                                          \\
			Fantasy     & (fantasy, fairy, fairytale)                                                  & (magic, colorful, comical, lavish, delightfully, dazzling, colors) & (fantasy, surreal, elm, fairy, fairytale, dreams, dream)                 \\
			Film-Noir   & (femme, fatale)                                                              & (noir, vintage)                                                    & (femme, fatale)                                                          \\
			History     & (events, accuracy, accurate, facts, confusing)                               & (historically)                                                     & (western, historical, musicians, ..., biography, propaganda)             \\
			Horror      & (horror, creepy, slasher, ..., scares, scare)                                & (creepy, scare, spooky, scary, monster, menacing, nightmares)      & (horror, creepy, scares, ..., spooky, chills)                            \\
			Music       & (musicals, songs, singing, ..., song, numbers)                               & (songs, musical, song, ..., sing, dancing)                         & (songs, singing, song, ..., sings, singer)                               \\
			Musical     & (musicals, songs, singing, ..., song, numbers)                               & (musicals, broadway)                                               & (songs, singing, song, ..., sings, singer)                               \\
			Mystery     & (detective, investigation, mystery, clues, mysterious, investigating)        & (mystery, dramatic, fiction, ..., psychological, slowly)           & (thriller, suspense, hitchcock, ..., thrillers, suspenseful)             \\
			Romance     & (romantic, charming, romance, ..., charm, chick)                             & (romance, romantic, chemistry, ..., handsome, scenery)             & (wedding, marry, marries, marrying, bride)                               \\
			Sci-Fi      & (sci, alien, space, aliens, outer, invasion)                                 & (sci, science, futuristic)                                         & (science, scientific, scientist, ..., scientists, investigating)         \\
			Short       & (episodes, episode, seasons, aired, television, storylines)  (python, monty) & (agree, soundtrack, product, ..., happening, inside)               & (geocities, html, aol, faustus)                                          \\
			Sport       & (coach, sports, team, sport, football)                                       & (sports, sport, coach)                                             & (boxing)                                                                 \\
			Thriller    & (thriller, suspense, adventure, ..., suspenseful, tension)                   & (thriller, suspense, thrillers, suspenseful, thrills, thrilling)   & (thriller, suspense, hitchcock, ..., thrillers, suspenseful)             \\
			War         & (war, soldiers, vietnam, ..., military, troops)                              & (wwii, german)                                                     & (war, soldiers, vietnam, ..., military, troops)                          \\
			Western     & (westerns, western)                                                          & (westerns)                                                         & (outlaw)                                                                
		\end{tabular}\caption{The top node of the Decision Tree for Movies}
	\end{table}
\end{landscape}
%%%  Top dirs

%\begin{table}[]\label{For movies Top directions +}
%	content...
%\end{table}
 
%In the movie review domain, the original MDS space had high NDCG scores for noise, e.g.\ "berardinelli" a reviewer's name, or "listinfo" which is review metadata. This was also present in the neural network representation where this vector space was used as input. However, it was not present when starting from a bag-of-words. This is interesting, because the neural network performed worse with the bag-of-words as input, and very well with the vector space as input, despite their directions performing about the same on a depth-3 decision tree.

%%%  Top dir differences

%\begin{table}[]\label{difference how top directions in original rep have changed}
%	content...
%\end{table}

\subsubsection{Top-Ranking Entities on Decision Tree Cluster-Features in the Place-types Domain}\label{ch5:place-typesentities}

The intention of this section is to investigate the rankings of entities in the embeddings and see if they make sense. The intuition is that despite neural network embeddings obtaining directions that make reasonable sense, their entities may contain irrelevant entities or be inaccurate. This experiment is used as further verification that features derived from neural networks from  are indeed semantic.  In Table \ref{ch5:placeent} the five entities with the highest dot products, the five highest ranked entities, are shown alongside directions used as the top node of the best performing decision tree. Note that it may not seem meaningful to take only the top five entities, but as there are only 391 entities overall for the class, and very few of those are positive instances of the class, these  top five entities are the most relevant for classifying the associated class correctly. Essentially, if these top five instances seem semantic, we can reasonably assume that the class will be classified correctly if the direction used is relevant to the class. 

To begin, we look at the clusters used to classify "CollegeAndUniversity", the unsupervised representation uses the cluster "(annarbor, graduation, eugene, institute, highschool, cal)" that seems particularly relevant to educational institutions, and its associated entities also seem very meaningful and relevant "(college, campus, college campus, university, school)". For NNET-B, the cluster itself seems absurd "(investment)", but the associated entities are relevant to that cluster "(commercial real estate, large construction, retirement home, ..., rental property)". Similarly, the cluster for NNET-U does not seem relevant to the class and neither do its top entities "(snowboard, turism, amusementpark, ..., waltdisney, disneyworld)" "(video game store, theme park, space shuttle, launch pad, speedway)". So in this case, the entities seem reasonable and meaningful for their associated directions.

In general, the results for this table follow a similar trend. Entities align with the meaning of cluster-features, even in neural network embeddings. The problems of classification come from poor selection of these cluster-features, rather than the cluster-features of those entities being incorrect. There is an interesting difference in the case of classifying NightLifeSpot, where three distinct but relevant clusters and associated entities are found and used in each of the embeddings. In the initial embedding, a cluster-feature related to music was used "(audience, instrument, musicians, ..., dancers, bands)" with top-ranked entities  "(stage, bar, rock, sound, music venue)". In the case of NNET-B, a cluster for drinking alcohol was used instead "(booze, vodka, whiskey, liquor)" with the top-ranked entities corresponding to places where  alcohol can be bought "(dive bar, beer garden, karaoke bar, hotel bar, cocktail bar)", finally the NNET-U identifies a cluster more related to sex but also including tattoo and dance studios (shoulder, darkroom, topless, ..., boobs, lips) with associated entities  "(dance studio, tattoo studio, shoulder, topless beach, strip club)". 

In conclusion for this section, entities seem to make sense in the case of place-types where they align well with their cluster-feature. Additionally, combining the use of a Decision Tree, the associated cluster terms with the cluster-feature, as well as the top entities gives great insight into the representation and what it is doing, as each provides valuable context for each other. 

\begin{landscape}
	
\begin{table}[]
	\scriptsize
	\centering
	\begin{tabular}{lll}
		\textbf{Class}       & \textbf{Decision Tree Cluster-Feature}                                             & \textbf{Top 5 Entities}                     \\
		\toprule
ArtsAndEntertainment       & (snack, vegetable, lemon, pepper, vegetables)                                             & (restaurant, market, asian restaurant, seafood restaurant, japanese restaurant)                     \\
CollegeAndUniversity       & (annarbor, graduation, eugene, institute, highschool, cal)                                & (college, campus, college campus, university, school)                                               \\
Food                       & (cuisine)                                                                                 & (restaurant, asian restaurant, japanese restaurant, seafood restaurant, chinese restaurant)         \\
NightlifeSpot              & (audience, instrument, musicians, ..., dancers, bands)                                    & (stage, bar, rock, sound, music venue)                                                              \\
ParksAndOutdoors           & (cloudscape, solitary, hazy, ..., fluffy, intense)                                        & (coast, shore, coastline, shoreline, beach)                                                         \\
ProfessionalAndOtherPlaces & (nationalhistoriclandmark, register, revival, jefferson, independence, pioneer, civilwar) & (court house, courthouse, government building, state, cemetery)                                     \\
Residence                  & (laundry, dwelling)                                                                       & (house, home, residential building, apartment, historic building)                                   \\
ShopsAndService            & (mannequin, etsy, crafts, boutique, bags, jewelry, necklace)                              & (market, boutique, store, clothing store, vintage store)                                            \\
TravelAndTransport         & (donkey, costarica, tanzania, ..., reisen, andes)                                         & (market, desert, national park, crater, volcano)                                                    \\
& NNET-B                                                                                       &                                                                                                     \\
ArtsAndEntertainment       & (ruraldecay)                                                                              & (sugar mill, silo, corral, abandoned farm, ranch)                                                   \\
CollegeAndUniversity       & (investment)                                                                              & (commercial real estate, large construction, retirement home, ..., rental property) \\
Food                       & (dolphins)                                                                                & (football field, training camp, strait, baseball stadium, continent)                                \\
NightlifeSpot              & (booze, vodka, whiskey, liquor)                                                           & (dive bar, beer garden, karaoke bar, hotel bar, cocktail bar)                                       \\
ParksAndOutdoors           & (condos, venice, entertainment, ..., socks, americanflag)                                 & (piano bar, public housing, waiting room, residential street, dining room)                          \\
ProfessionalAndOtherPlaces & (gardening, tiki, cocktail, ..., candle, horticulture)                                    & (sand bar, vineyard, monsoon forest, ski lodge, topless beach)                                      \\
Residence                  & (tribute, plains, des, acropolis, aa)                                                     & (battlefield, national monument, battlefield park, cliff dwelling, space shuttle)                   \\
ShopsAndService            & (cloister)                                                                                & (diocese, convent, tomb, college theater, study)                                                    \\
TravelAndTransport         & (dmctz3, panasonicdmctz3, panasonictz3)                                                   & (railroad tunnel, terminal, airport tram, railroad signal, railroad yard)                           \\
& NNET-U                                                                                    &                                                                                                     \\
ArtsAndEntertainment       & (messy, schoolhouse, beatles, ..., halftimbered, publictransport)                         & (cubicle, newsroom, workroom, bedroom closet, detached house)                                       \\
CollegeAndUniversity       & (snowboard, turism, amusementpark, ..., waltdisney, disneyworld)                          & (video game store, theme park, space shuttle, launch pad, speedway)                                 \\
Food                       & (boutique, liquor, olive, ..., counter, dessert)                                          & (molecular gastronomy restaurant, whisky bar, gourmet shop, cocktail bar, juice bar)                \\
NightlifeSpot              & (shoulder, darkroom, topless, ..., boobs, lips)                                           & (dance studio, tattoo studio, shoulder, topless beach, strip club)                                  \\
ParksAndOutdoors           & (nesting, tits, mouth, ..., gulls, avian)                                                 & (rookery, hunting reserve, wetland, wildlife reserve, nest)                                         \\
ProfessionalAndOtherPlaces & (islam, muslim, supermarket, ..., sales, marruecos)                                       & (retail outlet, electronics store, tourist information center, jewelry store, souk)                 \\
Residence                  & (heath, ward, lock, ..., stained, argyll)                                                 & (ventilation shaft, abandoned prison, sanatorium, abandoned complex, reformatory)                   \\
ShopsAndService            & (video, candy, clothing, ..., skin, blonde)                                               & (video game store, jewelry store, thrift store, workroom, nail salon)                               \\
TravelAndTransport         & (survey, mammal, antenna, ..., paws, fur)                                                 & (crop farm, flowerbed, tongue, nest, crop circle)                                                  
                                              
	\end{tabular}\caption{The placetypes clusters used at the top of the decision tree and the associated top-ranked entities}\label{ch5:placeent}
\end{table}
\end{landscape}
%%%%  Decision trees obtained from single dirs


\subsubsection{Top Entities for Common Terms}\label{ch5:commonterms}

The previous section gave insight into the  top five ranked entities, but did not illustrate the difference between the top five ranked entities  of  common directions derived from  the initial embedding and the neural   embeddings.  In this section, we investigate the differences between top entities for common terms. The method is the same as Section \ref{ch5:commonunique} to obtain these common terms, but instead of taking the top 2,000 terms we take the top 500, so as not to include low-scoring terms that will have poor entity representations across the embeddings. 

The results show that in most cases, entities are semantically similar, meaning that common directions across both neural embeddings and the best-performing initial embeddings have common directions that also have common top entities. For example, the term "arrival (ticket, departure)" always has entities related to airports "(taxiway, airport tram, airport, aircraft cabin, airport gate)" even in the case of NNET-U, where the entity "toll booth" is included "(airport tram, airport control tower, airport, airport lounge, toll booth)".  This further validates the idea given earlier that if different disentangled representations are able to achieve similar performance when used as input to low-depth decision trees, then they likely have found common but meaningful directions that are relevant to the task. This is particularly interesting in the case of NNET-B, as it started from a bag-of-words, but remained very similar to an unsupervised entity embedding. % scores when using direction and cluster features because they find common terms that can be used in the classes. Essentially, they are able to find the same relevant terms that matter for classification.

\begin{table}[]
	\scriptsize
	\setlength\extrarowheight{-1pt}
	\begin{tabular}{ll}
		\textbf{Cluster Features}                           & \textbf{Top 5 Ranked Entities}                                                                  \\
		\toprule
		Initial Embedding                     &                                                                                        \\
		bass (drums, tom)                          & (indie theater, jazz club, music studio, music venue, rock club)                       \\
		arrival (ticket, departure)                & (taxiway, airport tram, airport, aircraft cabin, airport gate)                         \\
		condo (condominium, apartments)            & (rental property, condominium, condo, villa, plaza)                                    \\
		palazzo (ff, notte)                        & (campanile, palace, villa, triumphal arch, cuesta)                                     \\
		fans (ball, player)                        & (basketball stadium, hockey arena, soccer stadium, baseball stadium, football stadium) \\
		animalplanet (feeder, natureselegantshots) & (zoo, zoological garden, nest, moais, animal shelter)                                  \\
		rent (homes, villas)                       & (rental property, real estate offices, cuesta, condo, condominium)                     \\
		agriculture (agricultural, fields)         & (cropland, conifer forest, olive grove, arboretum, sugar plantation)                   \\
		champions (win, league)                    & (football stadium, soccer stadium, basketball stadium, college stadium, cuesta)        \\
		cielo (azul, nubes)                        & (cuesta, campanile, plaza, villa, playa)                                               \\
		drummer (drums, who)                       & (jazz club, cuesta, moais, music venue, indie theater)                                 \\
		accommodation (accomodation, guest)        & (rental property, amenities, hotel pool, suite, ski chalet)                            \\
		decayed (forgotten, exploring)             & (abandoned airfield, sanatorium, concentration camp, abandoned prison, hospital ward)  \\
		yankees (louis, victory)                   & (baseball stadium, basketball stadium, baseball field, hockey arena, stadium)          \\
		NNET-B                                     &                                                                                        \\
		bass (drums, tom)                          & (indie theater, jazz club, music studio, music venue, rock club)                       \\
		arrival (ticket, departure)                & (taxiway, airport tram, airport, aircraft cabin, airport gate)                         \\
		condo (condominium, apartments)            & (rental property, condominium, condo, villa, plaza)                                    \\
		palazzo (ff, notte)                        & (campanile, palace, villa, triumphal arch, cuesta)                                     \\
		fans (ball, player)                        & (basketball stadium, hockey arena, soccer stadium, baseball stadium, football stadium) \\
		animalplanet (feeder, natureselegantshots) & (zoo, zoological garden, nest, moais, animal shelter)                                  \\
		rent (homes, villas)                       & (rental property, real estate offices, cuesta, condo, condominium)                     \\
		agriculture (agricultural, fields)         & (cropland, conifer forest, olive grove, arboretum, sugar plantation)                   \\
		champions (win, league)                    & (football stadium, soccer stadium, basketball stadium, college stadium, cuesta)        \\
		cielo (azul, nubes)                        & (cuesta, campanile, plaza, villa, playa)                                               \\
		drummer (drums, who)                       & (jazz club, cuesta, moais, music venue, indie theater)                                 \\
		accommodation (accomodation, guest)        & (rental property, amenities, hotel pool, suite, ski chalet)                            \\
		decayed (forgotten, exploring)             & (abandoned airfield, sanatorium, concentration camp, abandoned prison, hospital ward)  \\
		yankees (louis, victory)                   & (baseball stadium, basketball stadium, baseball field, hockey arena, stadium)          \\
		NNET-U                                     &                                                                                        \\
		bass (tom, drums)                          & (music studio, jazz club, rock club, music school, piano bar)                          \\
		arrival (departure, journey)               & (airport tram, airport control tower, airport, airport lounge, toll booth)             \\
		condo (condominium, condominiums)          & (condo, condominium, rental property, suite, hotel pool)                               \\
		palazzo (palais, efs)                      & (campanile, palace, cathedral, villa, plaza)                                           \\
		fans (arena, name)                         & (football stadium, basketball stadium, soccer stadium, hockey arena, stadium)          \\
		animalplanet (tiere, vosplusbellesphotos)  & (zoo, zoological garden, nest, animal shelter, rookery)                                \\
		rent (rental, animalplanet)                & (rental property, condo, suite, condominium, real estate offices)                      \\
		agriculture (farming, petrol)              & (crop farm, olive grove, arboretum, cropland, wheatfield)                              \\
		champions (win, victory)                   & (football stadium, soccer stadium, stadium, basketball stadium, hockey arena)          \\
		cielo (ciel, nubes)                        & (campanile, cuesta, cathedral, plaza, villa)                                           \\
		drummer (guitarist, drums)                 & (jazz club, rock club, stage, piano bar, music venue)                                  \\
		accommodation (condominium, hostel)        & (amenities, rental property, suite, hotel pool, resort)                                \\
		decayed (forgotten, decaying)              & (sanatorium, abandoned airfield, barracks, abbey, military barracks)                   \\
		yankees (mlb, league)                      & (baseball stadium, baseball field, avenue, basketball stadium, hockey arena)          
	\end{tabular}\caption{Terms common for all embeddings and the associated ranking of entities on those term directions for each embedding. Arranged by NDCG-score in the initial embedding.}
\end{table}

%One general pattern that is noticable in these results is that entities for NNET-B seem to make less sense than those of the unsupervised or NNET-U embeddings. For example, the term "abandonedbuilding" which has meaningful entities in the case of the initial embedding "(abandoned township, sanatorium, vacant house, abandoned factory, sugar refinery)" and the case of NNET-U "(sanatorium, abandoned factory, abandoned prison, elevator shaft, abandoned home)" seems both nonsensical and general for NNET-B "(bathroom, gay bar, road, room, hill)". This is interesting, because the neural network results although worse than the unsupervised SVM representation were better than NNET-U. This could be explained by the neural network prioritizng terms relevant to the Foursquare task, as "abandonedbuilding" is not a useful term for classifying these classes, it was not separable in the space. This idea can be reinforced by the term "haunted (eerie, peeling)" also not containing relevant entities "(border, house, home, square, backyard)", these terms are just not relevant to the task and so are not prioritized.

%Although this is an assumption, it is backed by the previous qualitative results. Those directions chosen for the decision trees had meaningful entities in the top 5, even if these terms did not. We can conclude that although these common terms do contain sometimes irrelevant entities, this is because those directions were not prioritized in the representation. We verify this further quantitatively by finding the difference in NDCG score between NNET-B and the unsupervised representation:



%Decision trees of depth-3 in the quantiative results performed as well as neural networks, however the trees obtained from this representation are simplistic in the form of "if HORROR then HORROR" rather than representing some complex domain knowledge.



%%%%  Clusters

%%%%  Cluster diffs (new clusters? same terms but diff clusters?)

%%%%  Decision trees obtained from clusters

%\begin{table}[]\label{ch5:clustersusedintrees}
%	Horror: Most important clusters used for space 1, most important for space 2
%\end{table}

%%%%  Ranks obtained from single directions

%%%%  Ranks obtained from cluster directions

%%%%  Ranks obtained from single directions differences

%\begin{table}[]\label{Biggest difference in entities (entities that have gone down/gone up the most) for the same directions }
%	Entities, terms that have gone down the most from the original space, terms that have gone up the most
%\end{table}

%\begin{table}[]
%	\scriptsize
%\begin{tabular}{ll}%
%	Unsupervised Embedding                      &                                                                                   \\
%	statua (stern, 1st)                         & (triumphal arch, cuesta, cathedral, palace, campanile)                            \\
%	commuter (underpass, muni)                  & (underpass road, overpass road, freeway, elevated roadway, highway ramp)          \\
%	frankreich (francia, nov)                   & (campanile, triumphal arch, castle, cathedral, villa)                             \\
%	homes (residential, apartment)              & (rental property, villa, condominium, condo, real estate offices)                 \\
%	abandonedbuilding (abandonned, dilapidated) & (abandoned township, sanatorium, vacant house, abandoned factory, sugar refinery) \\
%	blooming (blooms, petals)                   & (arboretum, olive grove, botanical garden, garden, flowerbed)                     \\
%	haunted (spooky, eerie)                     & (cemetery, grave, mausoleum, tomb, sanatorium)                                    \\
%	adriatic (dalmatia, fishingboat)            & (campanile, triumphal arch, villa, palace, moais)                                 \\
%	cascades (cascade, dubai)                   & (ski trail, summit crater, summit, ski chairlift, ski resort)                     \\
%	designer (flor, virtual)                    & (bridal shop, tanning salon, bedroom closet, cosmetics shop, cuesta)              \\
%	dj (bands, punk)                            & (indie theater, jazz club, cuesta, music studio, rock club)                       \\
%	poor (poverty, documentary)                 & (slum, depression, villa, cuesta, caliphate)                                      \\
%	agricultural (tractor, farming)             & (cropland, crop farm, farmland, pasture, corn field)                              \\
%	concerts (instrument, solo)                 & (jazz club, indie theater, concert hall, cuesta, playhouse)                       \\
%	NNET Bow                                    &                                                                                   \\
%	statua (estatua, colonnade)                 & (crucifix, palace, tomb, monument, altar)                                         \\
%	commuter (lightrail, commute)               & (apartment, bus station, roadway, bus line, rock)                                 \\
%	frankreich (allemagne, austria)             & (castle, campanile, house, estate, rock)                                          \\
%	homes (residential, balcony)                & (house, home, slum, room, rental property)                                        \\
%	abandonedbuilding (decrepit, neglect)       & (bathroom, gay bar, road, room, hill)                                             \\
%	blooming (blossoms, daffodils)              & (conservatory, mountain, house, garden, greenhouse)                               \\
%	haunted (eerie, peeling)                    & (border, house, home, square, backyard)                                           \\
%	adriatic (turkiye, dalmatia)                & (catena, palace, rock, gay bar, campanile)                                        \\
%	cascades (tundra, prey)                     & (rock, bridge, house, summit, ski area)                                           \\
%	designer (gorgeous, mannequin)              & (room, bathroom, house, home, bedroom)                                            \\
%	dj (pop, smoking)                           & (gay bar, dive bar, rock club, pub, nightclub)                                    \\
%	poor (westminster, bag)                     & (house, thatched roof building, neighborhood, home, campanile)                    \\
%	agricultural (farmer, farming)              & (silo, farmstead, farmhouse, abandoned farm, thatched roof building)              \\
%	concerts (stalls, folk)                     & (rock club, casino, thatched roof building, nightclub, pub)                       \\
%	NNET Embedding                              &                                                                                   \\
%	statua (palacio, estatua)                   & (cathedral, triumphal arch, campanile, palace, convent)                           \\
%	commuter (muni, passenger)                  & (freeway, bus line, elevated roadway, ferry station, light rail)                  \\
%	frankreich (francia, normandy)              & (cathedral, campanile, abbey, castle, triumphal arch)                             \\
%	homes (condo, pine)                         & (rental property, thatched roof building, condo, villa, real estate offices)      \\
%	abandonedbuilding (vandalism, demolished)   & (sanatorium, abandoned factory, abandoned prison, elevator shaft, abandoned home) \\
%	blooming (petals, bloom)                    & (olive grove, arboretum, botanical garden, garden, conservatory)                  \\
%	haunted (spooky, scary)                     & (cemetery, abbey, sanatorium, graveyard, manor)                                   \\
%	adriatic (dalmatia, tilt)                   & (campanile, triumphal arch, villa, walled town, castle)                           \\
%	cascades (reservoir, glacier)               & (summit crater, hanging valley, summit, ridge line, mountain)                     \\
%	designer (shuttle, mic)                     & (tanning salon, suite, bedroom closet, bedroom, laundry room)                     \\
%	dj (hip, rave)                              & (playhouse, jazz club, rock club, stage, piano bar)                               \\
%	poor (poverty, documentary)                 & (slum, cathedral, homeless shelter, town square, church)                          \\
%	agricultural (plow, barns)                  & (crop farm, corn field, wheatfield, olive grove, cropland)                        \\
%	concerts (kunst, league)                    & (jazz club, cirque, rock club, stage, playhouse)                                 
%\end{tabular}\caption{Terms common for all embeddings and the associated ranking of entities on those term directions for each embedding. Arranged by NDCG-score in the unsupervised embedding.}
%\end{table}


%There are common word directions, e.g.\ horror. But how have the rankings changed for these directions? This is investigated by looking at the biggest differences in rankings for the same word between the original representation and the fine-tuned representation. We use the place-types results for this as they have easy to understand entities. We can see that e.g.

%%%%  Ranks obtained from top directions differences

%%%%  Common ranks

%%%%  Unique ranks

%%%%  PCA projections

%%%%  Scores of different classes









\subsection{Summary of Results}

First, directions in the movie domain were examined. When looking at top scoring terms in Section \ref{ch5:topscore}, it was found that NNET-U retained highly separable metadata directions. This seems to indicate that despite directions being uninformative, the neural network \hmark{does} not change the representation enough to make these directions less separable, \hmark{despite improving other directions so that the embedding   better suits the task.} In Section \ref{ch5:commonunique} it was found that NNET-U did not contain many unique directions in its top 2,000 scoring terms, but NNET-B did. \hmark{This is expected, as NNET-B is not starting from a vector space, however, despite starting from a bag-of-words the directions that were common to all embeddings seemed to be the most relevant properties to the task.} This  \hmark{explains} the reason that the disentangled feature representations derived from NNET-U, NNET-B and the initial embedding all performed similarly:  they all contained the directions that are most relevant to the task. 

In \ref{ch5:diffsection} the largest differences in score between directions was examined. It was found that terms relevant to the task had an increased associated NDCG score in the NNET-U representation, and NNET-B had the highest score increases for directions that were not relevant to the task. This gives some explanation for why NNET-B performed worse than NNET-U, NNET-B \hmark{ better represented terms that were not relevant to the task, while NNET-U focused on improving existing relevant directions.} In \ref{ch5:cluster-features} it was hypothesized that \hmark{the reason that the disentangled feature representation derived from NNET-B  did not result in overall higher performance  in a low-depth decision tree is because although it had good performance on the "Adventure" genre, it didn't have a property to classify the "Animation" genre.}  \hmark{In summary, the feed-forward neural network that performed well refined directions relevant to the task in-order to achieve a good classification accuracy, rather than finding new properties.}

In Section \ref{ch5:newsgroupsquant} cluster-features derived from NNET-B in the newsgroups domain were examined, as they performed as well as NNET-B. It was found that for NNET-B, the features used as the top decision tree nodes seemed to capture abstract concepts more relevant to the task than in the case of NNET-U or the initial embedding.\hmark{ This explains the increase in performance for the cluster-features used in the decision tree, NNET-U achieves strong performance by focusing on improving the feed-forward neural network features, but NNET-B achieves strong performance when using the cluster features because it is able to find unique properties that are relevant to the task.}

In Section \ref{ch5:place-typesentities} it was found that entities for features are meaningful in almost all cases, and across all embedding models. This further verifies that features are semantic across domains even when derived from neural network embeddings. It is speculated that using meaningful entities could make the features more understandable. This is further investigated in section \ref{ch5:commonterms}, where it is found that single-term directions common to all embeddings have similar entities across all embeddings. This further validates the idea in section \ref{ch5:commonunique} that each embedding model in the movies domain found similar relevant directions and associated entities that are most relevant to the task, as it shows that common directions are likely common in entity rankings for the domain as well.









%The way that knowledge is represented is changed significantly in the neural network with bow input compared to the original representation. This can be verified by what clusters are used in the decision trees to classify the entities. For the neural network with bow, 
%But what if these directions are not new properties, but instead the same rankings but with different words labelling them?






%\subsubsection{Directions}

%Two groups of single word directions and clusters were obtained from three different domains, in each one is from the second layer of  a two-layer neural network starting from a bag-of-words and the other is from the first layer of a one-layer neural network starting from an unsupervised representation. The examples in this section for the clusters are from the clusters that performed the best in depth-3 decision-trees. When investigating single word directions, sometimes it is unclear what they mean. In this case, the most similar single word-directions are found and used to provide additional context.

% Compare the same directions but see how their rankings have changed



% Compare the top directions for NDCG, top for Kappa



%\subsubsection{Starting from vector space}

%\subsubsection{Starting from bag-of-words}

%Obtaining directions from this space takes much longer than normal vector spaces.






\section{Evaluation of Auto-encoders}

\hmark{This section is separated into four parts. First, the parameters and methodology of the semantic features to be used in the experiments is explained in Section \ref{howto}. Then, the auto-encoders are quantitatively evaluated in Section \ref{ch5:quanevalffnnet} to determine how good the derived semantic features are in each layer. In summary, it is found that these derived semantic features do not perform well, but they seem to contain interesting properties. In particular, it seems that the deeper that the layers are in the auto-encoder, the more general the properties seem to be. These properties are further investigated qualitatively in Section \ref{auto-encoder-qual} and validated quantitatively in Section \ref{freq-eval}, where it is found that the deeper the layers of the auto-encoder are the more frequent highly separable properties become.}

\subsection{Software, Architecture and Settings}\label{howto}

We base our experiments on the movie reviews domain. To collect the terms that are likely to correspond to property names, we collect adjectives and nouns that occur at least 200 times in the movie review data set, collecting 17,840 terms overall.  These parameters are used for all entity embeddings induced by the auto-encoder. 

To implement the denoising auto-encoders, the Keras\footnote{\href{https://keras.io/}{Keras}}  library is used. As in Chapter \ref{ch3}, scikit-learn is used for the SVM implementation.  A 200 dimensional MDS space is used as the input to our stack of auto-encoders, as this performed the best in Chapter \ref{ch3}. The network is trained using stochastic gradient descent and the mean squared error loss function. For the encoders and decoders, the tanh activation function is used. For the first auto-encoder,  the same size layer as the input is maintained. Afterwards, the hidden representation size is halved each time it is used  as input to another auto-encoder, and this process is repeated three times, resulting in four new hidden representations $\{Input: 200, Hidden: 200, 100, 50, 25\}$. The input space is corrupted using  Gaussian noise with a standard deviation of 0.6 each time. This was chosen as higher noise values seemed to result in meaningless directions in initial results, but lesser noise values did not result in much change. As the lower layers are closer to the bag-of-words representation and are higher dimensional, the Kappa scores are higher in earlier spaces, as it is easier to separate entities. We address this in the clusters by setting the  score threshold $T$ using Kappa score  such that the number of terms we choose from is twice the number of dimensions in the space. Similarly, we set the threshold for the frequency of the words such that 12,000 directions are available to assign to the cluster centres in every space. 

\subsection{Quantitative Evaluation of Cluster Representation}\label{quan-auto}

% Explain what the experiment is and what it's for



% It's to see how good the properties are in auto-encoders

\hmark{This section evaluates if it is possible to obtain good  properties from the hidden layers of stacked denoising auto-encoders following the same procedure as in Chapter \ref{ch4}. A representation composed of cluster properties is obtained from the hidden layers of the stacked denoising auto-encoder  described in Section \ref{howto}, then these representations are used as input to low-depth decision trees to classify the genres, keywords and ratings tasks for the movies domain. The results are shown in Table \ref{quanautoresult}, where it can be seen that the accuracy and f1-scores of the properties obtained from the hidden layers of stacked auto-encoders are significantly lower than the properties obtained from the original representation. This indicates that the hidden-layers of these auto-encoders do not represent key domain properties. However, when observing examples these properties do seem to have meaning. This is further investigated in  Section \ref{auto-encoder-qual}.  }



\begin{table}[]
	\centering
	\begin{tabular}{lllllll}
		\textbf{Clusters}      & \textbf{Genres}   &          & \textbf{Keywords} &          & \textbf{Ratings}  &          \\
		\toprule
		 & Accuracy & F1-Score & Accuracy & F1-Score & Accuracy & F1-Score \\
		L0            & \textbf{0.91}     & \textbf{0.43}     & \textbf{0.71}     & \textbf{0.22}     & \textbf{0.65}     & \textbf{0.49}     \\
		L1            & 0.64     & 0.27     & 0.63     & 0.17     & 0.64     & 0.41     \\
		L2            & 0.71     & 0.32     & 0.65     & 0.19     & 0.64     & 0.44     \\
		L3            & 0.66     & 0.27     & 0.62     & 0.17     & 0.61     & 0.44     \\
		L4            & 0.66     & 0.26     & 0.54     & 0.15     & 0.55     & 0.40    
	\end{tabular}
\caption{The results of depth-3 trees for a variety of tasks in the movie domain where cluster representations derived from auto-encoder layers are used as input}\label{quanautoresult}
\end{table}



\subsection{Qualitative Evaluation of Cluster Representations}\label{auto-encoder-qual}

% Although there is a significant drop-off in accuracy, it seems that there is a meaningful difference between the properties. 

\hmark{In this section, the properties induced from the auto-encoder spaces described in Section \ref{quan-auto} are qualitatively investigated.} In Table \ref{ch5:labelcomparison}, we illustrate the differences between clusters obtained using standard auto-encoders and denoising auto-encoders. Layer 1 refers to the hidden representation of the first auto-encoder, and Layer 4 refers to the hidden representation of the final auto-encoder. As single labels can lead to ambiguity, in Table 1 we label clusters using the top three highest scoring terms in the cluster. Clusters are arranged from highest to lowest Kappa score. 

Both auto-encoders model different properties across their layers, but the properties obtained when using a denoising auto-encoder seem to be significantly different. For example, the normal auto-encoder captures properties like "Horror" and "Thriller", but does not capture properties like "Society" and "Relationship". Further, "Gore"  is the  most similar to the directions "Zombie" and "Zombies" in Layer 1, the most similar directions of "Budget" and "Effects" in Layer 4. We can assume that this means that the feature in Layer 4 represents the  idea of budget "Horror" movie, which is significantly different to a "Zombie" movie. This change, from "Zombie" to "Effects" seems to be a difference between a low-frequency word that is specific to only some movies "Zombie", and a high-frequency word that is meaningful for many movies. In Section \ref{freq-eval} the relationship between properties in the earlier layers and those in the later ones is more precisely quantified as a relationship of frequency, i.e. relevancy to many documents rather than few.

\begin{sidewaystable}
	\vspace{5 mm}
	
	\setlength\extrarowheight{-3pt}
	
	\begin{tabular}{llll}
		
		\bf Standard Auto-encoder &   & \textbf{Denoising Auto-encoder} &   \\
		Layer 1     &  Layer 4  &  Layer 1   &  Layer 4 \\
		\toprule
		horror: terror, horrific & horror: victims, nudity  & gore: zombie, zombies & society: view, understand \\
		thriller: thrillers, noir & documentary: perspective, insight  & jokes: chuckle, fart & emotional: insight, portrays \\
		comedies: comedy, timing & blood: killing, effects   & horror: terror, horrific & stupid: flick, silly  \\
		adults: disney, childrens & suspense: mysterious, tense & emotionally: tragic, strength &  gore: budget, effects  \\
		husband: wife, husbands &  thriller: thrillers, cop  & gags: zany, parodies & military: war, ship \\
		relationships: intimate, angst & gory: gruesome, zombie  & hindi: bollywood, indian & romance: younger, handsome \\
		nudity: naked, gratuitous & beautifully: satisfying, brilliantly & touching: teach, relate & ridiculous: awful, worse   \\
		political: politics, nation & emotional: complex, struggle & scary: frightening, terrifying & government: technology, footage  \\
		smart: slick, sophisticated &  laughed: laughing, loud & documentary: document, narration & awesome: chick, looked\\
		creepy: sinister, atmospheric & charming: delightful, loves & adults: disney, teaches &  political: country, documentary\\
		laughed: humorous, offensive & hilarious: funny, parody & laughed: brow, laughter & relationship: relationships, sensitive \\
		adventure: adventures, ship & scares: halloween, slasher & thriller: thrillers, procedural & horror: genre, dark  \\
		actions: reaction, innocent & funniest: funnier, gags & cgi: animated, animation &  waste: concept, plain   \\
		cute: adorable, rom & emotions: respect, relationships & suspense: clues, atmospheric & army: disc, studio\\
		british: england, accent & laugh: mom, crazy & dumb: mindless, car & combat: enemy, weapons  \\
		horrible: worse, cheap & filmmaker: approach, artist & political: propaganda, citizens & supporting: office, married \\
		narrative: filmmaker, structure & drama: portrayed, portrayal & witty: delightfully, sarcastic & amazon: bought, copy \\
		digital: dolby, definition & interviews: included, showed & laughing: outrageous, mouthed & study: details, detail \\
		gory: graphic, gruesome & comedic: comedies, humorous & relationships: ensemble, interactions & land: water, super \\
		romantic: handsome, attractive & emotionally: central, relationships & creepy: mysterious, eerie & chemistry, comedies, comedic \\
		
	\end{tabular}
	\caption{A comparison between the first layers and the fourth layers of two different kinds of auto-encoders.}
	\label{ch5:labelcomparison}
	
\end{sidewaystable}


\subsection{Quantitative Frequency Evaluation}\label{freq-eval}

\hmark{Although there is an indication that the properties in the later layers of the stacked denoising auto-encoder have some meaningful change, it is unclear what that change is. Following the qualitative analysis in Section \ref{auto-encoder-qual} the results seem to indicate later properties are more frequent across the corpus, while the earlier properties are less frequent. In other words, the earlier representations model properties that are more specific, in the sense that they are relevant to only some documents, and the later representations model properties that are more general, in the sense that they are relevant to more documents.} 

\hmark{To evaluate the frequency of the clusters, the average frequency of each term in the cluster is used. However, the first terms of a cluster are generally more separable in the space, as the highest kappa score terms are added to the cluster first. Given this, the first $N$ terms of each cluster for a layer of the auto-encoder are averaged rather than all terms. There is a maximum cluster size, and all terms must be added, so taking the top $N$ terms also removes the problem of irrelevant and less separable terms being included that were added to clusters just because they were the least dissimilar compared to the other clusters, despite not being relevant to the cluster.}


\hmark{ To determine the frequency of an auto-encoder layer, for each cluster the first $N$ words are taken and their frequencies are averaged. This is the cluster frequency. Then the overall frequency of the auto-encoder layer is obtained by taking the average of all clusters. This overall frequency for each layer is shown in Table \ref{quan-freq-table}. Additionally, results for the first $N$ words at different thresholds are shown. As suspected in the qualitative observations in Section \ref{auto-encoder-qual}, as the auto-encoder layers get deeper the frequency of the terms that are well modelled (i.e. terms the first terms of the cluster) increases.}

	

\begin{table}[]
	\centering
	\begin{tabular}{llllll}
		 \# Of Cluster Words  & \textbf{L0 Freq} & \textbf{L1 Freq} & \textbf{L2 Freq} & \textbf{ L3 Freq} & \textbf{L4 Freq} \\
		\toprule
		First 1 & 23298   & 29984   & 37277   & 41952   & 36489   \\
		First 2 & 20399   & 25356   & 31184   & 37764   & 35785   \\
		First 3 & 17065   & 22595   & 30465   & 36935   & 34893   \\
		First 4 & 14986   & 20555   & 28780   & 36798   & 35107   \\
		First 5 & 13352   & 18982   & 27724   & 35446   & 34705   \\
		ALL   & 7556    & 12187   & 16440   & 17684   & 9611   
	\end{tabular}
\caption{The average frequency of cluster words in the corpus. Here, the frequencies of the first words in the cluster are shown, up to five.}\label{quan-freq-table}
\end{table}




% To further investigate the idea that these properties are more frequent, they are investigated in this section

% We take the top X to represent the cluster because all terms don't make sense

% We test multiple top X to make sure that it isn't just a bias result or whatever

% We find that they are indeed more general/frequent


\section{Inducing Rules from Auto-Encoder Entity Embeddings}\label{InducingRulesFromEntityEmbeddings}

As described in the previous section, stacked denoising auto-encoders model increasingly frequent properties. Based on this observation, in this section a method is introduced  to characterize semantic features (i.e.\ clusters of directions)  modelled in one space  in terms of salient properties that are modelled in another space. To do so, the off-the-shelf rule learner JRip (Using the "RIPPER" algorithm \cite{Cohen1995})  is used to predict which entities will be highly ranked, according to a given cluster direction, using the rankings induced by the clusters of the preceding space   as features. To improve the readability of the resulting rules, rather than using the precise ranks as input, the ranks are aggregated by percentile, i.e $1\%, 2\%, ..., 100\%$, where an entity has a $1\%$ label if it is among the $1\%$ highest ranked entities, for a given cluster direction. For the class labels,  a movie is defined as a positive instance if it is among the highest ranked entities (e.g.\ top 2\%) of the considered cluster direction. Using the input features of each layer and the class labels  from the subsequent layer, these rules can be used to explain the semantic relationships between properties modelled by different vector spaces. \hmark{In this setting, attributes are discretized as understanding what is meant by the rules is the main focus, as  these rules are not intended for making predictions, but only for getting insight into data and potentially  generating explanations of domain knowledge.}


\subsection{Qualitative Evaluation of Induced Symbolic Rules}

% Based on the findings that they are frequent, perhaps it is possible to induce symbolic rules.
In this section, the method used to obtain rules that describe the semantic relationships between properties derived from the stacked denoising auto-encoder is qualitatively investigated. Since the number of all induced rules is large, here we only show high accuracy rules that cover 200 samples or more.  Still, we naturally cannot list even all the accurate rules covering more than 200 samples. Therefore we focus here on the rules which are either interesting in their own right or exhibit interesting properties, strengths or limitations of the proposed approach. 

For easier readability, we post-process the induced rules. For instance, the following is a rule obtained for the property "Gore" in the third layer of the network shown in the original format produced by JRip:

\begin{lstlisting}[mathescape=true]
IF scares-L2 <= 6 AND blood-L2 <= 8 AND funniest-L2 >= 22
=> classification=+ (391.0/61.0)
\end{lstlisting}

\noindent In this rule, $\texttt{scares-L2 <= 6}$ denotes the condition that the movie is in the top 6\% of rankings for the property "$\texttt{scares}$" derived from the hidden representation of the second auto-encoder. We will write such conditions simply as "$\texttt{Scares$_{2}$}$". Similarly, a condition such as $\texttt{funniest-L2 >= 22}$, which indicates that the property is not in the top $22\%$, will be written as "$\texttt{NOT Funniest}$$_{2}$". In this simpler notation the above rule will look as follows: %We thus obtain the following formulation for the above rule:

\begin{lstlisting}[mathescape=true]
IF Scares$_{2}$ AND Blood$_{2}$ AND NOT Funniest$_{2}$ THEN Gore$_{3}$
\end{lstlisting}

This rule demonstrates an interpretable relationship. However, we have observed that the meaning of a rule may not be clear from the property labels that are automatically selected. In such cases, it is beneficial to label them by including the most similar cluster terms. For example, using the cluster terms below we can see that "Flick" relates to "chick-flicks" and that "Amazon" relates to old movies:

\begin{lstlisting}[mathescape=true]
IF Flick$_{2}$ AND Sexual$_{2}$ AND Cheesy$_{2}$ AND NOT Amazon$_{2}$ THEN Nudity$_{3}$

Flick$_{2}$: {Flicks, Chick, Hot}
Amazon$_{2}$: {Vhs, Copy, Ago}
\end{lstlisting}

Rules derived from later layers use properties described by rules from previous layers. By seeing rules from earlier layers that contain properties in later layers, we can better understand what the components of later rules mean. Below, we have provided rules to explain the origins of components in a later rule:

\begin{lstlisting}[mathescape=true]
IF Emotions$_{2}$ AND Actions$_{2}$ THEN Emotions$_{3}$ 
IF Emotions$_{2}$ AND Emotion$_{2}$ AND Impact$_{2}$ THEN Journey$_{3}$ 
IF Emotions$_{3}$ AND Journey$_{3}$ THEN Adventure$_{4}$ 
\end{lstlisting}

We observe a general trend that as the size of the representations decreases and the entity embeddings become smaller, rules have fewer conditions, resulting in overall higher scoring and more interpretable rules. To illustrate this, we compare rules from an earlier layer to similar rules in a later layer: %Below, we present two of the layer-two rules.

\begin{lstlisting}[mathescape=true]
IF Romance$_{1}$ AND Poignant$_{1}$ AND NOT English$_{1}$ AND NOT French$_{1}$
AND NOT Gags$_{1}$  AND NOT Disc$_{1}$ THEN Relationships$_{2}$

IF Relationships$_{2}$ AND Emotions$_{2}$ AND Chemistry$_{2}$ THEN Romantic$_{3}$
IF Emotions$_{2}$ AND Compelling$_{2}$ THEN Beautifully$_{3}$ 
IF Warm$_{2}$ AND Emotions$_{2}$ THEN Charming$_{3}$ 
IF Emotions$_{2}$ AND Compelling$_{2}$ THEN Emotional$_{3}$ 
\end{lstlisting}

Rules in later layers also made effective use of a NOT component. Below, we demonstrate some of those rules:

\begin{lstlisting}[mathescape=true]
IF Touching$_{3}$ AND Emotions$_{3}$ AND NOT Unfunny$_{3}$ THEN  Relationship$_{4}$ 
IF Laughs$_{3}$ AND Laugh$_{3}$ AND NOT Compelling$_{3}$ THEN  Stupid$_{4}$
IF Touching$_{3}$ AND Social$_{3}$ AND NOT Slasher$_{3}$ THEN  Touching$_{4}$
\end{lstlisting}

As the same terms were used to find new properties for each space, the obtained rules sometimes use duplicate property names in their components. As the properties from later layers are a combination of properties from earlier layers, the properties in later layers are refinements of the earlier properties, despite having the same term. Below, we provide some examples to illustrate this:

\begin{lstlisting}[mathescape=true]
IF Emotions$_{2}$ AND Actions$_{2}$ THEN Emotions$_{3}$ 

Emotions$_{2}$: {Acted, Feelings, Mature}
Actions$_{2}$: {Control, Crime, Force}
Emotions$_{3}$: {Emotion, Issue, Choices}

IF Horror$_{2}$ AND Creepy$_{2}$ AND Scares$_{2}$ THEN Horror$_{3}$

Horror$_{2}$: {Terror, Horrific, Exploitation}
Creepy$_{2}$: {Mysterious, Twisted, Psycho}
Scares$_{2}$: {Slasher, Supernatural, Halloween}
Horror$_{3}$: {Creepy, Dark, Chilling}

IF Touching$_{2}$ AND Chemistry$_{2}$ THEN Touching$_{3}$ 
IF Touching$_{2}$ AND Emotions$_{2}$ THEN Touching$_{3}$ 
IF Compelling$_{2}$ AND Emotional$_{2}$ AND Suspense$_{2}$ THEN Compelling$_{3}$
If Romance$_{2}$ AND Touching$_{2}$ AND Chemistry$_{2}$ THEN Romance $_{3}$ 
IF Emotionally$_{2}$ AND Emotions$_{2}$ AND Compelling$_{2}$ THEN Emotionally$_{3}$ 
\end{lstlisting}



\section{Conclusion} \label{ch5:conclusion}

Feedforward neural networks and auto-encoders are qualitatively investigated using the methods outlined in Chapter \ref{ch3}. These properties are meaningful and interpretable for both types of neural network architecture. In the case of feed-forward networks, it is observed that associated entities of features are meaningful, and similar features are found in all embeddings. These associated top entities are particularly relevant when understanding the semantic features. Further, the performance of a feed-forward network seems to rely on making particularly relevant properties to the task separable, as seen in the case of NNET-B in the newsgroups domain. If properties that are relevant to the task are common to all embedding models, low-depth decision trees that use these disentangled feature representations as input perform similarly, for example in the movies domain.  

In the case of the auto-encoders as the space size reduces the representation becomes more abstract, and additionally through qualitative investigation rules show some promise in being used to link together properties in the layers and form some explanation of abstraction. However, it is found that the Kappa score of the directions derived from the embeddings is lower in more stacked auto-encoder embeddings, meaning that they are less separable. This is likely due to repeated non-linear transformations. This brings-up the question, is it possible to fine-tune the embedding such that features remain linearly separable but also become more abstract? This lead on to the work  in Chapter \ref{ch4}, where a fine-tuning process is introduced to improve the quality of the disentangled feature representations. To expand on the work in this chapter, it would be interesting to see how the interpretability of these disentangled representations could be compared to alternative approaches, and they could be used to benefit those that work on those  safety and fairness of neural network models, in particular to make black-box state-of-the-art models transparent by  explaining layer-to-layer connections.




%\subsection{Chapter 3 Space Types}
%\begin{landscape}
%	\begin{table}[]
%		\begin{tabular}{llllllllll}
%			& Genres     &         &         & Keywords  &         &         & Ratings  &         &         \\
%			Movies            & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
%			Space             & 50 PCA     & 50 MDS  & 100 MDS & 200 PCA   & 200 MDS & 200 MDS & 50 PCA   & 200 PCA & 50 PCA  \\
%			Single directions & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A     \\
%			&            &         &         &           &         &         &          &         &         \\
%			& Newsgroups &         &         & Sentiment &         &         & Reuters  &         &         \\
%			Rep               & 200 PCA    & 200 PCA & 100 PCA & PCA 100   & PCA 50  & PCA 50  & 200 PCA  & 200 PCA & 100 PCA \\
%			Single dir        & 200 MDS    & 100 D2V & 50 D2V  & D2V 100   & PCA 50  & D2V 100 & N/A      & N/A     & N/A     \\
%			&            &         &         &           &         &         &          &         &         \\
%			& Foursquare &         &         & OpenCYC   &         &         & Geonames &         &         \\
%			Placetypes        & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
%			Rep               & MDS 100    & AWV 50  & MDS 200 & AWV 50    & MDS 200 & AWV 50  & MDS 50   & MDS 50  & AWV 200 \\
%			Single dir        & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A    
%		\end{tabular}\caption{Space-types, clusters have the same as single directions.}
%	\end{table}
%\end{landscape}




  %This elevates neural networks from simply being representation learners to models that can discover and create new ideas in their representations in-order to solve the task.

%Theoretically if we are able to identify features from each layer, then we can also map how they interact with each other. 









%For each domain, we test two different network set-ups. The first is one where the hidden layer is the same size as the input layer, and the second is where the hidden layer is smaller than the hidden layer. The assumption is that by constraining the hidden layer size, we can force the network to generalize and find more abstract concepts in the hidden-layer, hopefully gaining some insight into the domain by comparing the input representation and the more general hidden representation.

%There are no additional techniques applied to improve the performance of the neural networks.