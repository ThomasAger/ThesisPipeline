\chapter{Investigating Neural Networks In Terms Of Directions}

\section{Chapter 5}\label{chapter5}

 Neural network models that encode spatial relationships in their hidden layers have achieved state-of-the-art in Text Classification by using transfer learning from a pre-trained Language Model \cite{Gong2018}. There have also been neural network models that produce an interpretable representation, for example InfoGan.
 Most state-of-the-art results rely on Vector Space Models. Ideally the method would be able to achieve strong results for simple interpretable classifiers by transforming an existing representation that performs well at the task.
 

\subsection{Chapter 3 Space Types}
\begin{landscape}
	\begin{table}[]
		\begin{tabular}{llllllllll}
			& Genres     &         &         & Keywords  &         &         & Ratings  &         &         \\
			Movies            & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
			Space             & 50 PCA     & 50 MDS  & 100 MDS & 200 PCA   & 200 MDS & 200 MDS & 50 PCA   & 200 PCA & 50 PCA  \\
			Single directions & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A     \\
			&            &         &         &           &         &         &          &         &         \\
			& Newsgroups &         &         & Sentiment &         &         & Reuters  &         &         \\
			Rep               & 200 PCA    & 200 PCA & 100 PCA & PCA 100   & PCA 50  & PCA 50  & 200 PCA  & 200 PCA & 100 PCA \\
			Single dir        & 200 MDS    & 100 D2V & 50 D2V  & D2V 100   & PCA 50  & D2V 100 & N/A      & N/A     & N/A     \\
			&            &         &         &           &         &         &          &         &         \\
			& Foursquare &         &         & OpenCYC   &         &         & Geonames &         &         \\
			Placetypes        & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
			Rep               & MDS 100    & AWV 50  & MDS 200 & AWV 50    & MDS 200 & AWV 50  & MDS 50   & MDS 50  & AWV 200 \\
			Single dir        & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A    
		\end{tabular}\caption{Space-types, clusters have the same as single directions.}
	\end{table}
\end{landscape}


We trained neural networks with non-linear activation functions so that they would ideally perform better than linear SVM's on the document representations. The assumption behind this is that if the network is able to achieve stronger results on the task than a baseline representation, it must be doing something more than representing the information. In other words, the representation must be better for the task than an unsupervised representation. The goal of this section is to discover why neural networks are able to perform so well, and explain it in terms of directions. 

The  differences between the supervised representation and the unsupervised representation are examined here, with the main question being how exactly does a neural network achieve such strong results on the task? Some general ideas or explanations that are given are that through non-linear transformations, the internal representation of the neural network is able to find a more complex or accurate representation of domain knowledge relevant to the task.  %This elevates neural networks from simply being representation learners to models that can discover and create new ideas in their representations in-order to solve the task.

Theoretically if we are able to identify features from each layer, then we can also map how they interact with each other. 

When obtaining the directions, we set the frequency cut-off to the top 10,000 words and the direction cut-off when classifying to 2000 features. This is arbitrary, as both of these cut-offs need to be tuned for the specific space-type and task in-order to achieve strong results. The reason that these parameters are not tuned in this case is because we are interested in the qualitative nature of the directions, not the performance on the text classification task. 

We investigate the task using the newsgroups as it gives a good example of a representation that contains many different concepts and aspects of domain knowledge, as well as a well defined classification task that if achieved well on clearly demonstrates the representation contains a good amount of information in the domain. The place-types task does not contain too many entities, making it unreliable for neural network training and the reuters task has similar problems. One alternative would be the movies domain, but as identifying the genres of movies is functionally similar to the newsgroups task, we assume that any results found for the newsgroups can easily generalize to any classification task.

To learn the feed-forward networks, we take the highest performing representation found in Table \ref{bg:repsresults} for depth-3 decision trees and use them as input representations.  For the newsgroups, this is the size 100 Doc2Vec space. Then, we set the hidden-layer to be the same size as the input representation and use a non-linear activation function.  The reasoning behind this is twofold: First, we can assume that the representation that performs well on depth-three decision trees contain good interpretable concepts that can be meaningfully adjusted by the network, and second that this allows for easy comparison between the original representation and the representation taken from the hidden layer of the network. These feedforward networks are simple, but do achieve stronger results than the linear SVM. These act as a "baseline network", by which we can see how fundamentally the most simple neural network representation may work. 

The primary difference between the linear SVM results and the results achieved using this neural network is that the neural network used is multi-label, meaning that all of the classification objectives are trained at the same time. Following results showing that simple neural networks can still perform well on text classification without needing a complex architecture  \cite{Lakhotia2018} \cite{Nam2014}, we use a neural network with the following changes: Cross-entropy loss, Adagrad trainer with default parameters, Dropout, Softmax activation on the output layer, ReLu or tanH activation function. We tune the network for the following values: Dropout {0.2, 0.4, 0.6, 0.8}, Hidden activation: {ReLu, Tanh}, epochs {100, 500, 1000} and class_weight {Balanced, Unbalanced}. We then obtain directions from the best performing network.

%For each domain, we test two different network set-ups. The first is one where the hidden layer is the same size as the input layer, and the second is where the hidden layer is smaller than the hidden layer. The assumption is that by constraining the hidden layer size, we can force the network to generalize and find more abstract concepts in the hidden-layer, hopefully gaining some insight into the domain by comparing the input representation and the more general hidden representation.

There are no additional techniques applied to improve the performance of the neural networks.