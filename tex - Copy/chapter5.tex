\chapter{Directions in Neural Networks}\label{chapter5}

\section{Introduction}

% Neural networks are not interpretable

% However, they clearly contain a lot of useful information, as they perform well at tasks

% There is usually a theory that these networks contain more complex relations the deeper they go, or more 'abstract' dimensions.

% Three questions, first: Is it possible to obtain these more complex, accurate to domain knowledge relations and use them as features of an interpretable classifier using the methods in Chapter 3? 

% Second: Can we discover what kind of relationships neural networks are finding by investigating the directions that we will use as features in the interpretable represenation obtained from the network? 

% Finally: In networks which obtain successively more abstract representations, can we create a mapping of specific to abstract concepts?

% The contribution of this chapter is as follows:

% We look at feed-forward networks and auto-encoders. The idea with auto-encoders is that we will be able to map specific terms to more complex ones by reducing the size of the vector space. With feed-forward networks, we want to see how the neural network achieves strong performance on a text classification task, aka how it transforms the representation such that it can achieve strong results on a supervised task.

% The results for auto-encoders was this, feed-forward networks was that

% Summary/whats coming next

\section{Background}

\subsection{Explanations}

% Explanation is a huge field

% The different kinds of explanations

% Where our directions fit-into this paradigm


\subsection{Rules and Neural Networks}

% We plan to use rules in-order to create a kind of mapping of domain knowledge from specific to general using auto-encoders

% There is a history of using rules to understand neural networks, pedagogical etc

% Where our method fits into this


\subsection{Investigation methods}

% There are many visualization/investigation tools

% Where our directions fit-into this paradigm

\subsection{Feed-forward networks}

% Feed-forward networks 

% Basic layout, input/hidden/output, weights

% Activation functions (Relu, Tanh) refer to paper

% Loss (binary_crossentropy), trainer, Adagrad

% Dropout

% Thresholding


\subsection{Auto-encoders}

% Input/output is the same

% Hidden layer constrained in some way so that the representation is more general

% Denoising auto-encoders

% Variations of auto-encoders in recent years. We just use the basic one.

\section{Method}

% Generally, we treat the hidden layers of neural networks as vector spaces.

% We use the same methods as described in chapter 3, obtaining directions and rankings

% Using these directions and rankings, we obtain interpretable representations

\subsection{Auto-encoders}

% Using the interpretable representation, we create a mapping from layer-to-layer using a rule-based classifier

\subsection{Feed-forward networks}

These representations these networks build should differ from those obtained in the unsupervised representation in a few different ways. First, the neural network used is multi-label, meaning that all of the classification objectives are trained at the same time. Second, a non-linear activation function is used, sometimes in multiple layers. Finally, the supervised objective should shape the space more than an unsupervised objective, although only objectives that are relevant to the domain are considered in this case - as we are not interested in a network that ignores a large amount of information, rather we want the network to construct a representation that more accurately represents domain knowledge.

 Following results showing that simple neural networks can still perform well on text classification without needing a complex architecture  \cite{Lakhotia2018} \cite{Nam2014}, we use a neural network with the following changes: Cross-entropy loss, Adagrad trainer with default parameters, Dropout, Softmax activation on the output layer, ReLu or tanH activation function. 

We trained neural networks with non-linear activation functions so that they would  perform better than linear SVM's on  document representations. The assumption behind this is that if the network is able to achieve stronger results on the task than a baseline representation, it must be doing something more than representing the information. In other words, the representation must be better for the task than an unsupervised representation. The goal of this section is to discover why neural networks are able to perform so well, and explain it in terms of directions. 

To learn a baseline feedforward network, we take the highest performing representation found in Table \ref{bg:repsresults} for clusters on depth-3 decision trees and use them as input representations.   For the newsgroups, this is the size 100 Doc2Vec space. Then, we set the hidden-layer to be the same size as the input representation and use a non-linear activation function.  The reasoning behind this is twofold: First, we can assume that the representation that performs well on depth-three decision trees contain good interpretable concepts that can be meaningfully adjusted by the network, and second that this allows for easy comparison between the original representation and the representation taken from the hidden layer of the network. These feedforward networks are simple, but do achieve stronger results than the linear SVM.  

The  differences between the supervised representation and the unsupervised representation are examined here, with the main question being how exactly does a neural network achieve such strong results on the task? Some general ideas or explanations that are given are that through non-linear transformations, the internal representation of the neural network is able to find a more complex or accurate representation of domain knowledge relevant to the task.


We additionally learn a neural network starting from the bag-of-words  with two hidden-layers, the first of size 1000 and the second of size 100. In preliminary experiments, the network with two layers outperformed the single layer network consistently, and this neural network performs much more strongly than the one starting from the Doc2Vec space. 

% We simply take the hidden-layer representation and look at what's going on. 



\section{Parameters}

We use three different domains: Newsgroups, Placetypes and Movie reviews. We choose newsgroups and movie reviews as they have a relevant task that can be used to verify if the network has learned general domain concepts, e.g. in the case of newsgroups the natural categories of the documents and the movie reviews the genres of movies. Placetypes are chosen not because we expect them to perform well with neural networks, but rather because their entities are easy to understand and in-turn explain. In fact, as placetypes has such a low number of entities for its tasks we cannot expect good results with neural networks which typically perform well with a large number of entities.

When obtaining the directions, we set the frequency cut-off to the top 10,000 words and the direction cut-off when classifying to 2000 features. This is arbitrary, as both of these cut-offs need to be tuned for the specific space-type and task in-order to achieve strong results. The reason that these parameters are not tuned in this case is because we are interested in the qualitative nature of the directions, not the performance on the text classification task. 

We investigate the task using the newsgroups as it gives a good example of a representation that contains many different concepts and aspects of domain knowledge, as well as a well defined classification task that if achieved well on clearly demonstrates the representation contains a good amount of information in the domain. The place-types task does not contain too many entities, making it unreliable for neural network training and the reuters task has similar problems. One alternative would be the movies domain, but as identifying the genres of movies is functionally similar to the newsgroups task, we assume that any results found for the newsgroups can easily generalize to any classification task.

We tune the network for the following values: 

epoch = [100, 200, 300]
activation_function = ["relu", "tanh"]
dropout = [0.1, 0.25, 0.5, 0.75]
hidden_layer_size = [1,  2, 3, 4]

When using the bag-of-words as input to the neural network for the newsgroups, we found that it worked well with default parameters.  The parameters for this network are not tuned as the baseline network is, instead an initial hidden layer of size 1000 was chosen following recommendations in \cite{Nam2014a} and the secondary layer was chosen as it has a size similar to the Doc2Vec space and converged quickly. As the method described in chapter \ref{ch3} scales poorly to larger dimensional spaces, this secondary layer was necessary. The batch size is 100 for all experiments, excluding the place-types which used a batch-size of 10 as there were so few entities.

\subsection{Feed-forward Networks}

% We use hyper-parameter optimization to get the best performing neural network

\section{Quantitative Results}

Here, we show the results for the neural networks, the results for the directions obtained from the hidden layers of those networks on depth-3 decision trees, and the results of the clustering of those directions on depth-3 decision trees. The intention of these results is not to achieve state-of-the-art on the task, instead it is to create networks that can perform reasonably well on the task given a simple configuration. This is because the goal of this work is to use directions from neural networks as feature and investigatory tools, and to do that we need to know if the directions obtained from the network are still usable as features.

So, this section is not intending to show that supervised models outperform unsupervised ones, as that is not necessarily the case. Rather, it intends to provide insight into what the directions in the neural networks are using the quantitative results. The main focus is not on if it outperforms other results, but rather how it performs. If it does perform well, then that gives some insight into the neural network, in particular that it must be arranging the space in a better way than an unsupervised representation can to perform well at the associated task. Similarly if it performs poorly, it must be arranging the space poorly. 

These results are designed to give these kind of preliminary insights into what we expect the directions to be in the qualitative analysis in Section \ref{ch5:qual}.

\subsection{Feed-forward networks}

\subsubsection{Neural Network Results}

Compared to newsgroups, the neural network that used a vector space as input performed significantly better than the linear SVM on the unsupervised representation. It obtained an F1-score of 0.574, when the linear SVM's score was 0.532. The assumption follows that if it actually has found more meaningful concepts in the space, then the directions should also perform well on a depth-3 decision tree.

For the place-types domain, the neural network that used a vector space as input performed significantly worse than the SVM, as it scored 0.53 in F1 score compared to 0.622. However, this is not particularly meaningful as there are so few entities. Similarly for the network starting from a bag of words, it scored 0.58 at best.

In the movies domain, the network that used a vector space as input performed better than the original by a large margin. The original scored 0.53 and the network scored 0.57.

Obtaining the directions for spaces transformed by the relu activation function were a lot slower than those of the tanh activation function.


\subsubsection{Directions}


\subsubsection{Clusters}

\section{Qualitative Investigation}\label{ch5:qual}

\subsection{Feed-forward networks}

\subsubsection{Neural Network Results}

Compared to newsgroups, the neural network that used a vector space as input performed significantly better than the linear SVM on the unsupervised representation. It obtained an F1-score of 0.574, when the linear SVM's score was 0.532. The assumption follows that if it actually has found more meaningful concepts in the space, then the directions should also perform well on a depth-3 decision tree.

For the place-types domain, the neural network that used a vector space as input performed significantly worse than the SVM, as it scored 0.53 in F1 score compared to 0.622. However, this is not particularly meaningful as there are so few entities.

In the movies domain, the network that used a vector space as input performed better than the original by a large margin. The original scored 0.53 and the network scored 0.57.

 


\subsubsection{Directions}

Two groups of single word directions and clusters were obtained from three different domains, in each one is from the second layer of  a two-layer neural network starting from a bag-of-words and the other is from the first layer of a one-layer neural network starting from an unsupervised representation. The examples in this section for the clusters are from the clusters that performed the best in depth-3 decision-trees. When investigating single word directions, sometimes it is unclear what they mean. In this case, the most similar single word-directions are found and used to provide additional context.

% Compare the same directions but see how their rankings have changed



% Compare the top directions for NDCG, top for Kappa



\subsubsection{Starting from vector space}

\subsubsection{Starting from bag-of-words}

Obtaining directions from this space takes much longer than normal vector spaces.

\subsection{Auto-encoders}

\section{Conclusion}



 Neural network models that encode spatial relationships in their hidden layers have achieved state-of-the-art in Text Classification by using transfer learning from a pre-trained Language Model \cite{Gong2018}. There have also been neural network models that produce an interpretable representation, for example InfoGan.
 Most state-of-the-art results rely on Vector Space Models. Ideally the method would be able to achieve strong results for simple interpretable classifiers by transforming an existing representation that performs well at the task.
 

\subsection{Chapter 3 Space Types}
\begin{landscape}
	\begin{table}[]
		\begin{tabular}{llllllllll}
			& Genres     &         &         & Keywords  &         &         & Ratings  &         &         \\
			Movies            & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
			Space             & 50 PCA     & 50 MDS  & 100 MDS & 200 PCA   & 200 MDS & 200 MDS & 50 PCA   & 200 PCA & 50 PCA  \\
			Single directions & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A     \\
			&            &         &         &           &         &         &          &         &         \\
			& Newsgroups &         &         & Sentiment &         &         & Reuters  &         &         \\
			Rep               & 200 PCA    & 200 PCA & 100 PCA & PCA 100   & PCA 50  & PCA 50  & 200 PCA  & 200 PCA & 100 PCA \\
			Single dir        & 200 MDS    & 100 D2V & 50 D2V  & D2V 100   & PCA 50  & D2V 100 & N/A      & N/A     & N/A     \\
			&            &         &         &           &         &         &          &         &         \\
			& Foursquare &         &         & OpenCYC   &         &         & Geonames &         &         \\
			Placetypes        & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
			Rep               & MDS 100    & AWV 50  & MDS 200 & AWV 50    & MDS 200 & AWV 50  & MDS 50   & MDS 50  & AWV 200 \\
			Single dir        & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A    
		\end{tabular}\caption{Space-types, clusters have the same as single directions.}
	\end{table}
\end{landscape}




  %This elevates neural networks from simply being representation learners to models that can discover and create new ideas in their representations in-order to solve the task.

Theoretically if we are able to identify features from each layer, then we can also map how they interact with each other. 









%For each domain, we test two different network set-ups. The first is one where the hidden layer is the same size as the input layer, and the second is where the hidden layer is smaller than the hidden layer. The assumption is that by constraining the hidden layer size, we can force the network to generalize and find more abstract concepts in the hidden-layer, hopefully gaining some insight into the domain by comparing the input representation and the more general hidden representation.

There are no additional techniques applied to improve the performance of the neural networks.