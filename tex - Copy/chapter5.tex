\chapter{Directions in Neural Networks}\label{ch4}

\section{Introduction}

The previous chapter showed how in a vector space of entities (e.g. different movies) fine-grained semantic relationships can be identified with directions (e.g. more violent than) and used as features in a document classification task. This Chapter explores how these fine-grained semantic relationships can be used in relationship to neural networks. Two kinds of neural networks are investigated, feed-forward networks and auto-encoders.  Vector space embeddings in the hidden layers of feedforward networks are re-organized into interpretable feauture representations, and they are qualitatively investigated using examples and low-depth decision trees. Interpretable feature-representations are obtained from the layers of auto-encoders, and their use is investigated in the context of learning about relationships between properties in the domain. Specifically, auto-encoders are used to obtain a sequence of increasingly small entity embeddings that model increasingly abstract relationships. After identifying directions that model important properties of entities in each of these vector spaces, symbolic rules are induced that relate specific properties to more general ones. Illustrative examples are provided of the hidden layers and these rules.

%To investigate the denoising auto-encoders, the problem of how to learn symbolic rules from unstructured text documents that describe entities of interest in terms of increasingly abstract properties is considered, e.g. how the property of "Blood" relates to the property of "Violence" for movies. A straightforward approach to obtain a similar result might be to directly learn rules from bag-of-words representations of documents. However, such an approach would typically lead to a large number of rules of little interest, e.g. rules pertaining more to which words are used together rather than capturing capturing meaningful semantic relationships.  %Obtaining meaningful and interpretable symbolic rules is important in fields like exploratory data analysis, or explaining classifier decisions, as they can be interpreted easily by human users.

The approach in this Chapter builds on the method from Chapter \ref{ch3}, finding directions which correspond to interpretable properties in  neural network vector spaces, labelled using adjectives and nouns that appear in the text collection. In order to obtain the symbolic rules, first a series of increasingly general entity embeddings is derived using auto-encoders (see Section \ref{InducingRulesFromEntityEmbeddings}). To induce rules from embeddings, properties derived from those embeddings are linked  together. As an example, below is one of the rules derived with this method, where the first two terms are from one entity embedding, and the final term is from a more abstract entity embedding.
\begin{align}\label{rule1}\texttt{IF Emotions  AND  Journey THEN Adventure}\end{align} 

Generally, there are two goals to achieve for this method for the auto-encoders: First, qualitatively investigate if neural networks can help identify relationships between properties. Second, to investigate the kind of properties encoded in auto-encoders. One potential application of this work in recommendations is in the robustness of explanations using these properties. In the domain of movies, we may have a situation where the synopsis or reviews mention the words ``Emotions" and ``Journey", from which the system could derive that it is probably an ``Adventure" movie and use that term as a part of its supporting explanation e.g. "You liked how this movie was an emotional and a journey, so you may like this adventure movie." Of-course if these properties are indeed salient and can be linked in this way, these properties are also be useful for investigating how deep neural networks represent information.

Additionally, Feed-forward networks are qualitatively investigated using properties and associated entities ranked on those properties obtained by  using the hidden-layer as the starting vector space representation. It is found that interpretable features can indeed be obtained from these neural network representations, and that the rankings of entities on these properties make sense. Additionally, in the case of the newsgroups the predictive performance of a Depth-3 Decision tree that uses the cluster-features as input can outperform the original neural network, and in general the performance of the neural network is retained or slightly decreased in comparison to depth-3 decision trees learned using properties derived from their hidden layers.

In conclusion, feed-forward networks and auto-encoders are investigated using the methods described in Chapter \ref{ch3}. In Section \label{ch5:bg} some neural-network specific methods for interpretability are discussed. Following that in Section \ref{ch5:method}, the detail for how we obtain the hidden layer of the feed-forward networks and how the method from Chapter \ref{ch3} is used to investigate stacked denoising auto-encoders, and how rules are induced that explain the semantic relationships between the properties that are discovered. Next in Section \ref{ch5:results} the results of the qualitative and quantitative investigation of the feed-forward networks is discussed followed by the qualitative results for the auto-encoders. Finally in \ref{ch5:conclusion} conclusions are drawn as to the usefulness of the method and its limitations. In particular, we find that the properties are meaningful for neural networks and provide some insight but it would be useful to improve the properties such that they retain interpretability even when being made abstract by denoising auto-encoders, which is what is covered in Chapter \ref{ch4}.

% Section \ref{InducingRulesFromEntityEmbeddings}  In Section \ref{QualitativeEvaluation} these properties and rules are qualitatively examined, and in Section \ref{RelatedWork} we place our work in the context of related work. Finally, in Section \ref{Conclusions} we provide our conclusions.


% Neural networks are not interpretable

% However, they clearly contain a lot of useful information, as they perform well at tasks

% There is usually a theory that these networks contain more complex relations the deeper they go, or more 'abstract' dimensions.

% Write things that cannot be challenged, everything can and will be used against you

%%%% Aim is to do a comprehensive qualitative analysis of using these interpretable features obtained from neural networks

 %First: Can we verify that the  neural networks are finding meaningful directions for the task by investigating the directions that we will use as features in the interpretable represenation obtained from the network? 

% Three questions, first: Is it possible to obtain these more complex, accurate to domain knowledge relations and use them to produce interpretable representations that achieve similar results to neural networks on tasks even when using a limited number of features in a simple interpretable classifier e..g a low-depth decision tree, not replacing neural network, just using it. NOT transfer learning, cant really justify that (good space that can be used for anything)

% Finally: In networks which obtain successively more abstract representations, can we create a mapping of specific to abstract concepts?

%Conclusion: The rankings are sometimes weird/absurd, would be nice if it was more interpretable, thats where fine-tuning comes in



% The contribution of this chapter is as follows:

% We look at feed-forward networks and auto-encoders. The idea with auto-encoders is that we will be able to map specific terms to more complex ones by reducing the size of the vector space. With feed-forward networks, we want to see how the neural network achieves strong performance on a text classification task, aka how it transforms the representation such that it can achieve strong results on a supervised task.

% The results for auto-encoders was this, feed-forward networks was that

% Summary/whats coming next

\section{Background}\label{ch5:bg}

There are many methods for obtaining an explanation of a black-box after it has been learned using a 'proxy model', a model that approximates the decisions made by the network but is interpretable. There are three general approaches: LIME that produces a linear model that explains the network in terms of input features   \cite{Ribeiro2016}, DeepRed that creates a decision tree that is faithful to the original network \cite{Zilke2016}, and automatic rule extraction \cite{Hailesilassie2016}. LIME differs from the work in this thesis as it focuses on the relationship between input and output, rather than explaining the internal layers of the network. Essentially, the work on this thesis focuses on layers, while their work focuses on explaining a black-box in terms of its input. 

Although the decision tree method seems similar on the surface, it differs from the work here as the decision tree in this work is obtained from meaningful interpretable features from the re-organized neural network space, rather than attempting to produce a proxy model decision tree that is faithful to the neural network. Essentially, the work in this chapter is investigating the re-organization of the neural network into interpretable features, and a decision tree is a simple interpretable model that can help us investigate those interpretable features. A variety of simple interpretable models could be used.

The existing neural network rule extraction algorithms can be categorized as either decompositional, pedagogical or eclectic \cite{Andrews1995a}. Decompositional approaches derive rules by analysing the units of the network, while pedagogical approaches treat the network as a black box, and examine the global relationships between inputs and outputs. Eclectic approaches use elements of both decompositional and pedagogical approaches. Our method could be classified as decompositional, as we re-organize the hidden layer of the neural network. We will now describe some similar approaches and explain how our methods differs.

The algorithm in \cite{Kim2000} is a decompositional approach that applies to a neural network with two hidden layers. It uses hyperplanes based on the weight parameters of the first layer, and then combines them into a decision tree. NeuroLinear \cite{Setiono1997c} is a decompositional approach applied to a neural network with a single hidden layer that discretizes hidden unit activation values and uses a hyperplane rule to represent the relationship between the discretized values and the first layer's weights. HYPINV \cite{Saad2007b} is a pedagogical approach that calculates changes to the input of the network to find hyperplane rules that explain how the network functions. 

The main difference in our work is that our method induces rules from properties derived from the re-organized layers of a network, rather than learning rules that describe the relationships between units in the network itself. Additionally, the focus is on a qualitative investigation of the network and its potential to learn increasingly general entity embeddings from hidden representations rather than tuning network parameters such that weights directly relate to good rules. 

Another recent topic that relates to our work is improving neural networks and entity embeddings using symbolic rules \cite{Hu2016}. In \cite{Rocktaschel2015} a combination of first-order logic formulae and matrix factorization is used to capture semantic relationships between concepts that were not in the original text. This results in relations that are able to generalize well from input data.  

This is essentially the opposite of the task we consider in this paper: using embeddings to learn better rules. The rules that we derive are not intended to explain how the network functions but rather to describe the semantic relationships that hold in the considered domain. In other words, our aim is to investigate the use of neural network representations in the hidden layer as a tool for learning logical domain theories, where the focus is on producing rules that capture meaningful semantic relationships.

%There are two typical methods for making neural networks interpretable, explaining the neural network after it has been learned and modifying the way the neural network is learned such that it is interpretable. However, our method does not easily fit into either of these categories, as we do not explain the neural network in terms of its predictions or making it so that it learns an additional interpretable objective.

%The method that we introduce is a post-processing method on the hidden layer of a neural network to re-organize it's internal semantic structure into interpretable features. This satisfies a number of important criteria in the literature. It provides interpretability of the models representation, completeness of that representation, doesn't rely on input vectors instead using information directly from the model and is not limited to a model or a classifier. 

%One relevant work in the image domain is TCAV which separates user-specified entities using linear regression and obtains a direction using the weights for that concept. 

%As the method can be used to post-process vector spaces that encode semantic directions, it can be applied to many existing models without the need for re-training or adopting a new architecture. 

%Interpretability and completeness

%Interpretability of information inside the network versus predictions


%Post-hoc versus baking it into the model

%Creating networks that are easier to explain:  Disentangled representations, GAN, PCA< Independent Component Analytsis, Nonnegative matrix factorization, Variational autoencoding, Beta-VAE, InfoGAN, Construction of graphs, decision trees

%Deep networks that are trained to make their own explanations 

%Linear proxy models

%Decision trees from neural networks  (super deep and bad)

%Rule extraction from neural networks

%Concept Activation Vectors "are a framework for interpretation of a neural nets representations by identifying and probing driections that align with human interpretable concepts"

%Persuasive versus transparent

%Saliency maps "One of the most popular approaches in interpreting NN is saliency methods (24; 22; 25; 8; 5).
%These techniques seek to identify regions of the input most relevant to the classification output by
%the network. Qualitatively, these methods often successfully label regions of the input which seem
%semantically relevant to the classification.
%" - copy pasted MIGHT BE WRONG

%"Even a truthful explanation may be misleading if it is only locally truthful (20). For example, since
%the importance of features only needs to be truthful in the vicinity of the data point of interest, there
%is no guarantee that the method will not generate two completely conflicting explanations. These
%inconsistencies may result in decreased user trust at a minimum. On the other hand, making a
%globally truthful explanation may be difficult as the networks decision boundaries may be complex.
%TCAV produces globally explanations, and uses model’s output to generate explanations to maintain
%consistency between explanations and the model’s reasoning.
%" - copy pasted

%" there is evidence that they work by gradually disentangling concepts of interest, layer by
%layer (2; 3)."

%"although feedforward networks learn highly nonlinear
%functions there is evidence that they work by gradually disentangling concepts of interest, layer by
%layer (2; 3). It has also been shown that representations are not necessarily contained in individual
%%neurons but more generally in linear combinations of neurons (19; 27). Thus the space of neuron
%activations in layers of a neural network may have a meaningful global linear structure. Furthermore,
%if such a structure exists, we can uncover it by training a linear classifier mapping the representation
%in a single layer to a human selected set of concepts."

%[2] Understanding intermediate layers using linear classifier
%probes.

%[3] Network dissection: Quantifying interpretability of deep visual representations

%[19] Svcca: Singular
%vector canonical correlation analysis for deep understanding and improvement

%[27] Intriguing properties of neural networks.

%\subsection{Explanations}

% Explanation is a huge field

% The different kinds of explanations

% Where our directions fit-into this paradigm


%\subsection{Rules and Neural Networks}

% We plan to use rules in-order to create a kind of mapping of domain knowledge from specific to general using auto-encoders

% There is a history of using rules to understand neural networks, pedagogical etc

% Where our method fits into this


%\subsection{Investigation methods}

% There are many visualization/investigation tools

% Where our directions fit-into this paradigm

%\subsection{Feed-forward networks}

% Feed-forward networks 

% Basic layout, input/hidden/output, weights

% Activation functions (Relu, Tanh) refer to paper

% Loss (binary_crossentropy), trainer, Adagrad

% Dropout

% Thresholding


%\subsection{Auto-encoders}

% Input/output is the same

% Hidden layer constrained in some way so that the representation is more general

% Denoising auto-encoders

% Variations of auto-encoders in recent years. We just use the basic one.

\section{Method}\label{ch5:method}

A bag-of-words representation when used as input to a decision-tree classifies using many individual words, for example the class of "comedy" would be have a node for each feature, and those features would be words like "laughed" "witty" "charming" and so on. Because of this, a larger tree is required to classify well. 

Neural networks with large hidden layers can also be expected to have similar behaviour. If the hidden layer is larger, then the representation will contain directions that correspond to more granular concepts. For example, to classify comedy there may be directions like "gags, slapstick, gag", "witty, charming, wit", "comedies, comedy" "eccentric", simple concepts that directly relate to the class.

As the layers get smaller, the concepts become more condensed and general. To see an example of this, see \ref{ch5:dtng} where example clustered features from the best performing decision trees are shown. The method in this section for the feedforward network is essentially to use a reasonably sized hidden layer that will result in good properties, and then take that hidden layer representation and investigate it qualitatively. In the case of the auto-encoders, instead successively smaller, more abstract, less disentangled representations are obtained and they and the relationships between them are investigated.

% Generally, we treat the hidden layers of neural networks as vector spaces.

% We use the same methods as described in chapter 3, obtaining directions and rankings

% Using these directions and rankings, we obtain interpretable representations

\subsection{Inducing Rules from Auto-Encoder Entity Embeddings}\label{InducingRulesFromEntityEmbeddings}

In this section, we explain how we obtain a series of increasingly general entity embeddings, and how we can learn symbolic rules that link properties from subsequent spaces together. 

To construct more general embeddings from the initial embedding provided by the MDS method, we use stacked denoising auto-encoders \cite{Vincent2008a}. Standard auto-encoders are composed of an ``encoder" that maps the input representation into a hidden layer, and a ``decoder" that aims to recreate the input from the hidden layer. Auto-encoders are normally trained using an objective function that minimizes information loss (e.g. Mean Squared Error) between the input and output layer~\cite{Bengio2009}. The task of recreating the input is made non-trivial by constraining the size of the hidden layer to be smaller than the input layer, forcing the information to be represented using fewer dimensions, or in denoising auto-encoders by corrupting the input with random noise, forcing the auto-encoder to use more general commonalities between the input features. By repeatedly  using the hidden layer as input to another auto-encoder, we can obtain increasingly general representations. To obtain the entity representations from our auto-encoders, we use the activations of the neurons in a hidden layer as the coordinates of entities in a new vector space. 

The main novelty of our approach is that we characterize the salient properties (i.e. clusters of directions) modelled in one space in terms of salient properties that are modelled in another space. Specifically, we use the off-the-shelf rule learner JRip [7] to predict which entities will be highly ranked, according to a given cluster direction, using as features the rankings induced by the clusters of the preceding space. To improve the readability of the resulting rules, rather than using the precise ranks as input, we aggregate the ranks by percentile, i.e $1\%, 2\%, ..., 100\%$, where an entity has a $1\%$ label if it is among the $1\%$ highest ranked entities, for a given cluster direction. For the class labels, we define a movie as a positive instance if it is among the highest ranked entities (e.g. top 2\%) of the considered cluster direction. Using the input features of each layer and the class labels  from the subsequent layer, these rules can be used to explain the semantic relationships between properties modelled by different vector spaces. We note that one drawback of discretizing continuous attributes is that the accuracy of the rules extracted from the network may decrease \cite{Setiono2008a}.  However, in our setting, interpretability is more important than accuracy, as we do not aim to use these rules for making predictions, but use them only for generating explanations and getting insight into data.

% Using the interpretable representation, we create a mapping from layer-to-layer using a rule-based classifier

\subsection{Feed-forward networks}

We train two neural networks with non-linear activation functions. The first is NNET-U, a neural network with a single hidden layer of a size learned using hyper-parameter optimization that uses an already learned unsupervised representation as input. The second is NNET-B, a neural network that starts with the PPMI BOW as input. The output layer is the of size $C_n$ where $C_n$ is the number of classes. Each network is trained on one task from each domain. As the goal is a qualitative analysis, these tasks were chosen as it is clear what properties will correspond to them. To obtain a representation from this neural network, we simply take the entiies as they are represented by the trained model in the hidden layer.

The representations these networks build should differ from those obtained in the unsupervised representation in a few different ways. First, the neural network used is multi-label, meaning that all of the classification objectives are trained at the same time. Second, a non-linear activation function is used, sometimes in multiple layers. Finally, the supervised objective should shape the space more than an unsupervised objective, although only objectives that are relevant to the domain are considered in this case - as we are not interested in a network that ignores a large amount of information, rather we want the network to construct a representation that more accurately represents domain knowledge.

 Following results showing that simple neural networks can still perform well on text classification without needing a complex architecture  \cite{Lakhotia2018} \cite{Nam2014}, we use a neural network with the following parameters: Cross-entropy loss, Adagrad trainer with default parameters, Dropout, Sigmoid activation on the output layer. Instead of Relu which we found did not obtain good directions, we used the tanh activation function. Dropout is a regularizer that drops out units during training and makes the representation more general and robust. Adagrad is a trainer that adjusts the learning rate during training. 

 %The assumption behind this is that if the network is able to achieve stronger results on the task than a baseline representation, it must be doing something more than representing the information. In other words, the representation must be better for the task than an unsupervised representation. The goal of this section is to discover why neural networks are able to perform so well, and explain it in terms of directions. 


%The  differences between the supervised representation and the unsupervised representation are examined here, with the main question being how exactly does a neural network achieve such strong results on the task? Some general ideas or explanations that are given are that through non-linear transformations, the internal representation of the neural network is able to find a more complex or accurate representation of domain knowledge relevant to the task.


We additionally learn a neural network starting from the bag-of-words  with two hidden-layers, the first of size 1000 and the second of size 100. In preliminary experiments, the network with two layers outperformed the single layer network consistently, and in the case of newsgroups performed  more strongly than an SVM on the unsupervised representation. When taking a representation from this neural network, we take this final hidden layer of size 100. This is the representation used in the qualitative examples.

% We simply take the hidden-layer representation and look at what's going on. 



\section{Investigating Feedforward Neural Networks}

\subsection{Parameters}

We use three different domains: Newsgroups, Placetypes and Movie reviews. We choose newsgroups and movie reviews as they have a relevant task that can be used to verify if the network has learned general domain concepts, e.g. in the case of newsgroups the natural categories of the documents and the movie reviews the genres of movies. Placetypes are chosen not because we expect them to perform well with neural networks, but rather because their entities are easy to understand and in-turn explain. In fact, as placetypes has such a low number of entities for its tasks we cannot expect good results with neural networks which typically perform well with a large number of entities.

When obtaining the directions, we set the frequency cut-off to the top 10,000 words and the direction cut-off when classifying to 2000 features. This is arbitrary, as both of these cut-offs need to be tuned for the specific space-type and task in-order to achieve strong results. The reason that these parameters are not tuned in this case is because we are interested in the qualitative nature of the directions, not the performance on the text classification task. Additionally, as Normalized Discounted Cumulative Gain (See Section \ref{ch3:NDCG}) was shown to perform well for a variety of embedding types and domains, it was the only scoring method used.

%We investigate the task using the newsgroups as it gives a good example of a representation that contains many different concepts and aspects of domain knowledge, as well as a well defined classification task that if achieved well on clearly demonstrates the representation contains a good amount of information in the domain. The place-types task does not contain too many entities, making it unreliable for neural network training and the reuters task has similar problems. One alternative would be the movies domain, but as identifying the genres of movies is functionally similar to the newsgroups task, we assume that any results found for the newsgroups can easily generalize to any classification task.

To learn a baseline feedforward network, the highest performing representation found in Table \ref{bg:repsresults} for cluster-features is used as input to depth-3 decision trees and use those representations as input to the neural network.   For the newsgroups, this is the size 100 Doc2Vec space. Then, we set the hidden-layer to be the same size as the input representation and use the "tanh" activation function.  The reasoning behind this is twofold: First, we can assume that the representation that performs well on depth-three decision trees contain good interpretable concepts that can be meaningfully adjusted by the network, and second that this allows for easy comparison between the original representation and the representation taken from the hidden layer of the network, as the highest performing directions can be compared.

The network is hyper-parameter tuned with the following values: 

\begin{itemize}
	\item ${epoch} = [100, 200, 300]$
	\item ${dropout} = [0.1, 0.25, 0.5, 0.75]$
	\item ${hidden_layer_size} =[1,  2, 3, 4]$
\end{itemize}

The batch size is 100 for all experiments, excluding the place-types which used a batch-size of 10 as there were so few entities. When using the bag-of-words as input to the neural network we followed "default" parameters as recommended by \cite{Nam2014a} with an initial hidden layer of size 1000. However, a secondary hidden layer of size 100 is added for this work, chosen as it is close to the size of the original unsupervised representations. 

For the linear SVM and the Decision Trees of depth-3, hyper-parameter optimization for these models was used with the same parameters as described in Section \ref{ch3:hyperparam}.


\subsection{Quantitative Results for Feedforward Network}


The results for place-types were obtained as their entities can be easily understood without expert knowledge (e.g. "tree" and "cliff" rather than "The Shining" in the case of the movies). It is not unexpected that they performed poorly in the neural networks, as they have a very low number of entities (391) and it is also an unbalanced task. 

\begin{table}[]
\begin{tabular}{llll}
	& Movies      & Placetypes  & Newsgroups  \\
	Unsupervised (Linear SVM)   & 0.532082959 & 0.630040432 & 0.628171051 \\
	NNET-U                      & 0.559090895 & 0.563313632 & 0.627577055 \\
	NNET-B                      & 0.435345945 & 0.597008771 & 0.673618458 \\
	Directions Unsupervised DT3 & 0.493072458 & 0.508163669 & 0.536686468 \\
	Directions NNET-U DT3       & 0.505155338 & 0.460648739 & 0.50988517  \\
	Directions NNET-B DT3       & 0.501545907 & 0.521526974 & 0.664793407 \\
	Clusters Unsupervised DT3   & 0.506096231 & 0.544001448 & 0.513367341 \\
	Clusters NNET-U DT3         & 0.472297312 & 0.541887565 & 0.478077988 \\
	Clusters NNET-B DT3         & 0.501510396 & 0.5967867   & 0.682928611
\end{tabular}\caption{F1-scores for each embedding type and domain}
\end{table}

Here, we show quantitative results for:

\begin{itemize}
	\item The best performing linear SVM result on an unsupervised representation from Chapter \ref{ch3}.
	\item The predictions of the neural networks. 
	\item The results for the directions obtained from the hidden layers of the neural networks on depth-3 decision trees.
	\item The results of the clustering of those directions on depth-3 decision trees.
\end{itemize}

The intention of the results in this section is not to achieve state-of-the-art on the task, rather it is to show that depth-3 decision trees that use the directions obtained from a simple neural network  perform reasonably well on the task or sometimes better. As in Chapter \ref{ch3}, this validates that these features are indeed important for the domain.  The idea that interpretable features can be obtained from the neural networks using the method in Chapter \ref{ch3} is validated and not much predictive power is lost over the original neural network in some cases. As these features are also able to perform well in the simple classifier, it validates the use of Decision Trees in the qualitative investigation in Section \ref{ch5:qual}. 

These results are also preliminary insights that inform what we expect the directions to be like in in Section \ref{ch5:qual}. In the case of the movies we know that the neural network with the unsupervised embedding as input (NNET-U) performed better than the linear SVM with the unsupervised embedding as input, which means that  properties which are relevant to the task are likely better separated in the embedding. Similarly, as the neural network with a bag-of-words input (NNET-B) performed poorly, we can expect that properties that are separable in this representation will be less relevant to the task. As all of these representations performed similarly in the depth-3 decision trees, we can expect that they have each been able to identify the key properties that are particularly relevant to the class, e.g. one that corresponds to Horror, another to Comedy, and so on. So likely  all embeddings  share important properties relevant to the task, as well as introducing other properties. 

The reason that NNET-U performed well in the neural network but poorly in the directions is likely because it did not disentangle properties such that those relevant to the class were grouped together. This means that although properties like "comedies" are more separable in the space, as they are not similar to the cluster-direction for "comedy", instead forming their own cluster-direction, the "comedies" cluster-feature is not used in the depth-3 decision tree, making it meaningless for the interpretable classifier but meaningful for the neural network. We find evidence to support this idea in Section \ref{ch5:qual}.

%This section  provides insight into the directions contained within the neural network hidden layers.  If the neural network performs better than a linear svm with the unsupervised representation as input, it gives the insight that it must contain more relevant or refined directions than the unsupervised representation. Similarly if it performs poorly, it must be arranging the entities poorly. 

In the case of movies the neural networks perform slightly better than the unsupervised representation when the unsupervised representation is used as input (NNET-U) but much worse when the PPMI bag-of-words is used as input (NNET-B). This is likely because the vocabulary for the movies is large (limited to 100,000 unique terms for movies from the original vocabulary size of 551,080 versus 51,064 for newsgroups, see Section \ref{ch2.5:technical}) resulting in overfitting.  It is interesting to see that for the place-types the single-term features and cluster-features for NNET-B outperformed the other embeddings, and for both the movies and the place-types the difference is minimal between the highest-scoring cluster-feature results on a depth-3 decision tree and the linear SVM on the unsupervised representation. This can lead us to assume that the properties obtained from the neural network embeddings are indeed meaningful.

NNET-B for the newsgroups outperformed the linear SVM with an unsupervised embedding by a large margin, and continued to do so for both the single-term features and the cluster-features. Interestingly, the cluster-features in a depth-3 decision tree also performed at the same level as the neural network. This can be understood to mean that the properties and subsequent cluster-features obtained from the NNET-B embedding must be particularly relevant to the task and also disentangled into concepts that directly correspond to the classes. Similarly the cluster-features for the place-types also performed at the same level as the neural network, but neither of them performed better than the Linear SVM. This indicates that when learning to classify using a neural network starting from a bag-of-words, properties relevant to the task with good rankings of entities are identified in the embedding before the output layer. 

In conclusion, disentangled properties that can be used to classify entities can be identified in the hidden layer before the output layer of neural networks, and in the case of newsgroups the interpretable features that correspond to these properties perform equivalent or better than the neural network. Some assumptions are made about what kind-of properties these embeddings represent, in particular that in neural networks that perform well embeddings will likely contain unique properties and also improved rankings for properties that are particularly relevant to the task. Additionally, for the NNET-B embedding in the newsgroups domain the properties will be disentangled.

%\subsubsection{Neural Network Results}

%For the newsgroups, the neural network with bow input performed significantly better than the one with a vector space as input. 

%Compared to newsgroups, the neural network that used a vector space as input performed significantly better than the linear SVM on the unsupervised representation. It obtained an F1-score of 0.574, when the linear SVM's score was 0.532. The assumption follows that if it actually has found more meaningful properties in the space, then the directions should also perform well on a depth-3 decision tree.

%For the place-types domain, the neural network that used a vector space as input performed significantly worse than the SVM, as it scored 0.53 in F1 score compared to 0.622. However, this is not particularly meaningful as there are so few entities.

\subsubsection{Direction results}

As expected, the score of a depth-3  decision tree classifier when using the directions obtained from the neural network are close to the performance of the neural network. 

There are a few reasons that a neural network representation would perform better than an unsupervised one, which are explained here with examples from the movies domain:

\begin{itemize}
	\item Noise is no longer encoded spatially, making it easier to obtain meaningful features. (e.g. metadata like e-mail list names "listinfo" "robomod" are no longer highly separable, meaning that the only features found are meaningful)
	\item New features are encoded in the representation that contribute to the task (e.g. "Blood" may be in the unsupervised representation, corresponding to if the movie was funny, but in the neural network representation a feature for "Zombie" was introduced which helps classify if the movie is a horror)
	\item The properties that are relevant to the class are clustered together, resulting in a disentangled representation where features correspond more closely to the task, (e.g. The cluster-direction for "Blood, Gore" and the cluster-direction for "Scary, Terrifying" are instead clustered together to better represent the Horror genre in a single cluster-direction "Blood, Gore, Scary, Terrifying")
	\item The rankings for properties that are also encoded in the unsupervised representation are better (e.g. The "Comedy" direction more correctly ranks movies based on how funny they are)
\end{itemize}

If the directions perform well and use similar features it is reasonable to assume that the rankings are better than in the unsupervised representation. The next section qualitatively investigates directions in these embeddings.

\subsection{Qualitative Investigation of Feedforward Networks}\label{ch5:qual}

The qualitative section  focuses on qualitatively investigating feed-forward neural network embeddings using the interpretable features. There are three main goals of this qualitative investigation section:

\begin{itemize}
	\item Qualitatively investigate the interpretable feature representation obtained from the neural network embeddings.
	\item Qualitatively investigate the differences between between the embeddings.
	\item Qualitatively investigate the use of decision trees learned from the interpretable feature representation.
\end{itemize}

The first goal is simple to achieve: If the interpretable features obtained from the neural network makes intuitive sense, then it can be qualitatively investigating  these features. The second goal is achieved by comparing the interpretable features obtained from the embeddings to the interpretable features obtained from the unsupervised space. Finally,   decision trees are investigated by examining the properties that are used in them and their associated rankings. 

When showing results for single directions, the words that these directions are labelled with e.g. "Blood" will be accompanied by the terms for two of its most similar directions e.g. "Blood (Gore, Horror)". This is to provide context for the word such that it can be understood more easily. Unlike the clustered features, the direction is not changed. Three embeddings are used in this section for each domain: The unsupervised embedding, the neural network embedding that used the vector space as input, and the neural network embedding that used the bag-of-words as input. 

Terms are scored using NDCG \ref{ch3:NDCG} as this has good results for a variety of tasks and spaces. The frequency threshold was set to the top 10,000 terms and the score threshold was set to the top 2,000 scoring terms in NDCG. 

\subsubsection{Top Scoring Terms}

In this section, we look at the top-scoring terms for three different domains. These are intended to give a general impression of what these terms and their associated similar directions are like, as well as get some initial insights into what the embeddings are representing.

In Table \ref{ch5:topscores} the top NDCG scoring terms are listed for each domain and embedding type. One immediate observation is that the unsupervised embedding and the NNET with the unsupervised embedding as input ("NNET-U") have similar top scoring terms, e.g. "listinfo" "mailing". As expected, the directions including the noise in the original unsupervised embedding were not transformed significantly by the neural network. In the case of the neural network using BOW as input (NNET-B) however, these noisy terms are not present. This brings up the question, what exactly has the neural network learned such that it performs higher in F1-score in the case of the unsupervised embedding as input? In-order to further investigate this, terms that were unique to each representation were obtained. This was obtained using a basic procedure where if terms did not occur in either of the top 2,000 highest scoring terms for the other embeddings then they were considered not unique.


\begin{table}[]
	\scriptsize
	\begin{tabular}{lll}
		Movies                                   &                                                   &                                                          \\
		Unsupervised Embedding                                  & NNET BOW Input                                       & NNET Embedding Input                                             \\
		listinfo (mailman, rec)                  & horror (pacing, dialog)                           & listinfo (mailman, rec)                                  \\
		robomod (mailman, rec)                   & westerns (russian, digitally)                     & robomod (mailman, rec)                                   \\
		mailing (mailman, listinfo)              & documentary (joke, ultimate)                      & mailing (mailman, listinfo)                              \\
		noir (fatale, femme)                     & comedies (actress, entertain)                     & noir (fatale, femme)                                     \\
		martial (kung, fight)                    & hilarious (disappointment, dozen)                 & horror (scary, horrific)                                 \\
		gay (homosexual, homosexuality)          & laughs (woods, rights)                            & martial (kung, arts)                                     \\
		horror (scary, scares)                   & sci (equivalent, intent)                          & gay (homosexual, homosexuality)                          \\
		prison (jail, prisoners)                 & adults (grown, rushed)                            & prison (jail, convicted)                                 \\
		arts (rec, listinfo)                     & songs (battle, speak)                             & animation (animated, cartoon)                            \\
		musicals (musical, singing)              & war (military, james)                             & arts (rec, listinfo)                                     \\
		mailman (listinfo, robomod)              & western (don, stick)                              & musicals (musical, numbers)                              \\
		Placetypes                               &                                                   &                                                          \\
		southcoast (filters, reala)              & leafs (botanic, f100)                             & interchange (midday, elevated)                           \\
		interchange (underpass, hk)              & aerialphotography (beaver, kiteaerialphotography) & canonrebel (controluce, pigeons)                         \\
		canonrebel (1855mm, nikond300s)          & irvine (jay, meet)                                & rave (dj, erin)                                          \\
		statua (stern, 1st)                      & centralcoast (published, aloha)                   & winnipeg (konicaminolta, twincities)                     \\
		municipal (citizen, farbe)               & swell (fin, polar)                                & windmills (goldenhour, puffy)                            \\
		madrid (noir, df)                        & pacificocean (puerto, waves)                      & statua (palacio, estatua)                                \\
		reizen (seventies, canonef2470mmf28lusm) & trunks (birch, pair)                              & reizen (1022mm, t3)                                      \\
		commuter (underpass, muni)               & southflorida (fla, hawaiian)                      & f456 (300mm, a550)                                       \\
		crime (illegal, violence)                & lapland (alberi, topv333)                         & gibraltar (iow, upon)                                    \\
		leafs (iris, cyan)                       & sunbathing (underwater, relaxed)                  & song (singing, juni)                                     \\

		Newsgroups                               &                                                   &                                                          \\
		solid (design, single)                   & vol (vast, foot)                                  & temperature (discoveries, surveys)                       \\
		struggle (grew, landed)                  & volt (amplifier, soldered)                        & testify (ali, concentrated)                              \\
		spreading (pursued, tolerant)            & voltage (cautious, scanned)                       & solid (company, sides)                                   \\
		salt (drinking, combinations)            & volume (fairly, distinguish)                      & tea (tech, buck)                                         \\
		random (attacks, described)              & volumes (expressed, scenario)                     & widely (recently, 1982)                                  \\
		widely (developed, tend)                 & voluntary (demonstrating, explore)                & denial (judaism, kurds)                                  \\
		temperature (layers, consumption)        & volvo (aftermarket, horsepower)                   & detecting (skeptical, signals)                           \\
		viable (motivation, emphasized)          & tigers (carter, brady)                            & dick (quoted, seen)                                      \\
		vol (published, journal)                 & postage (straightforward, engaged)                & pitches (screw, headers)                                 \\
		volt (garage, voltage)                   & povray (animations, pbmplus)                      & random (dropped, atheism)                                \\
		voltage (amps, resistor)                 & starters (worthwhile, fame)                       & salt (stations, alike)                                   \\
		volume (hundred, frequently)             & secular (descendants, fundamentalists)            & vol (contents, students)                                 \\
		volumes (historians, turkish)            & ring (increasing, behind)                         & volt (disagreement, clouds)                              \\
		voluntary (heterosexual, posed)          & utterly (des, retain)                             & voltage (criteria, disclosed)                            \\
		volvo (saab, chrysler)                   & occurring (communicate, contacted)                & volume (although, quickly)                               \\
		tea (nagornokarabakh, mothers)           & single (entire, prove)                            & volumes (raster, josephus)                               \\
		quick (careful, usual)                   & oxygen (speeding, extraordinary)                  & voluntary (transmitted, satan)                           \\
		utterly (violates, sinners)              & widgets (shells, experimentation)                 & volvo (outlet, 302)                                     
	\end{tabular}\caption{NDCG Top-Scoring directions}\label{ch5:topscores}
\end{table}

\subsubsection{Common and Unique Terms to Each Embedding}\label{ch5:commonunique}

This section evaluates terms from the movies domain in terms of if they are unique to the embedding or common to all there embeddings. The intention is to see if the embeddings have introduced any new properties to the top 2,000.

A term  is  common if the term occurs in the 2,000 top scoring terms of both the other two embeddings. Interestingly, there were only 14 unique terms for the unsupervised embedding, and only 19 for the neural network that used the unsupervised embedding as input. Meanwhile, there were 770 unique terms for the neural network with bag-of-words as input. This shows that representing new directions was not the reason for superior performance for NNET-U. Rather, it is likely that terms that are relevant to the genre that were already in the top 2,000 top-scoring terms are given higher performance. In-order to qualitatively investigate further what has changed between the embeddings, the differences between the scores of the terms was obtained.

% Most difference
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\begin{table}[]
	\scriptsize
	\begin{tabular}{L{3.8cm}L{3.8cm}L{3.8cm}L{3.8cm}}
Movies                               &                                             &                                        &                                            \\
Unique Unsupervised                  & Unique NNET-B                           & Unique NNET-U                & Common                                     \\
immigrants (immigrant, america)      & unlike (terrific, efforts)                  & carry (recent, hoping)                 & noir (fatale, femme)                       \\
flashback (flashbacks, present)      & efforts (detail, directors)                 & federal (fbi, agents)                  & martial (kung, fight)                      \\
bloated (spectacle, overlong)        & impression (throw, truth)                   & possessed (demonic, forces)            & horror (scary, scares)                     \\
chapter (previous, installment)      & viewed (catch, escape)                      & faustus (geocities, html)              & arts (rec, robomod)                        \\
rebel (rebellious, freedom)          & suggest (wondering, trip)                   & spike (african, lees)                  & musicals (musical, singing)                \\
assault (attack, violent)            & focuses (sadly, thoughts)                   & dashing (handsome, excitement)         & hilarious (funniest, laughing)             \\
client (lawyer, attorney)            & terms (theatre, marvelous)                  & wartime (wwii, bombing)                & westerns (western, cowboy)                 \\
predecessor (sequel, sequels)        & suppose (heres, greatly)                    & phantom (sees, opera)                  & jokes (laughs, joke)                       \\
competitive (competition, sport)     & credit (clever, sequence)                   & theories (theory, conspiracy)          & romantic (romance, romances)               \\
jealous (attraction, crush)          & exception (beat, passed)                    & robots (sci, princess)                 & animation (animated, cartoon)              \\
betrayal (loyalty, affair)           & heres (suppose, negative)                   & abused (abuse, abusive)                & western (westerns, west)                   \\
artsy (pretentious, artistic)        & fare (twenty, concerned)                    & fiance (fianc, engaged)                & songs (song, lyrics)                       \\
hotel (manager, vacation)            & unable (convey, accept)                     & hatred (hate, racism)                  & comedies (comedic, laughs)                 \\
stereotypical (stereotypes, clich)   & deliver (depth, limited)                    & mysteries (mystery, clues)             & war (soldiers, military)                   \\                                   
	\end{tabular}\caption{Terms from three different document embeddings, the unsupervised embedding, the neural network that used a bag-of-words as ioput and the neural network that used the unsupervised vector space as input. Arranged by NDCG, from highest to lowest.}
\end{table}


\subsubsection{The Difference Between The Term Scores In The Embeddings}\label{ch5:diffsection}

In this section, the differences between the scores of terms in the movies domain are qualitatively analysed. The intention is to determine what some differences are between the unsupervised embeddings and the neural network embeddings, and evaluate previous assumptions regarding the behaviour of the network.

These results are for all term directions obtained from each embedding, so all 10,000 terms. In the first column of  Table \ref{ch5:diff} the terms that the unsupervised representation has scored higher than NNET-U are not clearly related to the task of genres. For example, "yup", "wright", "wimpy" and "zoom" which are the top four highest score differences do not seem related to the task at all. Meanwhile, the terms that the NNET-U embedding has gained in score are clearly relevant to the task, e.g. "animation", "adventure", "comedies". Although, it has also increased the separability of some noise e.g. the term "listinfo", which is a problem already identified when using the unsupervised representation as input to the neural network. This gives us some indication of what the neural network is doing to solve the task. This behaviour of the neural network decreasing separability of noisy terms and increasing separability of terms relevant to the task could contribute to the difference in score where the neural network outperformed a linear SVM on the unsupervised embedding. 

Meanwhgile NNET-B seems to have elevated terms that are not relevant to the task. As the neural network seems to have overfit due to the large vocabulary, it has made terms separable that are not immediately meaningful, e.g. "attends" "adds" "foray". However, as previously mentioned it did not not have some of the same noise as the unsupervised embedding. However, it seems to have added its own noise. 
%Common terms: relevant to the class
%Unique terms: unique noise, vector space nnet performed better while bow nnet performed worse. should see something that reflects that
%Common terms btwn unsupervised and vector: common noise
%Most distance: 

%Generally, we see a similar trend in other domains. If noise is in the original space, using it as input to a neural network will not remove that noise. In that case, what changes between the original vector space and the neural network? To answer this question, the terms arranged by the difference between them is listed in table 

\begin{landscape}
\begin{table}[]
	\scriptsize
	\centering
	\begin{tabular}{llll}
		Unsupervised - NNET-U          & Unsupervised - NNET-B              & NNET-U - Unsupervised                 & NNET-B - Unsupervised                \\
		zoom (cuts, editing)           & zoom (cuts, editing)               & allens (woody, allen)                 & atmosphere (mood, atmospheric)       \\
		wimpy (villian, instance)      & zombies (zombie, undead)           & animation (animated, cartoon)         & biased (agree, showed)               \\
		wright (forrest, buyer)        & williams (jim, includes)           & adults (children, adult)              & abomination (insult, horrible)       \\
		yards (shoot, yard)            & willingly (explicitly, risk)       & arts (rec, listinfo)                  & absent (absence, previous)           \\
		yup (junk, downright)          & wrecked (crashes, crashed)         & allen (woody, allens)                 & car (cars, driving)                  \\
		watchers (underlying, holding) & welles (orson, kane)               & adaptation (adaptations, adapted)     & abandons (leaving, decides)          \\
		valiant (merit, admirable)     & writers (write, finish)            & animated (animation, cartoon)         & attends (attending, attend)          \\
		wee (pee, pick)                & willingness (conventional, goal)   & aboard (ship, board)                  & adds (added, addition)               \\
		winding (road, wheel)          & wars (enemy, war)                  & adventure (adventures, adventurous)   & broader (greater, balanced)          \\
		yea (hey, wanna)               & winner (winning, won)              & australian (australia, sydney)        & artwork (painting, animation)        \\
		wax (inclined, usage)          & walt (disneys, disney)             & addiction (addict, addicted)          & cheerfully (gleefully, bursts)       \\
		woke (yeah, wake)              & unstable (psychotic, mentally)     & berardinelli (employers, distributor) & foray (marks, venture)               \\
		wrecked (crashes, crashed)     & walking (walk, walks)              & abuse (abused, abusive)               & aboard (ship, board)                 \\
		winded (significant, deliver)  & wrath (gods, angry)                & actress (actresses, lady)             & absurd (ridiculous, absurdity)       \\
		wine (bottle, drink)           & widely (recently, equally)         & apes (monkey, monkeys)                & civilization (civilized, survival)   \\
		wrapping (product, needed)     & wondering (figured, wanting)       & comedies (comedic, laughs)            & abrupt (lacked, ended)               \\
		yahoo (faustus, faust)         & yawn (dull, tedious)               & listinfo (mailman, rec)               & active (helped, allowed)             \\
		whip (beat, kick)              & zeal (enthusiasm, celluloid)       & africa (african, countries)           & admiration (respect, remarkable)     \\
		zeal (enthusiasm, celluloid)   & wolf (wolves, pack)                & bogart (casablanca, noir)             & consciousness (profound, meaning)    \\
		zone (twilight, concept)       & wire (wires, walls)                & animals (animal, humans)              & abandoned (deserted, forced)         \\
		wayward (cross, join)          & undead (zombie, zombies)           & anime (animation, japan)              & attitude (respect, reasons)          \\
		visitor (arrival, visiting)    & winter (snow, frozen)              & african (racial, racist)              & graphically (graphic, violent)       \\
		weaving (weaves, crafted)      & yesteryear (throwback, nostalgic)  & band (bands, rock)                    & acceptable (rhodes, internetreviews) \\
		yearns (desires, affection)    & voted (vote, awards)               & accurate (accuracy, accurately)       & advances (developed, attractive)     \\
		youngster (kid, childhood)     & unquestionably (arguably, closing) & accuracy (accurate, inaccuracies)     & depths (beneath, surface)            \\
		yell (angry, yelling)          & wifes (husband, marriage)          & artist (artists, artistic)            & advice (decided, spent)              \\
		walker (damage, bill)          & worthwhile (skip, include)         & catholic (priest, church)             & explanation (explained, explain)     \\
		\end{tabular}\caption{The largest differences in term scores between embeddings for the movies. The first two columns are the terms that were most reduced in score, while the third and fourth columns are the terms that increased most in score}\label{ch5:diff}
\end{table}
\end{landscape}

\subsubsection{Newsgroups Decision Tree Cluster-Features}

In this section we qualitatively investigate the clusters for the newsgroups, with particular interest in why the NNET-B clusters performed as well as the neural network and better than the unsupervised representation by a wide margin.

In Table \ref{chh5:dtng}, we examine the clustered features used in the decision trees for the newsgroups. In particular, 
we obtain the top decision tree node for each class. This is to give an overview of the different kinds of clusters and how they perform for each task. When examining the difference between the cluster-features for the unsupervised embedding and NNET-B, we can see that these important features seem more relevant to the task. Rather than, refining the rankings of the previous properties, new cluster-directions have been found that more closely correspond to the task. For example, the cluster-feature used for comp.graphics contains many relevant terms  "(pbmplus, sgis, sunview, phigs, pex, colormap, pixmap)" compared to the unsupervised embedding "(povray, raytracing)" as does the cluster-feature used for rec.autos "(sedan, camry, saab, ..., diesel, coupe)" versus "(sedan, camry)". Following this the conclusion is that the BOW representation simply better arranges the entities such that these natural cluster directions are meaningful to the class. 

Strikingly, there is not a single clustered direction for NNET-U that is has combined more than a single term. This is because the cluster-directions for the newsgroups are such that there are many large cluster-directions full of noisy terms, and then many single-term cluster-features for the relevant terms. These larger cluster-features were not used in the depth-3 decision tree. However, this did not lead to good quantitative results. For example, "bikes" is particularly relevant to the class rec.motorcycles but the accompanying directions seen in the unsupervised and NNET-B embeddings are not present. This is likely due to the hyper-parameter for the clusters, but it is interesting as that means these single-directions performed better than clustering terms together. 




%%% Unique dirs

%\begin{table}[]\label{Unique word directions identified in the bow representation}
%	Entities, terms that have gone down the most from the original space, terms that have gone up the most
%\end{table}

%However, this only covers those terms which were also in the original space. There are also unique word directions identified in the bow representation. %Noisy entities have gone down, prototypical entities have gone up?

%The clusters obtained from the bag-of-words as input neural network directions seemed to have a general trend as well, they grouped noisy terms that were not related to the genres together, and kept the genre clusters tight without excess terms. The hypothesis that follows is that the neural network focused on optimizing the rankings for these directions, moving anything not directly related to the objective away. 

%To demonstrate this, we show a comparison of the neural network with bow input representation and the original representation each projected using PCA into two dimensions.

%Additionally, we investigate the decision trees associated with these clusters. In these decision trees, we can see that the same features were used repeatedly with more refinement, speaking to the fact that rather than neural networks finding new concepts in the domain, they instead refine those that are relevant to the task. This makes explanation of why an entity is classified a certain way easy, as it can simply be put in terms of relative scale. For example, you could say that this movie was classified as a horror, as it was more {horror, scary, bloody} than {example movie}.

%The difference between the rankings of common single word directions is also investigated. Hypothetically, the rankings should make more intuitive sense, and noise should be removed, if the neural network does indeed prioritize the rankings of this important domain knowledge.


%%% Common dirs
 
\begin{landscape}
	\begin{table}[]
		\scriptsize
		\centering
\begin{tabular}{llll}
	Newsgroups               & Unsupervised Embedding                                     & NNET-B                                                               & NNET-U  \\
	alt.atheism              & (celestial, creationist, psalm, ..., fallacy, baptism)     & (messenger, faiths)                                                    & (homosexuality) \\
	comp.graphics            & (povray, raytracing)                                       & (pbmplus, sgis, sunview, phigs, pex, colormap, pixmap)                 & (tiff)          \\
	comp.os.ms-windows.misc  & (winini, systemini, cica)                                  & (cirrus, workgroups, hicolor, ..., keystrokes, sdk)                    & (xlib)          \\
	comp.sys.ibm.pc.hardware & (lpt1, irq, chipset, mfm, logitech)                        & (qemm, emm386, hernandez, ..., com2, brewers)                          & (motherboards)  \\
	comp.sys.mac.hardware    & (centris, lciii)                                           & (democrats, emissions, helicopters, enforced, tribes, reign, clintons) & (quadra)        \\
	comp.windows.x           & (xdm, makefile, r4, ..., rainer, ow)                       & (xmotif, tvtwm, xfree86, ..., xsun, polygon)                           & (xlib)          \\
	misc.forsale             & (diffiehellman, exterior, strips, ..., uucp, slots)        & (267, saga, warriors, ..., pov, carnage)                               & (obo)           \\
	rec.autos                & (sedan, camry)                                             & (sedan, camry, saab, ..., diesel, coupe)                               & (toyota)        \\
	rec.motorcycles          & (porsche, countersteering, msf)                            & (carb, bikes, reed)                                                    & (bikes)         \\
	rec.sport.baseball       & (gant, duke, padres, ..., marlins, slg)                    & (larkin, marlins, pennant, ..., platoon, coaching)                     & (pitching)      \\
	rec.sport.hockey         & (providence, keenan, hawks, ..., champs, ahl)              & (nyr, messier, motto, ..., goaltending, keenan)                        & (stanley)       \\
	sci.crypt                & (playback, ciphertext, cryptanalysis, ..., escrow, nist)   & (authentication, diffiehellman, publickey, 80bit, vesselin, skipjack)  & (rsa)           \\
	sci.electronics          & (ham, amp, reactor, watts, amplifier, amps)                & (omega, diode, charger, ..., antennas, joystick)                       & (voltage)       \\
	sci.med                  & (intake, calcium, antibiotic, ..., kidney, quack)          & (bethesda, sensation, defects, ..., therapies, tablets)                & (patients)      \\
	sci.space                & (reboost, fusion, astronomers, ..., pasadena, galaxy)      & (shafer, reusable, pluto, ..., orbiter, billboards)                    & (lunar)         \\
	soc.religion.christian   & (divorce, sinless, moses, ..., corinthians, baptized)      & (pastor, baptized, congregation, corinthians, repent, pagan)           & (scripture)     \\
	talk.politics.guns       & (federalist, unregistered, tyranny, ..., wright, shotguns) & (tpg, progun, 9mm, ..., rkba, shotguns)                                & (batf)          \\
	talk.politics.mideast    & (arab, rebellion, syrian, ..., zionist, gaza)              & (asalasdpaarf, revisionism, zionism, ..., hamas, grandparents)         & (palestinian)   \\
	talk.politics.misc       & (croats, redraw, shelling, ..., bosnians, yugoslavia)      & (wiretapping, safeguards, vlsi)                                        & (homosexual)    \\
	talk.religion.misc       & (divorce, sinless, moses, ..., corinthians, baptized)      & (thy, forgiveness, churchs, ..., protestant, ephesians)                & (scripture)    
\end{tabular}\caption{The top node of the Decision Tree for newsgroups}\label{ch5:dtng}
\end{table}
\end{landscape}


\subsubsection{Movies Decision Tree Cluster-Features}

In this section, we qualitatively investigate the cluster-features used in the top decision tree nodes for the movies domain, with a particular interest in why all embeddings have similar overall predictive performance despite containing very different properties (as talked about in Section \ref{ch5:diffsection}).

One observation that follows the discussion in Section \ref{ch5:diffsection} is that the Adventure genre is classified using a cluster-feature that contains the word "adventure" in the case of NNET-U (adventure, spectacle, exciting, ..., seat, boyfriend), rather than the "(animation, animated, anime, voiced, cartoon)" cluster-ffeature that is re-used in the unsupervised embedding. This follows the previous observation as the term "adventure" was one of the biggest differences between the unsupervised representation and NNET-U, where it had increased in NDCG score for NNET-U. However, despite this the performance overall is the same. 

The reason for the similar performance is likely that at the cost of re-organizing the representation such that the adventure cluster-direction existed, other cluster-directions were disrupted. For example the history class is classified in the unsupervised representation by the seemingly relevant "(events, accuracy, accurate, facts, confusing)", but in the case of NNET-U this cluster is disrupted by many seemingly noisy terms "(western, historical, musicians, ..., biography, propaganda)". Note that we do not really see this in the newsgroups Table \label{ch5:dtng} for NNET-B, where instead new and meaningful cluster-features are found that are relevant to the task.

\begin{landscape}
	
	\begin{table}[]
		\scriptsize
		\centering
		\begin{tabular}{lp{7cm}p{7cm}p{7cm}}
			Movies      & Unsupervised Embedding                                                       & NNET Bow                                                           & NNET Embedding                                                           \\
			Action      & (fight, fighting, epic, ..., battle, weapons)                                & (martial)                                                          & (martial, fight, fighting, fights, choreography, choreographed, fighter) \\
			Adventure   & (animation, animated, anime, voiced, cartoon)                                & (adventure, adventures)                                            & (adventure, spectacle, exciting, ..., seat, boyfriend)                   \\
			Animation   & (animation, animated, anime, voiced, cartoon)                                & (voiced)                                                           & (voice, voiced, recording, voices, vocal, listening)                     \\
			Biography   & (biography, biopic)                                                          & (biopic)                                                           & (gritty, historically, fiction, ..., accurate, accuracy)                 \\
			Comedy      & (hilarious, jokes, comedies, ..., funniest, funnier)                         & (comedies, funniest, funnier, ..., laughed, slapstick)             & (hilarious, jokes, comedies, ..., funniest, funnier)                     \\
			Crime       & (noir, crime, caper, criminal, criminals, crimes)                            & (crime, police, criminal, cop, cops)                               & (gangster, gangsters)                                                    \\
			Documentary & (documentary, footage, documentaries, interviews, interviewed, informative)  & (documentary, footage, documentaries)                              & (documentary, footage, documentaries, ..., extras, interviewed)          \\
			Drama       & (emotional, magical, waves, ..., silent, evidence)                           & (emotional, emotions, emotionally, ..., relationships, study)      & (bond, emotional, families, ..., powerful, gripping)                     \\
			Family      & (adults, children, childrens, ..., parents, daughter)                        & (adult, ages, children, parents, teenager)                         & (disneys, walt)                                                          \\
			Fantasy     & (fantasy, fairy, fairytale)                                                  & (magic, colorful, comical, lavish, delightfully, dazzling, colors) & (fantasy, surreal, elm, fairy, fairytale, dreams, dream)                 \\
			Film-Noir   & (femme, fatale)                                                              & (noir, vintage)                                                    & (femme, fatale)                                                          \\
			History     & (events, accuracy, accurate, facts, confusing)                               & (historically)                                                     & (western, historical, musicians, ..., biography, propaganda)             \\
			Horror      & (horror, creepy, slasher, ..., scares, scare)                                & (creepy, scare, spooky, scary, monster, menacing, nightmares)      & (horror, creepy, scares, ..., spooky, chills)                            \\
			Music       & (musicals, songs, singing, ..., song, numbers)                               & (songs, musical, song, ..., sing, dancing)                         & (songs, singing, song, ..., sings, singer)                               \\
			Musical     & (musicals, songs, singing, ..., song, numbers)                               & (musicals, broadway)                                               & (songs, singing, song, ..., sings, singer)                               \\
			Mystery     & (detective, investigation, mystery, clues, mysterious, investigating)        & (mystery, dramatic, fiction, ..., psychological, slowly)           & (thriller, suspense, hitchcock, ..., thrillers, suspenseful)             \\
			Romance     & (romantic, charming, romance, ..., charm, chick)                             & (romance, romantic, chemistry, ..., handsome, scenery)             & (wedding, marry, marries, marrying, bride)                               \\
			Sci-Fi      & (sci, alien, space, aliens, outer, invasion)                                 & (sci, science, futuristic)                                         & (science, scientific, scientist, ..., scientists, investigating)         \\
			Short       & (episodes, episode, seasons, aired, television, storylines)  (python, monty) & (agree, soundtrack, product, ..., happening, inside)               & (geocities, html, aol, faustus)                                          \\
			Sport       & (coach, sports, team, sport, football)                                       & (sports, sport, coach)                                             & (boxing)                                                                 \\
			Thriller    & (thriller, suspense, adventure, ..., suspenseful, tension)                   & (thriller, suspense, thrillers, suspenseful, thrills, thrilling)   & (thriller, suspense, hitchcock, ..., thrillers, suspenseful)             \\
			War         & (war, soldiers, vietnam, ..., military, troops)                              & (wwii, german)                                                     & (war, soldiers, vietnam, ..., military, troops)                          \\
			Western     & (westerns, western)                                                          & (westerns)                                                         & (outlaw)                                                                
		\end{tabular}\caption{The top node of the Decision Tree for Movies}
	\end{table}
\end{landscape}
%%%  Top dirs

%\begin{table}[]\label{For movies Top directions +}
%	content...
%\end{table}
 
%In the movie review domain, the original MDS space had high NDCG scores for noise, e.g. "berardinelli" a reviewer's name, or "listinfo" which is review metadata. This was also present in the neural network representation where this vector space was used as input. However, it was not present when starting from a bag-of-words. This is interesting, because the neural network performed worse with the bag-of-words as input, and very well with the vector space as input, despite their directions performing about the same on a depth-3 decision tree.

%%%  Top dir differences

%\begin{table}[]\label{difference how top directions in original rep have changed}
%	content...
%\end{table}

\subsubsection{Top-Ranking Entities On Decision Tree Cluster-Features in the Place-types Domain}

The intention of this section is to investigate the rankings of entities in the embeddings and see if they make sense. The intuition is that despite neural network embeddings obtaining directions that make reasonable sense, their entities may be disrupted. This is determined by taking the top 5 entities for each top node cluster-feature from decision trees as in the previous section. Note that it may not seem meaningful to take only the top 5 entities, but as there are only 391 entities overall for the class, and very few of those are positive instances of the class, they are actually extremely relevant. Essentially, if the entities do not make sense for the cluster directions in the neural network embedding but do for the unsupervised embedding, it is reasonable to say that the interpretability of the features is not as good as in an unsupervised representation.

To begin, we look at the clusters used to classify "CollegeAndUniversity", the unsupervised representation uses the cluster "(annarbor, graduation, eugene, institute, highschool, cal)" that seems particularly relevant to educational institutions, and its associated entities also seem very meaningful and relevant "(college, campus, college campus, university, school)". For NNET-B, the cluster itself seems absurd "(investment)", but the associated entities are relevant to that cluster "(commercial real estate, large construction, retirement home, ..., rental property)". Similarly, the cluster for NNET-U does not seem relevant to the class and neither do its top entities "(snowboard, turism, amusementpark, ..., waltdisney, disneyworld)" "(video game store, theme park, space shuttle, launch pad, speedway)". So in this case, the entities seem to hold.

In general, the results for this table follow a similar trend. Entities align with the meaning of cluster-features, even in neural network embeddings. The problems of classification come from poor selection of these cluster-features, rather than the cluster-features of those entities being incorrect. There is an interesting difference in the case of classifying NightLifeSpot, where three distinct but relevant clusters and associated entities are found and used in each of the embeddings. In the unsupervised representation, a cluster-feature related to music was used "(audience, instrument, musicians, ..., dancers, bands)" with relevant entities for that "(stage, bar, rock, sound, music venue)", meanwhile in the case of NNET-B, a cluster for drinking alcohol was used instead "(booze, vodka, whiskey, liquor)" with top entities of places to get that alcohol "(dive bar, beer garden, karaoke bar, hotel bar, cocktail bar)", finally the NNET-U identifies a cluster more related to sex but also including tattoo and dance studios (shoulder, darkroom, topless, ..., boobs, lips) with associated entities  "(dance studio, tattoo studio, shoulder, topless beach, strip club)". 

In conclusion for this section, entities seem to make sense in the case of place-types where they align well with their cluster-feature. Additionally, combining the use of a Decision Tree, the associated cluster terms with the cluster-feature, as well as the top entities gives great insight into the representation and what it is doing, as each provides valuable context for each other. 

\begin{landscape}
	
\begin{table}[]
	\scriptsize
	\centering
	\begin{tabular}{lll}
		Class       & Decision Tree Cluster-Feature                                             & Top 5 Entities                     \\
                          & Unsupervised                                                                              &                                                                                                     \\
ArtsAndEntertainment       & (snack, vegetable, lemon, pepper, vegetables)                                             & (restaurant, market, asian restaurant, seafood restaurant, japanese restaurant)                     \\
CollegeAndUniversity       & (annarbor, graduation, eugene, institute, highschool, cal)                                & (college, campus, college campus, university, school)                                               \\
Food                       & (cuisine)                                                                                 & (restaurant, asian restaurant, japanese restaurant, seafood restaurant, chinese restaurant)         \\
NightlifeSpot              & (audience, instrument, musicians, ..., dancers, bands)                                    & (stage, bar, rock, sound, music venue)                                                              \\
ParksAndOutdoors           & (cloudscape, solitary, hazy, ..., fluffy, intense)                                        & (coast, shore, coastline, shoreline, beach)                                                         \\
ProfessionalAndOtherPlaces & (nationalhistoriclandmark, register, revival, jefferson, independence, pioneer, civilwar) & (court house, courthouse, government building, state, cemetery)                                     \\
Residence                  & (laundry, dwelling)                                                                       & (house, home, residential building, apartment, historic building)                                   \\
ShopsAndService            & (mannequin, etsy, crafts, boutique, bags, jewelry, necklace)                              & (market, boutique, store, clothing store, vintage store)                                            \\
TravelAndTransport         & (donkey, costarica, tanzania, ..., reisen, andes)                                         & (market, desert, national park, crater, volcano)                                                    \\
& NNET-B                                                                                       &                                                                                                     \\
ArtsAndEntertainment       & (ruraldecay)                                                                              & (sugar mill, silo, corral, abandoned farm, ranch)                                                   \\
CollegeAndUniversity       & (investment)                                                                              & (commercial real estate, large construction, retirement home, ..., rental property) \\
Food                       & (dolphins)                                                                                & (football field, training camp, strait, baseball stadium, continent)                                \\
NightlifeSpot              & (booze, vodka, whiskey, liquor)                                                           & (dive bar, beer garden, karaoke bar, hotel bar, cocktail bar)                                       \\
ParksAndOutdoors           & (condos, venice, entertainment, ..., socks, americanflag)                                 & (piano bar, public housing, waiting room, residential street, dining room)                          \\
ProfessionalAndOtherPlaces & (gardening, tiki, cocktail, ..., candle, horticulture)                                    & (sand bar, vineyard, monsoon forest, ski lodge, topless beach)                                      \\
Residence                  & (tribute, plains, des, acropolis, aa)                                                     & (battlefield, national monument, battlefield park, cliff dwelling, space shuttle)                   \\
ShopsAndService            & (cloister)                                                                                & (diocese, convent, tomb, college theater, study)                                                    \\
TravelAndTransport         & (dmctz3, panasonicdmctz3, panasonictz3)                                                   & (railroad tunnel, terminal, airport tram, railroad signal, railroad yard)                           \\
& NNET-U                                                                                    &                                                                                                     \\
ArtsAndEntertainment       & (messy, schoolhouse, beatles, ..., halftimbered, publictransport)                         & (cubicle, newsroom, workroom, bedroom closet, detached house)                                       \\
CollegeAndUniversity       & (snowboard, turism, amusementpark, ..., waltdisney, disneyworld)                          & (video game store, theme park, space shuttle, launch pad, speedway)                                 \\
Food                       & (boutique, liquor, olive, ..., counter, dessert)                                          & (molecular gastronomy restaurant, whisky bar, gourmet shop, cocktail bar, juice bar)                \\
NightlifeSpot              & (shoulder, darkroom, topless, ..., boobs, lips)                                           & (dance studio, tattoo studio, shoulder, topless beach, strip club)                                  \\
ParksAndOutdoors           & (nesting, tits, mouth, ..., gulls, avian)                                                 & (rookery, hunting reserve, wetland, wildlife reserve, nest)                                         \\
ProfessionalAndOtherPlaces & (islam, muslim, supermarket, ..., sales, marruecos)                                       & (retail outlet, electronics store, tourist information center, jewelry store, souk)                 \\
Residence                  & (heath, ward, lock, ..., stained, argyll)                                                 & (ventilation shaft, abandoned prison, sanatorium, abandoned complex, reformatory)                   \\
ShopsAndService            & (video, candy, clothing, ..., skin, blonde)                                               & (video game store, jewelry store, thrift store, workroom, nail salon)                               \\
TravelAndTransport         & (survey, mammal, antenna, ..., paws, fur)                                                 & (crop farm, flowerbed, tongue, nest, crop circle)                                                  
                                              
	\end{tabular}\caption{The placetypes clusters used at the top of the decision tree and the associated top-ranked entities}
\end{table}
\end{landscape}
%%%%  Decision trees obtained from single dirs


\subsubsection{Top Entities for Common Terms}

Despite the previous section giving insight into the cluster-features by providing context with the top 5 entities, it did not illustrate the differences in entity rankings between the embeddings for the same properties. Instead, it showed that different embeddings used different properties as the top node of the decision tree for a class, often covering different facets in different embeddings. In this section, we investigate the differences between top entities for common terms. The method is the same as Section \ref{ch5:commonunique} to obtain these common terms, but instead of taking the top 2,000 terms we take the top 500, so as not to include low-scoring terms that will have poor entity representations across the embeddings. 

Interestingly, the resulting terms have very common top entities in their rankings, including the neural network representation that used the bag-of-words as input. This further validates the idea given earlier that the representations are able to achieve common scores when using direction and cluster features because they find common terms that can be used in the classes. Essentially, they are able to find the same relevant terms that matter for classification.

\begin{table}[]
	\scriptsize
	\begin{tabular}{ll}
		Cluster Features                           & Top 5 Ranked Entities                                                                  \\
		Unsupervised Embedding                     &                                                                                        \\
		bass (drums, tom)                          & (indie theater, jazz club, music studio, music venue, rock club)                       \\
		arrival (ticket, departure)                & (taxiway, airport tram, airport, aircraft cabin, airport gate)                         \\
		condo (condominium, apartments)            & (rental property, condominium, condo, villa, plaza)                                    \\
		palazzo (ff, notte)                        & (campanile, palace, villa, triumphal arch, cuesta)                                     \\
		fans (ball, player)                        & (basketball stadium, hockey arena, soccer stadium, baseball stadium, football stadium) \\
		animalplanet (feeder, natureselegantshots) & (zoo, zoological garden, nest, moais, animal shelter)                                  \\
		rent (homes, villas)                       & (rental property, real estate offices, cuesta, condo, condominium)                     \\
		agriculture (agricultural, fields)         & (cropland, conifer forest, olive grove, arboretum, sugar plantation)                   \\
		champions (win, league)                    & (football stadium, soccer stadium, basketball stadium, college stadium, cuesta)        \\
		cielo (azul, nubes)                        & (cuesta, campanile, plaza, villa, playa)                                               \\
		drummer (drums, who)                       & (jazz club, cuesta, moais, music venue, indie theater)                                 \\
		accommodation (accomodation, guest)        & (rental property, amenities, hotel pool, suite, ski chalet)                            \\
		decayed (forgotten, exploring)             & (abandoned airfield, sanatorium, concentration camp, abandoned prison, hospital ward)  \\
		yankees (louis, victory)                   & (baseball stadium, basketball stadium, baseball field, hockey arena, stadium)          \\
		NNET-B                                     &                                                                                        \\
		bass (drums, tom)                          & (indie theater, jazz club, music studio, music venue, rock club)                       \\
		arrival (ticket, departure)                & (taxiway, airport tram, airport, aircraft cabin, airport gate)                         \\
		condo (condominium, apartments)            & (rental property, condominium, condo, villa, plaza)                                    \\
		palazzo (ff, notte)                        & (campanile, palace, villa, triumphal arch, cuesta)                                     \\
		fans (ball, player)                        & (basketball stadium, hockey arena, soccer stadium, baseball stadium, football stadium) \\
		animalplanet (feeder, natureselegantshots) & (zoo, zoological garden, nest, moais, animal shelter)                                  \\
		rent (homes, villas)                       & (rental property, real estate offices, cuesta, condo, condominium)                     \\
		agriculture (agricultural, fields)         & (cropland, conifer forest, olive grove, arboretum, sugar plantation)                   \\
		champions (win, league)                    & (football stadium, soccer stadium, basketball stadium, college stadium, cuesta)        \\
		cielo (azul, nubes)                        & (cuesta, campanile, plaza, villa, playa)                                               \\
		drummer (drums, who)                       & (jazz club, cuesta, moais, music venue, indie theater)                                 \\
		accommodation (accomodation, guest)        & (rental property, amenities, hotel pool, suite, ski chalet)                            \\
		decayed (forgotten, exploring)             & (abandoned airfield, sanatorium, concentration camp, abandoned prison, hospital ward)  \\
		yankees (louis, victory)                   & (baseball stadium, basketball stadium, baseball field, hockey arena, stadium)          \\
		NNET-U                                     &                                                                                        \\
		bass (tom, drums)                          & (music studio, jazz club, rock club, music school, piano bar)                          \\
		arrival (departure, journey)               & (airport tram, airport control tower, airport, airport lounge, toll booth)             \\
		condo (condominium, condominiums)          & (condo, condominium, rental property, suite, hotel pool)                               \\
		palazzo (palais, efs)                      & (campanile, palace, cathedral, villa, plaza)                                           \\
		fans (arena, name)                         & (football stadium, basketball stadium, soccer stadium, hockey arena, stadium)          \\
		animalplanet (tiere, vosplusbellesphotos)  & (zoo, zoological garden, nest, animal shelter, rookery)                                \\
		rent (rental, animalplanet)                & (rental property, condo, suite, condominium, real estate offices)                      \\
		agriculture (farming, petrol)              & (crop farm, olive grove, arboretum, cropland, wheatfield)                              \\
		champions (win, victory)                   & (football stadium, soccer stadium, stadium, basketball stadium, hockey arena)          \\
		cielo (ciel, nubes)                        & (campanile, cuesta, cathedral, plaza, villa)                                           \\
		drummer (guitarist, drums)                 & (jazz club, rock club, stage, piano bar, music venue)                                  \\
		accommodation (condominium, hostel)        & (amenities, rental property, suite, hotel pool, resort)                                \\
		decayed (forgotten, decaying)              & (sanatorium, abandoned airfield, barracks, abbey, military barracks)                   \\
		yankees (mlb, league)                      & (baseball stadium, baseball field, avenue, basketball stadium, hockey arena)          
	\end{tabular}\caption{Terms common for all embeddings and the associated ranking of entities on those term directions for each embedding. Arranged by NDCG-score in the unsupervised embedding.}
\end{table}

%One general pattern that is noticable in these results is that entities for NNET-B seem to make less sense than those of the unsupervised or NNET-U embeddings. For example, the term "abandonedbuilding" which has meaningful entities in the case of the unsupervised embedding "(abandoned township, sanatorium, vacant house, abandoned factory, sugar refinery)" and the case of NNET-U "(sanatorium, abandoned factory, abandoned prison, elevator shaft, abandoned home)" seems both nonsensical and general for NNET-B "(bathroom, gay bar, road, room, hill)". This is interesting, because the neural network results although worse than the unsupervised SVM representation were better than NNET-U. This could be explained by the neural network prioritizng terms relevant to the Foursquare task, as "abandonedbuilding" is not a useful term for classifying these classes, it was not separable in the space. This idea can be reinforced by the term "haunted (eerie, peeling)" also not containing relevant entities "(border, house, home, square, backyard)", these terms are just not relevant to the task and so are not prioritized.

%Although this is an assumption, it is backed by the previous qualitative results. Those directions chosen for the decision trees had meaningful entities in the top 5, even if these terms did not. We can conclude that although these common terms do contain sometimes irrelevant entities, this is because those directions were not prioritized in the representation. We verify this further quantitatively by finding the difference in NDCG score between NNET-B and the unsupervised representation:



%Decision trees of depth-3 in the quantiative results performed as well as neural networks, however the trees obtained from this representation are simplistic in the form of "if HORROR then HORROR" rather than representing some complex domain knowledge.



%%%%  Clusters

%%%%  Cluster diffs (new clusters? same terms but diff clusters?)

%%%%  Decision trees obtained from clusters

%\begin{table}[]\label{ch5:clustersusedintrees}
%	Horror: Most important clusters used for space 1, most important for space 2
%\end{table}

%%%%  Ranks obtained from single directions

%%%%  Ranks obtained from cluster directions

%%%%  Ranks obtained from single directions differences

%\begin{table}[]\label{Biggest difference in entities (entities that have gone down/gone up the most) for the same directions }
%	Entities, terms that have gone down the most from the original space, terms that have gone up the most
%\end{table}

%\begin{table}[]
%	\scriptsize
%\begin{tabular}{ll}%
%	Unsupervised Embedding                      &                                                                                   \\
%	statua (stern, 1st)                         & (triumphal arch, cuesta, cathedral, palace, campanile)                            \\
%	commuter (underpass, muni)                  & (underpass road, overpass road, freeway, elevated roadway, highway ramp)          \\
%	frankreich (francia, nov)                   & (campanile, triumphal arch, castle, cathedral, villa)                             \\
%	homes (residential, apartment)              & (rental property, villa, condominium, condo, real estate offices)                 \\
%	abandonedbuilding (abandonned, dilapidated) & (abandoned township, sanatorium, vacant house, abandoned factory, sugar refinery) \\
%	blooming (blooms, petals)                   & (arboretum, olive grove, botanical garden, garden, flowerbed)                     \\
%	haunted (spooky, eerie)                     & (cemetery, grave, mausoleum, tomb, sanatorium)                                    \\
%	adriatic (dalmatia, fishingboat)            & (campanile, triumphal arch, villa, palace, moais)                                 \\
%	cascades (cascade, dubai)                   & (ski trail, summit crater, summit, ski chairlift, ski resort)                     \\
%	designer (flor, virtual)                    & (bridal shop, tanning salon, bedroom closet, cosmetics shop, cuesta)              \\
%	dj (bands, punk)                            & (indie theater, jazz club, cuesta, music studio, rock club)                       \\
%	poor (poverty, documentary)                 & (slum, depression, villa, cuesta, caliphate)                                      \\
%	agricultural (tractor, farming)             & (cropland, crop farm, farmland, pasture, corn field)                              \\
%	concerts (instrument, solo)                 & (jazz club, indie theater, concert hall, cuesta, playhouse)                       \\
%	NNET Bow                                    &                                                                                   \\
%	statua (estatua, colonnade)                 & (crucifix, palace, tomb, monument, altar)                                         \\
%	commuter (lightrail, commute)               & (apartment, bus station, roadway, bus line, rock)                                 \\
%	frankreich (allemagne, austria)             & (castle, campanile, house, estate, rock)                                          \\
%	homes (residential, balcony)                & (house, home, slum, room, rental property)                                        \\
%	abandonedbuilding (decrepit, neglect)       & (bathroom, gay bar, road, room, hill)                                             \\
%	blooming (blossoms, daffodils)              & (conservatory, mountain, house, garden, greenhouse)                               \\
%	haunted (eerie, peeling)                    & (border, house, home, square, backyard)                                           \\
%	adriatic (turkiye, dalmatia)                & (catena, palace, rock, gay bar, campanile)                                        \\
%	cascades (tundra, prey)                     & (rock, bridge, house, summit, ski area)                                           \\
%	designer (gorgeous, mannequin)              & (room, bathroom, house, home, bedroom)                                            \\
%	dj (pop, smoking)                           & (gay bar, dive bar, rock club, pub, nightclub)                                    \\
%	poor (westminster, bag)                     & (house, thatched roof building, neighborhood, home, campanile)                    \\
%	agricultural (farmer, farming)              & (silo, farmstead, farmhouse, abandoned farm, thatched roof building)              \\
%	concerts (stalls, folk)                     & (rock club, casino, thatched roof building, nightclub, pub)                       \\
%	NNET Embedding                              &                                                                                   \\
%	statua (palacio, estatua)                   & (cathedral, triumphal arch, campanile, palace, convent)                           \\
%	commuter (muni, passenger)                  & (freeway, bus line, elevated roadway, ferry station, light rail)                  \\
%	frankreich (francia, normandy)              & (cathedral, campanile, abbey, castle, triumphal arch)                             \\
%	homes (condo, pine)                         & (rental property, thatched roof building, condo, villa, real estate offices)      \\
%	abandonedbuilding (vandalism, demolished)   & (sanatorium, abandoned factory, abandoned prison, elevator shaft, abandoned home) \\
%	blooming (petals, bloom)                    & (olive grove, arboretum, botanical garden, garden, conservatory)                  \\
%	haunted (spooky, scary)                     & (cemetery, abbey, sanatorium, graveyard, manor)                                   \\
%	adriatic (dalmatia, tilt)                   & (campanile, triumphal arch, villa, walled town, castle)                           \\
%	cascades (reservoir, glacier)               & (summit crater, hanging valley, summit, ridge line, mountain)                     \\
%	designer (shuttle, mic)                     & (tanning salon, suite, bedroom closet, bedroom, laundry room)                     \\
%	dj (hip, rave)                              & (playhouse, jazz club, rock club, stage, piano bar)                               \\
%	poor (poverty, documentary)                 & (slum, cathedral, homeless shelter, town square, church)                          \\
%	agricultural (plow, barns)                  & (crop farm, corn field, wheatfield, olive grove, cropland)                        \\
%	concerts (kunst, league)                    & (jazz club, cirque, rock club, stage, playhouse)                                 
%\end{tabular}\caption{Terms common for all embeddings and the associated ranking of entities on those term directions for each embedding. Arranged by NDCG-score in the unsupervised embedding.}
%\end{table}


%There are common word directions, e.g. horror. But how have the rankings changed for these directions? This is investigated by looking at the biggest differences in rankings for the same word between the original representation and the fine-tuned representation. We use the place-types results for this as they have easy to understand entities. We can see that e.g.

%%%%  Ranks obtained from top directions differences

%%%%  Common ranks

%%%%  Unique ranks

%%%%  PCA projections

%%%%  Scores of different classes



















%The way that knowledge is represented is changed significantly in the neural network with bow input compared to the original representation. This can be verified by what clusters are used in the decision trees to classify the entities. For the neural network with bow, 
%But what if these directions are not new properties, but instead the same rankings but with different words labelling them?






%\subsubsection{Directions}

%Two groups of single word directions and clusters were obtained from three different domains, in each one is from the second layer of  a two-layer neural network starting from a bag-of-words and the other is from the first layer of a one-layer neural network starting from an unsupervised representation. The examples in this section for the clusters are from the clusters that performed the best in depth-3 decision-trees. When investigating single word directions, sometimes it is unclear what they mean. In this case, the most similar single word-directions are found and used to provide additional context.

% Compare the same directions but see how their rankings have changed



% Compare the top directions for NDCG, top for Kappa



%\subsubsection{Starting from vector space}

%\subsubsection{Starting from bag-of-words}

%Obtaining directions from this space takes much longer than normal vector spaces.

\section{Qualitative Evaluation of Auto-encoders}

We base our experiments on the movie reviews domain. To collect the terms that are likely to correspond to property names, we collect adjectives and nouns that occur at least 200 times in the movie review data set, collecting 17,840 terms overall.  These parameters are used for all entity embeddings induced by the auto-encoder. 

\subsection{Software, Architecture and Settings}

To implement the denoising auto-encoders, the Keras \cite{keras}  library is used. As in \ref{ch3}, scikit-learn~\cite{scikit} is used for the SVM implementation. All of the code and data for this Chapter is available on GitHub\footnote{\url{https://github.com/eygrr/RulesFromAuto-encoders}}. A 200 dimensional MDS space is used as the input to our stack of auto-encoders, as this performed the best in Chapter \ref{ch3}. The network is trained using stochastic gradient descent and the mean squared error loss function. For the encoders and decoders, the tanh activation function is used. For the first auto-encoder,  the same size layer as the input is maintained. Afterwards, the hidden representation size is halved each time it is used  as input to another auto-encoder, and this process is repeated three times, resulting in four new hidden representations $\{Input: 200, Hidden: 200, 100, 50, 25\}$. The input space is corrupted using  Gaussian noise with a standard deviation of 0.6 each time. As the lower layers are closer to the bag-of-words representation and are higher dimensional, the Kappa scores are higher in earlier spaces, as it is easier to separate entities. We address this in the clusters by setting the high Kappa score threshold $T^+$ such that the number of terms we choose from is twice the number of dimensions in the space. Similarly, we set $T^-$ such that 12,000 directions are available to assign to the cluster centres in every space. 

\subsection{Qualitative Evaluation of Induced Clusters}

In Table \ref{ch5:labelcomparison}, we illustrate the differences between clusters obtained using standard auto-encoders and denoising auto-encoders. Layer 1 refers to the hidden representation of the first auto-encoder, and Layer 4 refers to the hidden representation of the final auto-encoder. As single labels can lead to ambiguity, in Table 1 we label clusters using the top three highest scoring terms in the cluster. Clusters are arranged from highest to lowest Kappa score. 

Both auto-encoders model increasingly general properties, but the properties obtained when using denoising auto-encoder properties are more general. For example, the normal auto-encoder contains properties like ``Horror" and ``Thriller", but does not contain more general properties like ``Society" and ``Relationship". Further, ``Gore"  has the most similar properties ``Zombie" and ``Zombies" in Layer 1, and has the most similar properties of ``Budget" and ``Effects" in Layer 4. By representing a category of movie where ``Budget" and ``Effects" are important, the property is more general.

\begin{sidewaystable}
	\label{ch5:labelcomparison}
	\caption{A comparison between the first layers and the fourth layers of two different kinds of auto-encoders.}
	\vspace{5 mm}
	\begin{adjustbox}{center}
		\begin{tabular}{|ll|ll|}
			\hline
			\bf Standard Auto-encoder &   & \textbf{Denoising Auto-encoder} &   \\
			\bf Layer 1  \bf   & \bf Layer 4 \bf &  \textbf{Layer 1}  \bf   & \bf Layer 4 \bf \\
			\hline
			horror: terror, horrific & horror: victims, nudity  & gore: zombie, zombies & society: view, understand \\
			thriller: thrillers, noir & documentary: perspective, insight  & jokes: chuckle, fart & emotional: insight, portrays \\
			comedies: comedy, timing & blood: killing, effects   & horror: terror, horrific & stupid: flick, silly  \\
			adults: disney, childrens & suspense: mysterious, tense & emotionally: tragic, strength &  gore: budget, effects  \\
			husband: wife, husbands &  thriller: thrillers, cop  & gags: zany, parodies & military: war, ship \\
			relationships: intimate, angst & gory: gruesome, zombie  & hindi: bollywood, indian & romance: younger, handsome \\
			nudity: naked, gratuitous & beautifully: satisfying, brilliantly & touching: teach, relate & ridiculous: awful, worse   \\
			political: politics, nation & emotional: complex, struggle & scary: frightening, terrifying & government: technology, footage  \\
			smart: slick, sophisticated &  laughed: laughing, loud & documentary: document, narration & awesome: chick, looked\\
			creepy: sinister, atmospheric & charming: delightful, loves & adults: disney, teaches &  political: country, documentary\\
			laughed: humorous, offensive & hilarious: funny, parody & laughed: brow, laughter & relationship: relationships, sensitive \\
			adventure: adventures, ship & scares: halloween, slasher & thriller: thrillers, procedural & horror: genre, dark  \\
			actions: reaction, innocent & funniest: funnier, gags & cgi: animated, animation &  waste: concept, plain   \\
			cute: adorable, rom & emotions: respect, relationships & suspense: clues, atmospheric & army: disc, studio\\
			british: england, accent & laugh: mom, crazy & dumb: mindless, car & combat: enemy, weapons  \\
			horrible: worse, cheap & filmmaker: approach, artist & political: propaganda, citizens & supporting: office, married \\
			narrative: filmmaker, structure & drama: portrayed, portrayal & witty: delightfully, sarcastic & amazon: bought, copy \\
			digital: dolby, definition & interviews: included, showed & laughing: outrageous, mouthed & study: details, detail \\
			gory: graphic, gruesome & comedic: comedies, humorous & relationships: ensemble, interactions & land: water, super \\
			romantic: handsome, attractive & emotionally: central, relationships & creepy: mysterious, eerie & chemistry, comedies, comedic \\
			
			\hline 
		\end{tabular}
	\end{adjustbox}
\end{sidewaystable}

\subsection{Qualitative Evaluation of Induced Symbolic Rules}

Our aim in this work is to derive symbolic rules that can be used to explain the semantic relationships between properties derived from increasingly general entity embeddings. We provide examples of such rules in this section. Since the number of all induced rules is large, here we only show high accuracy rules that cover 200 samples or more.  Still, we naturally cannot list even all the accurate rules covering more than 200 samples. Therefore we focus here on the rules which are either interesting in their own right or exhibit interesting properties, strengths or limitations of the proposed approach. The complete list of induced rules is available online from our GitHub repository\footnote{https://github.com/eygrr/RulesFromAuto-encoders}.

For easier readability, we post-process the induced rules. For instance, the following is a rule obtained for the property ``Gore'' in the third layer of the network shown in the original format produced by JRip:

\begin{lstlisting}[mathescape=true]
IF scares-L2 <= 6 AND blood-L2 <= 8 AND funniest-L2 >= 22
=> classification=+ (391.0/61.0)
\end{lstlisting}

\noindent In this rule, $\texttt{scares-L2 <= 6}$ denotes the condition that the movie is in the top 6\% of rankings for the property ``$\texttt{scares}$" derived from the hidden representation of the second auto-encoder. We will write such conditions simply as ``$\texttt{Scares$_{2}$}$". Similarly, a condition such as $\texttt{funniest-L2 >= 22}$, which indicates that the property is not in the top $22\%$, will be written as “$\texttt{NOT Funniest}$$_{2}$”. In this simpler notation the above rule will look as follows: %We thus obtain the following formulation for the above rule:

\begin{lstlisting}[mathescape=true]
IF Scares$_{2}$ AND Blood$_{2}$ AND NOT Funniest$_{2}$ THEN Gore$_{3}$
\end{lstlisting}

This rule demonstrates an interpretable relationship. However, we have observed that the meaning of a rule may not be clear from the property labels that are automatically selected. In such cases, it is beneficial to label them by including the most similar cluster terms. For example, using the cluster terms below we can see that ``Flick" relates to ``chick-flicks" and that ``Amazon" relates to old movies:

\begin{lstlisting}[mathescape=true]
IF Flick$_{2}$ AND Sexual$_{2}$ AND Cheesy$_{2}$ AND NOT Amazon$_{2}$ THEN Nudity$_{3}$

Flick$_{2}$: {Flicks, Chick, Hot}
Amazon$_{2}$: {Vhs, Copy, Ago}
\end{lstlisting}

Rules derived from later layers use properties described by rules from previous layers. By seeing rules from earlier layers that contain properties in later layers, we can better understand what the components of later rules mean. Below, we have provided rules to explain the origins of components in a later rule:

\begin{lstlisting}[mathescape=true]
IF Emotions$_{2}$ AND Actions$_{2}$ THEN Emotions$_{3}$ 
IF Emotions$_{2}$ AND Emotion$_{2}$ AND Impact$_{2}$ THEN Journey$_{3}$ 
IF Emotions$_{3}$ AND Journey$_{3}$ THEN Adventure$_{4}$ 
\end{lstlisting}

We observe a general trend that as the size of the representations decreases and the entity embeddings become smaller, rules have fewer conditions, resulting in overall higher scoring and more interpretable rules. To illustrate this, we compare rules from an earlier layer to similar rules in a later layer: %Below, we present two of the layer-two rules.

\begin{lstlisting}[mathescape=true]
IF Romance$_{1}$ AND Poignant$_{1}$ AND NOT English$_{1}$ AND NOT French$_{1}$
AND NOT Gags$_{1}$  AND NOT Disc$_{1}$ THEN Relationships$_{2}$

IF Relationships$_{2}$ AND Emotions$_{2}$ AND Chemistry$_{2}$ THEN Romantic$_{3}$
IF Emotions$_{2}$ AND Compelling$_{2}$ THEN Beautifully$_{3}$ 
IF Warm$_{2}$ AND Emotions$_{2}$ THEN Charming$_{3}$ 
IF Emotions$_{2}$ AND Compelling$_{2}$ THEN Emotional$_{3}$ 
\end{lstlisting}

Rules in later layers also made effective use of a NOT component. Below, we demonstrate some of those rules:

\begin{lstlisting}[mathescape=true]
IF Touching$_{3}$ AND Emotions$_{3}$ AND NOT Unfunny$_{3}$ THEN  Relationship$_{4}$ 
IF Laughs$_{3}$ AND Laugh$_{3}$ AND NOT Compelling$_{3}$ THEN  Stupid$_{4}$
IF Touching$_{3}$ AND Social$_{3}$ AND NOT Slasher$_{3}$ THEN  Touching$_{4}$
\end{lstlisting}

As the same terms were used to find new properties for each space, the obtained rules sometimes use duplicate property names in their components. As the properties from later layers are a combination of properties from earlier layers, the properties in later layers are refinements of the earlier properties, despite having the same term. Below, we provide some examples to illustrate this:

\begin{lstlisting}[mathescape=true]
IF Emotions$_{2}$ AND Actions$_{2}$ THEN Emotions$_{3}$ 

Emotions$_{2}$: {Acted, Feelings, Mature}
Actions$_{2}$: {Control, Crime, Force}
Emotions$_{3}$: {Emotion, Issue, Choices}

IF Horror$_{2}$ AND Creepy$_{2}$ AND Scares$_{2}$ THEN Horror$_{3}$

Horror$_{2}$: {Terror, Horrific, Exploitation}
Creepy$_{2}$: {Mysterious, Twisted, Psycho}
Scares$_{2}$: {Slasher, Supernatural, Halloween}
Horror$_{3}$: {Creepy, Dark, Chilling}

IF Touching$_{2}$ AND Chemistry$_{2}$ THEN Touching$_{3}$ 
IF Touching$_{2}$ AND Emotions$_{2}$ THEN Touching$_{3}$ 
IF Compelling$_{2}$ AND Emotional$_{2}$ AND Suspense$_{2}$ THEN Compelling$_{3}$
If Romance$_{2}$ AND Touching$_{2}$ AND Chemistry$_{2}$ THEN Romance $_{3}$ 
IF Emotionally$_{2}$ AND Emotions$_{2}$ AND Compelling$_{2}$ THEN Emotionally$_{3}$ 
\end{lstlisting}



\section{Conclusion}

Feedforward neural networks and auto-encoders are qualitatively investigated using the methods outlined in Chapter \ref{ch3}. These properties are meaningful and interpretable in both representations, and in the case of the auto-encoders as the space size reduces the representation becomes more abstract. Through qualitative investigation rules show some promise in being used to link together properties in the layers and form some explanation of abstraction. Depth-3 decision trees on the final hidden layer of the neural network are found to retain some or all of the performance of the original neural network and also remain interpretable. For future work, the interpretability of these disentangled representations could be compared to alternative approaches, and they could be used to benefit those that work on those  safety and fairness of neural network models, in particular to make black-box state-of-the-art models transparent by  explaining layer-to-layer connections.




%\subsection{Chapter 3 Space Types}
%\begin{landscape}
%	\begin{table}[]
%		\begin{tabular}{llllllllll}
%			& Genres     &         &         & Keywords  &         &         & Ratings  &         &         \\
%			Movies            & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
%			Space             & 50 PCA     & 50 MDS  & 100 MDS & 200 PCA   & 200 MDS & 200 MDS & 50 PCA   & 200 PCA & 50 PCA  \\
%			Single directions & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A     \\
%			&            &         &         &           &         &         &          &         &         \\
%			& Newsgroups &         &         & Sentiment &         &         & Reuters  &         &         \\
%			Rep               & 200 PCA    & 200 PCA & 100 PCA & PCA 100   & PCA 50  & PCA 50  & 200 PCA  & 200 PCA & 100 PCA \\
%			Single dir        & 200 MDS    & 100 D2V & 50 D2V  & D2V 100   & PCA 50  & D2V 100 & N/A      & N/A     & N/A     \\
%			&            &         &         &           &         &         &          &         &         \\
%			& Foursquare &         &         & OpenCYC   &         &         & Geonames &         &         \\
%			Placetypes        & D1         & D2      & D3      & D1        & D2      & D3      & D1       & D2      & D3      \\
%			Rep               & MDS 100    & AWV 50  & MDS 200 & AWV 50    & MDS 200 & AWV 50  & MDS 50   & MDS 50  & AWV 200 \\
%			Single dir        & N/A        & N/A     & N/A     & N/A       & N/A     & N/A     & N/A      & N/A     & N/A    
%		\end{tabular}\caption{Space-types, clusters have the same as single directions.}
%	\end{table}
%\end{landscape}




  %This elevates neural networks from simply being representation learners to models that can discover and create new ideas in their representations in-order to solve the task.

%Theoretically if we are able to identify features from each layer, then we can also map how they interact with each other. 









%For each domain, we test two different network set-ups. The first is one where the hidden layer is the same size as the input layer, and the second is where the hidden layer is smaller than the hidden layer. The assumption is that by constraining the hidden layer size, we can force the network to generalize and find more abstract concepts in the hidden-layer, hopefully gaining some insight into the domain by comparing the input representation and the more general hidden representation.

%There are no additional techniques applied to improve the performance of the neural networks.