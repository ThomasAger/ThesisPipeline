% This thesis template has been put together and edited by Diego Pizzocaro
% It's a collection/edit of files produced by Frank C Langbein for his PhD thesis
% and then edited by Ahmed Alazzawi, Hmood Al Dossari, Manar I. Hosny, H.Z. Al-Dossari 
% and kindly shared by them.
%
% Following, the original GNU license chosen by Frank C Langbein, 
% which is the same license for distributing this package.
%
% beautification.tex -- Beautification of Reverse Engineered Geometric Models
%% PhD Thesis by Frank C. Langbein%
% Maintainer: Frank C. Langbein <frank@langbein.org>
% Version: 1.0
% Copyright (c) 2003 Frank C Langbein.
% Permission is granted to copy, distribute and/or modify this document
% under the terms of the GNU Free Documentation License, Version 1.2 or
% any later version published by the Free Software Foundation; with no
% Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
% copy of the license is included in the section entitled "GNU Free
% Documentation License".
%


\documentclass[a4paper,oneside,onecolumn,openright,12pt]{book}


\makeatletter
% Copy pasted junk

\newcommand{\green}[1]{\textcolor{green}{#1}}


%Graphics
%\usepackage{graphicx}
% Langbein added these - Tom
\usepackage{caption}
\usepackage{footmisc}
\usepackage{paralist}
\usepackage{array}
\usepackage{epsf}
\usepackage{amsmath}
\usepackage{rotating}
\usepackage{listings}
\usepackage{varwidth}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{afterpage}
\usepackage{subfig}
%\usepackage{algorithm}
%\usepackage{rotating}
%\usepackage{amssymb}
\usepackage{multibib}
\usepackage{booktabs}
% Fonts, encoding, etc.
\usepackage{type1cm}
\usepackage[latin1]{inputenc}
\usepackage[british]{babel}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{longtable}
%\usepackage{algorithm}
\usepackage[noend]{algorithmic}
%\usepackage{rotating}
\usepackage{multirow}
\usepackage{lscape}
\usepackage{float}
%\usepackage{subfigure}
% Line spacing
\def\baselinestretch{1.5}
\parindent0cm
\parskip1.5ex\@plus.7ex\@minus.1ex\relax
\lstset{
	basicstyle=\ttfamily,
	columns=fullflexible,
	keepspaces=true,
}

%Acronyms
 \usepackage{acronym}
 %Inline lists
 \usepackage{paralist}

% Page dimensions
\usepackage{vmargin}
\setpapersize{A4}
\setmarginsrb{25mm}{20mm}{25mm}{10mm}{14.5pt}{8mm}{0pt}{11mm}

% Footer and header
\usepackage{afterpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot{}
\renewcommand{\chaptermark}[1]{}
\renewcommand{\sectionmark}[1]%
             {\markboth{\thesection\ #1}{\thesection\ #1}}
\renewcommand{\subsectionmark}[1]{}
\fancypagestyle{plain}{%
  \fancyhead{}
  \fancyhead[LE,RO]{\thepage}
  \fancyfoot{}
  \renewcommand{\headrulewidth}{.6pt}
}

% Chapter
\def\@makechapterhead#1{%
  \ \\[-35.5pt]\hbox to \textwidth {%
    \hfill {\vbox{\hbox{\rule[5pt]{140pt}{4pt}}%
        \hbox to 140pt {\hfill\huge\bfseries\slshape \@chapapp\space\thechapter\/}}}}%
  \vskip55\p@%
  {\parindent \z@ \raggedright \normalfont%
    \interlinepenalty\@M%
    \Huge \bfseries #1\par\nobreak%
    \vskip 45\p@%
  }}
\def\@chapter[#1]#2{\ifnum \c@secnumdepth >\m@ne
                       \if@mainmatter
                         \refstepcounter{chapter}%
                         \typeout{\@chapapp\space\thechapter.}%
                         \addcontentsline{toc}{chapter}%
                                   {\protect\numberline{\thechapter}#1}%
                       \else
                         \addcontentsline{toc}{chapter}{#1}%
                       \fi
                    \else
                      \addcontentsline{toc}{chapter}{#1}%
                    \fi
                    \chaptermark{#1}%
                    \addtocontents{lof}{\protect\addvspace{10\p@}}%
                    \addtocontents{lot}{\protect\addvspace{10\p@}}%
                    \addtocontents{loa}{\protect\addvspace{10\p@}}%
                    \if@twocolumn
                      \@topnewpage[\@makechapterhead{#2}]%
                    \else
                      \@makechapterhead{#2}%
                      \@afterheading
                    \fi}
\def\@schapter#1{\addcontentsline{toc}{chapter}{#1}%
                 \markboth{#1}{#1}%
                 \addtocontents{lof}{\protect\addvspace{10\p@}}%
                 \addtocontents{lot}{\protect\addvspace{10\p@}}%
                 \addtocontents{loa}{\protect\addvspace{10\p@}}%
                 \if@twocolumn%
                    \@topnewpage[\@makeschapterhead{#1}]%
                 \else%
                    \@makeschapterhead{#1}%
                    \@afterheading%
                 \fi}
\def\@makeschapterhead#1{%
  \ \\[-35.5pt]\hbox to \textwidth {%
    \hfill {\vbox{\hbox{\rule[5pt]{140pt}{0pt}\rule[5pt]{0pt}{4pt}}%
        \hbox to 140pt {\hfill\huge\bfseries\slshape \ \/}}}}%
  \vskip55\p@%
  {\parindent \z@ \raggedright \normalfont%
    \interlinepenalty\@M%
    \Huge \bfseries  #1\par\nobreak%
    \vskip 45\p@%
  }}

% Table of contents
\def\contentsname{Contents}
\renewcommand\tableofcontents{%
    \if@twocolumn%
      \@restonecoltrue\onecolumn%
    \else%
      \@restonecolfalse%
    \fi%
    \chapter*{\contentsname}%
    \@starttoc{toc}%
    \if@restonecol\twocolumn\fi%
    }

% Bibliography
\renewenvironment{thebibliography}[1]
     {\chapter*{\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}

% Floats
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{\textbf{#1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    \textbf{#1: #2.}\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\renewcommand{\topfraction}{0.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.9}

% Tables
\usepackage{dcolumn}
\usepackage{hhline}

% Graphics
\usepackage[dvips]{graphicx}
\usepackage[usenames,dvipsnames]{color}
\usepackage{rotating}
\usepackage{psfrag}
\usepackage{epic}
\usepackage{tabularx}
\usepackage{eepic}

% Algorithm environment
\newcounter{algorithm}[chapter]
\renewcommand{\thealgorithm}{\thechapter.\@arabic\c@algorithm}
\def\fps@algorithm{t}
\def\ftype@algorithm{1}
\def\ext@algorithm{loa}
\def\fnum@algorithm{Algorithm~\thealgorithm}
\newenvironment{algorithm}{\@float{algorithm}}{\end@float}
\newenvironment{algorithm*}{\@dblfloat{algorithm}}{\end@dblfloat}
\newenvironment{alevel}%
   {\begin{list}{}{%
      \setlength{\topsep}{0pt}%
      \setlength{\parskip}{0pt}%
      \setlength{\partopsep}{0pt}%
      \setlength{\parsep}{0pt}%
      \setlength{\itemsep}{0pt}}}%
   {\end{list}}
\newcommand\listalgorithmsname{List of Algorithms}
\newcommand\listofalgorithms{%
    \if@twocolumn
      \@restonecoltrue\onecolumn
    \else
      \@restonecolfalse
    \fi
    \chapter*{\listalgorithmsname}%
      \@mkboth{\listalgorithmsname}{\listalgorithmsname}%
    \@starttoc{loa}%
    \if@restonecol\twocolumn\fi
    }
\newcommand*\l@algorithm{\@dottedtocline{1}{1.5em}{2.3em}}

% List of figures and tables
\renewcommand\listoffigures{%
    \if@twocolumn
      \@restonecoltrue\onecolumn
    \else
      \@restonecolfalse
    \fi
    \chapter*{\listfigurename}%
      \@mkboth{\listfigurename}{\listfigurename}%
    \@starttoc{lof}%
    \if@restonecol\twocolumn\fi
    }
\renewcommand\listoftables{%
    \if@twocolumn
      \@restonecoltrue\onecolumn
    \else
      \@restonecolfalse
    \fi
    \chapter*{\listtablename}%
      \@mkboth{\listtablename}{\listtablename}%
    \@starttoc{lot}%
    \if@restonecol\twocolumn\fi
    }

% Math symbols, fonts, etc.
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\S}{\mathbb{S}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\Cf}{\mathfrak{C}}
\newcommand{\Pf}{\mathfrak{P}}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\avg}{avg}
\DeclareMathOperator{\floor}{floor}
\DeclareMathOperator{\ceil}{ceil}
\DeclareMathOperator{\round}{round}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\absd}[1]{\left\lvert#1\right\rvert}
\providecommand{\card}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

% URLs
\usepackage{url}
%% Define a new 'leo' style for the package that will use a smaller font.
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\footnotesize\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{leo}



\makeatother


\begin{document}


\frontmatter


\begin{titlepage}

\begin{center}
\vspace*{3ex}
\textbf{\Huge Disentangling low-dimensional vector space representations of text documents}\\[2ex]
%\textbf{\Huge Title line 2}\\[12ex]
\textbf{\large A thesis submitted in partial fulfilment}\\[1ex]
\textbf{\large of the requirement for the degree of Doctor of
  Philosophy}\\[16ex]
\textbf{\LARGE Thomas J. Ager}\\
\vfill
\textbf{\LARGE January 2020}\\
\vfill
\textbf{\LARGE Cardiff University}\\[1ex]
\textbf{\LARGE School of Computer Science \& Informatics}\\[4ex]
\end{center}

\end{titlepage}
\newpage\thispagestyle{empty}\cleardoublepage


\thispagestyle{plain}

\vspace*{6ex}

\textbf{\large Declaration}

This work has not previously been accepted in substance for any degree and is not concurrently submitted in candidature for any degree.\\[2ex]
Signed \dotfill \ (candidate) \hspace*{10em}\\[1ex]
Date\ \ \ \ \ \dotfill \hspace*{18em}

\vfill

\textbf{\large Statement 1}

This thesis is being submitted in partial fulfillment of the requirements for the degree of PhD.\\[2ex]
Signed \dotfill \ (candidate) \hspace*{10em}\\[1ex]
Date\ \ \ \ \ \dotfill \hspace*{18em}

\textbf{\large Statement 2}

This thesis is the result of my own independent work/investigation,
except where otherwise stated. Other sources are acknowledged by
explicit references.\\[2ex]
Signed \dotfill \ (candidate) \hspace*{10em}\\[1ex]
Date\ \ \ \ \ \dotfill \hspace*{18em}

\vfill

\textbf{\large Statement 3}

I hereby give consent for my thesis, if accepted, to be available for photocopying and for inter-library loan,
 and for the title and summary to be made available to outside organisations.\\[2ex]
Signed \dotfill \ (candidate) \hspace*{10em}\\[1ex]
Date\ \ \ \ \ \dotfill \hspace*{18em}

\vfill

\cleardoublepage

\thispagestyle{plain}
\ \vfill{\small
Copyright \copyright\ 2011 Name Lastname.\\
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2 or
any later version published by the Free Software Foundation; with no
Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A
copy of the license is included in the section entitled ``GNU Free
Documentation License''.}\\[3.5ex]
A copy of this document in various transparent and opaque
machine-readable formats and related software is available at
\url{http://yourwebsite}.
\cleardoublepage


\ \vspace*{1.11cm}
\markboth{Dedication}{}
\begin{flushright}
\textbf{\large To Steven and my parents.}\\
\textbf{\large for your invaluable and unwavering support.}
\end{flushright}
\newpage
\markboth{}{}
\cleardoublepage


\chapter*{Abstract}

%%%%%%%%%%%%%%%%% OLD WORK

%NESY 
%Vector space embeddings can be used as a tool for learning semantic relationships from unstructured text documents. Among others, earlier work has shown how in a vector space of entities (e.g. different movies) fine-grained semantic relationships can be identified with directions (e.g. more violent than).

%IJCAI 
%In many applications, it is important for classification models to be interpretable. Unfortunately, however, prominent While dimensionality reduction methods could be used in principle, the resulting dimensions would typically not be interpretable. To address this shortcoming, we propose a method to learn interpretable features from a given low-dimensional vector space representation.

%CONLL
%In this paper we consider semantic spaces consisting of objects from some particular domain (e.g.\ IMDB movie reviews). and 

%%%%%%%%%%%%%%%%% OLD WORK


% General problem introduction, why it matters, what past work did, what the results will achieve

% Most vector spaces are entangled

In this thesis vector space document embeddings consisting of entities from some particular domain (e.g.\ IMDB movie reviews) are used. Representations of text documents induced from text data are typically either sparse and separated into clear semantic features (e.g. in the case of a bag-of-words, where each feature is the number of times a word occurs), or they are dense vector space document embeddings where the individual features are not clearly meaningful.  However, despite the individual features of these document embeddings not being meaningful, similarity data  encoded spatially in these document embeddings is meaningful. In work by Derrac \cite{Derrac2015}, it was  observed that such document embeddings often model important features of the domain (e.g.\ how "scary" a movie is, where that movie representation is obtained from text data of its reviews) as directions. By calculating the dot product of  these feature directions,  a value is obtained that corresponds to how  far up entities are on a feature direction. This allows  entities to be ranked according to how much they 'have' the corresponding feature, e.g.\ movies can be ranked on how "scary" they are.

The objective of this thesis is to obtain feature representations from existing document embeddings where each feature is clear and semantic. For example, in a domain of movie reviews, each feature would rank entities correctly on some salient domain concept, e.g.\ "Scary", "Funny", "Romantic". This acts as an unsupervised post-processing step on any existing document embedding. This thesis concerns itself with application of this methodology to a variety of document embeddings, of particular interest is the application to embeddings derived from neural networks. Obtaining these semantic features from neural networks offers a useful contribution to work in improving these models and making them more transparent. These semantic features are beneficial for a variety of purposes, in particular that they provide an inductive bias to classifiers, generalize well to unseen entities, and enable transfer learning. Further, for neural network practitioners, they may provide insight into what the neural network is doing correctly or incorrectly in its learning process. In terms of application, a representation like this can be used in entity oriented search engines, recommendation systems, among others, or perhaps even be used as input to a simple classifier as an alternative to the hard to decipher original neural network. Vector space embeddings have only become more widespread in use, especially as neural network architectures have advanced to  state-of-the-art performance in many tasks. However, semantic features like these are usually obtained by using a specialized learning procedure, e.g.\ by enforcing sparsity constraints or orthogonality between features, and learned from the original text data.

The work by Derrac \cite{Derrac2015} obtained semantic features from Multi-Dimensional Scaling (MDS) document embeddings and validated their work by classifying entities using a rule-based classifier (FOIL), resulting in rules of the form "\textbf{IF} \textit{x} is more scary than most horror films \textbf{THEN} \textit{x} is a horror film.". The work in this thesis is a deeper investigation and improvement of this method. It  experimentally validates that these features are clear and semantic across a variety of unsupervised document embedding models and domains, including from neural network embeddings. Further,  variants of the method are introduced that improve performance. These results are verified on  a document classification task that  classified decision trees with a limited depth. This limitation verifies that the feature representation has captured features that correspond to domain knowledge, as if the classifier can perform well with a low amount of features those features must be semantic. 

As mentioned previously, the method is used on supervised and unsupervised neural network hidden layers. In particular, a qualitative analysis of feed-forward networks and auto-encoders is conducted, finding that feed-forward networks have better feature representations of features relevant to the task, and auto-encoders when stacked (such that the hidden-layer  of one auto-encoder is the input to another) represent increasingly abstract features. After identifying directions that model salient properties of entities in each of these vector spaces, symbolic rules are induced that relate specific properties to more general ones. In addition to introducing variants of the method in \cite{Derrac2015}, this thesis also introduces an additional post-processing step for improving the derived feature representations. Methods for learning document embeddings,  are mostly aimed at modelling similarity. In Chapter \ref{ch5}, it is argued that there is an inherent trade-off between capturing similarity and faithfully modelling features as directions. Following this observation, a simple method to fine-tune existing document embeddings is proposed, with the aim of improving the quality of the feature directions obtained from them. Crucially, this  method is also fully unsupervised, requiring only a bag-of-words representation of the objects as input.  In particular,  first  clusters of terms are identified that refer to salient properties of the considered domain, and a simple neural network model is used to learn a new representation in which each of these properties is faithfully modelled as a direction. As in Chapter \ref {ch3}, the usefulness of this approach is validated using low-depth decision trees, where it is found that in most cases performance improves.

Overall, this thesis offers methods to post-process existing document embeddings in-order to obtain clear semantic features, quantitatively finds that these methods are effective, and qualitatively investigates these features. New variants of an existing method are introduced, and a qualitative investigation of  neural networks using these properties observes characteristics like auto-encoders finding more abstract features the more stacked the representation is. Finally, a new methodology to fine-tune the vector space such that a better feature representation is obtained is introduced and quantitatively validated. 

%Can this method be applied to a variety of representations?
%Can this method be applied to more domains?
%Does this method result in a representation of clear semantic features?
%Can these semantic features be used to form a useful alternative representation?
%Are the features meaningful qualitatively in a variety of domains and for a variety of representations?

%The work in Chapter \ref{ch3} addresses these questions, and introduces variants to their method that result in  better performance on tasks.

%There are a variety of approaches to obtaining vector space representations where features are clear and semantic. 

%simple models such as (small) decision trees cannot readily be applied to text documents due to the size of the underlying feature space. 
 

  





% Disentanglement and why it matters

% Existing disentanglement methods and why they dont fit the gap

% What we actually did to solve this problem 1st chapter

% What the results were 1st chapter

% What we did 2nd

% Results 2nd

% What we did 3rd

% Results 3rd

% Overall conclusions

%Vector space representations of documents can be re-organized  such that their dimensions correspond to clear semantic features. Furthermore, these disentangled representations can be used to provide a useful inductive bias to classifiers, and as the features are disentangled, they can be used to qualitatively investigate the hidden layers of neural networks to gain valuable insights into how they represent documents. 
%\def\baselinestretch{1}\normalfont

\def\baselinestretch{1.5}\normalfont


\chapter*{Acknowledgements}

\tableofcontents

\input{listofpublications.tex}

\listoffigures

\listoftables

\listofalgorithms

\input{acronyms.tex}

\mainmatter
%begin each chapter chapter1.tex by \Chapter{chapter1}, for example, then the sections and so on...
\input{chapter1}
\input{chapter2}
\input{chapter25}
\input{chapter3}
\input{chapter5}
\input{chapter4}
\input{chapter6}
\input{appendix}
\backmatter

\def\baselinestretch{1}\normalfont
\input{fdl.tex}
\def\baselinestretch{1.24}\normalfont

\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}
