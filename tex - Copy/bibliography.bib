@article{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Fine-Tuning Vector Space Representations for Interpretable Text Classification.pdf:pdf},
title = {{Fine-Tuning Vector Space Representations for Interpretable Text Classification}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - vpetite.txt:txt},
title = {vpetite}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - TalesOfTheDervishes (2).pdf.pdf:pdf},
title = {{TalesOfTheDervishes (2).pdf}}
}
@article{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - CM1102 Web Applications Exercise 3 CSS for multi-column layout and drop down menus Objectives of the exercise.pdf:pdf},
pages = {1--2},
title = {{CM1102 Web Applications Exercise 3 : CSS for multi-column layout and drop down menus Objectives of the exercise :}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - S000437020800101X.htm:htm},
title = {{S000437020800101X}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Ganesha.docx:docx},
title = {{Ganesha}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - ScienceDirect - Artificial Intelligence Symbolic knowledge extraction from trained neural networks A sound approach.pdf:pdf},
title = {{ScienceDirect - Artificial Intelligence : Symbolic knowledge extraction from trained neural networks: A sound approach}},
url = {http://www.sciencedirect.com/science/article/pii/S0004370200000771}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 1704.03296.pdf.pdf:pdf},
title = {1704.03296.pdf}
}
@article{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
title = {{No Title}}
}
@article{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Interim Report Student Details Section A Student Self-Assessment A . 1 Thesis Title and Hypothesis A . 2 Overall Pr.pdf:pdf},
title = {{Interim Report Student Details Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis A . 2 Overall Progress}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 1606.03657.pdf.pdf:pdf},
title = {1606.03657.pdf}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - arffs.groovy:groovy},
title = {arffs}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - arffs.groovy:groovy},
title = {arffs}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - self.txt:txt},
title = {self}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Ganesha.docx:docx},
title = {{Ganesha}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Zhou- Extracting symbolic rules from trained neural.url:url},
title = {{Zhou- Extracting symbolic rules from trained neural}}
}
@misc{,
title = {{{\~{}}1245963944HI}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.dropbox:dropbox},
title = {{No Title}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Bull.jar:jar},
title = {{Bull}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 4ae075de7bf9ea2cac01fe02e2920ee5c789.pdf.pdf:pdf},
mendeley-groups = {Report},
title = {4ae075de7bf9ea2cac01fe02e2920ee5c789.pdf}
}
@misc{,
mendeley-groups = {Report},
title = {{2001-Li-ICDM}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Bull.jar:jar},
title = {{Bull}}
}
@misc{,
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The-Ten-Principal-Upanishads.pdf.pdf:pdf},
title = {{The-Ten-Principal-Upanishads.pdf}}
}
@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.1038/nn.3331},
eprint = {1603.04467},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
isbn = {0010-0277},
issn = {0270-6474},
mendeley-groups = {!Paper 3},
pmid = {16411492},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@article{ActuarialSocietyofSouthAfrica2016,
author = {{Actuarial Society of South Africa}},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Actuarial Society of South Africa - 2016 - Student handbook 2016.pdf:pdf},
pages = {1--51},
title = {{Student handbook 2016}},
url = {http://www.actuarialsociety.org.za/Portals/2/Documents/Education Office Documents/2016 Policies/2016 Student Handbook{\_}19022016.pdf},
year = {2016}
}
@article{Ager,
author = {Ager, Thomas},
title = {{Annual Review Year 2 : Obtaining interpretable classifiers from text-based neural networks}}
}
@article{Ager,
author = {Ager, Thomas and Schockaert, Steven},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders.pdf:pdf},
title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Ager2012,
author = {Ager, Thomas and Schockaert, Steven},
file = {:D$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}LabelFix (2).pdf:pdf},
title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
year = {2012}
}
@article{Agerb,
author = {Ager, Thomas and Schockaert, Steven},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders(2).pdf:pdf},
mendeley-groups = {Annotated/Past work,Progress Report},
title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Agera,
author = {Ager, Thomas and Schockaert, Steven},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders.pdf:pdf},
title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Ai2016,
author = {Ai, Qingyao and Yang, Liu and Guo, Jiafeng and Croft, W Bruce},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ai et al. - 2016 - Analysis of the Paragraph Vector Model for Information Retrieval.pdf:pdf},
isbn = {9781450344975},
keywords = {language model,paragraph vector},
mendeley-groups = {Annotated/Document representation},
title = {{Analysis of the Paragraph Vector Model for Information Retrieval}},
year = {2016}
}
@article{Alon2009,
abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Alon, Uri},
journal = {Molecular Cell},
number = {6},
pages = {726--728},
title = {{How To Choose a Good Scientific Problem}},
volume = {35},
year = {2009}
}
@article{Amato2009,
author = {Amato, Claudia and Fanizzi, Nicola and Fazzinga, Bettina},
file = {:C$\backslash$:/Users/Workk/Documents/Combining{\_}Semantic{\_}Web{\_}Search{\_}with{\_}the{\_}Power{\_}of{\_}In.pdf:pdf},
mendeley-groups = {11Thesis/Conceptual Spaces {\&} Properties},
number = {June 2014},
title = {{Combining Semantic Web Search with the Power of Inductive Reasoning . Combining Semantic Web Search with the Power of Inductive Reasoning}},
year = {2009}
}
@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
doi = {1606.06565},
eprint = {1606.06565},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
isbn = {0387310738},
mendeley-groups = {!Paper 3,11Thesis/Interpretability/Safety},
pages = {1--29},
title = {{Concrete Problems in AI Safety}},
url = {http://arxiv.org/abs/1606.06565},
year = {2016}
}
@article{Ananny2016,
abstract = {Models for understanding and holding systems accountable have long rested upon ideals and logics of transparency. Being able to see a system is sometimes equated with being able to know how it works and govern it—a pattern that recurs in recent work about transparency and computational systems. But can “black boxes' ever be opened, and if so, would that ever be sufficient? In this article, we critically interrogate the ideal of transparency, trace some of its roots in scientific and sociotechnical epistemological cultures, and present 10 limitations to its application. We specifically focus on the inadequacy of transparency for understanding and governing algorithmic systems and sketch an alternative typology of algorithmic accountability grounded in constructive engagements with the limitations of transparency ideals.},
author = {Ananny, Mike and Crawford, Kate},
doi = {10.1177/1461444816676645},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ananny, Crawford - 2016 - Seeing without knowing Limitations of the transparency ideal and its application to algorithmic accountability.pdf:pdf},
issn = {1461-4448},
journal = {New Media {\&} Society},
keywords = {accountability,algorithms,critical infrastructure studies,platform governance},
mendeley-groups = {!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
pages = {146144481667664},
title = {{Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability}},
url = {http://journals.sagepub.com/doi/10.1177/1461444816676645},
year = {2016}
}
@article{Andrews1995a,
abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy. {\textcopyright} 1995.},
author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
doi = {10.1016/0950-7051(96)81920-4},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Diederich, Tickle - 1995 - Survey and critique of techniques for extracting rules from trained artificial neural networks.pdf:pdf},
isbn = {09507051 (ISSN)},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {fuzzy neural networks,inferencing,knowledge insertion,rule extraction,rule refinement},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
number = {6},
pages = {373--389},
title = {{Survey and critique of techniques for extracting rules from trained artificial neural networks}},
volume = {8},
year = {1995}
}
@article{Andrews1995,
abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy. {\textcopyright} 1995.},
author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
doi = {10.1016/0950-7051(96)81920-4},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Diederich, Tickle - 1995 - Survey and critique of techniques for extracting rules from trained artificial neural networks.pdf:pdf},
isbn = {09507051 (ISSN)},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {fuzzy neural networks,inferencing,knowledge insertion,rule extraction,rule refinement},
number = {6},
pages = {373--389},
title = {{Survey and critique of techniques for extracting rules from trained artificial neural networks}},
volume = {8},
year = {1995}
}
@article{Andrews2002,
abstract = {This paper describes RULEX, a technique for providing an explanation component for local cluster (LC) neural networks. RULEX extracts symbolic rules from the scikit-learns of a trained LC net. LC nets are a special class of multilayer perceptrons that use sigmoid functions to generate localised functions. LC nets are well suited to both function approximation and discrete classification tasks. The restricted LC net is constrained in such a way that the local functions are 'axis parallel' thus facilitating rule extraction. This paper presents results for the LC net on a wide variety of benchmark problems and shows that RULEX produces comprehensible, accurate rules that exhibit a high degree of fidelity with the LC network from which they were extracted. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {Andrews, Robert and Geva, Shloma},
doi = {10.1016/S0925-2312(01)00577-X},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Geva - 2002 - Rule extraction from local cluster neural nets.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Knowledge extraction,Local response networks,Rule extraction},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Progress Report,Report},
number = {August 2002},
pages = {1--20},
title = {{Rule extraction from local cluster neural nets}},
volume = {47},
year = {2002}
}
@article{Apte1994,
abstract = {We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67{\%} recall/precision breakeven point to 80.5{\%}. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.},
author = {Apt{\'{e}}, Chidanand and Damerau, Fred and Weiss, Sholom M.},
doi = {10.1145/183422.183423},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Apt{\'{e}}, Damerau, Weiss - 1994 - Automated learning of decision rules for text categorization.pdf:pdf},
isbn = {1046-8188},
issn = {1046-8188},
journal = {ACM Trans. Inf. Syst.},
mendeley-groups = {Annotated/Decision Trees},
number = {3},
pages = {233--251},
title = {{Automated learning of decision rules for text categorization}},
url = {http://portal.acm.org/citation.cfm?id=183423{\&}dl=},
volume = {12},
year = {1994}
}

@article{Lewis1995,
	abstract = {Text retrieval systems typically produce a ranking of documents and let a user decide how far down that ranking to go. In contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other IR systems must make decisions without human input or supervision. It is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed. We show how to do this for binary text classification systems, emphasizing that different goals for the system lead to different optimal behaviors. Optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used.},
	author = {Lewis, David D.},
	doi = {10.1145/215206.215366},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/10.1.1.33.867.pdf:pdf},
	isbn = {0897917146},
	issn = {01635840},
	journal = {SIGIR Forum (ACM Special Interest Group on Information Retrieval)},
	mendeley-groups = {11Thesis},
	pages = {246--254},
	title = {{Evaluating and optimizing autonomous text classification systems}},
	year = {1995}
}
@book{BreFriOlsSto84a,
	added-at = {2020-05-07T22:53:11.000+0200},
	address = {Monterey, CA},
	author = {Breiman, L. and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
	biburl = {https://www.bibsonomy.org/bibtex/27f293aa2bdfd10960ef36928f2795f1d/flashspys},
	interhash = {61f3e6d61ba17bb493014bd1c6dfa670},
	intrahash = {7f293aa2bdfd10960ef36928f2795f1d},
	keywords = {ma treelearning},
	publisher = {Wadsworth and Brooks},
	serial = {bre84a},
	timestamp = {2020-05-07T22:53:11.000+0200},
	title = {Classification and Regression Trees},
	year = 1984
}

@inproceedings{Arjovsky2017,
  author    = {Mart{\'{\i}}n Arjovsky and
               L{\'{e}}on Bottou},
  title     = {Towards Principled Methods for Training Generative Adversarial Networks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  crossref  = {DBLP:conf/iclr/2017},
  url       = {https://openreview.net/forum?id=Hk4\_qw5xe},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/iclr/ArjovskyB17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Trifonov2018,
	title = "Learning and Evaluating Sparse Interpretable Sentence Embeddings",
	author = "Trifonov, Valentin  and
	Ganea, Octavian-Eugen  and
	Potapenko, Anna  and
	Hofmann, Thomas",
	booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
	month = nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/W18-5422",
	doi = "10.18653/v1/W18-5422",
	pages = "200--210",
	abstract = "Previous research on word embeddings has shown that sparse representations, which can be either learned on top of existing dense embeddings or obtained through model constraints during training time, have the benefit of increased interpretability properties: to some degree, each dimension can be understood by a human and associated with a recognizable feature in the data. In this paper, we transfer this idea to sentence embeddings and explore several approaches to obtain a sparse representation. We further introduce a novel, quantitative and automated evaluation metric for sentence embedding interpretability, based on topic coherence methods. We observe an increase in interpretability compared to dense models, on a dataset of movie dialogs and on the scene descriptions from the MS COCO dataset.",
}
@article{Arora2012,
abstract = {Topic Modeling is an approach used for automatic comprehension and classification of data in a variety of settings, and perhaps the canonical application is in uncovering thematic structure in a corpus of documents. A number of foundational works both in machine learning and in theory have suggested a probabilistic model for documents, whereby documents arise as a convex combination of (i.e. distribution on) a small number of topic vectors, each topic vector being a distribution on words (i.e. a vector of word-frequencies). Similar models have since been used in a variety of application areas; the Latent Dirichlet Allocation or LDA model of Blei et al. is especially popular. Theoretical studies of topic modeling focus on learning the model's parameters assuming the data is actually generated from it. Existing approaches for the most part rely on Singular Value Decomposition(SVD), and consequently have one of two limitations: these works need to either assume that each document contains only one topic, or else can only recover the span of the topic vectors instead of the topic vectors themselves. This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main tool in this context, which is an analog of SVD where all vectors are nonnegative. Using this tool we give the first polynomial-time algorithm for learning topic models without the above two limitations. The algorithm uses a fairly mild assumption about the underlying topic matrix called separability, which is usually found to hold in real-life data. A compelling feature of our algorithm is that it generalizes to models that incorporate topic-topic correlations, such as the Correlated Topic Model and the Pachinko Allocation Model. We hope that this paper will motivate further theoretical results that use NMF as a replacement for SVD - just as NMF has come to replace SVD in many applications.},
archivePrefix = {arXiv},
arxivId = {1204.1956},
author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur},
doi = {10.1109/FOCS.2012.49},
eprint = {1204.1956},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Ge, Moitra - 2012 - Learning topic models - Going beyond SVD.pdf:pdf},
isbn = {978-0-7695-4874-6},
issn = {02725428},
journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
pages = {1--10},
title = {{Learning topic models - Going beyond SVD}},
year = {2012}
}
@article{Arras2017,
abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
archivePrefix = {arXiv},
arxivId = {1612.07843},
author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1371/journal.pone.0181142},
eprint = {1612.07843},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2017 - What is relevant in a text document An interpretable machine learning approach.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {Annotated/Explanations},
number = {8},
pages = {1--19},
title = {{"What is relevant in a text document?": An interpretable machine learning approach}},
volume = {12},
year = {2017}
}
@article{Arras2016,
abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
archivePrefix = {arXiv},
arxivId = {1612.07843},
author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
eprint = {1612.07843},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2016 - {\&}quotWhat is Relevant in a Text Document{\&}quot An Interpretable Machine Learning Approach.pdf:pdf},
isbn = {1111111111},
mendeley-groups = {Annotated/Explanations,!Paper 3/task/newsgroups},
pages = {1--23},
title = {{"What is Relevant in a Text Document?": An Interpretable Machine Learning Approach}},
url = {http://arxiv.org/abs/1612.07843},
year = {2016}
}
@book{wang2005support,
	title={Support vector machines: theory and applications},
	author={Wang, Lipo},
	volume={177},
	year={2005},
	publisher={Springer Science \& Business Media}
}
@book{gurney1997introduction,
	title={An introduction to neural networks},
	author={Gurney, Kevin},
	year={1997},
	publisher={CRC press}
}

@article{Hofmann1999,
	abstract = {Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain-specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.},
	author = {Hofmann, Thomas},
	doi = {10.1145/312624.312649},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/PLSIHoffman.pdf:pdf},
	isbn = {1581130961},
	journal = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 1999},
	mendeley-groups = {11Thesis},
	pages = {50--57},
	title = {{Probabilistic latent semantic indexing}},
	year = {1999}
}

@article{cavalli1997genes,
	title={Genes, peoples, and languages},
	author={Cavalli-Sforza, L Luca},
	journal={Proceedings of the National Academy of Sciences},
	volume={94},
	number={15},
	pages={7719--7724},
	year={1997},
	publisher={National Acad Sciences}
}

@article{steyvers2006multidimensional,
	title={Multidimensional scaling},
	author={Steyvers, Mark},
	journal={Encyclopedia of cognitive science},
	year={2006},
	publisher={Wiley Online Library}
}
@article{Augasta2012a,
abstract = {Though neural networks have achieved highest classification accuracy for many classification problems, the obtained results may not be interpretable as they are often considered as black box. To overcome this drawback researchers have developed many rule extraction algorithms. This paper has discussed on various rule extraction algorithms based on three different rule extraction approaches namely decompositional, pedagogical and eclectic. Also it evaluates the performance of those approaches by comparing different algorithms with these three approaches on three real datasets namely Wisconsin breast cancer, Pima Indian diabetes and Iris plants. 2012 IEEE.},
author = {Augasta, M Gethsiyal and Kathirvalavakumar, T},
doi = {10.1109/icprime.2012.6208380},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Augasta, Kathirvalavakumar - 2012 - Rule extraction from neural networks - A comparative study.pdf:pdf},
isbn = {9781467310390},
journal = {2012 International Conference on Pattern Recognition, Informatics and Medical Engineering, PRIME 2012, March 21, 2012 - March 23, 2012},
keywords = {Biomedical engineering,Classification (of information),Data mining,Information science,Neural networks,Pattern recognition},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
pages = {404--408},
title = {{Rule extraction from neural networks - A comparative study}},
url = {http://dx.doi.org/10.1109/ICPRIME.2012.6208380},
year = {2012}
}
@article{Author2015a,
archivePrefix = {arXiv},
arxivId = {1506.03694},
author = {Author, First and Author, Second and Author, Third},
eprint = {1506.03694},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Author, Author - 2015 - Learning language through pictures.pdf:pdf},
number = {2013},
pages = {1--2},
title = {{Learning language through pictures}},
url = {http://arxiv.org/pdf/1506.03694v2.pdf},
year = {2015}
}
@article{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
archivePrefix = {arXiv},
arxivId = {1607.06450},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
eprint = {1607.06450},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf:pdf},
isbn = {978-3-642-04273-7},
issn = {1607.06450},
mendeley-groups = {!Paper 3/Training LSTMs},
title = {{Layer Normalization}},
url = {http://arxiv.org/abs/1607.06450},
year = {2016}
}
@article{Bach2015,
abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1371/journal.pone.0130140},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.pdf:pdf},
isbn = {10.1371/journal.pone.0130140},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {Report},
number = {7},
pages = {1--46},
pmid = {26161953},
title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
volume = {10},
year = {2015}
}
@article{Bach2015a,
abstract = {Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally considered `black box' predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher Vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification, (3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data. All experiments are performed on the PASCAL VOC 2007 data set.},
archivePrefix = {arXiv},
arxivId = {1512.00172},
author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
doi = {10.1109/CVPR.2016.318},
eprint = {1512.00172},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - Analyzing Classifiers Fisher Vectors and Deep Neural Networks.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
mendeley-groups = {Annotated/Artifacts in the data},
title = {{Analyzing Classifiers: Fisher Vectors and Deep Neural Networks}},
url = {http://arxiv.org/abs/1512.00172},
year = {2015}
}
@article{Bader2004,
abstract = {Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.},
archivePrefix = {arXiv},
arxivId = {cs/0408069},
author = {Bader, Sebastian and Hitzler, Pascal and Hoelldobler, Steffen},
eprint = {0408069},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bader, Hitzler, Hoelldobler - 2004 - The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challe.pdf:pdf},
journal = {Network},
keywords = {logic programs,neural networks,neural symbolic integra},
mendeley-groups = {Categories/Logic},
pages = {12},
primaryClass = {cs},
title = {{The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challenge for Artificial Intelligence}},
url = {http://arxiv.org/abs/cs/0408069},
year = {2004}
}
@article{Baehrens2010,
abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
archivePrefix = {arXiv},
arxivId = {0912.1128},
author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Muller, Klaus-Robert},
eprint = {0912.1128},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baehrens et al. - 2010 - How to Explain Individual Classification Decisions.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Ames mutagenicity,black box model,explaining,kernel methods,nonlinear},
mendeley-groups = {Report/Explaining predictions},
pages = {1803--1831},
title = {{How to Explain Individual Classification Decisions}},
volume = {11},
year = {2010}
}
@article{Bai2018,
abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.},
archivePrefix = {arXiv},
arxivId = {1803.01271},
author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
eprint = {1803.01271},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Kolter, Koltun - 2018 - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf:pdf},
mendeley-groups = {!Paper 3/Training LSTMs},
title = {{An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling}},
url = {http://arxiv.org/abs/1803.01271},
year = {2018}
}
@article{Bai2004,
author = {Bai, Xue and Padman, Rema and Airoldi, Edoardo},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Padman, Airoldi - 2004 - Sentiment Extraction from Unstructured Text using Tabu Search-Enhanced Markov Blanket.pdf:pdf},
journal = {Proceedings of the International Workshop on Mining for and from the Semantic Web},
keywords = {bayesian models,opinion,semantic learning,sematic orientation,sentiments},
mendeley-groups = {Report},
number = {July},
pages = {24--35},
title = {{Sentiment Extraction from Unstructured Text using Tabu Search-Enhanced Markov Blanket}},
url = {http://ra.adm.cs.cmu.edu/anon/home/ftp/usr/ftp/isri2004/CMU-ISRI-04-127.pdf},
year = {2004}
}
@article{Bandara2017,
abstract = {With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks, and in particular Long Short-Term Memory (LSTM) networks have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context, when trained across all available time series. However, if the time series database is heterogeneous accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model using LSTMs on subgroups of similar time series, which are identified by time series clustering techniques. The proposed methodology is able to consistently outperform the baseline LSTM model, and it achieves competitive results on benchmarking datasets, in particular outperforming all other methods on the CIF2016 dataset.},
archivePrefix = {arXiv},
arxivId = {1710.03222},
author = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
eprint = {1710.03222},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bandara, Bergmeir, Smyl - 2017 - Forecasting Across Time Series Databases using Long Short-Term Memory Networks on Groups of Similar Ser.pdf:pdf},
keywords = {big data forecasting,lstm,neural networks,rnn,time series clustering},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
number = {Ml},
title = {{Forecasting Across Time Series Databases using Long Short-Term Memory Networks on Groups of Similar Series}},
url = {http://arxiv.org/abs/1710.03222},
volume = {1999},
year = {2017}
}
@article{Barnes2017,
abstract = {There has been a good amount of progress in sentiment analysis over the past 10 years, including the proposal of new methods and the creation of benchmark datasets. In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task. Accordingly, it is hard to understand how well a certain model generalizes across different tasks and datasets. In this paper, we contribute to this situation by comparing several models on six different benchmarks, which belong to different domains and additionally have different levels of granularity (binary, 3-class, 4-class and 5-class). We show that Bi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are particularly good at fine-grained sentiment tasks (i. e., with more than two classes). Incorporating sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the SenTube datasets.},
archivePrefix = {arXiv},
arxivId = {1709.04219},
author = {Barnes, Jeremy and Klinger, Roman and im Walde, Sabine Schulte},
eprint = {1709.04219},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnes, Klinger, Walde - 2017 - Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets.pdf:pdf},
mendeley-groups = {!Paper 3/task,!Paper 3/task/Large Movie Review},
pages = {2--12},
title = {{Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets}},
url = {http://arxiv.org/abs/1709.04219},
year = {2017}
}
@article{Barocas2016,
abstract = {Big data claims to be neutral. It isn't.Advocates of algorithmic techniques like data mining argue that they eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data mining can inherit the prejudices of prior decision-makers or reflect the widespread biases that persist in society at large. Often, the “patterns” it discovers are simply preexisting societal patterns of inequality and exclusion. Unthinking reliance on data mining can deny members of vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm's use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Article examines these concerns through the lens of American anti-discrimination law — more particularly, through Title VII's prohibition on discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining's victims would seem to lie in disparate impact doctrine. Case law and the EEOC's Uniform Guidelines, though, hold that a practice can be justified as a business necessity where its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. As a result, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others' discrimination against members of vulnerable groups, or flaws in the underlying data.Addressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, where the discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying anti-discrimination law: nondiscrimination and anti-subordination. Finding a solution to big data's disparate impact will require more than best efforts to stamp out prejudice and bias; it will require wholesale reexamination of the meanings of “discrimination” and “fairness.”},
author = {Barocas, Solon and Selbst, Andrew},
doi = {http://dx.doi.org/10.15779/Z38BG31},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barocas, Selbst - 2016 - Big Data ' s Disparate Impact.pdf:pdf},
issn = {9780262327343},
journal = {California law review},
mendeley-groups = {Annotated/Overarching Interpretability,11Thesis/Interpretability/Discrimination},
number = {1},
pages = {671--729},
title = {{Big Data ' s Disparate Impact}},
url = {https://ssrn.com/abstract=2477899},
volume = {104},
year = {2016}
}
@article{Bastani,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.08504v1},
author = {Bastani, Osbert and Kim, Carolyn},
eprint = {arXiv:1705.08504v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim - Unknown - Interpreting Blackbox Models via Model Extraction.pdf:pdf},
mendeley-groups = {Annotated},
title = {{Interpreting Blackbox Models via Model Extraction}}
}
@article{Bastani2017,
abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
archivePrefix = {arXiv},
arxivId = {1706.09773},
author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
eprint = {1706.09773},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim, Bastani - 2017 - Interpretability via Model Extraction.pdf:pdf},
keywords = {()},
mendeley-groups = {Annotated/Fairness},
title = {{Interpretability via Model Extraction}},
url = {http://arxiv.org/abs/1706.09773},
year = {2017}
}
@article{Bau2017,
abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
archivePrefix = {arXiv},
arxivId = {1704.05796},
author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/CVPR.2017.354},
eprint = {1704.05796},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1704.05796.pdf:pdf},
isbn = {9781538604571},
issn = {1530-1567},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Representations},
pages = {3319--3327},
pmid = {12882599},
title = {{Network dissection: Quantifying interpretability of deep visual representations}},
volume = {2017-Janua},
year = {2017}
}
@article{Bechavod2017,
abstract = {We present a regularization-inspired approach for reducing bias in learned classifiers. In particular, we focus on binary classification tasks over individuals from two populations, where, as our criterion for fairness, we wish to achieve similar false positive rates in both populations, and similar false negative rates in both populations. As a proof of concept, we implement our approach and empirically evaluate its ability to achieve both fairness and accuracy, using the COMPAS scores data for prediction of recidivism.},
archivePrefix = {arXiv},
arxivId = {1707.00044},
author = {Bechavod, Yahav and Ligett, Katrina},
eprint = {1707.00044},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechavod, Ligett - 2017 - Learning Fair Classifiers A Regularization-Inspired Approach.pdf:pdf},
mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
pages = {6--10},
title = {{Learning Fair Classifiers: A Regularization-Inspired Approach}},
url = {http://arxiv.org/abs/1707.00044},
year = {2017}
}
@article{Bechberger,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.04825v1},
author = {Bechberger, Lucas},
eprint = {arXiv:1706.04825v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechberger - Unknown - Neural Representations.pdf:pdf},
mendeley-groups = {Annotated/Conceptual Spaces and Neural Networks,Annotated/Generative Adversarial Nets},
title = {{Neural Representations}}
}
@article{Beltagy2013,
author = {Beltagy, Islam and Chau, Cuong and Boleda, Gemma and Garrette, Dan and Erk, Katrin},
file = {:C$\backslash$:/Users/Workk/Documents/S13-1002.pdf:pdf},
mendeley-groups = {11Thesis/Conceptual Spaces {\&} Properties},
pages = {11--21},
title = {{Montague Meets Markov : Deep Semantics with Probabilistic Logical Form}},
volume = {1},
year = {2013}
}

@article{Bengio2009,
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  added-at = {2018-02-10T18:29:43.000+0100},
  author = {Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/24ac7fbfc4e40b606733ec3d33b6c9e26/flint63},
  description = {Book ISBN 978-1-60198-294-0},
  doi = {10.1561/2200000006},
  file = {Journal Issue:2009/Bengio09.pdf:PDF},
  groups = {public},
  interhash = {30174ec5e2667a039cdc30c5d359dc47},
  intrahash = {4ac7fbfc4e40b606733ec3d33b6c9e26},
  issn = {1935-8237},
  journal = {Foundations and Trends in Machine Learning},
  keywords = {01801 paper numerical ai data pattern recognition analysis learn algorithm},
  number = 1,
  pages = {1--127},
  timestamp = {2018-04-16T12:36:58.000+0200},
  title = {Learning Deep Architectures for {AI}},
  username = {flint63},
  volume = 2,
  year = 2009
}




@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2013 - Representation learning A review and new perspectives.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
mendeley-groups = {Papers/Paper 1,Report},
number = {8},
pages = {1798--1828},
pmid = {23787338},
title = {{Representation learning: A review and new perspectives}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23459267{\%}5Cnhttp://arxiv.org/abs/1206.5538{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6472238},
volume = {35},
year = {2013}
}
@article{Bengio2012,
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2012 - Representation Learning A Review and New Perspectives.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
mendeley-groups = {Annotated/Representation Learning,Report/Features},
number = {8},
pages = {1798--1828},
pmid = {23787338},
title = {{Representation Learning: A Review and New Perspectives}},
volume = {35},
year = {2012}
}
@article{Benitez1997,
abstract = {Artificial neural networks are efficient computing models which have shown their strengths in solving hard problems in artificial intelligence. They have also been shown to be universal approximators. Notwithstanding, one of the major criticisms is their being black boxes, since no satisfactory explanation of their behavior has been offered. In this paper, we provide such an interpretation of neural networks so that they will no longer be seen as black boxes. This is stated after establishing the equality between a certain class of neural nets and fuzzy rule-based systems. This interpretation is built with fuzzy rules using a new fuzzy logic operator which is defined after introducing the concept of f-duality. In addition, this interpretation offers an automated knowledge acquisition procedure.},
author = {Ben{\'{i}}tez, J. M. and Castro, J. L. and Requena, I.},
doi = {10.1109/72.623216},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben{\'{i}}tez, Castro, Requena - 1997 - Are artificial neural networks black boxes.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Equality between neural nets and fuzzy rule-based,Fuzzy additive systems,Interpretation of neural nets,f-duality,i-or operator},
number = {5},
pages = {1156--1164},
pmid = {18255717},
title = {{Are artificial neural networks black boxes?}},
volume = {8},
year = {1997}
}
@article{Bergera,
author = {Berger, Mark J},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - Unknown - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
mendeley-groups = {Interim Review},
pages = {1--8},
title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}}
}
@article{Berger,
author = {Berger, Mark J},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - Unknown - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
pages = {1--8},
title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}}
}
@article{Besana1991,
abstract = {Forty-six heroin abusers were hospitalized and treated with meperidine either alone or in association with clonidine. Meperidine was given orally in rapidly decreasing doses according to three different schedules. The majority of patients (87{\%}) successfully completed the detoxification program. The best meperidine starting posology was 200 mg four times daily, which allowed stoppage of the opioid treatment after gradual reduction of the daily dose in a mean time of 9.5 days. Association with clonidine was not proven to be useful. This study shows that meperidine can be effectively used in rapidly decreasing doses in the pharmacological detoxification treatment of hospitalized heroin addicts.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Besana, Carlo and Memoli, Massimo and Salvioni, Piero M. and Finazzi, Renato A. and Inversi, Felice and Rugarli, Claudio},
doi = {10.3109/10826089109058901},
eprint = {1301.3781},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/N13-1090.pdf:pdf},
isbn = {0020-773X (Print)$\backslash$r0020-773X (Linking)},
issn = {10826084},
journal = {Substance Use and Misuse},
keywords = {Clonidine,Detoxification,Drug addiction,Heroin misuse,Meperidine},
number = {5},
pages = {505--513},
pmid = {1938007},
title = {{Meperidine in detoxification of hospitalized heroin addicts}},
url = {http://research.microsoft.com/en-},
volume = {26},
year = {1991}
}
@article{Bharti2015,
author = {Bharti, Kusum Kumari and Singh, Pramod Kumar},
doi = {10.1016/j.eswa.2014.11.038},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bharti, Singh - 2015 - Expert Systems with Applications Hybrid dimension reduction by integrating feature selection with feature extract.pdf:pdf},
issn = {0957-4174},
journal = {Expert Systems With Applications},
mendeley-groups = {Report/Features},
number = {6},
pages = {3105--3114},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications Hybrid dimension reduction by integrating feature selection with feature extraction method for text clustering}},
url = {http://dx.doi.org/10.1016/j.eswa.2014.11.038},
volume = {42},
year = {2015}
}
@article{Bhatia2015,
abstract = {Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that rescikit-learning discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classification-based methods.},
archivePrefix = {arXiv},
arxivId = {1509.01599},
author = {Bhatia, Parminder and Ji, Yangfeng and Eisenstein, Jacob},
eprint = {1509.01599},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatia, Ji, Eisenstein - 2015 - Better Document-level Sentiment Analysis from RST Discourse Parsing.pdf:pdf},
isbn = {9781941643327},
mendeley-groups = {!Paper 3/Structured LSTMs,!Paper 3/task/Sentiment treebank},
title = {{Better Document-level Sentiment Analysis from RST Discourse Parsing}},
url = {http://arxiv.org/abs/1509.01599},
year = {2015}
}
@article{Binns2017,
abstract = {The internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and are subject to the idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic content moderation systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants.},
archivePrefix = {arXiv},
arxivId = {1707.01477},
author = {Binns, Reuben and Veale, Michael and {Van Kleek}, Max and Shadbolt, Nigel},
doi = {10.1007/978-3-319-67256-4_32},
eprint = {1707.01477},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Binns et al. - 2017 - Like trainer, like bot Inheritance of bias in algorithmic content moderation.pdf:pdf},
isbn = {9783319672557},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Algorithmic accountability,Discussion platforms,Freedom of speech,Machine learning,Online abuse},
mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
pages = {405--415},
title = {{Like trainer, like bot? Inheritance of bias in algorithmic content moderation}},
volume = {10540 LNCS},
year = {2017}
}
@book{Bishop2006a,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M CM Christopher M.},
booktitle = {Pattern Recognition},
chapter = {Graphical},
doi = {10.1117/1.2819119},
editor = {Jordan, M and Kleinberg, J and Sch{\"{o}}lkopf, B},
eprint = {0-387-31073-8},
isbn = {978-0387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf{\%}5Cnhttp://soic.iupui.edu/syllabi/semesters/4142/INFO{\_}B529{\_}Liu{\_}s.pdf{\%}5Cnhttp://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Blei2006,
abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.1486v1},
author = {Blei, David M. and Lafferty, John D.},
doi = {10.1145/1143844.1143859},
eprint = {arXiv:0712.1486v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, Lafferty - 2006 - Correlated Topic Models.pdf:pdf},
isbn = {1595933832},
issn = {19326157},
journal = {Advances in Neural Information Processing Systems 18},
mendeley-groups = {Annotated/Topic models},
pages = {147--154},
pmid = {9013932},
title = {{Correlated Topic Models}},
url = {papers2://publication/uuid/1191CDB8-6BB3-4201-8EFB-6F7B8CBA0E8F},
year = {2006}
}
@article{Blei2010,
abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
archivePrefix = {arXiv},
arxivId = {1003.0783},
author = {Blei, David M. and McAuliffe, Jon D.},
doi = {10.1002/asmb.540},
eprint = {1003.0783},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, McAuliffe - 2010 - Supervised Topic Models.pdf:pdf},
isbn = {160560352X},
issn = {15241904},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
pages = {1--8},
title = {{Supervised Topic Models}},
url = {http://arxiv.org/abs/1003.0783},
year = {2010}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@article{Blitzer2007,
author = {Blitzer, J and Dredze, M and Pereira, F},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blitzer, Dredze, Pereira - 2007 - Biographies, Bollywood, boom-boxes and blenders Domain adaptation for sentiment classification.pdf:pdf},
journal = {Proc. Assoc. Comput. Linguist. (ACL},
title = {{Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification}},
year = {2007}
}
@article{Blodgett2017,
abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
archivePrefix = {arXiv},
arxivId = {1707.00061},
author = {Blodgett, Su Lin and O'Connor, Brendan},
eprint = {1707.00061},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blodgett, O'Connor - 2017 - Racial Disparity in Natural Language Processing A Case Study of Social Media African-American English.pdf:pdf},
mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
title = {{Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English}},
url = {http://arxiv.org/abs/1707.00061},
year = {2017}
}
@article{Bloehdorn2006,
abstract = {Recent work has shown improvements in text clustering and classification tasks by integrating conceptual features extracted from ontologies. In this paper we present text mining experiments in the medical domain in which the ontological structures used are acquired automatically in an unsupervised learning process from the text corpus in question. We compare results obtained using the automatically learned ontologies with those obtained using manually engineered ones. Our results show that both types of ontologies improve results on text clustering and classification tasks, whereby the automatically acquired ontologies yield a improvement competitive with the manually engineered ones.$\backslash$nER -},
author = {Bloehdorn, S and Cimiano, P and Hotho, A},
doi = {10.1007/3-540-31314-1_40},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bloehdorn, Cimiano, Hotho - 2006 - Learning Ontologies to Improve Text Clustering and Classification.pdf:pdf},
isbn = {9783540313137},
journal = {From Data and Information Analysis to Knowledge Engineering},
number = {2005},
pages = {334--341},
title = {{Learning Ontologies to Improve Text Clustering and Classification}},
year = {2006}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the scikit-learns of a neural network, called Bayes by Backprop. It regularises the scikit-learns by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the scikit-learns can be used to improve generalisation in non-linear regression problems, and how this scikit-learn uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blundell et al. - 2015 - scikit-learn Uncertainty in Neural Networks.pdf:pdf},
isbn = {9781510810587},
mendeley-groups = {!Paper 3/Bayesian Networks},
title = {{scikit-learn Uncertainty in Neural Networks}},
url = {http://arxiv.org/abs/1505.05424},
volume = {37},
year = {2015}
}
@misc{Bologna2004,
abstract = {Although many authors generated comprehensible models from individual networks, much less work has been done in the explanation of ensembles. DIMLP is a special neural network model from which rules are generated at the level of a single network and also at the level of an ensemble of networks. We applied ensembles of 25 DIMLP networks to several datasets of the public domain and a classification problem related to post-translational modifications of proteins. For the classification problems of the public domain, the average predictive accuracy of rulesets extracted from ensembles of neural networks was significantly better than the average predictive accuracy of rulesets generated from ensembles of decision trees. By varying the architectures of DIMLP networks we found that the average predictive accuracy of rules, as well as their complexity were quite stable. The comparison to other rule extraction techniques applied to neural networks showed that rules generated from DIMLP ensembles gave very good results. In the last problem related to bioinformatics, the best result obtained by ensembles of DIMLP networks was also significantly better than the best result obtained by ensembles of decision trees. Thus, although neural networks take much longer to train than decision trees and also rules are generated at a greater computational cost (however, still polynomial), at least for several classification problems it was worth using neural network ensembles, as extracted rules were more accurate, on average. The DIMLP software is available for PC-Linux under http://us.expasy.org/people/Guido.Bologna.html ?? 2004 Elsevier B.V. All rights reserved.},
author = {Bologna, Guido},
booktitle = {Journal of Applied Logic},
doi = {10.1016/j.jal.2004.03.004},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna - 2004 - Is it worth generating rules from neural network ensembles.htm:htm},
isbn = {1570-8683},
issn = {15708683},
keywords = {Decision trees,Ensembles,Neural networks,Proteins,Rule extraction},
number = {3},
pages = {325--348},
title = {{Is it worth generating rules from neural network ensembles?}},
volume = {2},
year = {2004}
}
@article{Bologna2000,
author = {Bologna, Guido},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna - 2000 - Rule Extraction from a Multi layer Perceptron with Staircase Activation Functions The DIMLP model.pdf:pdf},
mendeley-groups = {Progress Report},
pages = {0--5},
title = {{Rule Extraction from a Multi layer Perceptron with Staircase Activation Functions The DIMLP model}},
year = {2000}
}
@article{Bologna,
author = {Bologna, Guido and Pellegrini, Christian},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna, Pellegrini - Unknown - Accurate Decomposition of Standard MLP Classification Responses into Symbolic Rules.pdf:pdf},
mendeley-groups = {Progress Report},
title = {{Accurate Decomposition of Standard MLP Classification Responses into Symbolic Rules}}
}
@article{Bordes2012,
abstract = {Open-text (or open-domain) semantic parsers are designed to interpret any state- ment in natural language by inferring a corresponding meaning representation (MR). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose here a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words, which are mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (WordNet and ConceptNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and word-sense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.},
author = {Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio, Yoshua},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes et al. - 2012 - Joint learning of words and meaning representations for open-text semantic parsing.pdf:pdf},
issn = {15337928},
journal = {International {\ldots}},
mendeley-groups = {Progress Report},
pages = {127--135},
title = {{Joint learning of words and meaning representations for open-text semantic parsing}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}BordesGWB12.pdf},
volume = {22},
year = {2012}
}
@article{Bostrom2011,
author = {Bostrom, Nick},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/artificialintelligence.pdf:pdf},
keywords = {artificial intelligence, ethics},
mendeley-groups = {11Thesis/Interpretability/Discrimination},
pages = {1--20},
title = {{The Ethics of Artificial Intelligence}},
year = {2011}
}
@article{Bottou2012a,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, Leon},
doi = {10.1007/978-3-642-35289-8_25},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {2045-2322},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {421--436},
pmid = {25382349},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
volume = {1},
year = {2012}
}
@article{Boureau2010,
abstract = {Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter responses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be broken down into two steps: (1) a coding step, which performs a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pooling step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pooling schemes have been proposed in the literature. The goal of this paper is threefold. We seek to establish the relative importance of each step of mid-level feature extraction through a comprehensive cross evaluation of several types of coding modules (hard and soft vector quantization, sparse coding) and pooling schemes (by taking the average, or the maximum), which obtains state-of-the-art performance or better on several recognition benchmarks. We show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding. We provide theoretical and empirical insight into the remarkable performance of max pooling. By teasing apart components shared by modern mid-level feature extractors, our approach aims to facilitate the design of better recognition architectures.},
author = {Boureau, Y. Lan and Bach, Francis and LeCun, Yann and Ponce, Jean},
doi = {10.1109/CVPR.2010.5539963},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boureau et al. - 2010 - Learning mid-level features for recognition.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Report/Features,Progress Report},
pages = {2559--2566},
title = {{Learning mid-level features for recognition}},
year = {2010}
}
@article{Bowman2015,
abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
eprint = {1511.06349},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:pdf},
mendeley-groups = {Annotated/Generative Adversarial Nets},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2015}
}
@article{Boz,
author = {Boz, Olcay and Ave, Laurel},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boz, Ave - Unknown - Decision Tree DecText - Decision Tree Extractor.pdf:pdf},
mendeley-groups = {Annotated/Decision Trees},
pages = {1--7},
title = {{Decision Tree DecText - Decision Tree Extractor}}
}
@article{Burges1998a,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, C.J.C. J Christopher J C},
doi = {10.1023/A:1009715923555},
eprint = {1111.6189v1},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
title = {{A tutorial on support vector machines for pattern recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf},
volume = {2},
year = {1998}
}
@article{Burges2005a,
author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
doi = {10.1145/1102351.1102363},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:pdf},
isbn = {1595931805},
issn = {00243205},
journal = {Icml 2005},
keywords = {Learning to Rank,RankNet},
mendeley-groups = {Progress Report},
pages = {89--96},
pmid = {16483612},
title = {{Learning to rank using gradient descent}},
year = {2005}
}
@article{Burges2005,
author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
doi = {10.1145/1102351.1102363},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:pdf},
isbn = {1595931805},
issn = {00243205},
journal = {Icml 2005},
keywords = {Learning to Rank,RankNet},
mendeley-groups = {Progress Report},
pages = {89--96},
pmid = {16483612},
title = {{Learning to rank using gradient descent}},
year = {2005}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
eprint = {1111.6189v1},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf{\%}5Cnhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@misc{Cambria2013a,
abstract = {The way people express their opinions has radically changed in the past few years thanks to the advent of online collaborative media. The distillation of knowledge from this huge amount of unstructured information can be a key factor for marketers who want to create an identity for their product or brand in the minds of their customers. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. Existing approaches to opinion mining, in fact, are still far from being able to infer the cognitive and affective information associated with natural language as they mainly rely on knowledge bases that are too limited to efficiently process text at concept-level. In this context, standard clustering techniques have been previously employed on an affective common-sense knowledge base in attempt to discover how different natural language concepts are semantically and affectively related to each other and, hence, to accordingly mine on-line opinions. In this work, a novel cognitive model based on the combined use of multi-dimensional scaling and artificial neural networks is exploited for better modelling the way multi-word expressions are organised in a brain-like universe of natural language concepts. The integration of a biologically inspired paradigm with standard principal component analysis helps to better grasp the non-linearities of the resulting vector space and, hence, improve the affective common-sense reasoning capabilities of the system. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Cambria, Erik and Mazzocco, Thomas and Hussain, Amir},
booktitle = {Biologically Inspired Cognitive Architectures},
doi = {10.1016/j.bica.2013.02.003},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cambria, Mazzocco, Hussain - 2013 - Application of multi-dimensional scaling and artificial neural networks for biologically inspired op.htm:htm},
isbn = {1467351644},
issn = {2212683X},
keywords = {AI,ANN,Cognitive modelling,NLP,Sentic computing},
pages = {41--53},
title = {{Application of multi-dimensional scaling and artificial neural networks for biologically inspired opinion mining}},
volume = {4},
year = {2013}
}
@article{Cambria2013,
abstract = {The way people express their opinions has radically changed in the past few years thanks to the advent of online collaborative media. The distillation of knowledge from this huge amount of unstructured information can be a key factor for marketers who want to create an identity for their product or brand in the minds of their customers. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. Existing approaches to opinion mining, in fact, are still far from being able to infer the cognitive and affective information associated with natural language as they mainly rely on knowledge bases that are too limited to efficiently process text at concept-level. In this context, standard clustering techniques have been previously employed on an affective common-sense knowledge base in attempt to discover how different natural language concepts are semantically and affectively related to each other and, hence, to accordingly mine on-line opinions. In this work, a novel cognitive model based on the combined use of multi-dimensional scaling and artificial neural networks is exploited for better modelling the way multi-word expressions are organised in a brain-like universe of natural language concepts. The integration of a biologically inspired paradigm with standard principal component analysis helps to better grasp the non-linearities of the resulting vector space and, hence, improve the affective common-sense reasoning capabilities of the system. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Cambria, Erik and Mazzocco, Thomas and Hussain, Amir},
doi = {10.1016/j.bica.2013.02.003},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cambria, Mazzocco, Hussain - 2013 - Application of multi-dimensional scaling and artificial neural networks for biologically inspired op.pdf:pdf},
isbn = {1467351644},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
keywords = {AI,ANN,Cognitive modelling,NLP,Sentic computing},
pages = {41--53},
publisher = {Elsevier B.V.},
title = {{Application of multi-dimensional scaling and artificial neural networks for biologically inspired opinion mining}},
url = {http://dx.doi.org/10.1016/j.bica.2013.02.003},
volume = {4},
year = {2013}
}
@article{Cao2015,
author = {Cao, Tru and Lim, Ee Peng and Zhou, Zhi Hua and Ho, Tu Bao and Cheung, David and Motoda, Hiroshi},
doi = {10.1007/978-3-319-18038-0},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2015 - Advances in knowledge discovery and data mining 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam,.pdf:pdf},
isbn = {9783319180373},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Annotated/Document representation},
pages = {212--225},
title = {{Advances in knowledge discovery and data mining: 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam, May 19-22, 2015 proceedings, part I}},
volume = {9077},
year = {2015}
}
@misc{Cao2015,
abstract = {We develop a Ranking framework upon Recursive Neural Networks (R2N2) to rank sentences for multi-document sum- marization. It formulates the sentence ranking task as a hi- erarchical regression process, which simultaneously mea- sures the salience of a sentence and its constituents (e.g., phrases) in the parsing tree. This enables us to draw on word-level to sentence-level supervisions derived from refer- ence summaries. In addition, recursive neural networks are used to automatically learn ranking features over the tree, with hand-crafted feature vectors of words as inputs. Hier- archical regressions are then conducted with learned features concatenating raw features. Ranking scores of sentences and words are utilized to effectively select informative and non- redundant sentences to generate summaries. Experiments on the DUC 2001, 2002 and 2004 multi-document summariza- tion datasets show that R2N2 outperforms state-of-the-art ex- tractive summarization approaches. Introduction},
archivePrefix = {arXiv},
arxivId = {1509.00685},
author = {Cao, Ziqiang and Wei, Furu and Dong, Li and Li, Sujian and Zhou, Ming},
booktitle = {Aaai},
doi = {10.1162/153244303322533223},
eprint = {1509.00685},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2015 - Ranking with recursive neural networks and its application to multi-document summarization.pdf:pdf},
isbn = {9781577357018},
issn = {19909772},
keywords = {NLP and Knowledge Representation Track},
mendeley-groups = {Progress Report},
pages = {2153--2159},
pmid = {18244602},
title = {{Ranking with recursive neural networks and its application to multi-document summarization}},
year = {2015}
}
@book{Carroll1976,
abstract = {The authors' general objective was to demonstrate that three affective dimensions of meaning -- Evaluation, Potency, and Activity (E-P-A) -- are in fact pancultural. the study was done by 80 researchers in 20 different countries.},
author = {Carroll, John B},
booktitle = {The American Journal of Psychology},
isbn = {9780252004261},
keywords = {ACTIVITY,Dimension,affect,evaluation,meaning,potency,semantics,space,universals,word class},
number = {1},
pages = {172--178},
title = {{Cross-Cultural Universals of Affective Meaning}},
volume = {89},
year = {1976}
}
@article{Caruana2015,
abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
doi = {10.1145/2783258.2788613},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caruana et al. - 2015 - Intelligible Models for HealthCare.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
keywords = {additive models,classification,healthcare,intelligibility,interaction detection,logistic regression,risk prediction},
mendeley-groups = {Report/Explaining predictions},
pages = {1721--1730},
title = {{Intelligible Models for HealthCare}},
url = {http://dl.acm.org/citation.cfm?id=2783258.2788613},
year = {2015}
}
@article{Chandrashekar2014,
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrashekar, Sahin - 2014 - A survey on feature selection methods q.pdf:pdf},
issn = {0045-7906},
journal = {Computers and Electrical Engineering},
mendeley-groups = {Report/Features,Report},
number = {1},
pages = {16--28},
publisher = {Elsevier Ltd},
title = {{A survey on feature selection methods q}},
url = {http://dx.doi.org/10.1016/j.compeleceng.2013.11.024},
volume = {40},
year = {2014}
}
@article{Chaney2012,
abstract = {Managing large collections of documents is an important$\backslash$nproblem for many areas of science, industry, and$\backslash$nculture. Probabilistic topic modeling offers a promising$\backslash$nsolution. Topic modeling is an unsupervised machine$\backslash$nlearning method that learns the underlying themes in$\backslash$na large collection of otherwise unorganized documents.$\backslash$nThis discovered structure summarizes and organizes the$\backslash$ndocuments. However, topic models are high-level statistical$\backslash$ntools—a user must scrutinize numerical distributions$\backslash$nto understand and explore their results. In this$\backslash$npaper, we present a method for visualizing topic models.$\backslash$nOur method creates a navigator of the documents,$\backslash$nallowing users to explore the hidden structure that a$\backslash$ntopic model discovers. These browsing interfaces reveal$\backslash$nmeaningful patterns in a collection, helping end-users$\backslash$nexplore and understand its contents in new ways. We$\backslash$nprovide open source software of our method.},
author = {Chaney, Ajb and Blei, Dm},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaney, Blei - 2012 - Visualizing Topic Models.pdf:pdf},
isbn = {9781577355564},
journal = {Icwsm},
pages = {419--422},
title = {{Visualizing Topic Models.}},
year = {2012}
}
@article{Chang2009,
abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
doi = {10.1.1.100.1089},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Models.pdf:pdf},
isbn = {9781615679119},
issn = {1098-6596},
journal = {Advances in Neural Information Processing Systems 22},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
pages = {288----296},
pmid = {25246403},
title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
year = {2009}
}
@article{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. A particularly significant one is data augmentation, which achieves a boost in performance in shallow methods analogous to that observed with CNN-based methods. Finally, we are planning to provide the configurations and code that achieve the state-of-the-art performance on the PASCAL VOC Classification challenge, along with alternative configurations trading-off performance, computation speed and compactness.},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.5244/C.28.6},
eprint = {1405.3531},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatfield et al. - 2014 - Return of the Devil in the Details Delving Deep into Convolutional Nets.pdf:pdf},
isbn = {1-901725-52-9},
issn = {1-901725-52-9},
journal = {arXiv preprint arXiv: {\ldots}},
mendeley-groups = {Progress Report,Interim Review},
pages = {1--11},
title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
url = {http://arxiv.org/abs/1405.3531},
year = {2014}
}
@article{Chemiavsky1991a,
abstract = {properties for software complexity measures are explored. It is shown that a collection of properties suggested by Weyuker are inadequate for determining the quality of a software complexity measure},
author = {Chemiavsky, John C and Smith, Carl H},
doi = {10.1109/32.106988},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chemiavsky, Smith - 1991 - Concise Papers.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {Foundations},
number = {9144263},
pages = {636--638},
pmid = {158},
title = {{Concise Papers}},
volume = {17},
year = {1991}
}
@inproceedings{Chen2014b,
abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Chen, Danqi and Manning, Christopher},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1082},
eprint = {1512.00567},
isbn = {9781937284961},
issn = {9781937284961},
mendeley-groups = {Report/Features},
pages = {740--750},
pmid = {8190083},
title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
url = {http://aclweb.org/anthology/D14-1082},
year = {2014}
}
@article{Chen,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3618v2},
author = {Chen, Danqi and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
eprint = {arXiv:1301.3618v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - Neural Tensor Networks and Semantic Word Vectors.pdf:pdf},
mendeley-groups = {Annotated/Representation Learning},
pages = {1--4},
title = {{Neural Tensor Networks and Semantic Word Vectors}}
}
@article{Chen2014c,
abstract = {We have provided a model and framework as a foundation for transparent interfaces via our Situation Awareness-based Agent Transparency (SAT) model. In this report we discuss the implications of agent transparency for operator trust and workload; we also review potential user interface designs (information visualization and displaying uncertainty information) to support agent transparency. Finally, we provide examples of transparent interface design efforts currently ongoing at the U.S. Army Research Laboratory's Human Research and Engineering Directorate under the Autonomy Research Pilot Initiative.},
author = {Chen, Jessie Y. C. and Procci, Katelyn and Boyce, Michael and Wright, Julia and Garcia, Andre and Barnes, Michael J.},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Situation Awareness-Based Agent Transparency.pdf:pdf},
isbn = {ARL-TR-6905},
journal = {US Army Research Laboratory},
keywords = {autonomous systems,human-robot interaction,situation awareness (SA),transparency,trust},
mendeley-groups = {Annotated/Overarching Interpretability},
number = {April},
pages = {1--29},
title = {{Situation Awareness-Based Agent Transparency}},
year = {2014}
}
@article{Chen2014a,
abstract = {Abstract Denoising auto - encoders (DAEs) have been successfully used to learn new representations for a wide range of machine learning tasks. During training, DAEs make many passes over the training dataset and reconstruct it from partial corruption generated ... $\backslash$n},
author = {Chen, M and Weinberger, K and Sha, F and Bengio, Y},
doi = {10.1007/s11222-007-9033-z},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Marginalized Denoising Auto-encoders for Nonlinear Representations.pdf:pdf},
isbn = {9781634393973},
issn = {0960-3174},
journal = {Proceedings of The 31st {\ldots}},
keywords = {ICML2014},
title = {{Marginalized Denoising Auto-encoders for Nonlinear Representations}},
url = {http://jmlr.org/proceedings/papers/v32/cheng14.pdf{\%}5Cnpapers3://publication/uuid/B8D413D0-9096-41DC-A234-EBFC9C94CBAB},
volume = {32},
year = {2014}
}
@article{Chen2012a,
author = {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian and Sha, Fei},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2012 - Marginalized denoising autoencoders for domain adaptation.pdf:pdf},
journal = {arXiv preprint arXiv:1206.4683},
keywords = {boring formatting information, machine learning, I},
title = {{Marginalized denoising autoencoders for domain adaptation}},
year = {2012}
}
@article{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1606.03657},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
issn = {10495258},
mendeley-groups = {Annotated/Generative Adversarial Nets,Report},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@inproceedings{Chen2014,
abstract = {Distributional semantics and frame semantics are two representative views on language understanding in the statistical world and the linguistic world, respectively. In this paper, we combine the best of two worlds to automatically induce the semantic slots for spoken dialogue systems. Given a collection of unlabeled audio files, we exploit continuous-valued word embeddings to augment a probabilistic frame-semantic parser that identifies key semantic slots in an unsupervised fashion. In experiments, our results on a real-world spoken dialogue dataset show that the distributional word representations significantly improve the adaptation of FrameNet-style parses of ASR decodings to the target semantic space; that comparing to a state-of-the-art baseline, a 13{\%} relative average precision improvement is achieved by leveraging word vectors trained on two 100-billion words datasets; and that the proposed technology can be used to reduce the costs for designing task-oriented spoken dialogue systems.},
author = {Chen, Yun Nung and Wang, William Yang and Rudnicky, Alexander I.},
booktitle = {2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings},
doi = {10.1109/SLT.2014.7078639},
isbn = {9781479971299},
keywords = {Distributional semantics,Frame semantics,Unsupervised slot induction},
mendeley-groups = {Report/Features},
pages = {584--589},
title = {{Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems}},
year = {2014}
}
@article{Cheng2016,
abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
archivePrefix = {arXiv},
arxivId = {1601.06733},
author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
eprint = {1601.06733},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Dong, Lapata - 2016 - Long Short-Term Memory-Networks for Machine Reading.pdf:pdf},
title = {{Long Short-Term Memory-Networks for Machine Reading}},
url = {http://arxiv.org/abs/1601.06733},
year = {2016}
}
@article{Chih-WeiHsuChih-ChungChang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chih-Wei Hsu, Chih-Chung Chang - 2008 - A Practical Guide to Support Vector Classification.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@article{Chih-WeiHsuChih-ChungChang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
journal = {BJU international},
number = {1},
pages = {1396--400},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@article{Chorowski2015a,
abstract = {People can understand complex structures if they relate to more isolated yet understandable concepts. Despite this fact, popular pattern recognition tools, such as decision tree or production rule learners, produce only flat models which do not build intermediate data representations. On the other hand, neural networks typically learn hierarchical but opaque models. We show how constraining neurons' scikit-learns to be nonnegative improves the interpretability of a network's operation. We analyze the proposed method on large data sets: the MNIST digit recognition data and the Reuters text categorization data. The patterns learned by traditional and constrained network are contrasted to those learned with principal component analysis and nonnegative matrix factorization.},
author = {Chorowski, Jan and Zurada, Jacek M.},
doi = {10.1109/TNNLS.2014.2310059},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chorowski, Zurada - 2015 - Learning understandable neural networks with nonnegative scikit-learn constraints.pdf:pdf},
isbn = {2162-237X VO - 26},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Multilayer perceptron,pattern analysis,supervised learning,white-box models.},
mendeley-groups = {Annotated/Word Vectors},
number = {1},
pages = {62--69},
title = {{Learning understandable neural networks with nonnegative scikit-learn constraints}},
volume = {26},
year = {2015}
}
@article{Chrupaa2015,
abstract = {We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.},
archivePrefix = {arXiv},
arxivId = {1506.03694},
author = {Chrupa{\l}a, Grzegorz and K{\'{a}}d{\'{a}}r, {\'{A}}kos and Alishahi, Afra},
eprint = {1506.03694},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chrupa{\l}a, K{\'{a}}d{\'{a}}r, Alishahi - 2015 - Learning language through pictures.pdf:pdf},
mendeley-groups = {Progress Report},
number = {September},
pages = {8--9},
title = {{Learning language through pictures}},
url = {http://arxiv.org/abs/1506.03694},
year = {2015}
}
@article{Chuang2012,
abstract = {Topic models aid analysis of text corpora by identifying la- tent topics based on co-occurring words. Real-world de- ployments of topic models, however, often require intensive expert verification and model refinement. In this paper we present Termite, a visual analysis tool for assessing topic model quality. Termite uses a tabular layout to promote comparison of terms both within and across latent topics. We contribute a novel saliency measure for selecting relevant terms and a seriation algorithm that both reveals clustering structure and promotes the legibility of related terms. In a series of examples, we demonstrate how Termite allows analysts to identify coherent and significant themes.},
author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
doi = {10.1145/2254556.2254572},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chuang, Manning, Heer - 2012 - Termite Visualization Techniques for Assessing Textual Topic Models.pdf:pdf},
isbn = {9781450312875},
journal = {Proceedings of the International Working Conference on Advanced Visual Interfaces - AVI '12},
keywords = {seriation,text visualization,topic models},
pages = {74},
title = {{Termite : Visualization Techniques for Assessing Textual Topic Models}},
url = {http://dl.acm.org/citation.cfm?doid=2254556.2254572},
year = {2012}
}
@article{Collobert2000,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling, achieving or exceeding state-of-the-art performance in each on four benchmark tasks. Our goal was to design a flexible architecture that can learn representations useful for the tasks, thus avoiding excessive task-specific feature engineering (and therefore disregarding a lot of prior knowledge). Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabelled training data. This work is then used as a basis for building a freely available tagging system with excellent performance while requiring minimal computational resources.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1145/2347736.2347755},
eprint = {1103.0398},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2000 - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Deep Learning,Natural Language Processing,Neural Networks},
mendeley-groups = {Literature Review},
pages = {1--48},
title = {{Natural Language Processing (almost) from Scratch}},
volume = {1},
year = {2000}
}
@article{Collobert2000,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling, achieving or exceeding state-of-the-art performance in each on four benchmark tasks. Our goal was to design a flexible architecture that can learn representations useful for the tasks, thus avoiding excessive task-specific feature engineering (and therefore disregarding a lot of prior knowledge). Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabelled training data. This work is then used as a basis for building a freely available tagging system with excellent performance while requiring minimal computational resources.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1145/2347736.2347755},
eprint = {1103.0398},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2000 - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Deep Learning,Natural Language Processing,Neural Networks},
pages = {1--48},
title = {{Natural Language Processing (almost) from Scratch}},
volume = {1},
year = {2000}
}
@article{Conll,
author = {Conll, Anonymous},
file = {:D$\backslash$:/Downloads/Work/Paper submitted to CoNLL.pdf:pdf},
pages = {1--10},
title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Cooijmans2016,
abstract = {We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.},
archivePrefix = {arXiv},
arxivId = {1603.09025},
author = {Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'{e}}sar and G{\"{u}}l{\c{c}}ehre, {\c{C}}ağlar and Courville, Aaron},
doi = {10.1227/01.NEU.0000210260.55124.A4},
eprint = {1603.09025},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cooijmans et al. - 2016 - Recurrent Batch Normalization.pdf:pdf},
isbn = {9783319464657},
issn = {16113349},
mendeley-groups = {!Paper 3/Training LSTMs},
number = {Section 3},
pages = {1--13},
pmid = {26774160},
title = {{Recurrent Batch Normalization}},
url = {http://arxiv.org/abs/1603.09025},
year = {2016}
}
@article{Craven1996,
author = {Craven, Mark W},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven - 1996 - EXTRACTING COMPREHENSIBLE MODELS By.pdf:pdf},
isbn = {0591144956},
journal = {Evaluation},
title = {{EXTRACTING COMPREHENSIBLE MODELS By}},
year = {1996}
}
@article{Craven1994,
abstract = {Concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to...},
author = {Craven, Mark W and Shavlik, Jude W},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1994 - Using sampling and queries to extract rules from trained neural networks.pdf:pdf},
journal = {Proc.$\backslash$ Intl.$\backslash$ Conf.$\backslash$ Machine Learning},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
pages = {37--45},
title = {{Using sampling and queries to extract rules from trained neural networks}},
year = {1994}
}
@article{Craven,
author = {Craven, Mark W and Shavlik, Jude W},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - Unknown - Extracting Thee-Structured Representations of Thained Networks.pdf:pdf},
mendeley-groups = {Annotated/Decision Trees},
title = {{Extracting Thee-Structured Representations of Thained Networks}}
}
@article{Craven1993,
author = {Craven, Mark W and Shavlik, Jude W},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
journal = {Machine Learning: Proceedings of the Tenth International Conference},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
pages = {73--80},
title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
year = {1993}
}
@article{Craven1993,
author = {Craven, Mark W and Shavlik, Jude W},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
journal = {Machine Learning: Proceedings of the Tenth International Conference},
pages = {73--80},
title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
year = {1993}
}
@article{Cui,
author = {Cui, Hang and Za{\"{i}}ane, Osmar R and Canada, T H},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui, Za{\"{i}}ane, Canada - Unknown - Hierarchical Structural Approach to Improving the Browsability of Web Search Engine Results.pdf:pdf},
mendeley-groups = {Report/Clustering},
pages = {1--5},
title = {{Hierarchical Structural Approach to Improving the Browsability of Web Search Engine Results}}
}
@article{Curry,
author = {Curry, Edward and Buitelaar, Paul},
file = {:C$\backslash$:/Users/Workk/Documents/preprint{\_}nldb{\_}{\_}commonsense{\_}2014.pdf:pdf},
mendeley-groups = {11Thesis/Conceptual Spaces {\&} Properties},
pages = {1--12},
title = {{A Distributional Semantics Approach for Selective Reasoning on Commonsense Graph Knowledge Bases}}
}
@article{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
author = {Dai, Andrew M and Le, Quoc V},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
mendeley-groups = {!Paper 3/task/Large Movie Review,!Paper 3/task/Sentiment treebank,!Paper 3/task/newsgroups},
pages = {1--10},
pmid = {414454},
title = {{Semi-supervised Sequence Learning}},
url = {http://arxiv.org/abs/1511.01432},
year = {2015}
}
@article{Dai,
abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
archivePrefix = {arXiv},
arxivId = {1507.07998},
author = {Dai, Andrew M and Olah, Christopher and Le, Quoc V.},
eprint = {1507.07998},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Olah - Unknown - Document Embedding with Paragraph Vectors.pdf:pdf},
mendeley-groups = {Annotated/Representation Learning},
pages = {1--8},
title = {{Document Embedding with Paragraph Vectors}},
url = {http://arxiv.org/abs/1507.07998},
year = {2015}
}
@article{Dasigi2014a,
abstract = {Automatically identifying anomalous newswire events is a hard problem. We discuss the com-plexity of the problem and introduce a novel technique to model events based on recursive neural networks to represent events as composition of their semantic arguments. Our model learns to differentiate between normal and anomalous events. We model anomaly detection as a binary classification problem and show that the model learns useful features to classify anomaly. We use headlines from the weird news category publicly available on newswire websites to extract anomalous training examples and those from Gigaword as normal examples. We evaluate the classifier on human annotated data and obtain an accuracy of 65.44{\%}. We also show that our model is at least as competent as the least competent human annotator in anomaly detection.},
author = {Dasigi, Pradeep and Hovy, Eduard},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasigi, Hovy - 2014 - Modeling Newswire Events using Neural Networks for Anomaly Detection.pdf:pdf},
isbn = {9781941643266},
journal = {Coling-2014},
pages = {1414--1422},
title = {{Modeling Newswire Events using Neural Networks for Anomaly Detection}},
year = {2014}
}
@article{David1992,
author = {David, Douglass R Cuttingl and Kargerl, R and Tukey, John W},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/David, Kargerl, Tukey - 1992 - Scatter Gather Browsing A Cluster-based Large Document Approach Collections to Scatter Gather.pdf:pdf},
mendeley-groups = {Report/Clustering},
title = {{Scatter / Gather : Browsing A Cluster-based Large Document Approach Collections to Scatter / Gather}},
year = {1992}
}
@article{Derrac2015,
abstract = {Commonsense reasoning patterns such as interpolation and a fortiori inference have proven useful for dealing with gaps in structured knowledge bases. An important difficulty in applying these reasoning patterns in practice is that they rely on fine-grained knowledge of how different concepts and entities are semantically related. In this paper, we show how the required semantic relations can be learned from a large collection of text documents. To this end, we first induce a conceptual space from the text documents, using multi-dimensional scaling. We then rely on the key insight that the required semantic relations correspond to qualitative spatial relations in this conceptual space. Among others, in an entirely unsupervised way, we identify salient directions in the conceptual space which correspond to interpretable relative properties such as 'more fruity than' (in a space of wines), resulting in a symbolic and interpretable representation of the conceptual space. To evaluate the quality of our semantic relations, we show how they can be exploited by a number of commonsense reasoning based classifiers. We experimentally show that these classifiers can outperform standard approaches, while being able to provide intuitive explanations of classification decisions. A number of crowdsourcing experiments provide further insights into the nature of the extracted semantic relations.},
author = {Derrac, Joaquin and Schockaert, Steven},
doi = {10.1016/j.artint.2015.07.002},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derrac, Schockaert - 2015 - Inducing semantic relations from conceptual spaces A data-driven approach to plausible reasoning.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Commonsense reasoning,Conceptual spaces,Dimensionality reduction,Qualitative spatial relations},
mendeley-groups = {Annotated/Past work,Papers/Paper 1,Categories/Commonsense Reasoning,Report/Features,Progress Report,Report},
pages = {66--94},
publisher = {Elsevier B.V.},
title = {{Inducing semantic relations from conceptual spaces: A data-driven approach to plausible reasoning}},
url = {http://dx.doi.org/10.1016/j.artint.2015.07.002},
volume = {228},
year = {2015}
}
@article{Derrac2014,
abstract = {Place types taxonomies tend to have a shallow structure, which limits their predictive value. Although existing place type taxonomies could in principle be refined, the result would inevitably be highly subjective and application-specific. Instead, in this paper, we propose a methodology to enrich place types taxonomies with a ternary betweenness relation derived from Flickr. In particular, we first construct a semantic space of place types by applying dimensionality reduction methods to tag co-occurrence data obtained from Flickr. Our hypothesis is that natural properties of place types should correspond to convex regions in this space. Specifically, knowing that places P 1,...,Pn have a given property, we could then induce that all places which are located in the convex hull of {\{}P1,...,P n{\}} in the semantic space are also likely to have this property. To avoid relying on computationally expensive convex hull algorithms, we propose to derive a ternary betweenness relation from the semantic space, and to approximate the convex hull at the symbolic level based on this relation. We present experimental results which support the usefulness of our approach. {\textcopyright} 2014 Springer International Publishing.},
author = {Derrac, Joaqu{\'{i}}n and Schockaert, Steven},
doi = {10.1007/978-3-319-04939-7_8},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derrac, Schockaert - 2014 - Enriching taxonomies of place types using Flickr.pdf:pdf},
isbn = {9783319049380},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
pages = {174--192},
title = {{Enriching taxonomies of place types using Flickr}},
volume = {8367 LNCS},
year = {2014}
}
@article{Diao2014,
abstract = {Recommendation and review sites offer a wealth of infor-mation beyond ratings. For instance, on IMDb users leave reviews, commenting on different aspects of a movie (e.g. actors, plot, visual effects), and expressing their sentiments (positive or negative) on these aspects in their reviews. This suggests that uncovering aspects and sentiments will allow us to gain a better understanding of users, movies, and the process involved in generating ratings. The ability to answer questions such as " Does this user care more about the plot or about the special effects? " or " What is the quality of the movie in terms of acting? " helps us to understand why certain ratings are generated. This can be used to provide more meaningful recommendations. In this work we propose a probabilistic model based on collaborative filtering and topic modeling. It allows us to capture the interest distribution of users and the content distribution for movies; it provides a link between inter-est and relevance on a per-aspect basis and it allows us to differentiate between positive and negative sentiments on a per-aspect basis. Unlike prior work our approach is entirely unsupervised and does not require knowledge of the aspect specific ratings or genres for inference. We evaluate our model on a live copy crawled from IMDb. Our model offers superior performance by joint modeling. Moreover, we are able to address the cold start problem — by utilizing the information inherent in reviews our model demonstrates improvement for new users and movies.},
author = {Diao, Qiming and Qiu, Minghui and Wu, Chao-Yuan and Smola, Alexander J. and Jiang, Jing and Wang, Chong},
doi = {10.1145/2623330.2623758},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Diao et al. - 2014 - Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS).pdf:pdf},
isbn = {9781450329569},
journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
mendeley-groups = {!Paper 3/task,!Paper 3/task/Yelp},
pages = {193--202},
title = {{Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2623758},
year = {2014}
}
@article{Ding2014,
author = {Ding, Shifei and Jia, Hongjie and Chen, Jinrong and Jin, Fengxiang},
doi = {10.1007/s10462-012-9313-7},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2014 - Granular neural networks.pdf:pdf},
isbn = {0269-2821},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Fuzzy neural networks,Granular neural networks,Rough neural networks},
number = {3},
pages = {373--384},
title = {{Granular neural networks}},
volume = {41},
year = {2014}
}
@article{Ding2015,
abstract = {We propose a deep learning method for event- driven stock market prediction. First, events are extracted from news text, and represented as dense vectors, trained using a novel neural tensor net- work. Second, a deep convolutional neural network is used to model both short-term and long-term in- fluences of events on stock price movements. Ex- perimental results show that our model can achieve nearly 6{\%} improvements on S{\&}P 500 index predic- tion and individual stock prediction, respectively, compared to state-of-the-art baseline methods. In addition, market simulation results show that our system is more capable of making profits than pre- viously reported systems trained on S{\&}P 500 stock historical data.},
author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2015 - Deep learning for event-driven stock prediction.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Technical Papers — Web Mining},
number = {Ijcai},
pages = {2327--2333},
title = {{Deep learning for event-driven stock prediction}},
volume = {2015-Janua},
year = {2015}
}
@article{Donahue2011,
abstract = {Traditional supervised visual learning simply asks annotators {\&}{\#}x201C;what{\&}{\#}x201D; label an image should have. We propose an approach for image classification problems requiring subjective judgment that also asks {\&}{\#}x201C;why{\&}{\#}x201D;, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the first, the annotator highlights the spatial region of interest he found most influential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to refine the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices.},
author = {Donahue, Jeff and Grauman, Kristen},
doi = {10.1109/ICCV.2011.6126394},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue, Grauman - 2011 - Annotator rationales for visual recognition.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {Report/Explaining predictions,Annotated},
number = {Iccv},
pages = {1395--1402},
title = {{Annotator rationales for visual recognition}},
year = {2011}
}
@article{Dong2017a,
abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
archivePrefix = {arXiv},
arxivId = {1703.04096},
author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
doi = {10.1109/CVPR.2017.110},
eprint = {1703.04096},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2017 - Improving Interpretability of Deep Neural Networks with Semantic Information(2).pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
pages = {4306--4314},
title = {{Improving Interpretability of Deep Neural Networks with Semantic Information}},
url = {http://arxiv.org/abs/1703.04096},
year = {2017}
}
@article{Dong2017,
abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
archivePrefix = {arXiv},
arxivId = {1703.04096},
author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
doi = {10.1109/CVPR.2017.110},
eprint = {1703.04096},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2017 - Improving Interpretability of Deep Neural Networks with Semantic Information(2).pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
pages = {4306--4314},
title = {{Improving Interpretability of Deep Neural Networks with Semantic Information}},
url = {http://arxiv.org/abs/1703.04096},
year = {2017}
}
@article{Donoho2004,
abstract = {We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that un- der certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sam- pling. For such databases there is a generative model in terms of “parts” and NMF correctly identifies the ”parts”. We show that our theoretical results are predictive of the performance of publishedNMFcode, by run- ning the published algorithms on one of our synthetic image articulation databases.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Donoho, Dl and Stodden, Vc},
doi = {10.1.1.85.8157},
eprint = {1512.00567},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho, Stodden - 2004 - When does non-negative matrix factorization give a correct decomposition into parts.pdf:pdf},
isbn = {9780262201520},
issn = {09501991},
journal = {Proc. Advances in Neural Information Processing Systems 16},
mendeley-groups = {Annotated/NMF},
pages = {1141--1148},
pmid = {11585793},
title = {{When does non-negative matrix factorization give a correct decomposition into parts?}},
url = {http://academiccommons.columbia.edu/catalog/ac:140175},
year = {2004}
}
@article{Doshi-Velez2017,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
eprint = {1702.08608},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf:pdf},
mendeley-groups = {Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
number = {Ml},
pages = {1--13},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
url = {http://arxiv.org/abs/1702.08608},
year = {2017}
}
@article{Dosilovic2018,
author = {Do{\v{s}}ilovi{\'{c}}, Filip Karlo and Br{\v{c}}i{\'{c}}, Mario and Hlupi{\'{c}}, Nikica},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/dsdc{\_}11{\_}4754.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Explanation},
pages = {232--237},
title = {{232 Mipro 2018/Ds-Dc}},
year = {2018}
}
@article{Dou2013,
abstract = {Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system--HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.},
author = {Dou, Wenwen and Yu, Li and Wang, Xiaoyu and Ma, Zhiqiang and Ribarsky, William},
doi = {10.1109/TVCG.2013.162},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dou et al. - 2013 - HierarchicalTopics Visually exploring large text collections using topic hierarchies.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Hierarchical topic representation,rose tree,topic modeling,visual analytics},
mendeley-groups = {Annotated/Explanations},
number = {12},
pages = {2002--2011},
pmid = {24051766},
title = {{HierarchicalTopics: Visually exploring large text collections using topic hierarchies}},
volume = {19},
year = {2013}
}
@article{Duch2001a,
abstract = {A new methodology of extraction, optimization, and application of sets of logical rules is described. Neural networks are used for initial rule extraction, local or global minimization procedures for optimization, and Gaussian uncertainties of measurements are assumed during application of logical rules. Algorithms for extraction of logical rules from data with real-valued features require determination of linguistic variables or membership functions. Contest-dependent membership functions for crisp and fuzzy linguistic variables are introduced and methods of their determination described. Several neural and machine learning methods of logical rule extraction generating initial rules are described, based on constrained multilayer perceptron, networks with localized transfer functions or on separability criteria for determination of linguistic variables. A tradeoff between accuracy/simplicity is explored at the rule extraction stage and between rejection/error level at the optimization stage. Gaussian uncertainties of measurements are assumed during application of crisp logical rules, leading to "soft trapezoidal" membership functions and allowing to optimize the linguistic variables using gradient procedures. Numerous applications of this methodology to benchmark and real-life problems are reported and very simple crisp logical rules for many datasets provided.},
author = {Duch, W??odzis??aw and Adamczak, Rafat and Gra??bczewski, Krzysztof},
doi = {10.1109/72.914524},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duch, Adamczak, Grabczewski - 2001 - A new methodology of extraction, optimization and application of crisp and fuzzy logical rules.pdf:pdf},
isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Backpropagation,Data mining,Decision trees,Feature selection,Fuzzy systems,Logical rule-based systems,Neural networks},
mendeley-groups = {Progress Report},
number = {2},
pages = {277--306},
pmid = {18244384},
title = {{A new methodology of extraction, optimization and application of crisp and fuzzy logical rules}},
volume = {12},
year = {2001}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
mendeley-groups = {Annotated/Software},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@book{Edwards2017,
abstract = {ABSTRACT This article reflects the kinds of situations and spaces where people and algorithms meet. In what situations do people become aware of algorithms? How do they experience and make sense of these algorithms, given their often hidden and invisible nature? To what extent does an awareness of algorithms affect people's use of these platforms, if at all? To help answer these questions, this article examines people's personal stories about the Facebook algorithm through tweets and interviews with 25 ordinary users. To understand the spaces where people and algorithms meet, this article develops the notion of the algorithmic imaginary. It is argued that the algorithmic imaginary - ways of thinking about what algorithms are, what they should be and how they function - is not just productive of different moods and sensations but plays a generative role in moulding the Facebook algorithm itself. Examining how algorithms make people feel, then, seems crucial if we want to understand their social power.},
author = {Edwards, Lilian and Veale, Michael},
booktitle = {SSRN Electronic Journal},
doi = {10.2139/ssrn.2972855},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards, Veale - 2017 - Slave to the Algorithm Why a Right to Explanationn is Probably Not the Remedy You are Looking for.pdf:pdf},
isbn = {3540445668},
issn = {1556-5068},
mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability},
pages = {1--65},
title = {{Slave to the Algorithm? Why a Right to Explanationn is Probably Not the Remedy You are Looking for}},
url = {https://www.ssrn.com/abstract=2972855},
volume = {2017},
year = {2017}
}
@article{Ekstrand2014,
author = {Ekstrand, Michael D and Harper, F Maxwell and Willemsen, Martijn C and Konstan, Joseph A},
file = {:D$\backslash$:/PhD/Papedrs/listcmp.pdf:pdf},
isbn = {9781450326681},
keywords = {12,21,algorithms with comparable accuracy,movie recommendation domain,of,recommender systems,those differences in the,to map out some,user study,we present},
mendeley-groups = {11Thesis/Interpretability/General},
title = {{User Perception of Differences in Recommender Algorithms}},
year = {2014}
}
@article{Elazmeh2007,
abstract = {The paper presents ongoing issues, challenges, and difficulties we face in applying machine learning methods to retrospectively collected clinical data. The objective of our research is to build a reliable prediction model for early assessment of emergency pediatric asthma exacerbations. This predictive model should be able to distinguish between patients with mild or moderate/severe asthma attacks at a medically acceptable level of performance. Our real-life data set presents us with some difficult challenges which we communicate in this paper. Our approach to overcoming some of these difficulties is to use external expert knowledge to aid with classification by decomposing the classification problem into a two-tier concept, where concepts can be explicitly described in terms of the external knowledge source. Such an approach also has the advantage of significantly reducing the size of the training set required. Copyright {\textcopyright} 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
author = {Elazmeh, W.a and Matwin, S.a and O'Sullivan, D.b and Michalowski, W.b and Farion, K.c},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elazmeh et al. - 2007 - Insights from predicting pediatric asthma exacerbations from retrospective clinical data.pdf:pdf},
isbn = {9781577353324},
journal = {AAAI Workshop - Technical Report},
keywords = {Artificial intelligence,Classification (of infor,Clinical data,Evaluation methods,Expert knowledg,Learning systems},
mendeley-groups = {Report/Medical domain},
pages = {10--15},
title = {{Insights from predicting pediatric asthma exacerbations from retrospective clinical data}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-51849160749{\&}partnerID=40{\&}md5=345e9c0a3f9c793b5c69b25dfab7d721},
volume = {WS-07-05},
year = {2007}
}
@article{Erhan2010a,
abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5538v1},
author = {Erhan, Dumitru and Courville, Aaron and Vincent, Pascal},
doi = {10.1145/1756006.1756025},
eprint = {arXiv:1206.5538v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan, Courville, Vincent - 2010 - Why Does Unsupervised Pre-training Help Deep Learning.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
mendeley-groups = {Papers/Paper 1,Report/Features,Progress Report,Report},
number = {2007},
pages = {625--660},
title = {{Why Does Unsupervised Pre-training Help Deep Learning ?}},
url = {http://portal.acm.org/citation.cfm?id=1756025},
volume = {11},
year = {2010}
}
@article{Erhan2014,
abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
archivePrefix = {arXiv},
arxivId = {1312.2249},
author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
doi = {10.1109/CVPR.2014.276},
eprint = {1312.2249},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan et al. - 2014 - Scalable Object Detection Using Deep Neural Networks.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Progress Report},
pages = {2155--2162},
title = {{Scalable Object Detection Using Deep Neural Networks}},
url = {http://arxiv.org/abs/1312.2249{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909673},
year = {2014}
}
@article{Escalante2018,
author = {Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jacques, Julio and Bar{\'{o}}, Xavier and Ayache, Stephane and Viegas, Evelyne and G{\"{u}}{\c{c}}l{\"{u}}t{\"{u}}rk, Yağmur and G{\"{u}}{\c{c}}l{\"{u}}, Umut and Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jacques, Julio and Madadi, Meysam and Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jr, Julio Jacques},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Escalante et al. - 2018 - Design of an Explainable Machine Learning Challenge for Video Interviews To cite this version HAL Id hal-016.pdf:pdf},
mendeley-groups = {!Paper 3/Interpreting Vision},
title = {{Design of an Explainable Machine Learning Challenge for Video Interviews To cite this version : HAL Id : hal-01668386 Design of an Explainable Machine Learning Challenge for Video Interviews}},
year = {2018}
}
@article{Etchells2006a,
abstract = {There is much interest in rule extraction from neural networks and a plethora of different methods have been proposed for this purpose. We discuss the merits of pedagogical and decompositional approaches to rule extraction from trained neural networks, and show that some currently used methods for binary data comply with a theoretical formalism for extraction of Boolean rules from continuously valued logic. This formalism is extended into a generic methodology for rule extraction from smooth decision surfaces fitted to discrete or quantized continuous variables independently of the analytical structure of the underlying model, and in a manner that is efficient even for high input dimensions. This methodology is then tested with Monks' data, for which exact rules are obtained and to Wisconsin's breast cancer data, where a small number of high-order rules are identified whose discriminatory performance can be directly visualized.},
author = {Etchells, Terence A. and Lisboa, Paulo J G},
doi = {10.1109/TNN.2005.863472},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Etchells, Lisboa - 2006 - Orthogonal Search-Based Rule Extraction (OSRE) for Trained Neural Networks A Practical and Efficient Approach.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Neura networks,Rule extraction},
number = {2},
pages = {374--384},
pmid = {16566465},
title = {{Orthogonal Search-Based Rule Extraction (OSRE) for Trained Neural Networks: A Practical and Efficient Approach}},
volume = {17},
year = {2006}
}
@article{Evans2017,
abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework ({\$}\backslashpartial{\$}ILP), which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
archivePrefix = {arXiv},
arxivId = {1711.04574},
author = {Evans, Richard and Grefenstette, Edward},
eprint = {1711.04574},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans, Grefenstette - 2017 - Learning Explanatory Rules from Noisy Data.pdf:pdf},
mendeley-groups = {!Paper 3},
pages = {1--64},
title = {{Learning Explanatory Rules from Noisy Data}},
url = {http://arxiv.org/abs/1711.04574},
volume = {61},
year = {2017}
}
@article{Exner2012,
abstract = {In this paper, we describe an end-to-end system that automatically extracts RDF triples describing entity relations and properties from unstructured text. This system is based on a pipeline of text processing modules that includes a semantic parser and a coreference solver. By using coreference chains, we group entity actions and properties described in different sentences and convert them{\textless}br/{\textgreater}{\textless}br{\textgreater}$\backslash$r$\backslash$ninto entity triples. We applied our system to over 114,000 Wikipedia articles and we could extract more than 1,000,000 triples. Using an ontology-mapping system that we bootstrapped using existing DBpedia triples, we mapped 189,000 extracted triples onto the DBpedia namespace. These extracted entities are availableonline in the N-Triple format.},
author = {Exner, Peter and Nugues, Pierre},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Exner, Nugues - 2012 - Entity extraction From unstructured text to dbpedia rdf triples.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
mendeley-groups = {Report},
number = {Iswc},
pages = {58--69},
title = {{Entity extraction: From unstructured text to dbpedia rdf triples}},
volume = {906},
year = {2012}
}
@article{F??rnkranz2008a,
abstract = {Label ranking studies the problem of learning a mapping from instances to rankings over a predefined set of labels. Hitherto existing approaches to label ranking implicitly operate on an underlying (utility) scale which is not calibrated in the sense that it lacks a natural zero point. We propose a suitable extension of label ranking that incorporates the calibrated scenario and substantially extends the expressive power of these approaches. In particular, our extension suggests a conceptually novel technique for extending the common learning by pairwise comparison approach to the multilabel scenario, a setting previously not being amenable to the pairwise decomposition technique. The key idea of the approach is to introduce an artificial calibration label that, in each example, separates the relevant from the irrelevant labels. We show that this technique can be viewed as a combination of pairwise preference learning and the conventional relevance classification technique, where a separate classifier is trained to predict whether a label is relevant or not. Empirical results in the area of text categorization, image classification and gene analysis underscore the merits of the calibrated model in comparison to state-of-the-art multilabel learning methods.},
author = {F??rnkranz, Johannes and H??llermeier, Eyke and {Loza Menc??a}, Eneldo and Brinker, Klaus},
doi = {10.1007/s10994-008-5064-8},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frnkranz et al. - 2008 - Multilabel classification via calibrated label ranking.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Multi-label classification,Preference learning,Ranking},
number = {2},
pages = {133--153},
title = {{Multilabel classification via calibrated label ranking}},
volume = {73},
year = {2008}
}
@article{Fader2011,
abstract = {Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the ReVerb Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TextRunner and woepos. More than 30{\%} of ReVerb's extractions are at precision 0.8 or higher---compared to virtually none for earlier systems. The paper concludes with a detailed analysis of ReVerb's errors, suggesting directions for future work.},
author = {Fader, Anthony and Soderland, Stephen and Etzioni, Oren},
doi = {10.1234/12345678},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fader, Soderland, Etzioni - 2011 - Identifying relations for open information extraction.pdf:pdf},
isbn = {978-1-937284-11-4},
issn = {1937284115},
journal = {Proceedings of the Conference on {\ldots}},
pages = {1535--1545},
title = {{Identifying relations for open information extraction}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145596{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2145596},
year = {2011}
}
@article{Fallis2013a,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fallis - 2013 - No Title No Title(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Fallis2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fallis - 2013 - No Title No Title.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Fares2017,
author = {Fares, Murhaf and Kutuzov, Andrey and Oepen, Stephan and Velldal, Erik},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fares et al. - 2017 - Word vectors , reuse , and replicability Towards a community repository of large-text resources.pdf:pdf},
mendeley-groups = {Report/Features},
number = {May},
pages = {271--276},
title = {{Word vectors , reuse , and replicability : Towards a community repository of large-text resources}},
year = {2017}
}
@article{Faruqui2015a,
abstract = {Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substan- tial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms.},
archivePrefix = {arXiv},
arxivId = {1411.4166},
author = {Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K and Dyer, Chris and Hovy, Eduard and Smith, Noah A},
doi = {10.3115/v1/N15-1184},
eprint = {1411.4166},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui et al. - 2015 - Retrofitting Word Vectors to Semantic Lexicons.pdf:pdf},
isbn = {9781941643495},
journal = {Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL},
mendeley-groups = {!Paper 3/task/Sentiment treebank},
number = {i},
pages = {1606--1615},
title = {{Retrofitting Word Vectors to Semantic Lexicons}},
year = {2015}
}
@article{Faruqui2015,
abstract = {Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninter-pretable components whose relationship to the categories of traditional lexical seman-tic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguis-tic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9{\%} sparse. We analyze their performance on state-of-the-art eval-uation methods for distributional models of word vectors and find they are competi-tive to standard distributional approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.05230v1},
author = {Faruqui, Manaal and Dyer, Chris},
doi = {10.3115/v1/P15-2076},
eprint = {arXiv:1506.05230v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui, Dyer - 2015 - Non-distributional Word Vector Representations.pdf:pdf},
isbn = {9781941643730},
journal = {Acl-2015},
mendeley-groups = {Progress Report,Interim Review},
pages = {464--469},
title = {{Non-distributional Word Vector Representations}},
year = {2015}
}
@article{Faruqui2015,
abstract = {Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {1506.02004},
author = {Faruqui, Manaal and Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah A.},
eprint = {1506.02004},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui et al. - 2015 - Sparse Overcomplete Word Vector Representations.pdf:pdf},
isbn = {9781941643723},
journal = {Acl-2015},
mendeley-groups = {Annotated/Word Vectors,Progress Report,Interim Review,Annotated/NMF},
pages = {1491--1500},
title = {{Sparse Overcomplete Word Vector Representations}},
url = {http://homes.cs.washington.edu/{~}nasmith/papers/faruqui+tsvetkov+yogatama+dyer+smith.acl15.pdf},
year = {2015}
}
@article{FenTan2016,
abstract = {Ensembles of decision trees have good prediction accuracy but suffer from a lack of interpretability. We propose a new approach for interpreting tree ensembles by finding prototypes in tree space, utilizing the naturally-learned similarity measure from the tree ensemble. Demonstrating the method on random forests, we show that the method benefits from two unique aspects of tree ensembles by leveraging tree structure to sequentially find prototypes, and utilizing the naturally-learned similarity measure from the tree ensemble. The method provides good prediction accuracy when found prototypes are used in nearest-prototype classifiers, while us-ing fewer prototypes than competitor methods. We are investigating the sensitivity of the method to different prototype-finding procedures and demonstrating it on higher-dimensional data.},
archivePrefix = {arXiv},
arxivId = {1611.07115},
author = {{Fen Tan}, Hui and Hooker, Giles J and Wells, Martin T},
eprint = {1611.07115},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fen Tan, Hooker, Wells - 2016 - Tree Space Prototypes Another Look at Making Tree Ensembles Interpretable.pdf:pdf},
journal = {Nips},
mendeley-groups = {Annotated/Decision Trees,!Paper 3/task/newsgroups,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
title = {{Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable}},
year = {2016}
}
@article{Fern2014,
author = {Fern, Manuel and Cernadas, Eva},
file = {:C$\backslash$:/Users/Workk/Documents/delgado14a.pdf:pdf},
pages = {3133--3181},
title = {{Do we Need Hundreds of Classifiers to Solve Real World Classification Problems ?}},
volume = {15},
year = {2014}
}
@article{Fischer2012a,
abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.0966v3},
author = {Fischer, Asja and Igel, Christian},
doi = {10.1007/978-3-642-33275-3_2},
eprint = {arXiv:1311.0966v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer, Igel - 2012 - An Introduction to Restricted Boltzmann Machines.pdf:pdf},
isbn = {978-3-642-33274-6},
issn = {1875-7855},
journal = {Lecture Notes in Computer Science: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
pages = {14--36},
pmid = {24309266},
title = {{An Introduction to Restricted Boltzmann Machines}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33275-3{\_}2},
volume = {7441},
year = {2012}
}
@article{Fong2017,
abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, e.g. problems in health, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we introduce a paradigm that learns the minimally salient part of an image by directly editing it and learning from the corresponding changes to its output. Unlike previous works, our method is model-agnostic and testable because it is grounded in replicable image perturbations.},
archivePrefix = {arXiv},
arxivId = {1704.03296},
author = {Fong, Ruth and Vedaldi, Andrea},
eprint = {1704.03296},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong, Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meaningful Perturbation.pdf:pdf},
mendeley-groups = {Report/Explaining predictions,Report},
title = {{Interpretable Explanations of Black Boxes by Meaningful Perturbation}},
url = {http://arxiv.org/abs/1704.03296},
year = {2017}
}
@article{Franca2014a,
abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph min-ing and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integratedBCP with a well-known neural-symbolic system, C-IL2 P, to perform learning from nu-merical vectors. C-IL2 P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, han-dles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary re-sults indicating that a reduction of more than 90{\{}{\%}{\}} of features can be achieved with a small loss of accuracy.},
author = {Fran{\c{c}}a, Manoel V M and Zaverucha, Gerson and {D'Avila Garcez}, Artur S.},
doi = {10.1007/s10994-013-5392-1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}a, Zaverucha, D'Avila Garcez - 2014 - Fast relational learning using bottom clause propositionalization with artificial neural netw.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Artificial neural networks,Inductive logic programming,Neural-symbolic integration,Propositionalization,Relational learning},
mendeley-groups = {Progress Report},
number = {1},
pages = {81--104},
title = {{Fast relational learning using bottom clause propositionalization with artificial neural networks}},
volume = {94},
year = {2014}
}
@article{Franca2014,
abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph min-ing and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integratedBCP with a well-known neural-symbolic system, C-IL2 P, to perform learning from nu-merical vectors. C-IL2 P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, han-dles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary re-sults indicating that a reduction of more than 90{\{}{\%}{\}} of features can be achieved with a small loss of accuracy.},
author = {Fran{\c{c}}a, Manoel V M and Zaverucha, Gerson and {D'Avila Garcez}, Artur S.},
doi = {10.1007/s10994-013-5392-1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}a, Zaverucha, D'Avila Garcez - 2014 - Fast relational learning using bottom clause propositionalization with artificial neural netw.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Artificial neural networks,Inductive logic programming,Neural-symbolic integration,Propositionalization,Relational learning},
mendeley-groups = {Progress Report},
number = {1},
pages = {81--104},
title = {{Fast relational learning using bottom clause propositionalization with artificial neural networks}},
volume = {94},
year = {2014}
}
@article{Freitas2010,
author = {Freitas, AA and Wieser, DC and Apweiler, R},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas, Wieser, Apweiler - 2010 - On the importance of comprehensible classification models for protein function prediction.pdf:pdf},
journal = {IEEE/ACM Transactions on},
mendeley-groups = {Annotated/Applications/Scientific Discovery,Report/Biologicla domain},
number = {1},
pages = {172--182},
title = {{On the importance of comprehensible classification models for protein function prediction}},
url = {http://dl.acm.org/citation.cfm?id=1719290},
volume = {7},
year = {2010}
}
@article{Freitas2013,
abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
author = {Freitas, Alex A.},
doi = {10.1145/2594473.2594475},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas - 2013 - Comprehensible Classification Models - a position paper.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
keywords = {bayesian network classifiers,decision table,decision tree,monotonicity constraint,nearest neighbors,rule induction},
mendeley-groups = {Report/Just about interpretability,Annotated/Overarching Interpretability,Report,11Thesis/Interpretability,11Thesis/Interpretability/General},
number = {1},
pages = {1--10},
title = {{Comprehensible Classification Models - a position paper}},
url = {http://dl.acm.org.miman.bib.bth.se/citation.cfm?id=2594475},
volume = {15},
year = {2013}
}
@article{Fu1994,
abstract = {The neural network approach has proven useful for the development of artificial intelligence systems.  However, a disadvantage with this approach is that the knowledge embedded in the neural network is opaque.  In this paper, we show how to interpret neural network knowledge in sympobic form.  We lay down required definitions for this treatment, formulate the interpretation algorithm, and formally verigy its soundness.  The main result is a formalized relationship between a neural network and a rule-based system.  In addition, it has been demonstrated that the neural network generates rules of better performance than the decision tree approach in noisy conditions.},
author = {Fu, LiMin},
doi = {10.1109/21.299696},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu - 1994 - Rule generation from neural networks.pdf:pdf},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {8},
pages = {1114--1124},
title = {{Rule generation from neural networks}},
volume = {24},
year = {1994}
}
@article{Fu1998a,
author = {Fu, Limin},
doi = {10.1109/72.712152},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu - 1998 - A neural-network model for learning domain rules based on its activation function characteristics.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Certainty factor,Generalization,Machine learning,Neural network,Rule learning,Sample complexity,VC-dimension},
number = {5},
pages = {787--795},
title = {{A neural-network model for learning domain rules based on its activation function characteristics}},
volume = {9},
year = {1998}
}
@article{Funahashi1989,
author = {Funahashi, K.},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Funahashi - 1989 - On the Approximate Realisation of Continuous Mappings by Neural Networks.pdf:pdf},
journal = {Neural Networks},
keywords = {--neural network,alization,back propagation,continuous mapping,hidden layer,output function,re-,sigmoid function,unit},
number = {3},
pages = {183--192},
title = {{On the Approximate Realisation of Continuous Mappings by Neural Networks}},
volume = {2},
year = {1989}
}
@article{Fyshe2014,
abstract = {Vector space models (VSMs) represent word meanings as points in a high dimensional space. VSMs are typically created using a large text corpora, and so represent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics. Evaluations show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies. {\textcopyright} 2014 Association for Computational Linguistics.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Fyshe, Alona and Talukdar, Pp and Murphy, Brian and Mitchell, Tm},
doi = {10.14440/jbm.2015.54.A},
eprint = {15334406},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - 2014 - Interpretable Semantic Vectors from a Joint Model of Brain-and Text-Based Meaning.pdf:pdf},
isbn = {9781937284725},
issn = {0036-8075},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
pages = {489--499},
pmid = {25792328},
title = {{Interpretable Semantic Vectors from a Joint Model of Brain-and Text-Based Meaning}},
url = {http://www.cs.cmu.edu/{~}afyshe/papers/acl2014/jnnse{\_}acl2014.pdf},
volume = {1},
year = {2014}
}
@article{Fyshe,
author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - Unknown - A Compositional and Interpretable Semantic Space.pdf:pdf},
mendeley-groups = {Annotated/Interpretable representations,Annotated/Word Vectors},
title = {{A Compositional and Interpretable Semantic Space}}
}
@article{Fyshe2015,
abstract = {Vector Space Models (VSMs) of Semantics are useful tools for exploring the semantics of single words, and the composition of words to make phrasal meaning. While many meth- ods can estimate the meaning (i.e. vector) of a phrase, few do so in an interpretable way. We introduce a new method (CNNSE) that al- lows word and phrase vectors to adapt to the notion of composition. Our method learns a VSM that is both tailored to support a chosen semantic composition operation, and whose resulting features have an intuitive interpreta- tion. Interpretability allows for the exploration of phrasal semantics, which we leverage to an- alyze performance on a behavioral task. 1},
author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - 2015 - A compositional and interpretable semantic space.pdf:pdf},
isbn = {9781941643495},
journal = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015)},
mendeley-groups = {Progress Report,Interim Review,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
pages = {32--41},
title = {{A compositional and interpretable semantic space}},
year = {2015}
}
@article{G.Towell1993a,
author = {{G. Towell}, Geoffrey and {W. Shavlik}, Jude},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G. Towell, W. Shavlik - 1993 - Interpretation of artificial neural networks Mapping knowledge based neural networks into rules.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {977--984},
title = {{Interpretation of artificial neural networks: Mapping knowledge based neural networks into rules}},
volume = {5},
year = {1993}
}
@article{Gal2015,
abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
archivePrefix = {arXiv},
arxivId = {1512.05287},
author = {Gal, Yarin and Ghahramani, Zoubin},
doi = {10.1201/9781420049176},
eprint = {1512.05287},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal, Ghahramani - 2015 - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:pdf},
isbn = {9789537619084},
issn = {0302-9743},
mendeley-groups = {!Paper 3/Training LSTMs,!Paper 3/task/Sentiment treebank},
number = {Nips},
pmid = {21803542},
title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.05287},
year = {2015}
}
@article{Galloway1982,
abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic similarity, pos-induction and word-analogy tasks.},
author = {Galloway, Patricia},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/P15-1126.pdf:pdf},
journal = {ALLC Journal},
keywords = {*Diachronic Linguistics (di1),*French (fr2),*Poetry (pl2),*Statistical Analysis of Style (st3),5710: poetics/literary theory; poetics,article,cluster analysis, typology, Lai de l'Ombre manuscr},
mendeley-groups = {11Thesis/Interpretability/Visual},
number = {1},
pages = {1--8},
title = {{Clustering Variants in the Lai de l'Ombre Manuscripts: Techniques and Principles}},
url = {http://search.proquest.com/docview/85463650?accountid=8330{\%}5Cnhttp://library.anu.edu.au:4550/resserv?genre=article{\&}issn={\&}title=ALLC+Journal{\&}volume=3{\&}issue=1{\&}date=1982-04-01{\&}atitle=Clustering+Variants+in+the+Lai+de+l'Ombre+Manuscripts:+Techniques+and+Princ},
volume = {3},
year = {1982}
}
@article{Ganin2015,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
issn = {1475-7516},
mendeley-groups = {Progress Report},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://arxiv.org/abs/1505.07818},
volume = {17},
year = {2015}
}
@article{Ganin2015a,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
issn = {1475-7516},
mendeley-groups = {Progress Report},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://arxiv.org/abs/1505.07818},
volume = {17},
year = {2015}
}
@article{Garc??a2009,
abstract = {The experimental analysis on the performance of a proposed method is a crucial and necessary task to carry out in a research. This paper is focused on the sta- tistical analysis of the results in the field of genetics-based machine Learning. It presents a study involving a set of techniques which can be used for doing a rigorous com- parison among algorithms, in terms of obtaining successful classification models. Two accuracy measures for multi- class problems have been employed: classification rate and Cohen's kappa. Furthermore, two interpretability measures have been employed: size of the rule set and number of antecedents. We have studied whether the samples of results obtained by genetics-based classifiers, using the performance measures cited above, check the necessary conditions for being analysed by means of parametrical tests. The results obtained state that the fulfillment of these conditions are problem-dependent and indefinite, which supports the use of non-parametric statistics in the experi- mental analysis. In addition, non-parametric tests can be satisfactorily employed for comparing generic classifiers over various data-sets considering any performance measure. According to these facts, we propose the use of the most powerful non-parametric statistical tests to carry out multiple comparisons. However, the statistical analysis conducted on interpretability must be carefully considered.},
author = {Garc??a, S. and Fern??ndez, Alberto and Luengo, Julian and Herrera, F.},
doi = {10.1007/s00500-008-0392-y},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garca et al. - 2009 - A study of statistical techniques and performance measures for genetics-based machine learning Accuracy and interp.pdf:pdf},
issn = {14327643},
journal = {Soft Computing},
keywords = {Classification,Cohen's kappa,Genetic algorithms,Genetics-based machine learning,Interpretability,Non-parametric tests,Statistical tests},
number = {10},
pages = {959--977},
title = {{A study of statistical techniques and performance measures for genetics-based machine learning: Accuracy and interpretability}},
volume = {13},
year = {2009}
}
@article{Garc??a2009,
abstract = {Classification in imbalanced domains is a recent challenge in data mining. We refer to imbalanced classification when data presents many examples from one class and few from the other class, and the less representative class is the one which has more interest from the point of view of the learning task. One of the most used techniques to tackle this problem consists in preprocessing the data previously to the learning process. This preprocessing could be done through under-sampling; removing examples, mainly belonging to the majority class; and over-sampling, by means of replicating or generating new minority examples. In this paper, we propose an under-sampling procedure guided by evolutionary algorithms to perform a training set selection for enhancing the decision trees obtained by the C4.5 algorithm and the rule sets obtained by PART rule induction algorithm. The proposal has been compared with other under-sampling and over-sampling techniques and the results indicate that the new approach is very competitive in terms of accuracy when comparing with over-sampling and it outperforms standard under-sampling. Moreover, the obtained models are smaller in terms of number of leaves or rules generated and they can considered more interpretable. The results have been contrasted through non-parametric statistical tests over multiple data sets. Crown Copyright ?? 2009.},
author = {Garc??a, Salvador and Fern??ndez, Alberto and Herrera, Francisco},
doi = {10.1016/j.asoc.2009.04.004},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garca, Fernndez, Herrera - 2009 - Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with.pdf:pdf},
isbn = {15684946},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Data reduction,Decision trees,Evolutionary algorithms,Imbalanced classification,Rule induction,Training set selection},
mendeley-groups = {Annotated/Rule-based classiifers},
number = {4},
pages = {1304--1314},
title = {{Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with evolutionary training set selection over imbalanced problems}},
volume = {9},
year = {2009}
}
@article{Garcez2003a,
author = {d'Avila Garcez, Artur S and Lamb, Luis C},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez, Lamb - 2003 - Reasoning about Time and Knowledge in neural-symbolic learning systems.pdf:pdf},
isbn = {0262201526},
issn = {10495258},
journal = {Advances in neural information processing systems},
title = {{Reasoning about Time and Knowledge in neural-symbolic learning systems}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}CS03.pdf},
year = {2003}
}
@article{Garcez2014,
abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
doi = {10.13140/2.1.1779.4243},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez et al. - 2014 - Neural-Symbolic Learning and Reasoning Contributions and Challenges.pdf:pdf},
journal = {Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, Stanford},
number = {November},
pages = {18--21},
title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
year = {2014}
}
@article{Garcez2014,
abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
doi = {10.13140/2.1.1779.4243},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez et al. - 2014 - Neural-Symbolic Learning and Reasoning Contributions and Challenges(2).pdf:pdf},
journal = {Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, Stanford},
pages = {18--21},
title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
year = {2014}
}
@article{Gardenfors2014,
author = {G{\"{a}}rdenfors, Peter},
doi = {10.1007/978-1-4020-9877-2},
file = {:C$\backslash$:/Users/Workk/Documents/Conceptual{\_}Spaces.pdf:pdf},
isbn = {9781402098772},
mendeley-groups = {11Thesis/Conceptual Spaces {\&} Properties},
number = {September},
title = {{Conceptual spaces}},
year = {2014}
}
@article{Garrette2014,
abstract = {First-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, scikit-learned knowledge, for example regarding word meaning. This paper describes a mapping between predicates of logical form and points in a vector space. This mapping is then used to project distributional inferences to inference rules in logical form. We then describe first steps of an approach that uses this mapping to recast first-order semantics into the probabilistic models that are part of Statistical Relational AI. Specifically, we show how Discourse Representation Structures can be combined with distributional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as negation and factivity as well as scikit-learned information on word meaning in context.},
author = {Garrette, Dan and Erk, Katrin and Mooney, Raymond},
doi = {10.1007/978-94-007-7284-7_3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garrette, Erk, Mooney - 2014 - A Formal Approach to Linking Logical Form and Vector-Space Lexical Semantics.pdf:pdf},
isbn = {978-94-007-7283-0},
journal = {Computing Meaning SE - 3},
mendeley-groups = {Progress Report},
pages = {27--48},
title = {{A Formal Approach to Linking Logical Form and Vector-Space Lexical Semantics}},
url = {http://link.springer.com/chapter/10.1007/978-94-007-7284-7{\_}3{\%}5Cnhttp://dx.doi.org/10.1007/978-94-007-7284-7{\_}3},
volume = {47},
year = {2014}
}
@article{GethsiyalAugasta2012a,
abstract = {Artificial neural networks often achieve high classification accuracy$\backslash$nrates, but they are considered as black boxes due to their lack of$\backslash$nexplanation capability. This paper proposes the new rule extraction$\backslash$nalgorithm RxREN to overcome this drawback. In pedagogical approach the$\backslash$nproposed algorithm extracts the rules from trained neural networks for$\backslash$ndatasets with mixed mode attributes. The algorithm relies on reverse$\backslash$nengineering technique to prune the insignificant input neurons and to$\backslash$ndiscover the technological principles of each significant input neuron$\backslash$nof neural network in classification. The novelty of this algorithm lies$\backslash$nin the simplicity of the extracted rules and conditions in rule are$\backslash$ninvolving both discrete and continuous mode of attributes.$\backslash$nExperimentation using six different real datasets namely iris, wbc,$\backslash$nhepatitis, pid, ionosphere and creditg show that the proposed algorithm$\backslash$nis quite efficient in extracting smallest set of rules with high$\backslash$nclassification accuracy than those generated by other neural network$\backslash$nrule extraction methods.},
author = {{Gethsiyal Augasta}, M. and Kathirvalavakumar, T.},
doi = {10.1007/s11063-011-9207-8},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gethsiyal Augasta, Kathirvalavakumar - 2012 - Reverse engineering the neural networks for rule extraction in classification problems(2).pdf:pdf},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
number = {2},
pages = {131--150},
title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
volume = {35},
year = {2012}
}
@article{GethsiyalAugasta2012,
abstract = {Artificial neural networks often achieve high classification accuracy$\backslash$nrates, but they are considered as black boxes due to their lack of$\backslash$nexplanation capability. This paper proposes the new rule extraction$\backslash$nalgorithm RxREN to overcome this drawback. In pedagogical approach the$\backslash$nproposed algorithm extracts the rules from trained neural networks for$\backslash$ndatasets with mixed mode attributes. The algorithm relies on reverse$\backslash$nengineering technique to prune the insignificant input neurons and to$\backslash$ndiscover the technological principles of each significant input neuron$\backslash$nof neural network in classification. The novelty of this algorithm lies$\backslash$nin the simplicity of the extracted rules and conditions in rule are$\backslash$ninvolving both discrete and continuous mode of attributes.$\backslash$nExperimentation using six different real datasets namely iris, wbc,$\backslash$nhepatitis, pid, ionosphere and creditg show that the proposed algorithm$\backslash$nis quite efficient in extracting smallest set of rules with high$\backslash$nclassification accuracy than those generated by other neural network$\backslash$nrule extraction methods.},
author = {{Gethsiyal Augasta}, M. and Kathirvalavakumar, T.},
doi = {10.1007/s11063-011-9207-8},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gethsiyal Augasta, Kathirvalavakumar - 2012 - Reverse engineering the neural networks for rule extraction in classification problems.pdf:pdf},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
number = {2},
pages = {131--150},
title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
volume = {35},
year = {2012}
}
@article{Ghemawat2003,
abstract = {We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.},
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
journal = {ACM SIGOPS Operating Systems Review},
keywords = {clustered storage,data storage,fault tolerance,scalability},
number = {5},
pages = {29},
title = {{The Google file system}},
volume = {37},
year = {2003}
}
@article{Giatsoglou2017,
abstract = {Sentiment analysis and opinion mining are valuable for extraction of useful subjective information out of text documents. These tasks have become of great importance, especially for business and marketing professionals, since online posted products and services reviews impact markets and consumers shifts. This work is motivated by the fact that automating retrieval and detection of sentiments expressed for certain products and services embeds complex processes and pose research challenges, due to the textual phenomena and the language specific expression variations. This paper proposes a fast, flexible, generic methodology for sentiment detection out of textual snippets which express people's opinions in different languages. The proposed methodology adopts a machine learning approach with which textual documents are represented by vectors and are used for training a polarity classification model. Several documents' vector representation approaches have been studied, including lexicon-based, word embedding-based and hybrid vectorizations. The competence of these feature representations for the sentiment classification task is assessed through experiments on four datasets containing online user reviews in both Greek and English languages, in order to represent high and weak inflection language groups. The proposed methodology requires minimal computational resources, thus, it might have impact in real world scenarios where limited resources is the case.},
author = {Giatsoglou, Maria and Vozalis, Manolis G. and Diamantaras, Konstantinos and Vakali, Athena and Sarigiannidis, George and Chatzisavvas, Konstantinos Ch},
doi = {10.1016/j.eswa.2016.10.043},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giatsoglou et al. - 2017 - Sentiment analysis leveraging emotions and word embeddings.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Hybrid vectorization,Machine learning,Multilingual sentiment analysis,Online user reviews,Text analysis,Vector representation},
mendeley-groups = {!Paper 3/task,!Paper 3/task/Sentiment treebank},
pages = {214--224},
publisher = {Elsevier Ltd},
title = {{Sentiment analysis leveraging emotions and word embeddings}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.10.043},
volume = {69},
year = {2017}
}
@article{Gilpin,
archivePrefix = {arXiv},
arxivId = {arXiv:1806.00069v2},
author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
eprint = {arXiv:1806.00069v2},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1806.00069.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
title = {{Explaining Explanations : An Approach to Evaluating Interpretability of Machine Learning}}
}
@article{Giraud-Carrier1998,
author = {Giraud-Carrier, Christophe},
doi = {10.1002/9781119990413.ch1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giraud-Carrier - 1998 - Beyond predictive accuracy what.pdf:pdf},
isbn = {9780470749838},
journal = {Proceedings of the ECML-98 Workshop on Upgrading Learning to Meta-Level: Model Selection and Data Transformation},
pages = {78--85},
title = {{Beyond predictive accuracy: what?}},
year = {1998}
}
@inproceedings{Gladkova2016,
abstract = {This paper presents an analysis of exist- ing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evalua- tions is “interpretability” of word embed- dings: a “good” embedding produces re- sults that make sense in terms of tradi- tional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of dis- tributional meaning representations. We argue for a shift from abstract ratings of word embedding “quality” to exploration of their strengths and weaknesses},
author = {Gladkova, Anna and Drozd, Aleksandr},
booktitle = {Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP},
doi = {10.18653/v1/W16-2507},
isbn = {9781945626142},
mendeley-groups = {Report/Features,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
pages = {36--42},
title = {{Intrinsic Evaluations of Word Embeddings: What Can We Do Better?}},
year = {2016}
}
@article{Gladkova2016,
abstract = {This paper presents an analysis of exist- ing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evalua- tions is “interpretability” of word embed- dings: a “good” embedding produces re- sults that make sense in terms of tradi- tional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of dis- tributional meaning representations. We argue for a shift from abstract ratings of word embedding “quality” to exploration of their strengths and weaknesses},
author = {Gladkova, Anna and Drozd, Aleksandr},
doi = {10.18653/v1/W16-2507},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gladkova, Drozd - 2016 - Intrinsic Evaluations of Word Embeddings What Can We Do Better.pdf:pdf},
isbn = {9781945626142},
journal = {Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP},
mendeley-groups = {Report/Features},
number = {August},
pages = {36--42},
title = {{Intrinsic Evaluations of Word Embeddings: What Can We Do Better?}},
year = {2016}
}
@article{Glorot2011a,
author = {Glorot, X and Bordes, a and Bengio, Y},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain adaptation for large-scale sentiment classification A deep learning approach.pdf:pdf},
keywords = {auto-encoders,deep-learning},
mendeley-groups = {Papers/Paper 1,Interim Review,Report},
number = {1},
title = {{Domain adaptation for large-scale sentiment classification: A deep learning approach}},
url = {http://eprints.pascal-network.org/archive/00008597/},
year = {2011}
}
@article{Glorot2011,
author = {Glorot, X and Bordes, a and Bengio, Y},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain adaptation for large-scale sentiment classification A deep learning approach(2).pdf:pdf},
keywords = {auto-encoders,deep-learning},
mendeley-groups = {Literature Review,Progress Report},
number = {1},
title = {{Domain adaptation for large-scale sentiment classification: A deep learning approach}},
url = {http://eprints.pascal-network.org/archive/00008597/},
year = {2011}
}
@article{Glorot2011,
abstract = {The exponential increase in the availability of online reviews and recommendations makes sentiment classi cation an interesting topic in academic and industrial research. Reviews can span so many di erent domains that it is dicult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classi ers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classi ers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain Adaptation for Large-Scale Sentiment Classification A Deep Learning Approach(3).pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning},
number = {1},
pages = {513--520},
title = {{Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach}},
url = {http://www.icml-2011.org/papers/342{\_}icmlpaper.pdf},
year = {2011}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1017/CBO9781139058452},
eprint = {arXiv:1406.2661v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
mendeley-groups = {Annotated/Representation Learning,Annotated/Generative Adversarial Nets},
pages = {2672--2680},
pmid = {1000183096},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{Goodman2016,
abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {Goodman, Bryce and Flaxman, Seth},
eprint = {1606.08813},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
isbn = {978-0-674-36827-9},
journal = {2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)},
keywords = {machine learning},
mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
number = {Whi},
pages = {26--30},
title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
url = {http://arxiv.org/abs/1606.08813},
year = {2016}
}
@article{Google,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep re-current architecture that combines recent advances in com-puter vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target de-scription sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descrip-tions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4555v2},
author = {Google, Oriol Vinyals and Google, Alexander Toshev and Google, Samy Bengio and Google, Dumitru Erhan},
doi = {10.1109/CVPR.2015.7298935},
eprint = {arXiv:1411.4555v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Google et al. - Unknown - Show and Tell A Neural Image Caption Generator.pdf:pdf},
isbn = {9781467369640},
issn = {9781467369640},
title = {{Show and Tell: A Neural Image Caption Generator}}
}
@article{Goyal2017,
abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\~{}}90{\%} scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.},
archivePrefix = {arXiv},
arxivId = {1706.02677},
author = {Goyal, Priya and Doll{\'{a}}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
doi = {10.1561/2400000003},
eprint = {1706.02677},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet in 1 Hour.pdf:pdf},
isbn = {9781601987167},
issn = {2167-3888},
mendeley-groups = {!Paper 3/Training LSTMs},
title = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
url = {http://arxiv.org/abs/1706.02677},
year = {2017}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {1308.0850},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {1308.0850},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
isbn = {2000201075},
issn = {18792782},
mendeley-groups = {!Paper 3/Training LSTMs},
pages = {1--43},
pmid = {23459267},
title = {{Generating Sequences With Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Greff2016,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2016 - LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
mendeley-groups = {Annotated/Representation Learning},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
year = {2016}
}
@article{Greff2017,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
mendeley-groups = {!Paper 3/Training LSTMs},
number = {10},
pages = {2222--2232},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
volume = {28},
year = {2017}
}
@article{Grgic-Hlaca2017,
abstract = {Consider a binary decision making process where a single machine learning classifier replaces a multitude of humans. We raise questions about the resulting loss of diversity in the decision making process. We study the potential benefits of using random classifier ensembles instead of a single classifier in the context of fairness-aware learning and demonstrate various attractive properties: (i) an ensemble of fair classifiers is guaranteed to be fair, for several different measures of fairness, (ii) an ensemble of unfair classifiers can still achieve fair outcomes, and (iii) an ensemble of classifiers can achieve better accuracy-fairness trade-offs than a single classifier. Finally, we introduce notions of distributional fairness to characterize further potential benefits of random classifier ensembles.},
archivePrefix = {arXiv},
arxivId = {1706.10208},
author = {Grgi{\'{c}}-Hla{\v{c}}a, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
eprint = {1706.10208},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grgi{\'{c}}-Hla{\v{c}}a et al. - 2017 - On Fairness, Diversity and Randomness in Algorithmic Decision Making.pdf:pdf},
mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
title = {{On Fairness, Diversity and Randomness in Algorithmic Decision Making}},
url = {http://arxiv.org/abs/1706.10208},
year = {2017}
}
@article{Grodzicki2008a,
abstract = {This paper considers the multilabel classification problem, which$\backslash$nis a generalization of traditional two-class or multi-class classification$\backslash$nproblem. In multilabel classification a set of labels (categories)$\backslash$nis given and each training instance is associated with a subset of$\backslash$nthis label-set. The task is to output the appropriate subset of labels$\backslash$n(generally of unknown size) for a given, unknown testing instance.$\backslash$nSome improvements to the existing neural network multilabel classification$\backslash$nalgorithm, named BP-MLL, are proposed here. The modifications concern$\backslash$nthe form of the global error function used in BP-MLL. The modified$\backslash$nclassification system is tested in the domain of functional genomics,$\backslash$non the yeast genome data set. Experimental results show that proposed$\backslash$nmodifications visibly improve the performance of the neural network$\backslash$nbased multilabel classifier. The results are statistically significant.$\backslash$n漏 2008 Springer-Verlag Berlin Heidelberg.},
author = {Grodzicki, Rafa{\l} and Ma{\'{n}}dziuk, Jacek and Wang, Lipo},
doi = {10.1007/978-3-540-87700-4_41},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grodzicki, Ma{\'{n}}dziuk, Wang - 2008 - Improved multilabel classification with neural networks.pdf:pdf},
isbn = {3540876995},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Backpropagation,Bioinformatics,Functional genomics,Learning system,Multilabel,Neural network},
number = {2},
pages = {409--416},
title = {{Improved multilabel classification with neural networks}},
volume = {5199 LNCS},
year = {2008}
}
@article{Grosenick2008,
author = {Grosenick, Logan and Greer, Stephanie and Knutson, Brian},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grosenick, Greer, Knutson - 2008 - Interpretable classi ers for FMRI improve prediction of purchases.pdf:pdf},
journal = {Analysis},
mendeley-groups = {Report},
number = {Xx},
pages = {1--10},
title = {{Interpretable classi ers for FMRI improve prediction of purchases}},
volume = {X},
year = {2008}
}
@article{Grosenick2008,
abstract = {{\textless}para{\textgreater} Despite growing interest in applying machine learning to neuroimaging analyses, few studies have gone beyond classifying sensory input to directly predicting behavioral output. With spatial resolution on the order of millimeters and temporal r...},
author = {Grosenick, Logan and Greer, Stephanie and Knutson, Brian},
doi = {10.1109/TNSRE.2008.926701},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grosenick, Greer, Knutson - 2008 - Interpretable Classifiers for fMRI Improve Prediction of Purchases.pdf:pdf},
isbn = {1558-0210 (Electronic)},
issn = {15580210},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
keywords = {Accumbens,classification,discriminant,elastic net,frontal,functional magnetic resonance imaging (fMRI),human,insula,lasso,penalized discriminant analysis (PDA),prediction,purchasing,single-trial,sparse,spatiotemporal,support vector machine (SVM)},
mendeley-groups = {Annotated/Rule-based classiifers},
number = {6},
pages = {539--548},
pmid = {19144586},
title = {{Interpretable Classifiers for fMRI Improve Prediction of Purchases}},
volume = {16},
year = {2008}
}
@article{Guarini2013,
author = {Guarini, Marcello},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guarini - 2013 - Scholarship at UWindsor Case Classification , Similarities , Spaces of Reasons , and Coherences C ASE C LASSIFICATION ,.pdf:pdf},
pages = {187--201},
title = {{Scholarship at UWindsor Case Classification , Similarities , Spaces of Reasons , and Coherences C ASE C LASSIFICATION , S IMILARITIES ,}},
year = {2013}
}
@article{Guidotti2018,
archivePrefix = {arXiv},
arxivId = {1802.01933},
author = {Guidotti, Riccardo and Monreale, Anna and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
eprint = {1802.01933},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guidotti et al. - 2018 - A Survey Of Methods For Explaining Black Box Models.pdf:pdf},
keywords = {KDD Lab,survey},
mendeley-groups = {!Paper 3},
pages = {1--45},
title = {{A Survey Of Methods For Explaining Black Box Models}},
year = {2018}
}
@article{Guillame-Bert2010,
abstract = {Artificial Neural Networks have previously been applied in neuro-symbolic learning to learn ground logic program rules. However, there are few results of learning relations using neuro-symbolic learning. This paper presents the system PAN, which can learn relations. The inputs to PAN are one or more atoms, representing the conditions of a logic rule, and the output is the conclusion of the rule. The symbolic inputs may include functional terms of arbitrary depth and arity, and the output may include terms constructed from the input functors. Symbolic inputs are encoded as an integer using an invertible encoding function, which is used in reverse to extract the output terms. The main advance of this system is a convention to allow construction of Artificial Neural Networks able to learn rules with the same power of expression as first order definite clauses. The system is tested on three examples and the results are discussed.},
author = {Guillame-Bert, M. and Broda, K. and d'Avila Garcez, a.},
doi = {10.1109/IJCNN.2010.5596491},
isbn = {978-1-4244-6916-1},
issn = {1098-7576},
journal = {Neural Networks (IJCNN), The 2010 International Joint Conference on},
pages = {18--23},
title = {{First-order logic learning in Artificial Neural Networks}},
year = {2010}
}
@article{Guo2014,
abstract = {Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presented approaches can be integrated into most of the classical linear models in NLP. Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features, among which the distributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score},
author = {Guo, Jiang and Che, Wanxiang and Wang, Haifeng and Liu, Ting},
isbn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
mendeley-groups = {Report/Features},
number = {2005},
pages = {110--120},
pmid = {1685741},
title = {{Revisiting Embedding Features for Simple Semi-supervised Learning}},
url = {http://www.aclweb.org/anthology/D14-1012.pdf},
year = {2014}
}
@article{Gupta2015,
abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05192v1},
author = {Gupta, Abhinav and Efros, Alexei a},
doi = {10.1109/ICCV.2015.167},
eprint = {arXiv:1505.05192v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta, Efros - 2015 - Unsupervised Visual Representation Learning by Context Prediction.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
journal = {arXiv preprint},
mendeley-groups = {Report/Features},
pages = {1422--1430},
pmid = {903},
title = {{Unsupervised Visual Representation Learning by Context Prediction}},
year = {2015}
}
@article{H.~Zou2006,
abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
archivePrefix = {arXiv},
arxivId = {1205.0121v2},
author = {H.{\~{}}Zou and T.{\~{}}Hastie and R.{\~{}}Tibshirani and Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1198/106186006X113430},
eprint = {1205.0121v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H.{\~{}}Zou et al. - 2006 - Sparse principal component analysis.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {arrays,ca 94305,composition,d student in the,department of statistics at,edu,elastic net,email,gene expression,gene expression arrays,hui zou is a,hzou,lasso,multivariate analysis,ph,singular,singular value de-,stanford,stanford university,stat,thresholding,value decomposition},
mendeley-groups = {Annotated/NMF},
number = {2},
pages = {265--286},
pmid = {21811560},
title = {{Sparse principal component analysis}},
volume = {15},
year = {2006}
}
@article{Hamilton2014,
abstract = {The rise in prevalence of algorithmically curated feeds in online news and social media sites raises a new question for designers, critics, and scholars of media: how aware are users of the role of algorithms and filters in their news sources? This paper situates this problem within the history of design for interaction, with an emphasis on the contemporary challenges of studying, and designing for, the algorithmic "curation" of feeds. Such a problem presents particular challenges when, as is common, neither the user nor the researcher has access to the actual proprietary algorithms at work. Author},
author = {Hamilton, Kevin and Karahalios, Karrie and Sandvig, Christian and Eslami, Motahhare},
doi = {10.1145/2559206.2578883},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamilton et al. - 2014 - A path to understanding the effects of algorithm awareness.pdf:pdf},
isbn = {9781450324748},
journal = {Proceedings of the extended abstracts of the 32nd annual ACM conference on Human factors in computing systems - CHI EA '14},
mendeley-groups = {Annotated/Psychology},
pages = {631--642},
title = {{A path to understanding the effects of algorithm awareness}},
url = {http://dl.acm.org/citation.cfm?doid=2559206.2578883},
year = {2014}
}
@article{Han2010a,
abstract = {Due to the visually polysemous barrier, videos and images may be annotated by multiple tags. Discovering the correlations among different tags can significantly help predicting precise labels for videos and images. Many of recent studies toward multi-label learning construct a linear subspace embed- ding with encoded multi-label information, such that data points sharing many common labels tend to be close to each other in the embedded subspace. Motivated by the advances of compressive sensing research, a sparse representation that selects a compact subset to describe the input data can be more discriminative. In this paper, we propose a sparse multi-label learning method to circumvent the visually polysemous barrier of multiple tags. Our approach learns a multi-label encoded sparse linear embedding space from a related dataset, and maps the target data into the learned new representation space to achieve better annotation performance. Instead of using l1-norm penalty (lasso) to induce sparse representation, we propose to formulate the multi-label learning as a penalized least squares optimization problem with elastic-net penalty. By casting the video concept detection and image annotation tasks into a sparse multi-label transfer learning framework in this paper, ridge regression, lasso, elastic net, and the multi-label extended sparse discriminant analysis methods are, respectively, well explored and compared},
author = {Han, Yahong and Wu, Fei and Zhuang, Yueting and He, Xiaofei},
doi = {10.1109/TCSVT.2010.2057015},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2010 - Multi-label transfer learning with sparse representation.pdf:pdf},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Image annotation,multi-label learning,sparse representation,transfer learning,video concept detection},
number = {8},
pages = {1110--1121},
title = {{Multi-label transfer learning with sparse representation}},
volume = {20},
year = {2010}
}
@article{Hara2016,
abstract = {Tree ensembles such as random forests and boosted trees are renowned for their high prediction performance; however, their interpretability is critically limited. One way of interpreting a complex tree ensemble is to obtain its simplified representation, which is formalized as a model selection problem: Given a complex tree ensemble, we want to obtain the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm. Our approach has three appealing features: the prediction performance is maintained, the coverage is sufficiently large, and the computation is reasonably feasible. Our synthetic data experiment and real world data applications show that complicated tree ensembles are approximated reasonably as interpretable.},
archivePrefix = {arXiv},
arxivId = {1606.09066},
author = {Hara, Satoshi and Hayashi, Kohei},
eprint = {1606.09066},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara, Hayashi - 2016 - Making Tree Ensembles Interpretable A Bayesian Model Selection Approach.pdf:pdf},
mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
number = {Whi},
title = {{Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach}},
url = {http://arxiv.org/abs/1606.09066},
year = {2016}
}
@article{Hardt2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.02413},
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
eprint = {arXiv:1610.02413},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/6374-equality-of-opportunity-in-supervised-learning.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability/Discrimination},
number = {Nips},
title = {{Equality of Opportunity in Supervised Learning}},
year = {2016}
}
@article{Hatzilygeroudis2004,
abstract = {In this paper, we first present and compare existing categorization schemes for neuro-symbolic$\backslash$napproaches. We then stress the point that not all hybrid neuro-symbolic approaches can be$\backslash$naccommodated by existing categories. Such a case is rule-based neuro-symbolic approaches that propose$\backslash$na unified knowledge representation scheme suitable for use in expert systems. That kind of integrated$\backslash$nschemes have the two component approaches tightly and indistinguishably integrated, offer an$\backslash$ninteractive inference engine and can provide explanations. Therefore, we introduce a new category of$\backslash$nneuro-symbolic integrations, namely {\{}{\^{a}}{\}}€˜representational integrations{\{}{\^{a}}{\}}€™. Furthermore, two sub-categories of$\backslash$nrepresentational integrations are distinguished, based on which of the two component approaches of the$\backslash$nintegrations is given pre-eminence. Representative approaches as well as advantages and disadvantages$\backslash$nof both sub-categories are discussed.},
author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatzilygeroudis, Prentzas - 2004 - Neuro-symbolic approaches for knowledge representation in expert systems.pdf:pdf},
journal = {International Journal of Hybrid Intelligent Systems},
keywords = {connectionist expert systems,neuro-symbolic integrations,rule-based expert systems},
mendeley-groups = {Progress Report},
number = {3, 4},
pages = {111--126},
title = {{Neuro-symbolic approaches for knowledge representation in expert systems}},
url = {http://dl.acm.org/citation.cfm?id=1232821},
volume = {1},
year = {2004}
}
@misc{Hatzilygeroudis2010,
abstract = {Neurules are a kind of integrated rules integrating neurocomputing and production rules. Each neurule is represented as an adaline unit. Thus, the corresponding neurule base consists of a number of autonomous adaline units (neurules). In this paper, we present the construction process and the inference mechanism of neurules and explore their generalization capabilities. The construction process, which also implements corresponding learning algorithm, creates neurules from a given empirical data set. The inference mechanism of neurules is integrated in its nature; it combines neurocomputing with symbolic processes. It is also interactive, i.e., it interacts with the user asking him/her to provide values for some variables necessary to carry on inference. As shown via experiments, the neurules' integrated inference mechanism is more efficient than the inference mechanism used in connectionist expert systems. Furthermore, neurules generalize much better than their constituent neural component (adaline unit) and are comparable to the backpropagation neural net (BPNN).},
author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2010.79},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatzilygeroudis, Prentzas - 2010 - Integrated rule-based learning and inference.htm:htm},
isbn = {1041-4347},
issn = {10414347},
keywords = {Neurosymbolic integration,integrated inference,neurocomputing,rule-based reasoning},
number = {11},
pages = {1549--1562},
title = {{Integrated rule-based learning and inference}},
volume = {22},
year = {2010}
}
@article{Haury2011,
abstract = {Biomarker discovery from high-dimensional data is a crucial problem with enormous applications in biology and medicine. It is also extremely challenging from a statistical viewpoint, but surprisingly few studies have investigated the relative strengths and weaknesses of the plethora of existing feature selection methods. In this study we compare feature selection methods on public gene expression datasets for breast cancer prognosis, in terms of predictive performance, stability and functional interpretability of the signatures they produce. We observe that the feature selection method has a significant influence on the accuracy, stability and interpretability of signatures. Surprisingly, complex wrapper and embedded methods generally do not outperform simple univariate feature selection methods, and ensemble feature selection has generally no positive effect. Overall a simple Student's t-test seems to provide the best results.},
archivePrefix = {arXiv},
arxivId = {1101.5008},
author = {Haury, Anne Claire and Gestraud, Pierre and Vert, Jean Philippe},
doi = {10.1371/journal.pone.0028210},
eprint = {1101.5008},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haury, Gestraud, Vert - 2011 - The influence of feature selection methods on accuracy, stability and interpretability of molecular signa.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {Annotated/Applications/Scientific Discovery},
number = {12},
pages = {1--12},
pmid = {22205940},
title = {{The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures}},
volume = {6},
year = {2011}
}
@article{Hauser2010,
abstract = {The authors test methods, based on cognitively simple decision rules, that predict which products consumers select for their consideration sets. Drawing on qualitative research, the authors propose disjunctions-of- conjunctions (DOC) decision rules that generalize well-studied decision models, such as disjunctive, conjunctive, lexicographic, and subset conjunctive rules. They propose two machine-learning methods to estimate cognitively simple DOC rules. They observe consumers' consideration sets for global positioning systems for both calibration and validation data.They compare the proposed methods with both machine- learning and hierarchical Bayes methods, each based on five extant compensatory and noncompensatory rules. For the validation data, the cognitively simple DOC-based methods predict better than the ten benchmark methods on an information theoretic measure and on hit rates. The results are robust with respect to format by which consideration is measured, sample, and presentation of profiles. The article closes with an illustration of how DOC-based rules can affect managerial decisions.},
author = {Hauser, John R and Toubia, Olivier and Evgeniou, Theodoros and Befurt, Rene and Dzyabura, Daria},
doi = {10.1509/jmkr.47.3.485},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hauser et al. - 2010 - Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets.pdf:pdf},
isbn = {00222437},
issn = {0022-2437},
journal = {Journal of Marketing Research},
keywords = {cognitive,conjoint analysis,consideration sets,consumer,decision theory,disjunctions of conjunctions,heuristics,lexicography,machine learning,noncompensatory decisions,simplicity},
mendeley-groups = {Annotated/Applications/Marketing},
number = {3},
pages = {485--496},
pmid = {50522113},
title = {{Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets}},
volume = {47},
year = {2010}
}
@article{Hayes2017,
abstract = {Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.},
author = {Hayes, Bradley and Shah, Julie A.},
doi = {10.1145/2909824.3020233},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hayes, Shah - 2017 - Improving Robot Controller Transparency Through Autonomous Policy Explanation.pdf:pdf},
isbn = {9781450343367},
issn = {21672148},
journal = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17},
mendeley-groups = {Annotated/Overarching Interpretability},
pages = {303--312},
title = {{Improving Robot Controller Transparency Through Autonomous Policy Explanation}},
url = {http://dl.acm.org/citation.cfm?doid=2909824.3020233},
year = {2017}
}
@article{Hechtlinger2016,
abstract = {State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a "black box". In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing.},
archivePrefix = {arXiv},
arxivId = {1611.07634},
author = {Hechtlinger, Yotam},
eprint = {1611.07634},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hechtlinger - 2016 - Interpretation of Prediction Models Using the Input Gradient.pdf:pdf},
mendeley-groups = {Report/Explaining predictions},
number = {Nips},
title = {{Interpretation of Prediction Models Using the Input Gradient}},
url = {http://arxiv.org/abs/1611.07634},
year = {2016}
}
@article{Herlocker2000,
abstract = {Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.},
archivePrefix = {arXiv},
arxivId = {48},
author = {Herlocker, Jonathan L and Konstan, Joseph a and Riedl, John},
doi = {10.1145/358916.358995},
eprint = {48},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herlocker, Konstan, Riedl - 2000 - Explaining collaborative filtering recommendations.pdf:pdf},
isbn = {1581132220},
issn = {00318655},
journal = {Proceedings of the 2000 ACM conference on Computer supported cooperative work},
keywords = {Explanations,GroupLens,MovieLens,collaborative filtering,recommender systems},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
pages = {241--250},
pmid = {8234466},
title = {{Explaining collaborative filtering recommendations}},
url = {http://dl.acm.org/citation.cfm?id=358995},
year = {2000}
}
@article{Hinton2015a,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network.pdf:pdf},
journal = {NIPS 2014 Deep Learning Workshop},
mendeley-groups = {Literature Review,Interim Review},
pages = {1--9},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Ho2006,
abstract = {An accurate classifier with linguistic interpretability using a small number of relevant genes is beneficial to microarray data analysis and development of inexpensive diagnostic tests. Several frequently used techniques for designing classifiers of microarray data, such as support vector machine, neural networks, k-nearest neighbor, and logistic regression model, suffer from low interpretabilities. This paper proposes an interpretable gene expression classifier (named iGEC) with an accurate and compact fuzzy rule base for microarray data analysis. The design of iGEC has three objectives to be simultaneously optimized: maximal classification accuracy, minimal number of rules, and minimal number of used genes. An "intelligent" genetic algorithm IGA is used to efficiently solve the design problem with a large number of tuning parameters. The performance of iGEC is evaluated using eight commonly-used data sets. It is shown that iGEC has an accurate, concise, and interpretable rule base (1.1 rules per class) on average in terms of test classification accuracy (87.9{\%}), rule number (3.9), and used gene number (5.0). Moreover, iGEC not only has better performance than the existing fuzzy rule-based classifier in terms of the above-mentioned objectives, but also is more accurate than some existing non-rule-based classifiers. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Ho, Shinn Ying and Hsieh, Chih Hung and Chen, Hung Ming and Huang, Hui Ling},
doi = {10.1016/j.biosystems.2006.01.002},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ho et al. - 2006 - Interpretable gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis.pdf:pdf},
issn = {03032647},
journal = {BioSystems},
keywords = {Fuzzy classifier,Gene expression,Intelligent genetic algorithm,Microarray data analysis,Pattern recognition},
mendeley-groups = {Report},
number = {3},
pages = {165--176},
pmid = {16490299},
title = {{Interpretable gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis}},
volume = {85},
year = {2006}
}
@article{Hornik1993a,
abstract = {We show that standard feedforward networks with as few as a single hidden layer can uniformly approximate continuous functions on compacta provided that the activation function $\psi$ is locally Riemann integrable and nonpolynomial, and have universal Lp ($\mu$) approximation capabilities for finite and compactly supported input environment measures $\mu$ provided that $\psi$ is locally bounded and nonpolynomial. In both cases, the input-to-hidden scikit-learns and hidden layer biases can be constrained to arbitrarily small sets; if in addition $\psi$ is locally analytic a single universal bias will do.},
author = {Hornik, Kurt},
doi = {10.1016/S0893-6080(09)80018-X},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik - 1993 - Some new results on neural network approximation.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {-universal approximation capabilit ies,feedfor wa rd networks,small scikit-learn sets,universal bias},
number = {8},
pages = {1069--1072},
publisher = {Pergamon Press Ltd.},
title = {{Some new results on neural network approximation}},
url = {http://www.sciencedirect.com/science/article/pii/S089360800980018X},
volume = {6},
year = {1993}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Hoyer2004,
abstract = {Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of `sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.},
archivePrefix = {arXiv},
arxivId = {cs/0408058},
author = {Hoyer, Patrik O.},
doi = {10.1109/ICMLC.2011.6016966},
eprint = {0408058},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoyer - 2004 - Non-negative matrix factorization with sparseness constraints.pdf:pdf},
isbn = {0780395174},
issn = {1532-4435},
keywords = {data-adaptive representations,non-negative matrix factorization,sparseness},
mendeley-groups = {Annotated/NMF},
pages = {1457--1469},
pmid = {1000253614},
primaryClass = {cs},
title = {{Non-negative matrix factorization with sparseness constraints}},
url = {http://arxiv.org/abs/cs/0408058},
volume = {5},
year = {2004}
}
@misc{Hruschka2006a,
abstract = {Multilayer perceptrons adjust their internal parameters performing vector mappings from the input to the output space. Although they may achieve high classification accuracy, the knowledge acquired by such neural networks is usually incomprehensible for humans. This fact is a major obstacle in data mining applications, in which ultimately understandable patterns (like classification rules) are very important. Therefore, many algorithms for rule extraction from neural networks have been developed. This work presents a method to extract rules from multilayer perceptrons trained in classification problems. The rule extraction algorithm basically consists of two steps. First, a clustering genetic algorithm is applied to find clusters of hidden unit activation values. Then, classification rules describing these clusters, in relation to the inputs, are generated. The proposed approach is experimentally evaluated in four datasets that are benchmarks for data mining applications and in a real-world meteorological dataset, leading to interesting results. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Hruschka, Eduardo R. and Ebecken, Nelson F F},
booktitle = {Neurocomputing},
doi = {10.1016/j.neucom.2005.12.127},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hruschka, Ebecken - 2006 - Extracting rules from multilayer perceptrons in classification problems A clustering-based approach.htm:htm},
issn = {09252312},
keywords = {Clustering,Genetic algorithms,Rule extraction from neural networks},
number = {1-3},
pages = {384--397},
title = {{Extracting rules from multilayer perceptrons in classification problems: A clustering-based approach}},
volume = {70},
year = {2006}
}
@article{Hu2016,
abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the scikit-learns of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
archivePrefix = {arXiv},
arxivId = {1603.06318},
author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
eprint = {1603.06318},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Papers/Paper 1,Literature Review,Annotated/Rule-based classiifers,Report},
pages = {1--18},
title = {{Harnessing Deep Neural Networks with Logic Rules}},
url = {http://arxiv.org/abs/1603.06318},
year = {2016}
}
@article{Hu2016,
abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the scikit-learns of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
archivePrefix = {arXiv},
arxivId = {1603.06318},
author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
doi = {10.18653/v1/P16-1228},
eprint = {1603.06318},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules(2).pdf:pdf},
journal = {Acl},
mendeley-groups = {Progress Report,Interim Review},
pages = {1--18},
title = {{Harnessing Deep Neural Networks with Logic Rules}},
url = {http://arxiv.org/abs/1603.06318},
year = {2016}
}
@article{Hu2017,
abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
archivePrefix = {arXiv},
arxivId = {1703.00955},
author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1703.00955},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2017 - Toward Controlled Generation of Text.pdf:pdf},
title = {{Toward Controlled Generation of Text}},
url = {http://arxiv.org/abs/1703.00955},
year = {2017}
}
@article{Huang,
author = {Huang, Sheng and Peng, Xueping and Niu, Zhendong and Wang, Kunshan},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - News Topic Detection based on Hierarchical Clustering and Named Entity.pdf:pdf},
isbn = {9781612847290},
keywords = {-news topic detection,agglomerative hierarchical,clustering,named entity,vector space model},
mendeley-groups = {Report/Clustering},
pages = {280--284},
title = {{News Topic Detection based on Hierarchical Clustering and Named Entity}}
}
@article{Hullermeier2008,
abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison (RPC), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (scikit-learned) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare RPC to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that RPC is superior in terms of computational efficiency, and at least competitive in terms of accuracy. ?? 2008 Elsevier B.V. All rights reserved.},
author = {H{\"{u}}llermeier, Eyke and F{\"{u}}rnkranz, Johannes and Cheng, Weiwei and Brinker, Klaus},
doi = {10.1016/j.artint.2008.08.002},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{u}}llermeier et al. - 2008 - Label ranking by learning pairwise preferences.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Constraint classification,Pairwise classification,Preference learning,Ranking},
mendeley-groups = {Annotated/Ranking,Progress Report},
number = {16-17},
pages = {1897--1916},
title = {{Label ranking by learning pairwise preferences}},
volume = {172},
year = {2008}
}
@article{Huynh2011a,
abstract = {The production of relatively large and opaque scikit-learn matrices by error backpropagation learning has inspired substantial research on how to extract symbolic human-readable rules from trained networks. While considerable progress has been made, the results at present are still relatively limited, in part due to the large numbers of symbolic rules that can be generated. Most past work to address this issue has focused on progressively more powerful methods for rule extraction (RE) that try to minimize the number of scikit-learns and/or improve rule expressiveness. In contrast, here we take a different approach in which we modify the error backpropagation training process so that it learns a different hidden layer representation of input patterns than would normally occur. Using five publicly available datasets, we show via computational experiments that the modified learning method helps to extract fewer rules without increasing individual rule complexity and without decreasing classification accuracy. We conclude that modifying error backpropagation so that it more effectively separates learned pattern encodings in the hidden layer is an effective way to improve contemporary RE methods.},
author = {Huynh, Thuan Q. and Reggia, James A.},
doi = {10.1109/TNN.2010.2094205},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huynh, Reggia - 2011 - Guiding hidden layer representations for improved rule extraction from neural networks.pdf:pdf},
isbn = {1941-0093 (Electronic)$\backslash$n1045-9227 (Linking)},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Hidden layer representation,neural networks,penalty function,rule extraction},
number = {2},
pages = {264--275},
pmid = {21138801},
title = {{Guiding hidden layer representations for improved rule extraction from neural networks}},
volume = {22},
year = {2011}
}
@article{Huysmans2011,
abstract = {An important objective of data mining is the development of predictive models. Based on a number of observations, a model is constructed that allows the analysts to provide classifications or predictions for new observations. Currently, most research focuses on improving the accuracy or precision of these models and comparatively little research has been undertaken to increase their comprehensibility to the analyst or end-user. This is mainly due to the subjective nature of 'comprehensibility', which depends on many factors outside the model, such as the user's experience and his/her prior knowledge. Despite this influence of the observer, some representation formats are generally considered to be more easily interpretable than others. In this paper, an empirical study is presented which investigates the suitability of a number of alternative representation formats for classification when interpretability is a key requirement. The formats under consideration are decision tables, (binary) decision trees, propositional rules, and oblique rules. An end-user experiment was designed to test the accuracy, response time, and answer confidence for a set of problem-solving tasks involving the former representations. Analysis of the results reveals that decision tables perform significantly better on all three criteria, while post-test voting also reveals a clear preference of users for decision tables in terms of ease of use. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Huysmans, Johan and Dejaeger, Karel and Mues, Christophe and Vanthienen, Jan and Baesens, Bart},
doi = {10.1016/j.dss.2010.12.003},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huysmans et al. - 2011 - An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models.pdf:pdf},
isbn = {0167-9236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Classification,Comprehensibility,Data mining,Decision tables,Knowledge representation},
mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
number = {1},
pages = {141--154},
publisher = {Elsevier B.V.},
title = {{An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models}},
url = {http://dx.doi.org/10.1016/j.dss.2010.12.003},
volume = {51},
year = {2011}
}
@article{Inan2016,
abstract = {Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.},
archivePrefix = {arXiv},
arxivId = {1611.01462},
author = {Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
eprint = {1611.01462},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inan, Khosravi, Socher - 2016 - Tying Word Vectors and Word Classifiers A Loss Framework for Language Modeling.pdf:pdf},
mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
pages = {1--13},
title = {{Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling}},
url = {http://arxiv.org/abs/1611.01462},
year = {2016}
}
@article{Isinkaye2015,
author = {Isinkaye, F O},
doi = {10.1016/j.eij.2015.06.005},
file = {:D$\backslash$:/PhD/PGR/1-s2.0-S1110866515000341-main.pdf:pdf},
keywords = {collaborative filtering,content-based filtering,hybrid filtering technique},
pages = {261--273},
publisher = {Ministry of Higher Education and Scientific Research},
title = {{Recommendation systems : Principles , methods and evaluation}},
year = {2015}
}
@article{Iyyer2015a,
author = {Iyyer, M and Manjunatha, V},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iyyer, Manjunatha - 2015 - Deep unordered composition rivals syntactic methods for text classification.pdf:pdf},
journal = {Proceedings of the 53rd  {\ldots}},
mendeley-groups = {Literature Review},
title = {{Deep unordered composition rivals syntactic methods for text classification}},
url = {https://www.cs.colorado.edu/{~}jbg/docs/2015{\_}acl{\_}dan.pdf},
year = {2015}
}
@article{Jacobsson2005,
abstract = {Rule extraction (RE) from recurrent neural networks (RNNs) refers to finding models of the underlying RNN, typically in the form of finite state machines, that mimic the network to a satisfactory degree while having the advantage of being more transparent. RE from RNNs can be argued to allow a deeper and more profound form of analysis of RNNs than other, more or less ad hoc methods. RE may give us understanding of RNNs in the intermediate levels between quite abstract theoretical knowledge of RNNs as a class of computing devices and quantitative performance evaluations of RNN instantiations. The development of techniques for extraction of rules from RNNs has been an active field since the early 1990s. This article reviews the progress of this development and analyzes it in detail. In order to structure the survey and evaluate the techniques, a taxonomy specifically designed for this purpose has been developed. Moreover, important open research issues are identified that, if addressed properly, possibly ca...},
author = {Jacobsson, Henrik},
doi = {10.1162/0899766053630350},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobsson - 2005 - Rule Extraction from Recurrent Neural Networks ATaxonomy and Review.pdf:pdf},
isbn = {0899766053630350},
issn = {0899-7667},
journal = {Neural Computation},
number = {6},
pages = {1223--1263},
title = {{Rule Extraction from Recurrent Neural Networks: ATaxonomy and Review}},
volume = {17},
year = {2005}
}
@article{Ji2017,
abstract = {We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.},
archivePrefix = {arXiv},
arxivId = {1702.01829},
author = {Ji, Yangfeng and Smith, Noah},
doi = {10.18653/v1/P17-1092},
eprint = {1702.01829},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji, Smith - 2017 - Neural Discourse Structure for Text Categorization.pdf:pdf},
isbn = {9781945626753},
mendeley-groups = {!Paper 3/Structured LSTMs,!Paper 3/task/Yelp},
title = {{Neural Discourse Structure for Text Categorization}},
url = {http://arxiv.org/abs/1702.01829},
year = {2017}
}
@article{Jiang2016,
abstract = {This paper involves deriving high quality information from unstructured text data through the integration of rich document representations to improve machine learning text classification problems. Previous research has applied Neural Network Language Models (NNLMs) to document classification performance, and word vector representations have been used to measure semantics among text. Never have they been combined together and shown to have improved text classification performance. Our belief is that the inference and clustering abilities of word vectors coupled with the power of a neural network can create more accurate classification predictions. The first phase our work focused on word vector representations for classification purposes. This approach included analyzing two distinct text sources with pre-marked binary outcomes for classification, creating a benchmark metric, and comparing against word vector representations within the feature space as a classifier. The results showed promise, obtaining an area under the curve of 0.95 utilizing word vectors, relative to the benchmark case of 0.93. The second phase of the project focused on utilizing an extension of the neural network model used in phase one to represent a document in its entirety as opposed to being represented word by word. Preliminary results indicated a slight improvement over the baseline model of approximately 2-3 percent.},
author = {Jiang, Suqi and Lewris, Jason and Voltmer, Michael and Wang, Hongning},
doi = {10.1109/SIEDS.2016.7489319},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2016 - Integrating rich document representations for text classification.pdf:pdf},
isbn = {9781509009701},
journal = {2016 IEEE Systems and Information Engineering Design Symposium, SIEDS 2016},
keywords = {Natural Language Processing,Text Classification,Text Mining,Word2vec},
mendeley-groups = {Annotated/Document representation},
pages = {303--308},
title = {{Integrating rich document representations for text classification}},
year = {2016}
}
@article{Jiang2003,
abstract = {A new approach of constructing andtraining neural networks for pattern classi{\$}cation is proposed. Data clusters are generated andtrainedsequentially basedon distinct local subsets of the training data. Obtainedclusters are then usedto construct a feed-forwardnetwork, which is further trainedusing standardalgorithms operating on the global training set. The network obtained using this approach e6ectively inherits the knowledge from the local training procedure before improving on its generalization ability through the subsequent global training. Various experiments demonstrate the superiority of this approach over competing methods.},
author = {Jiang, Xudong and {Kam Sie Wah}, Alvin Harvey},
doi = {10.1016/S0031-3203(02)00087-0},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Kam Sie Wah - 2003 - Constructing and training feed-forward neural networks for pattern classification.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Classification,Clustering,Generalization,Local andglobal training,Neural networks},
pages = {853--867},
title = {{Constructing and training feed-forward neural networks for pattern classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320302000870},
volume = {36},
year = {2003}
}
@article{Joachims1998,
abstract = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.},
author = {Joachims, Thorsten},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims - 1998 - 1 Introduction 2 Text Categorization 3 Support Vector Machines.pdf:pdf},
isbn = {3540644172},
journal = {Machine Learning},
number = {LS-8 Report 23},
pages = {137--142},
title = {{1 Introduction 2 Text Categorization 3 Support Vector Machines}},
url = {http://www.springerlink.com/index/drhq581108850171.pdf},
volume = {1398},
year = {1998}
}
@article{Johansson2009,
abstract = {Some data mining problems require predictive models to be not only accurate but also comprehensible. Comprehensibility enables human inspection and understanding of the model, making it possible to trace why individual predictions are made. Since most high-accuracy techniques produce opaque models, accuracy is, in practice, regularly sacrificed for comprehensibility. One frequently studied technique, often able to reduce this accuracy vs. comprehensibility tradeoff, is rule extraction, i.e., the activity where another, transparent, model is generated from the opaque. In this paper, it is argued that techniques producing transparent models, either directly from the dataset, or from an opaque model, could benefit from using an oracle guide. In the experiments, genetic programming is used to evolve decision trees, and a neural network ensemble is used as the oracle guide. More specifically, the datasets used by the genetic programming when evolving the decision trees, consist of several different combinations of the original training data and ldquooracle datardquo, i.e., training or test data instances, together with corresponding predictions from the oracle. In total, seven different ways of combining regular training data with oracle data were evaluated, and the results, obtained on 26 UCI datasets, clearly show that the use of an oracle guide improved the performance. As a matter of fact, trees evolved using training data only had the worst test set accuracy of all setups evaluated. Furthermore, statistical tests show that two setups, both using the oracle guide, produced significantly more accurate trees, compared to the setup using training data only.},
author = {Johansson, Ulf and Niklasson, Lars},
doi = {10.1109/CIDM.2009.4938655},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johansson, Niklasson - 2009 - Evolving decision trees using oracle guides.pdf:pdf},
isbn = {9781424427659},
journal = {2009 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2009 - Proceedings},
mendeley-groups = {Report/Medical domain},
pages = {238--244},
title = {{Evolving decision trees using oracle guides}},
year = {2009}
}
@article{Johnson2015,
abstract = {Convolutional neural network (CNN) is a neu-ral network that can make use of the inter-nal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embed-ding of small text regions for use in classifi-cation. In addition to a straightforward adap-tation of CNN from image to text, a sim-ple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1412.1058},
author = {Johnson, Rie and Zhang, Tong},
eprint = {1412.1058},
file = {:C$\backslash$:/Users/Workk/Documents/1412.1058.pdf:pdf},
isbn = {9781941643495},
journal = {Naacl},
mendeley-groups = {11Thesis},
number = {2011},
pages = {103--112},
title = {{Effective Use of Word Order for Text Categorization with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1412.1058{\%}5Cnhttp://arxiv.org/abs/1412.1058v1},
year = {2015}
}
@article{Johnson2016,
abstract = {One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson {\&} Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1602.02373},
author = {Johnson, Rie and Zhang, Tong},
eprint = {1602.02373},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson, Zhang - 2016 - Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
mendeley-groups = {!Paper 3/task/Large Movie Review},
title = {{Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings}},
url = {http://arxiv.org/abs/1602.02373},
volume = {48},
year = {2016}
}
@article{Joulin2016,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
doi = {1511.09249v1},
eprint = {1607.01759},
file = {:D$\backslash$:/Downloads/Work/1607.01759.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
mendeley-groups = {Annotated/Interpretable Classifiers,!Paper 3/Word vectors,!Paper 3/task/Yelp},
pmid = {1000303116},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {http://arxiv.org/abs/1607.01759},
year = {2016}
}
@article{Jurman2016,
author = {Jurman, Nicholas},
file = {:C$\backslash$:/Users/Workk/Documents/Research{\_}Student{\_}Travel{\_}Application{\_}completed.pdf:pdf},
title = {{N / a}},
year = {2016}
}
@article{Kaikhah,
author = {Kaikhah, Khosrow and Doddameti, Sandesh},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaikhah, Doddameti - Unknown - Discovering Trends in Large Datasets Using Neural Networks.pdf:pdf},
mendeley-groups = {Papers/Paper 1,Report},
pages = {1--23},
title = {{Discovering Trends in Large Datasets Using Neural Networks}}
}
@article{Kalchbrenner2014,
abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25{\%} error reduction in the last task with respect to the strongest baseline.},
archivePrefix = {arXiv},
arxivId = {1404.2188v1},
author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
doi = {10.3115/v1/P14-1062},
eprint = {1404.2188v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalchbrenner, Grefenstette, Blunsom - 2014 - A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
isbn = {9781937284725},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
mendeley-groups = {!Paper 3/task/Sentiment treebank,Categories},
pages = {655--665},
title = {{A Convolutional Neural Network for Modelling Sentences}},
url = {http://aclweb.org/anthology/P14-1062},
year = {2014}
}
@article{Kamkarhaghighi2017,
author = {Kamkarhaghighi, Mehran and Makrehchi, Masoud},
doi = {10.1016/j.eswa.2017.08.021},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamkarhaghighi, Makrehchi - 2017 - Content Tree Word Embedding for document representation.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Word embedding,Content tree,Word2Vec,GloVe,Sentime},
mendeley-groups = {Annotated/Document representation},
pages = {241--249},
publisher = {Elsevier Ltd},
title = {{Content Tree Word Embedding for document representation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417417305596},
volume = {90},
year = {2017}
}
@article{Karaletsos2015,
abstract = {Representation learning systems typically rely on massive amounts of labeled data in order to be trained effectively. Recently, high-dimensional parametric models like convolutional neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Human-in-the-loop systems like crowdsourcing are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. We propose to combine generative unsupervised feature learning with learning from similarity orderings in order to learn models which take advantage of privileged information coming from the crowd. We use a fast variational algorithm to learn the model on standard datasets and demonstrate applicability to two image datasets, where classification is drastically improved. We show how triplet-samples of the crowd can supplement labels as a source of information to shape latent spaces with rich semantic information.},
archivePrefix = {arXiv},
arxivId = {1506.05011},
author = {Karaletsos, Theofanis and Belongie, Serge and R{\"{a}}tsch, Gunnar},
doi = {10.1051/0004-6361/201527329},
eprint = {1506.05011},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karaletsos, Belongie, R{\"{a}}tsch - 2015 - Bayesian representation learning with oracle constraints.pdf:pdf},
journal = {Iclr},
mendeley-groups = {Progress Report},
pages = {1--9},
title = {{Bayesian representation learning with oracle constraints}},
url = {http://arxiv.org/abs/1506.05011},
year = {2015}
}
@article{Karaletsos2015a,
abstract = {Representation learning systems typically rely on massive amounts of labeled data in order to be trained effectively. Recently, high-dimensional parametric models like convolutional neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Human-in-the-loop systems like crowdsourcing are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. We propose to combine generative unsupervised feature learning with learning from similarity orderings in order to learn models which take advantage of privileged information coming from the crowd. We use a fast variational algorithm to learn the model on standard datasets and demonstrate applicability to two image datasets, where classification is drastically improved. We show how triplet-samples of the crowd can supplement labels as a source of information to shape latent spaces with rich semantic information.},
archivePrefix = {arXiv},
arxivId = {1506.05011},
author = {Karaletsos, Theofanis and Belongie, Serge and R{\"{a}}tsch, Gunnar},
doi = {10.1051/0004-6361/201527329},
eprint = {1506.05011},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karaletsos, Belongie, R{\"{a}}tsch - 2015 - Bayesian representation learning with oracle constraints.pdf:pdf},
journal = {Iclr},
mendeley-groups = {Progress Report},
pages = {1--9},
title = {{Bayesian representation learning with oracle constraints}},
url = {http://arxiv.org/abs/1506.05011},
year = {2015}
}
@article{Karpathy2015a,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\{}{\%}{\}} vs. 60.9{\{}{\%}{\}}) and the UCF-101 datasets with (88.6{\{}{\%}{\}} vs. 88.0{\{}{\%}{\}}) and without additional optical flow information (82.6{\{}{\%}{\}} vs. 72.8{\{}{\%}{\}}).},
archivePrefix = {arXiv},
arxivId = {1503.08909v2},
author = {Karpathy, a. and Fei-Fei, Li},
doi = {10.1109/CVPR.2015.7298932},
eprint = {1503.08909v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Image Des.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Cvpr2015},
mendeley-groups = {Progress Report},
title = {{Deep Visual-Semantic Alignments for Generating Image Des}},
year = {2015}
}
@article{Karpathy2015,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {1506.02078},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1506.02078},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
mendeley-groups = {!Paper 3,!Paper 3/Understanding LSTMs},
pages = {1--12},
pmid = {26353135},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078},
year = {2015}
}
@article{Karwath2002,
abstract = {BACKGROUND: The inference of homology between proteins is a key problem in molecular biology The current best approaches only identify approximately 50{\%} of homologies (with a false positive rate set at 1/1000).$\backslash$n$\backslash$nRESULTS: We present Homology Induction (HI), a new approach to inferring homology. HI uses machine learning to bootstrap from standard sequence similarity search methods. First a standard method is run, then HI learns rules which are true for sequences of high similarity to the target (assumed homologues) and not true for general sequences, these rules are then used to discriminate sequences in the twilight zone. To learn the rules HI describes the sequences in a novel way based on a bioinformatic knowledge base, and the machine learning method of inductive logic programming. To evaluate HI we used the PDB40D benchmark which lists sequences of known homology but low sequence similarity. We compared the HI methodology with PSI-BLAST alone and found HI performed significantly better. In addition, Receiver Operating Characteristic (ROC) curve analysis showed that these improvements were robust for all reasonable error costs. The predictive homology rules learnt by HI by can be interpreted biologically to provide insight into conserved features of homologous protein families.$\backslash$n$\backslash$nCONCLUSIONS: HI is a new technique for the detection of remote protein homology--a central bioinformatic problem. HI with PSI-BLAST is shown to outperform PSI-BLAST for all error costs. It is expect that similar improvements would be obtained using HI with any sequence similarity method.},
author = {Karwath, Andreas and King, Ross D},
doi = {10.1186/1471-2105-3-11},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karwath, King - 2002 - Homology induction the use of machine learning to improve sequence similarity searches.pdf:pdf},
isbn = {10.1186/1471-2105-3-11},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Animals,Artificial Intelligence,Computational Biology,Computational Biology: methods,Databases, Protein,Fungal Proteins,Fungal Proteins: genetics,Internet,Mice,Oryza sativa,Plant Proteins,Plant Proteins: genetics,Predictive Value of Tests,Retroviridae Proteins,Retroviridae Proteins: genetics,Sensitivity and Specificity,Sequence Alignment,Sequence Alignment: methods,Sequence Homology, Amino Acid},
mendeley-groups = {Report/Biologicla domain},
number = {1},
pages = {11},
pmid = {11972320},
title = {{Homology induction: the use of machine learning to improve sequence similarity searches.}},
url = {http://www.biomedcentral.com/1471-2105/3/11},
volume = {3},
year = {2002}
}
@article{Kayande2009,
abstract = {Model-based decision support systems (DSS) improve performance in many contexts that are data-rich, uncertain, and require repetitive decisions. But such DSS are often not designed to help users understand and internalize the underlying factors driving DSS recommendations. Users then feel uncertain about DSS recommendations, leading them to possibly avoid using the system. We argue that a DSS must be designed to induce an alignment of a decision maker's mental model with the decision model embedded in the DSS. Such an alignment requires effort from the decision maker and guidance from the DSS. We experimentally evaluate two DSS design characteristics that facilitate such alignment: (i) feedback on the upside potential for performance improvement and (ii) feedback on corrective actions to improve decisions. We show that, in tandem, these two types of DSS feedback induce decision makers to align their mental models with the decision model, a process we call deep learning, whereas individually these two types of feedback have little effect on deep learning. We also show that deep learning, in turn, improves user evaluations of the DSS. We discuss how our findings could lead to DSS design improvements and better returns on DSS investments. [ABSTRACT FROM AUTHOR] Copyright of Information Systems Research is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Kayande, Ujwal and {De Bruyn}, Arnaud and Lilien, Gary L. and Rangaswamy, Arvind and van Bruggen, Gerrit H.},
doi = {10.1287/isre.1080.0198},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kayande et al. - 2009 - How incorporating feedback mechanisms in a DSS affects DSS evaluations.pdf:pdf},
isbn = {10477047},
issn = {10477047},
journal = {Information Systems Research},
keywords = {DSS design,Decision support systems,Evaluations,Feedback,Learning,Mental models},
mendeley-groups = {Annotated/Overarching Interpretability},
number = {4},
pages = {527--546},
title = {{How incorporating feedback mechanisms in a DSS affects DSS evaluations}},
volume = {20},
year = {2009}
}
@article{Keil2006,
abstract = {The study of explanation, while related to intuitive theories, concepts, and mental models, offers important new perspectives on high-level thought. Explanations sort themselves into several distinct types corresponding to patterns of causation, content domains, and explanatory stances, all of which have cognitive consequences. Although explanations are necessarily incomplete—often dramatically so in laypeople—those gaps are difficult to discern. Despite such gaps and the failure to recognize them fully, people do have skeletal explanatory senses, often implicit, of the causal structure of the world. They further leverage those skeletal understandings by knowing how to access additional explanatory knowledge in other minds and by being particularly adept at using situational support to build explanations on the fly in real time. Across development and cultures, there are differences in preferred explanatory schemes, but rarely are any kinds of schemes completely unavailable to a group},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Keil, Frank C.},
doi = {10.1146/annurev.psych.57.102904.190100},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keil - 2006 - Explanation and Understanding.pdf:pdf},
isbn = {0066-4308 1545-2085},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {abstract the study of,and mental models,causality,cognition,cognitive development,concepts,domain specificity,expla-,explanation,illusions of,knowing,offers important new perspectives,on high-level thought,stances,theories,while related to intuitive},
mendeley-groups = {Report/Explaining predictions},
number = {1},
pages = {227--254},
pmid = {16318595},
title = {{Explanation and Understanding}},
url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.57.102904.190100},
volume = {57},
year = {2006}
}
@article{Kheder2014,
author = {Kheder, Waad Ben and Matrouf, Driss and Bousquet, Pierre-Michel and Bonastre, Jean-Fran{\c{c}}ois and Ajili, Moez},
doi = {10.1007/978-3-319-11397-5},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kheder et al. - 2014 - Statistical Language and Speech Processing.pdf:pdf},
isbn = {978-3-319-11396-8},
journal = {Statistical Language and Speech Processing},
keywords = {i-vectors},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
pages = {97--107},
title = {{Statistical Language and Speech Processing}},
url = {http://link.springer.com/10.1007/978-3-319-11397-5{\%}5Cnhttp://link.springer.com/content/pdf/10.1007/978-3-319-11397-5.pdf},
volume = {8791},
year = {2014}
}
@article{Kim2015,
abstract = {We present the Mind the Gap Model (MGM), an approach for interpretable fea- ture extraction and selection. By placing interpretability criteria directly into the model, we allowfor the model to both optimize parameters related to interpretabil- ity and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and dis- ease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.},
author = {Kim, Been and Shah, Julie and Doshi-Velez, Finale},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Shah, Doshi-Velez - 2015 - Mind the Gap A Generative Approach to Interpretable Feature Selection and Extraction.pdf:pdf},
issn = {10495258},
journal = {Nips},
mendeley-groups = {Annotated/Interpretable representations},
pages = {1--9},
title = {{Mind the Gap : A Generative Approach to Interpretable Feature Selection and Extraction}},
year = {2015}
}
@article{Kim2018,
author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/kim18d.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
title = {{Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCAV )}},
year = {2018}
}
@article{Kim2000,
author = {Kim, D and Lee, J},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Lee - 2000 - Handling continuous-valued attributes in decision tree with neural network modeling.pdf:pdf},
isbn = {3-540-67602-3},
journal = {Machine Learning: Ecml 2000},
mendeley-groups = {Report/Decision Trees?,Papers/Paper 1,Progress Report,Report},
pages = {211--219},
title = {{Handling continuous-valued attributes in decision tree with neural network modeling}},
volume = {1810},
year = {2000}
}
@article{Kim2017,
abstract = {Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate scikit-learning scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others.},
author = {Kim, Han Kyul and Kim, Hyunjoong and Cho, Sungzoon},
doi = {10.1016/j.neucom.2017.05.046},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kim, Cho - 2017 - Bag-of-concepts Comprehending document representation through clustering words in distributed representation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Bag-of-concepts,Interpretable document representation,Word2vec clustering},
mendeley-groups = {Annotated/Word Vectors,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
pages = {336--352},
title = {{Bag-of-concepts: Comprehending document representation through clustering words in distributed representation}},
volume = {266},
year = {2017}
}
@article{Kim2017,
abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
archivePrefix = {arXiv},
arxivId = {1702.00887},
author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
eprint = {1702.00887},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2017 - Structured Attention Networks.pdf:pdf},
issn = {1702.00887},
mendeley-groups = {!Paper 3/Structured LSTMs},
pages = {1--21},
title = {{Structured Attention Networks}},
url = {http://arxiv.org/abs/1702.00887},
year = {2017}
}
@article{Kindermans2017,
abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
archivePrefix = {arXiv},
arxivId = {1711.00867},
author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"{u}}tt, Kristof T. and D{\"{a}}hne, Sven and Erhan, Dumitru and Kim, Been},
doi = {10.1016/j.jns.2003.09.014},
eprint = {1711.00867},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kindermans et al. - 2017 - The (Un)reliability of saliency methods.pdf:pdf},
isbn = {0022-510X (Print)},
issn = {0022510X},
mendeley-groups = {!Paper 3/Interpretability General},
pages = {1--12},
pmid = {14706220},
title = {{The (Un)reliability of saliency methods}},
url = {http://arxiv.org/abs/1711.00867},
year = {2017}
}
@article{Kingma2015,
abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
archivePrefix = {arXiv},
arxivId = {1506.02557},
author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
doi = {10.1016/S0733-8619(03)00096-3},
eprint = {1506.02557},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Salimans, Welling - 2015 - Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
isbn = {1506.02557},
issn = {10495258},
mendeley-groups = {!Paper 3/Bayesian Networks},
number = {Mcmc},
pages = {1--9},
pmid = {15062530},
title = {{Variational Dropout and the Local Reparameterization Trick}},
url = {http://arxiv.org/abs/1506.02557},
year = {2015}
}
@article{Kiros2014,
abstract = {In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2710v1},
author = {Kiros, Ryan and Zemel, Rs and Salakhutdinov, Ruslan},
eprint = {arXiv:1406.2710v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2014 - A Multiplicative Model for Learning Distributed Text-Based Attribute Representations.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Annotated/Word Vectors},
pages = {1--11},
title = {{A Multiplicative Model for Learning Distributed Text-Based Attribute Representations}},
url = {http://arxiv.org/abs/1406.2710},
year = {2014}
}
@article{Kiros,
author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros et al. - Unknown - Skip-Thought Vectors.pdf:pdf},
mendeley-groups = {Annotated/Representation Learning},
number = {786},
pages = {1--9},
title = {{Skip-Thought Vectors}}
}
@article{Koppula,
archivePrefix = {arXiv},
arxivId = {1802.03816},
author = {Koppula, Skanda and Sim, Khe Chai and Chin, Kean},
eprint = {1802.03816},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koppula, Sim, Chin - Unknown - Understanding Recurrent Neural State Using Memory Signatures.pdf:pdf},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
title = {{Understanding Recurrent Neural State Using Memory Signatures}}
}
@article{Korde2012,
abstract = {As most information (over 80{\%}) is stored as text, text mining is believed to have a high commercial potential value. knowledge may be discovered from many sources of information; yet, unstructured texts remain the largest readily available source of knowledge .Text classification which classifies the documents according to predefined categories .In this paper we are tried to give the introduction of text classification, process of text classification as well as the overview of the classifiers and tried to compare the some existing classifier on basis of few criteria like time complexity, principal and performance.},
author = {Korde, Vandana and Mahender, C Namrata},
doi = {10.5121/ijaia.2012.3208},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korde, Mahender - 2012 - Text Classification and Classifiers A Survey.pdf:pdf},
issn = {09762191},
journal = {International Journal of Artificial Intelligence {\&} Applications},
keywords = {classifiers,text classification,text representation},
mendeley-groups = {Annotated/Decision Trees},
number = {2},
pages = {85--99},
title = {{Text Classification and Classifiers: A Survey}},
url = {http://www.airccse.org/journal/ijaia/papers/3212ijaia08.pdf},
volume = {3},
year = {2012}
}
@article{Krakovna2016,
abstract = {As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks, state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining a long short-term memory (LSTM) model with a hidden Markov model (HMM), a simpler and more transparent model. We add the HMM state probabilities to the output layer of the LSTM, and then train the HMM and LSTM either sequentially or jointly. The LSTM can make use of the information from the HMM, and fill in the gaps when the HMM is not performing well. A small hybrid model usually performs better than a standalone LSTM of the same size, especially on smaller data sets. We test the algorithms on text data and medical time series data, and find that the LSTM and HMM learn complementary information about the features in the text.},
archivePrefix = {arXiv},
arxivId = {1611.05934},
author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
eprint = {1611.05934},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krakovna, Doshi-Velez - 2016 - Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models(2).pdf:pdf},
mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
number = {Whi},
title = {{Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}},
url = {http://arxiv.org/abs/1611.05934},
year = {2016}
}
@article{Krakovna2016a,
abstract = {As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks, state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining a long short-term memory (LSTM) model with a hidden Markov model (HMM), a simpler and more transparent model. We add the HMM state probabilities to the output layer of the LSTM, and then train the HMM and LSTM either sequentially or jointly. The LSTM can make use of the information from the HMM, and fill in the gaps when the HMM is not performing well. A small hybrid model usually performs better than a standalone LSTM of the same size, especially on smaller data sets. We test the algorithms on text data and medical time series data, and find that the LSTM and HMM learn complementary information about the features in the text.},
archivePrefix = {arXiv},
arxivId = {1611.05934},
author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
eprint = {1611.05934},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krakovna, Doshi-Velez - 2016 - Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models(2).pdf:pdf},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
number = {Whi},
title = {{Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}},
url = {http://arxiv.org/abs/1611.05934},
year = {2016}
}
@article{Kriegel2011,
author = {Kriegel, Hans-peter and Kr, Peer},
doi = {10.1002/widm.30},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kriegel, Kr - 2011 - Density-based clustering.pdf:pdf},
mendeley-groups = {Report/Clustering},
number = {June},
pages = {231--240},
title = {{Density-based clustering}},
volume = {1},
year = {2011}
}
@article{Krueger2016,
abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.},
archivePrefix = {arXiv},
arxivId = {1606.01305},
author = {Krueger, David and Maharaj, Tegan and Kram{\'{a}}r, J{\'{a}}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
eprint = {1606.01305},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krueger et al. - 2016 - Zoneout Regularizing RNNs by Randomly Preserving Hidden Activations.pdf:pdf},
mendeley-groups = {!Paper 3/Training LSTMs},
pages = {1--11},
title = {{Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations}},
url = {http://arxiv.org/abs/1606.01305},
year = {2016}
}
@article{Kuchaiev2017,
abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.},
archivePrefix = {arXiv},
arxivId = {1703.10722},
author = {Kuchaiev, Oleksii and Ginsburg, Boris},
doi = {10.1109/CVPR.2016.90},
eprint = {1703.10722},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuchaiev, Ginsburg - 2017 - Factorization tricks for LSTM networks.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
pages = {1--6},
pmid = {23554596},
title = {{Factorization tricks for LSTM networks}},
url = {http://arxiv.org/abs/1703.10722},
year = {2017}
}
@article{Kulkarni2015b,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
archivePrefix = {arXiv},
arxivId = {1503.03167},
author = {Kulkarni, Td and Whitney, W},
doi = {10.1063/1.4914407},
eprint = {1503.03167},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network(2).pdf:pdf},
issn = {10897550},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Progress Report,Interim Review},
pages = {2539--2547},
title = {{Deep Convolutional Inverse Graphics Network}},
url = {http://arxiv.org/abs/1503.03167},
year = {2015}
}
@article{Kulkarni2015a,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
archivePrefix = {arXiv},
arxivId = {1503.03167},
author = {Kulkarni, Td and Whitney, W},
doi = {10.1063/1.4914407},
eprint = {1503.03167},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network.pdf:pdf},
issn = {10897550},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Progress Report},
pages = {2539--2547},
title = {{Deep Convolutional Inverse Graphics Network}},
url = {http://arxiv.org/abs/1503.03167},
year = {2015}
}
@article{Kulkarni2015,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
archivePrefix = {arXiv},
arxivId = {1503.03167},
author = {Kulkarni, Td and Whitney, W},
doi = {10.1063/1.4914407},
eprint = {1503.03167},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network.pdf:pdf},
issn = {10897550},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Progress Report},
pages = {2539--2547},
title = {{Deep Convolutional Inverse Graphics Network}},
url = {http://arxiv.org/abs/1503.03167},
year = {2015}
}
@article{Kumar2015,
abstract = {{\textless}p{\textgreater} A framework for automated detection and classification of cancer from microscopic biopsy images using clinically significant and biologically interpretable features is proposed and examined. The various stages involved in the proposed methodology include enhancement of microscopic images, segmentation of background cells, features extraction, and finally the classification. An appropriate and efficient method is employed in each of the design steps of the proposed framework after making a comparative analysis of commonly used method in each category. For highlighting the details of the tissue and structures, the contrast limited adaptive histogram equalization approach is used. For the segmentation of background cells, {\textless}math id="M1"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}k{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -means segmentation algorithm is used because it performs better in comparison to other commonly used segmentation methods. In feature extraction phase, it is proposed to extract various biologically interpretable and clinically significant shapes as well as morphology based features from the segmented images. These include gray level texture features, color based features, color gray level texture features, Law's Texture Energy based features, Tamura's features, and wavelet features. Finally, the {\textless}math id="M2"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}K{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -nearest neighborhood method is used for classification of images into normal and cancerous categories because it is performing better in comparison to other commonly used methods for this application. The performance of the proposed framework is evaluated using well-known parameters for four fundamental tissues (connective, epithelial, muscular, and nervous) of randomly selected 1000 microscopic biopsy images. {\textless}/p{\textgreater}},
author = {Kumar, Rajesh and Srivastava, Rajeev and Srivastava, Subodh},
doi = {10.1155/2015/457906},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Srivastava, Srivastava - 2015 - Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significan.pdf:pdf},
issn = {2314-5129, 2314-5137},
journal = {Journal of Medical Engineering},
mendeley-groups = {Annotated/Applications/Medical},
number = {2015},
pages = {1--14},
pmid = {21329180},
title = {{Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significant and Biologically Interpretable Features}},
url = {http://www.hindawi.com/journals/jme/2015/457906/},
volume = {2015},
year = {2015}
}
@article{Lacoste-Julien2008,
abstract = {Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in finding  a reduced dimensionality representation.  Specifically, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood.  By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification task and show how our model can identify shared topics across classes as well as class-dependent topics.},
author = {Lacoste-Julien, Simon and Sha, Fei and Jordan, Michael I.},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lacoste-Julien, Sha, Jordan - 2008 - DiscLDA Discriminative Learning for Dimensionality Reduction and Classification.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems},
keywords = {Computational, Information-Theoretic Learning with,Information Retrieval {\&} Textual Information Access,Learning/Statistics {\&} Optimisation},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,!Paper 3/task/newsgroups},
pages = {1--8},
title = {{DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification}},
url = {http://eprints.pascal-network.org/archive/00004292/},
volume = {21},
year = {2008}
}
@article{Lai2016,
abstract = {We analyze three critical components of word embedding training: the model, the corpus, and the training parameters. We systematize existing neural-network-based word embedding algorithms and compare them using the same corpus. We evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks and using it to initialize neural networks. We also provide several simple guidelines for training word embeddings. First, we discover that corpus domain is more important than corpus size. We recommend choosing a corpus in a suitable domain for the desired task, after that, using a larger corpus yields better results. Second, we find that faster models provide sufficient performance in most cases, and more complex models can be used if the training corpus is sufficiently large. Third, the early stopping metric for iterating should rely on the development set of the desired task rather than the validation loss of training embedding.},
archivePrefix = {arXiv},
arxivId = {1507.05523},
author = {Lai, Siwei and Liu, Kang and He, Shizhu and Zhao, Jun},
doi = {10.1109/MIS.2016.45},
eprint = {1507.05523},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 2016 - How to generate a good word embedding.pdf:pdf},
issn = {15411672},
journal = {IEEE Intelligent Systems},
keywords = {distributed representation,intelligent systems,neural network,word embedding},
mendeley-groups = {!Paper 3/task,!Paper 3/task/Large Movie Review,!Paper 3/task/Sentiment treebank},
number = {6},
pages = {5--14},
title = {{How to generate a good word embedding}},
volume = {31},
year = {2016}
}
@article{Lai1990,
author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 1990 - Recurrent Convolutional Neural Networks for Text Classification.pdf:pdf},
keywords = {NLP and Machine Learning Track},
mendeley-groups = {Annotated/Representation Learning,!Paper 3/task/newsgroups},
pages = {2267--2273},
title = {{Recurrent Convolutional Neural Networks for Text Classification}},
year = {1990}
}
@article{Laine2016,
abstract = {In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63{\%} to 12.89{\%} in CIFAR-10 with 4000 labels and from 18.44{\%} to 6.83{\%} in SVHN with 500 labels.},
archivePrefix = {arXiv},
arxivId = {1610.02242},
author = {Laine, Samuli and Aila, Timo},
eprint = {1610.02242},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laine, Aila - 2016 - Temporal Ensembling for Semi-Supervised Learning.pdf:pdf},
mendeley-groups = {Progress Report},
number = {November 2016},
pages = {1--9},
title = {{Temporal Ensembling for Semi-Supervised Learning}},
url = {http://arxiv.org/abs/1610.02242},
year = {2016}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
archivePrefix = {arXiv},
arxivId = {10.1126/science.aab3050},
author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tnenbaum, Joshua B.},
doi = {10.1126/science.aab3050},
eprint = {science.aab3050},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tnenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075},
journal = {Science},
mendeley-groups = {Progress Report,Interim Review},
number = {6266},
pages = {1332--1338},
pmid = {26659050},
primaryClass = {10.1126},
title = {{Human-level concept learning through probabilistic program induction}},
url = {https://www.sciencemag.org/content/350/6266/1332.full.pdf},
volume = {350},
year = {2015}
}
@article{Lakkaraju2016,
author = {Lakkaraju, Himabindu and Bach, Stephen and Leskovec, Jure},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju, Bach, Leskovec - 2016 - Interpretable Decision Sets A Joint Framework for Description and Prediction.pdf:pdf},
journal = {The 22th {\{}ACM{\}} {\{}SIGKDD{\}} International Conference on Knowledge Discovery and Data Mining, {\{}KDD{\}} '16, San Fransisco, CA, USA, August, 2016 International Conference on Knowledge Discovery and Data Mining, {\{}{\{}{\}}KDD{\{}{\}}{\}} '16, San Fransisco, CA, USA, August, 2016},
title = {{Interpretable Decision Sets: A Joint Framework for Description and Prediction}},
volume = {1},
year = {2016}
}
@article{Lakkaraju2017,
abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
archivePrefix = {arXiv},
arxivId = {1707.01154},
author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
eprint = {1707.01154},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju et al. - 2017 - Interpretable {\&} Explorable Approximations of Black Box Models.pdf:pdf},
mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
title = {{Interpretable {\&} Explorable Approximations of Black Box Models}},
url = {http://arxiv.org/abs/1707.01154},
year = {2017}
}
@article{Lau2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.05368v1},
author = {Lau, Jey Han and Baldwin, Timothy},
eprint = {arXiv:1607.05368v1},
file = {:C$\backslash$:/Users/Workk/Documents/1607.05368.pdf:pdf},
title = {{Practical Insights into Document Embedding Generation}},
year = {2014}
}
@article{Lavra??1999,
abstract = {Widespread use of medical information systems and explosive growth of medical databases require traditional manual data analysis to be coupled with methods for efficient computer-assisted analysis. This paper presents selected data mining techniques that can be applied in medicine, and in particular some machine learning techniques including the mechanisms that make them better suited for the analysis of medical databases (derivation of symbolic rules, use of background knowledge, sensitivity and specificity of induced descriptions). The importance of the interpretability of results of data analysis is discussed and illustrated on selected medical applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lavra??, Nada},
doi = {10.1016/S0933-3657(98)00062-1},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavra - 1999 - Selected techniques for data mining in medicine.pdf:pdf},
isbn = {0933-3657},
issn = {09333657},
journal = {Artificial Intelligence in Medicine},
keywords = {Data mining,Machine learning,Medical applications},
mendeley-groups = {Report/Medical domain},
number = {1},
pages = {3--23},
pmid = {10225344},
title = {{Selected techniques for data mining in medicine}},
volume = {16},
year = {1999}
}
@article{Le2014,
author = {Le, Quoc and Mikolov, Tomas and Com, Tmikolov Google},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov, Com - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
mendeley-groups = {Annotated/Document representation,!Paper 3/task/Sentiment treebank},
title = {{Distributed Representations of Sentences and Documents}},
volume = {32},
year = {2014}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Qv and Mikolov, Tomas},
doi = {10.1145/2740908.2742760},
eprint = {1405.4053},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
journal = {International Conference on Machine Learning - ICML 2014},
mendeley-groups = {Progress Report,Interim Review},
pages = {1188--1196},
pmid = {9377276},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Lee1999,
abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Lee, D D and Seung, H S},
doi = {10.1038/44565},
eprint = {arXiv:1408.1149},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Seung - 1999 - Learning the parts of objects by non-negative matrix factorization.pdf:pdf},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
keywords = {Algorithms,Face,Humans,Learning,Models, Neurological,Perception,Perception: physiology,Semantics},
mendeley-groups = {Annotated/NMF},
number = {6755},
pages = {788--91},
pmid = {10548103},
title = {{Learning the parts of objects by non-negative matrix factorization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10548103},
volume = {401},
year = {1999}
}
@article{Leek2014,
author = {Leek, Jeffrey T},
doi = {10.1038/nrg2825.Tackling},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leek - 2014 - NIH Public Access.pdf:pdf},
mendeley-groups = {Annotated/Artifacts in the data},
number = {10},
pages = {1--15},
title = {{NIH Public Access}},
volume = {11},
year = {2014}
}
@article{Lei2016,
abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
archivePrefix = {arXiv},
arxivId = {1606.04155},
author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1606.04155},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lei, Barzilay, Jaakkola - 2016 - Rationalizing Neural Predictions.pdf:pdf},
mendeley-groups = {Report/Explaining predictions},
title = {{Rationalizing Neural Predictions}},
url = {http://arxiv.org/abs/1606.04155},
year = {2016}
}
@article{Leike2017,
abstract = {We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.},
archivePrefix = {arXiv},
arxivId = {1711.09883},
author = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A. and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
eprint = {1711.09883},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leike et al. - 2017 - AI Safety Gridworlds.pdf:pdf},
mendeley-groups = {!Paper 3},
title = {{AI Safety Gridworlds}},
url = {http://arxiv.org/abs/1711.09883},
year = {2017}
}
@article{Letham2015,
abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if {\ldots} then. . . statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.01644v1},
author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
doi = {10.1214/15-AOAS848},
eprint = {arXiv:1511.01644v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2015 - Interpretable classifiers using rules and bayesian analysis Building a better stroke prediction model.pdf:pdf},
isbn = {9781577356288},
issn = {19417330},
journal = {Annals of Applied Statistics},
keywords = {Bayesian analysis,Classification,Interpretability},
mendeley-groups = {Annotated/Rule-based classiifers,Report},
number = {3},
pages = {1350--1371},
title = {{Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model}},
volume = {9},
year = {2015}
}
@article{Letham2010,
abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. We introduce a generative model called the Bayesian List Machine for fitting decision lists, a type of interpretable classifier, to data. We use the model to predict stroke in atrial fibrillation patients, and produce predictive models that are simple enough to be understood by patients yet significantly outperform the medical scoring systems currently in use.},
author = {Letham, Benjamin and Rudin, Cynthia and Mccormick, Tyler H and Madigan, David},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2010 - An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis.pdf:pdf},
isbn = {9781577356288},
journal = {AAAI Technical Report WS-13-17},
keywords = {AAAI Technical Report WS-13-17},
mendeley-groups = {Annotated/Rule-based classiifers},
number = {609},
pages = {65--67},
title = {{An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis}},
year = {2010}
}
@article{Li2015,
abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
archivePrefix = {arXiv},
arxivId = {1506.01066},
author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
doi = {10.18653/v1/N16-1082},
eprint = {1506.01066},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Visualizing and Understanding Neural Models in NLP.pdf:pdf},
isbn = {9781941643914},
mendeley-groups = {Report/Explaining predictions},
title = {{Visualizing and Understanding Neural Models in NLP}},
url = {http://arxiv.org/abs/1506.01066},
year = {2015}
}
@article{Li2016a,
abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
archivePrefix = {arXiv},
arxivId = {1506.01066},
author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
eprint = {1506.01066},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Visualizing and Understanding Neural Models in NLP.pdf:pdf},
journal = {Naacl},
keywords = {Neural Network,Visualization},
pages = {1--10},
title = {{Visualizing and Understanding Neural Models in NLP}},
url = {http://arxiv.org/abs/1506.01066},
year = {2016}
}
@article{Li2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.00185v5},
author = {Li, Jiwei and Luong, Minh-thang and Jurafsky, Dan and Hovy, Eduard},
eprint = {arXiv:1503.00185v5},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - When Are Tree Structures Necessary for Deep Learning of.pdf:pdf},
mendeley-groups = {Annotated/Representation Learning},
title = {{When Are Tree Structures Necessary for Deep Learning of}},
year = {2014}
}
@article{Li2016,
abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
archivePrefix = {arXiv},
arxivId = {1612.08220},
author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
eprint = {1612.08220},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Monroe, Jurafsky - 2016 - Understanding Neural Networks through Representation Erasure.pdf:pdf},
mendeley-groups = {Report/Explaining predictions,!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/task/Sentiment treebank},
title = {{Understanding Neural Networks through Representation Erasure}},
url = {http://arxiv.org/abs/1612.08220},
year = {2016}
}
@article{Li2017,
abstract = {An important goal in behaviour analytics is to connect disease state or genome variation with observable differences in behaviour. Despite advances in sensor technology and imaging, informative behaviour quantification remains challenging. The nematode worm C. elegans provides a unique opportunity to test analysis approaches because of its small size, compact nervous system, and the availability of large databases of videos of freely behaving animals with known genetic differences. Despite its relative simplicity, there are still no reports of generative models that can capture essential differences between even well-described mutant strains. Here we show that a multilayer recurrent neural network (RNN) can produce diverse behaviours that are difficult to distinguish from real worms' behaviour and that some of the artificial neurons in the RNN are interpretable and correlate with observable features such as body curvature, speed, and reversals. Although the RNN is not trained to perform classification, we find that artificial neuron responses provide features that perform well in worm strain classification.},
author = {Li, Kezhi and Javer, Avelino and Keaveny, Eric E. and Brown, Andre E.X. and Buchanan, E Kelly and Linderman, Scott and Paninski, Liam},
doi = {10.1101/222208},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Recurrent Neural Networks with Interpretable Cells Predict and Classify Worm Behaviour.pdf:pdf},
journal = {Doi.Org},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
number = {Nips},
pages = {222208},
title = {{Recurrent Neural Networks with Interpretable Cells Predict and Classify Worm Behaviour}},
url = {https://www.biorxiv.org/content/early/2017/11/20/222208},
year = {2017}
}
@article{Li2014,
author = {Li, Li and Zhang, Longkai and Wang, Houfeng},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Wang - 2014 - Muli-label Text Categorization with Hidden Components.pdf:pdf},
journal = {Emnlp},
pages = {1816--1821},
title = {{Muli-label Text Categorization with Hidden Components}},
year = {2014}
}
@article{Li2014a,
author = {Li, Li and Zhang, Longkai and Wang, Houfeng},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Wang - 2014 - Muli-label Text Categorization with Hidden Components.pdf:pdf},
journal = {Emnlp},
mendeley-groups = {Interim Review},
pages = {1816--1821},
title = {{Muli-label Text Categorization with Hidden Components}},
year = {2014}
}
@article{Li2014,
abstract = {Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the '' forced topic'' problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.},
author = {Li, Ximing and Ouyang, Jihong and Lu, You and Zhou, Xiaotang and Tian, Tian},
doi = {10.1007/s10791-014-9244-9},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - Group topic model organizing topics into groups.pdf:pdf},
issn = {15737659},
journal = {Information Retrieval},
keywords = {Document clustering,Group,Latent Dirichlet allocation,Online learning,Topic modeling,Variational inference},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
number = {1},
pages = {1--25},
title = {{Group topic model: organizing topics into groups}},
volume = {18},
year = {2014}
}
@article{Liang2017a,
abstract = {This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.},
archivePrefix = {arXiv},
arxivId = {1703.03055},
author = {Liang, Xiaodan and Lin, Liang and Shen, Xiaohui and Feng, Jiashi and Yan, Shuicheng and Xing, Eric P.},
doi = {10.1109/CVPR.2017.234},
eprint = {1703.03055},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2017 - Interpretable Structure-Evolving LSTM(2).pdf:pdf},
issn = {1703.03055},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
number = {61622214},
pages = {1010--1019},
title = {{Interpretable Structure-Evolving LSTM}},
url = {http://arxiv.org/abs/1703.03055},
year = {2017}
}
@article{Liang2017,
abstract = {This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.},
archivePrefix = {arXiv},
arxivId = {1703.03055},
author = {Liang, Xiaodan and Lin, Liang and Shen, Xiaohui and Feng, Jiashi and Yan, Shuicheng and Xing, Eric P.},
doi = {10.1109/CVPR.2017.234},
eprint = {1703.03055},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2017 - Interpretable Structure-Evolving LSTM(2).pdf:pdf},
issn = {1703.03055},
mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
number = {61622214},
pages = {1010--1019},
title = {{Interpretable Structure-Evolving LSTM}},
url = {http://arxiv.org/abs/1703.03055},
year = {2017}
}
@article{Liang2016,
abstract = {By taking the semantic object parsing task as an exemplar application scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network, which is the generalization of LSTM from sequential data or multi-dimensional data to general graph-structured data. Particularly, instead of evenly and fixedly dividing an image to pixels or patches in existing multi-dimensional LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each arbitrary-shaped superpixel as a semantically consistent node, and adaptively construct an undirected graph for each image, where the spatial relations of the superpixels are naturally used as edges. Constructed on such an adaptive graph topology, the Graph LSTM is more naturally aligned with the visual patterns in the image (e.g., object boundaries or appearance similarities) and provides a more economical information propagation route. Furthermore, for each optimization step over Graph LSTM, we propose to use a confidence-driven scheme to update the hidden and memory states of nodes progressively till all nodes are updated. In addition, for each node, the forgets gates are adaptively learned to capture different degrees of semantic correlation with neighboring nodes. Comprehensive evaluations on four diverse semantic object parsing datasets well demonstrate the significant superiority of our Graph LSTM over other state-of-the-art solutions.},
archivePrefix = {arXiv},
arxivId = {1603.07063},
author = {Liang, Xiaodan and Shen, Xiaohui and Feng, Jiashi and Lin, Liang and Yan, Shuicheng},
doi = {10.1007/978-3-319-46448-0_8},
eprint = {1603.07063},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2016 - Semantic object parsing with graph LSTM.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Graph LSTM,Object parsing,Recurrent neural networks},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
pages = {125--143},
pmid = {4520227},
title = {{Semantic object parsing with graph LSTM}},
volume = {9905 LNCS},
year = {2016}
}
@article{Linegang2006,
abstract = {The US Navy is funding the development of advanced automation systems to plan and execute unmanned vehicles missions, pushing towards a higher level of autonomy for automated planning systems. With effective systems, the human could play a role of mission manager and automation systems could perform mission planning and execution tasks with limited human involvement. Evaluations of the automation systems currently under development are identifying critical conflicts between human operator expectations and automated planning results. This paper presents a model of this human-automation interaction system and summarizes the resulting system design effort. This model provides a theory explaining the source of conflict between human and automation, and predicts that an ecological approach to display design would reduce that conflict. Based on that prediction, the paper describes initial results of an ecological approach to system analysis and design, intended to improve human-automation interaction for these types of advanced automation systems.},
author = {Linegang, M. P. and Stoner, H. a. and Patterson, M. J. and Seppelt, B. D. and Hoffman, J. D. and Crittendon, Z. B. and Lee, John D.},
doi = {10.1177/154193120605002304},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Linegang et al. - 2006 - Human-Automation Collaboration in Dynamic Mission Planning A Challenge Requiring an Ecological Approach.pdf:pdf},
isbn = {10711813 (ISSN); 9780945289296 (ISBN)},
issn = {1071-1813},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
mendeley-groups = {Annotated/Overarching Interpretability},
number = {23},
pages = {2482--2486},
title = {{Human-Automation Collaboration in Dynamic Mission Planning: A Challenge Requiring an Ecological Approach}},
volume = {50},
year = {2006}
}
@article{Lipton2016,
abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
archivePrefix = {arXiv},
arxivId = {1606.03490},
author = {Lipton, Zachary C.},
eprint = {1606.03490},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
mendeley-groups = {Annotated/Overarching Interpretability,Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
number = {Whi},
title = {{The Mythos of Model Interpretability}},
url = {http://arxiv.org/abs/1606.03490},
year = {2016}
}
@article{Lisboa2002,
abstract = {The purpose of this review is to assess the evidence of healthcare benefits involving the application of artificial neural networks to the clinical functions of diagnosis, prognosis and survival analysis, in the medical domains of oncology, critical care and cardiovascular medicine. The primary source of publications is PUBMED listings under Randomised Controlled Trials and Clinical Trials. The r{\^{o}}le of neural networks is introduced within the context of advances in medical decision support arising from parallel developments in statistics and artificial intelligence. This is followed by a survey of published Randomised Controlled Trials and Clinical Trials, leading to recommendations for good practice in the design and evaluation of neural networks for use in medical intervention.},
author = {Lisboa, P.J.G.},
doi = {10.1016/S0893-6080(01)00111-3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lisboa - 2002 - A review of evidence of health benefit from artificial neural networks in medical intervention.pdf:pdf},
isbn = {0893-6080 (Print)$\backslash$r0893-6080 (Linking)},
issn = {08936080},
journal = {Neural Networks},
keywords = {clinical trials,decision support systems,diagnosis,prognosis,prospective studies,randomised controlled trials,review,survival analysis},
mendeley-groups = {Report/Features,Report},
number = {1},
pages = {11--39},
pmid = {11958484},
title = {{A review of evidence of health benefit from artificial neural networks in medical intervention}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608001001113},
volume = {15},
year = {2002}
}
@article{Liu2017,
abstract = {In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.},
archivePrefix = {arXiv},
arxivId = {1705.09207},
author = {Liu, Yang and Lapata, Mirella},
eprint = {1705.09207},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lapata - 2017 - Learning Structured Text Representations(2).pdf:pdf},
mendeley-groups = {!Paper 3/Structured LSTMs},
title = {{Learning Structured Text Representations}},
url = {http://arxiv.org/abs/1705.09207},
year = {2017}
}
@article{Liu2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.09207v2},
author = {Liu, Yang and Lapata, Mirella},
eprint = {arXiv:1705.09207v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lapata - 2017 - Learning Structured Text Representations.pdf:pdf},
mendeley-groups = {Annotated/Interpretable representations},
title = {{Learning Structured Text Representations}},
year = {2017}
}
@article{Lundberg2016,
abstract = {Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.},
archivePrefix = {arXiv},
arxivId = {1611.07478},
author = {Lundberg, Scott and Lee, Su-In},
eprint = {1611.07478},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Lee - 2016 - An unexpected unity among methods for interpreting model predictions.pdf:pdf},
mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
number = {Nips},
pages = {1--6},
title = {{An unexpected unity among methods for interpreting model predictions}},
url = {http://arxiv.org/abs/1611.07478},
year = {2016}
}

@article{Craven1993,
author = {{M.W. Craven}, J.W. Shavlik},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/M.W. Craven - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
mendeley-groups = {Papers/Paper 1,Report},
number = {4},
pages = {434--441},
title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
volume = {41},
year = {1993}
}
@article{Maas2011,
	abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
	author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	doi = {978-1-932432-87-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:pdf},
	isbn = {9781932432879},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Datasets,!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {142--150},
	title = {{Learning Word Vectors for Sentiment Analysis}},
	year = {2011}
}
@article{Macqueen,
author = {Macqueen, J},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Macqueen - Unknown - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MULTIVARIATE OBSERVATIONS.pdf:pdf},
mendeley-groups = {Report/Clustering},
number = {233},
pages = {281--297},
title = {{SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MULTIVARIATE OBSERVATIONS}},
volume = {233}
}
@article{Maire1999a,
abstract = {The core problem of rule-extraction from feed-forward networks is an inversion problem. In this article, we solve this inversion problem by backpropagating unions of polyhedra. We obtain as a by-product a new rule-extraction technique for which the fidelity of the extracted rules can be made arbitrarily high.},
author = {Maire, F.},
doi = {10.1016/S0893-6080(99)00013-1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maire - 1999 - Rule-extraction by backpropagation of polyhedra.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Artificial neural network,Inversion,Polyhedron,Rule-extraction},
number = {4-5},
pages = {717--725},
pmid = {12662679},
title = {{Rule-extraction by backpropagation of polyhedra}},
volume = {12},
year = {1999}
}
@article{Malioutov2013,
abstract = {We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean 'sensing' matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting. We show competitive classification accuracy using the proposed approach. Copyright 2013 by the author(s).},
author = {Malioutov, D M and Varshney, K R},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malioutov, Varshney - 2013 - Exact rule learning via Boolean compressed sensing.pdf:pdf},
journal = {30th International Conference on Machine Learning, ICML 2013},
keywords = {Classification accuracy; Exact recoveries; Group,Learning systems; Signal reconstruction,Recovery},
mendeley-groups = {Annotated/Interpretable Classifiers},
number = {PART 3},
pages = {1802--1810},
title = {{Exact rule learning via Boolean compressed sensing}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84897531301{\&}partnerID=40{\&}md5=896c81fac3b62d2c5b6e9f3cf3a5a96d},
year = {2013}
}
@article{Management2011,
author = {Management, Spatial and Naukowe, Bogucki Wydawnictwo},
doi = {10.2478/v10117-011-0021-1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Management, Naukowe - 2011 - COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ... COMPARISON OF VALUES OF P.pdf:pdf},
isbn = {9788362662623},
mendeley-groups = {Report/Features},
number = {2},
title = {{COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ... COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ON THE SAME SETS OF DATA}},
volume = {30},
year = {2011}
}
@article{Mao2014a,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.1090v1},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
eprint = {arXiv:1410.1090v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
journal = {arXiv:1410.1090 [cs]},
mendeley-groups = {Progress Report},
pages = {1--9},
title = {{Explain Images with Multimodal Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1410.1090{\%}5Cnhttp://www.arxiv.org/pdf/1410.1090.pdf},
year = {2014}
}
@article{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.1090v1},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
eprint = {arXiv:1410.1090v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
journal = {arXiv:1410.1090 [cs]},
mendeley-groups = {Progress Report},
pages = {1--9},
title = {{Explain Images with Multimodal Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1410.1090{\%}5Cnhttp://www.arxiv.org/pdf/1410.1090.pdf},
year = {2014}
}
@article{Marchant2009,
abstract = {The biomisation method is used to reconstruct Latin American vegetation at 6000±500 and 18 000±1000 radiocarbon years before present ({\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP) from pollen data. Tests using modern pollen data from 381 samples derived from 287 locations broadly reproduce potential natural vegetation. The strong temperature gradient associated with the Andes is recorded by a transition from high altitude cool grass/shrubland and cool mixed forest to mid-altitude cool temperate rain forest, to tropical dry, seasonal and rain forest at low altitudes. Reconstructed biomes from a number of sites do not match the potential vegetation due to local factors such as human impact, methodological artefacts and mechanisms of pollen representivity of the parent vegetation. At 6000±500 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP 255 samples are analysed from 127 sites. Differences between the modern and the 6000±500 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP reconstruction are comparatively small; change relative to the modern reconstruction are mainly to biomes characteristic of drier climate in the north of the region with a slight more mesic shift in the south. Cool temperate rain forest remains dominant in western South America. In northwestern South America a number of sites record transitions from tropical seasonal forest to tropical dry forest and tropical rain forest to tropical seasonal forest. Sites in Central America show a change in biome assignment, but to more mesic vegetation, indicative of greater plant available moisture, e.g. on the Yucat{\`{a}}n peninsula sites record warm evergreen forest, replacing tropical dry forest and warm mixed forest presently recorded. At 18 000±1000 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP 61 samples from 34 sites record vegetation reflecting a generally cool and dry environment. Cool grass/shrubland is prevalent in southeast Brazil whereas Amazonian sites record tropical dry forest, warm temperate rain forest and tropical seasonal forest. Southernmost South America is dominated by cool grass/shrubland, a single site retains cool temperate rain forest indicating that forest was present at some locations at the LGM. Some sites in Central Mexico and lowland Colombia remain unchanged in the biome assignments of warm mixed forest and tropical dry forest respectively, although the affinities that these sites have to different biomes do change between 18 000±1000 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP and present. The "unresponsive" nature of these sites results from their location and the impact of local edaphic influence. [ABSTRACT FROM AUTHOR] Copyright of Climate of the Past is the property of Copernicus Gesellschaft mbH and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Marchant, R and Cleef, A and Harrison, S P and Hooghiemstra, H and Markgraf, V and van Boxel, J and Ager, T and Almeida, L and Anderson, R and Baied, C and Behling, H and Berrio, J C and Burbridge, R and Bjorck, S and Byrne, R and Bush, M and Duivenvoorden, J and Flenley, J and {De Oliveira}, P and van Geel, B},
journal = {Climate of the Past},
keywords = {BIOTIC communities -- Research,CARBON,CLIMATIC changes -- Research,CLIMATOLOGY -- Research,ISOTOPES,LATIN America,RADIOCARBON dating,VEGETATION {\&} climate},
number = {4},
pages = {725--767},
publisher = {Copernicus Gesellschaft mbH},
title = {{Pollen-based biome reconstructions for Latin America at 0, 6000 and 18000 radiocarbon years ago}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=eih{\&}AN=47908619{\&}site=ehost-live},
volume = {5},
year = {2009}
}
@article{Marshal2016,
author = {Marshal, David and Lai, Yukun and Marshal, David},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marshal, Lai, Marshal - 2016 - Part 1 PhD progress review(2).pdf:pdf},
title = {{Part 1 : PhD progress review}},
year = {2016}
}
@article{Marshal2016,
author = {Marshal, David and Lai, Yukun and Marshal, David},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marshal, Lai, Marshal - 2016 - Part 1 PhD progress review.pdf:pdf},
number = {October},
title = {{Part 1 : PhD progress review}},
year = {2016}
}
@article{Martens,
author = {Martens, David},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens - Unknown - Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary.pdf:pdf},
journal = {Knowledge Creation Diffusion Utilization},
mendeley-groups = {Annotated/Applications/Financial Engineering},
pages = {1--2},
title = {{Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary}}
}
@article{Martens2007,
abstract = {In recent years, support vector machines (SVMs) were successfully applied to a wide range of applications. However, since the classifier is described as a complex mathematical function, it is rather incomprehensible for humans. This opacity property prevents them from being used in many real-life applications where both accuracy and comprehensibility are required, such as medical diagnosis and credit risk evaluation. To overcome this limitation, rules can be extracted from the trained SVM that are interpretable by humans and keep as much of the accuracy of the SVM as possible. In this paper, we will provide an overview of the recently proposed rule extraction techniques for SVMs and introduce two others taken from the artificial neural networks domain, being Trepan and G-REX. The described techniques are compared using publicly available datasets, such as Ripley's synthetic dataset and the multi-class iris dataset. We will also look at medical diagnosis and credit scoring where comprehensibility is a key requirement and even a regulatory recommendation. Our experiments show that the SVM rule extraction techniques lose only a small percentage in performance compared to SVMs and therefore rank at the top of comprehensible classification techniques. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Martens, David and Baesens, Bart and {Van Gestel}, Tony and Vanthienen, Jan},
doi = {10.1016/j.ejor.2006.04.051},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2007 - Comprehensible credit scoring models using rule extraction from support vector machines.pdf:pdf},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Classification,Credit scoring,Rule extraction,Support vector machine},
mendeley-groups = {Annotated/Applications/Credit scoring},
number = {3},
pages = {1466--1476},
title = {{Comprehensible credit scoring models using rule extraction from support vector machines}},
volume = {183},
year = {2007}
}
@article{Martensa,
author = {Martens, David and Provost, Foster},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens, Provost - Unknown - Explaining Data-Driven Document Classifications.pdf:pdf},
keywords = {comprehensibility,document classification,instance level explanation,text mining},
mendeley-groups = {Annotated/Explanations,!Paper 3/task/newsgroups},
title = {{Explaining Data-Driven Document Classifications *}}
}
@article{Martens2011,
abstract = {This paper proposes a complete framework to assess the overall performance of classification models from a user perspective in terms of accuracy, comprehensibility, and justifiability. A review is provided of accuracy and comprehensibility measures, and a novel metric is introduced that allows one to measure the justifiability of classification models. Furthermore, taxonomy of domain constraints is introduced, and an overview of the existing approaches to impose constraints and include domain knowledge in data mining techniques is presented. Finally, justifiability metric is applied to a credit scoring and customer churn prediction case. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Martens, David and Vanthienen, Jan and Verbeke, Wouter and Baesens, Bart},
doi = {10.1016/j.dss.2011.01.013},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2011 - Performance of classification models from a user perspective.pdf:pdf},
isbn = {0167-9236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Classification,Comprehensibility,Data mining,Justifiability,Metrics},
mendeley-groups = {Annotated/Overarching Interpretability},
number = {4},
pages = {782--793},
publisher = {Elsevier B.V.},
title = {{Performance of classification models from a user perspective}},
url = {http://dx.doi.org/10.1016/j.dss.2011.01.013},
volume = {51},
year = {2011}
}
@article{Martins2013,
abstract = {We present fast, accurate, direct non-projective dependency parsers with third-order features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-of- the-art accuracies for the largest datasets (English, Czech, and German).},
author = {Martins, Andre and Almeida, Miguel and Smith, Noah A},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins, Almeida, Smith - 2013 - Turning on the Turbo Fast Third-Order Non-Projective Turbo Parsers.pdf:pdf},
isbn = {9781937284510},
journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
pages = {617--622},
title = {{Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers}},
url = {http://www.aclweb.org/anthology/P13-2109{\%}5Cnhttp://www.cs.cmu.edu/{~}nasmith/papers/martins+almeida+smith.acl13.pdf},
year = {2013}
}
@article{Mcauley2013,
author = {Mcauley, Julian},
isbn = {9781450324090},
journal = {RecSys '13 Proceedings of the 7th ACM conference on Recommender systems},
keywords = {recommender systems,topic models},
mendeley-groups = {Annotated/Datasets},
pages = {165--172},
title = {{Hidden Factors and Hidden Topics : Understanding Rating Dimensions with Review Text}},
year = {2013}
}
@article{Melamud2016,
abstract = {We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from scikit-learned contexts of substitute words.},
archivePrefix = {arXiv},
arxivId = {1601.00893},
author = {Melamud, Oren and McClosky, David and Patwardhan, Siddharth and Bansal, Mohit},
eprint = {1601.00893},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melamud et al. - 2016 - The Role of Context Types and Dimensionality in Learning Word Embeddings.pdf:pdf},
isbn = {9781941643914},
mendeley-groups = {!Paper 3/task,!Paper 3/task/Sentiment treebank},
title = {{The Role of Context Types and Dimensionality in Learning Word Embeddings}},
url = {http://arxiv.org/abs/1601.00893},
year = {2016}
}
@article{Melis2017,
abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
archivePrefix = {arXiv},
arxivId = {1707.05589},
author = {Melis, G{\'{a}}bor and Dyer, Chris and Blunsom, Phil},
eprint = {1707.05589},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melis, Dyer, Blunsom - 2017 - On the State of the Art of Evaluation in Neural Language Models.pdf:pdf},
isbn = {9781604562170},
mendeley-groups = {!Paper 3/Language models},
pages = {1--10},
title = {{On the State of the Art of Evaluation in Neural Language Models}},
url = {http://arxiv.org/abs/1707.05589},
year = {2017}
}
@article{Mercado2016,
abstract = {OBJECTIVE We investigated the effects of level of agent transparency on operator performance, trust, and workload in a context of human-agent teaming for multirobot management. BACKGROUND Participants played the role of a heterogeneous unmanned vehicle (UxV) operator and were instructed to complete various missions by giving orders to UxVs through a computer interface. An intelligent agent (IA) assisted the participant by recommending two plans-a top recommendation and a secondary recommendation-for every mission. METHOD A within-subjects design with three levels of agent transparency was employed in the present experiment. There were eight missions in each of three experimental blocks, grouped by level of transparency. During each experimental block, the IA was incorrect three out of eight times due to external information (e.g., commander's intent and intelligence). Operator performance, trust, workload, and usability data were collected. RESULTS Results indicate that operator performance, trust, and perceived usability increased as a function of transparency level. Subjective and objective workload data indicate that participants' workload did not increase as a function of transparency. Furthermore, response time did not increase as a function of transparency. CONCLUSION Unlike previous research, which showed that increased transparency resulted in increased performance and trust calibration at the cost of greater workload and longer response time, our results support the benefits of transparency for performance effectiveness without additional costs. APPLICATION The current results will facilitate the implementation of IAs in military settings and will provide useful data to the design of heterogeneous UxV teams.},
author = {Mercado, Joseph E. and Rupp, Michael A. and Chen, Jessie Y. C. and Barnes, Michael J. and Barber, Daniel and Procci, Katelyn},
doi = {10.1177/0018720815621206},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mercado et al. - 2016 - Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management.pdf:pdf},
issn = {0018-7208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
keywords = {address correspondence to joseph,agent teaming,army,e,human,human research and engineering,intelligent agent transparency,mercado,multi-uxv management,research laboratory,s,u},
mendeley-groups = {Annotated/Overarching Interpretability},
number = {3},
pages = {401--415},
pmid = {26867556},
title = {{Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management}},
url = {http://journals.sagepub.com/doi/10.1177/0018720815621206},
volume = {58},
year = {2016}
}
@article{Merity2017a,
abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the scikit-learn-dropped LSTM which uses DropConnect on hidden-to-hidden scikit-learns as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
archivePrefix = {arXiv},
arxivId = {1708.02182},
author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
eprint = {1708.02182},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merity, Keskar, Socher - 2017 - Regularizing and Optimizing LSTM Language Models.pdf:pdf},
mendeley-groups = {!Paper 3/Language models},
title = {{Regularizing and Optimizing LSTM Language Models}},
url = {http://arxiv.org/abs/1708.02182},
year = {2017}
}
@article{Merity2017,
abstract = {Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing. Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance. Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations. We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling. Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures. These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.},
archivePrefix = {arXiv},
arxivId = {1708.01009},
author = {Merity, Stephen and McCann, Bryan and Socher, Richard},
eprint = {1708.01009},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merity, McCann, Socher - 2017 - Revisiting Activation Regularization for Language RNNs.pdf:pdf},
mendeley-groups = {!Paper 3/Training LSTMs},
title = {{Revisiting Activation Regularization for Language RNNs}},
url = {http://arxiv.org/abs/1708.01009},
year = {2017}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Nips},
mendeley-groups = {Annotated/Word Vectors,Interim Review},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lowerMikolov, T., Chen, K., Corrado, G., {\&} Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Arxiv, 1-12. http://doi.org/10.1162/153244303322533223 computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Arxiv},
mendeley-groups = {Annotated/Word Vectors,Report/Features,Interim Review,Report},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Miller1956,
abstract = {First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Miller, George A.},
doi = {10.1037/h0043158},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1956 - The magical number seven, plus or minus two some limits on our capacity for processing information.pdf:pdf},
isbn = {0198568770;},
issn = {1939-1471},
journal = {Psychological Review},
mendeley-groups = {Annotated/Psychology},
number = {2},
pages = {81--97},
pmid = {8022966},
title = {{The magical number seven, plus or minus two: some limits on our capacity for processing information.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0043158},
volume = {63},
year = {1956}
}
@article{Miller2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.07269v1},
author = {Miller, Tim},
eprint = {arXiv:1706.07269v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 2017 - Explanation in Artificial Intelligence Insights from the Social Sciences.pdf:pdf},
keywords = {explainability,explainable ai,explanation,interpretability,transparency},
mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
title = {{Explanation in Artificial Intelligence : Insights from the Social Sciences}},
year = {2017}
}
@article{Miller2017,
abstract = {In his seminal book The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity [2004, Sams Indianapolis, IN, USA], Alan Cooper ar-gues that a major reason why software is of-ten poorly designed (from a user perspective) is that programmers are in charge of design de-cisions, rather than interaction designers. As a result, programmers design software for them-selves, rather than for their target audience; a phenomenon he refers to as the 'inmates run-ning the asylum'. This paper argues that ex-plainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But ex-plainable AI is more likely to succeed if re-searchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science; and if evalu-ation of these models is focused more on people than on technology. From a light scan of litera-ture, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller, Howe, Sonenberg - 2017 - Explainable AI Beware of Inmates Running the Asylum.pdf:pdf},
journal = {IJCAI - Workshop on Explainable AI},
mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability},
title = {{Explainable AI: Beware of Inmates Running the Asylum}},
year = {2017}
}
@article{Mimno2011,
abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Un-fortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional sub- spaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mimno et al. - 2011 - Optimizing semantic coherence in topic models.pdf:pdf},
isbn = {9781937284114},
issn = {1937284115},
journal = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
keywords = {topic coherence,topic models,topics evaluation},
mendeley-groups = {Report/Clustering},
number = {2},
pages = {262--272},
title = {{Optimizing semantic coherence in topic models}},
year = {2011}
}
@article{Mitchell2015,
abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that per-formance on this task can be related to orthogonality within the space. Explic-itly designing such structure into a neu-ral network model results in represen-tations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within En-glish Wikipedia text to enable this de-composition can produce substantial im-provements on semantic-similarity, pos-induction and word-analogy tasks.},
author = {Mitchell, Jeff and Steedman, Mark},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/semsynacl2015{\_}final.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
mendeley-groups = {11Thesis/Conceptual Spaces {\&} Properties,11Thesis/Interpretability/Visual},
pages = {1301--1310},
title = {{Orthogonality of Syntax and Semantics within Distributional Spaces}},
url = {http://www.aclweb.org/anthology/P15-1126},
year = {2015}
}
@article{Mitchell1997a,
author = {Mitchell, Tom and Simon, Herb and Pomerleau, Dean},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell, Simon, Pomerleau - 1997 - Multitask Learning Rich Caruana 23 September1997.pdf:pdf},
number = {September},
title = {{Multitask Learning Rich Caruana 23 September1997}},
year = {1997}
}
@article{Miyato2016,
abstract = {Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.},
archivePrefix = {arXiv},
arxivId = {1605.07725},
author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
doi = {10.2507/daaam.scibook.2010.27},
eprint = {1605.07725},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyato, Dai, Goodfellow - 2016 - Adversarial Training Methods for Semi-Supervised Text Classification.pdf:pdf},
isbn = {2840601737},
issn = {18766102},
mendeley-groups = {!Paper 3/task/Large Movie Review},
pages = {1--11},
pmid = {19963286},
title = {{Adversarial Training Methods for Semi-Supervised Text Classification}},
url = {http://arxiv.org/abs/1605.07725},
year = {2016}
}
@article{Miyato2017,
abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the output distribution. Virtual adversarial loss is defined as the robustness of the model's posterior distribution against local perturbation around each input data point. Our method is similar to adversarial training, but differs from adversarial training in that it determines the adversarial direction based only on the output distribution and that it is applicable to a semi-supervised setting. Because the directions in which we smooth the model are virtually adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward and backpropagations. In our experiments, we applied VAT to supervised and semi-supervised learning on multiple benchmark datasets. With additional improvement based on entropy minimization principle, our VAT achieves the state-of-the-art performance on SVHN and CIFAR-10 for semi-supervised learning tasks.},
archivePrefix = {arXiv},
arxivId = {1704.03976},
author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
eprint = {1704.03976},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyato et al. - 2017 - Virtual Adversarial Training a Regularization Method for Supervised and Semi-supervised Learning.pdf:pdf},
pages = {1--14},
title = {{Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning}},
url = {http://arxiv.org/abs/1704.03976},
year = {2017}
}
@article{Mnih2008,
abstract = {Neural probabilistic language models (NPLMs) have been shown to be competi- tive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non- hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1},
author = {Mnih, Andriy and Hinton, Geoffrey E.},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih, Hinton - 2008 - A Scalable Hierarchical Distributed Language Model.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems},
pages = {1--8},
title = {{A Scalable Hierarchical Distributed Language Model.}},
url = {http://discovery.ucl.ac.uk/63249/},
year = {2008}
}
@article{Model2018,
author = {Model, Autoregressive Exogenous and Lu, Yao},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Model, Lu - 2018 - INTERPRETABLE LSTM NEURAL NETWORK FOR.pdf:pdf},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
pages = {1--7},
title = {{INTERPRETABLE LSTM NEURAL NETWORK FOR}},
year = {2018}
}
@book{Moewes,
author = {Moewes, Christian and N{\"{u}}rnberger, Andreas},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/The safe and interpretable machine learning part.pdf:pdf},
isbn = {9783642323775},
mendeley-groups = {11Thesis/Interpretability/Safety},
title = {{in Intelligent Data Analysis}}
}
@article{Murdoch2018,
abstract = {The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.},
archivePrefix = {arXiv},
arxivId = {1801.05453},
author = {Murdoch, W. James and Liu, Peter J. and Yu, Bin},
eprint = {1801.05453},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Liu, Yu - 2018 - Beyond Word Importance Contextual Decomposition to Extract Interactions from LSTMs.pdf:pdf},
mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank},
pages = {1--14},
title = {{Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs}},
url = {http://arxiv.org/abs/1801.05453},
year = {2018}
}
@article{Murdoch2017,
abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
archivePrefix = {arXiv},
arxivId = {1702.02540},
author = {Murdoch, W. James and Szlam, Arthur},
doi = {10.5121/ijci.2015.4221},
eprint = {1702.02540},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Szlam - 2017 - Automatic Rule Extraction from Long Short Term Memory Networks.pdf:pdf},
issn = {23208430},
mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/Interpretable LSTMs,!Paper 3/task,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank},
number = {2016},
pages = {1--12},
title = {{Automatic Rule Extraction from Long Short Term Memory Networks}},
url = {http://arxiv.org/abs/1702.02540},
year = {2017}
}
@article{Murphy,
abstract = {In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We find that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the first approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines. {\textcopyright} 2012 The COLING.},
author = {Murphy, Brian and Talukdar, Partha Pratim and Mitchell, Tom},
file = {::},
journal = {24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers},
keywords = {Distributional semantics,Interpretability,Neuro-semantics,Sparse coding,Vector-space models,Word embeddings},
mendeley-groups = {Thesis/Sparse reps},
number = {December 2012},
pages = {1933--1950},
title = {{Learning effective and interpretable semantic models using non-negative sparse embedding}},
year = {2012}
}

@article{Nam2014a,
abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
archivePrefix = {arXiv},
arxivId = {1312.5419},
author = {Nam, Jinseok and Kim, Jungi and {Loza Menc??a}, Eneldo and Gurevych, Iryna and F??rnkranz, Johannes},
doi = {10.1007/978-3-662-44851-9_28},
eprint = {1312.5419},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam et al. - 2014 - Large-scale multi-label text classification - Revisiting neural networks.pdf:pdf},
isbn = {9783662448502},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Literature Review,Progress Report},
number = {PART 2},
pages = {437--452},
title = {{Large-scale multi-label text classification - Revisiting neural networks}},
volume = {8725 LNAI},
year = {2014}
}
@article{Nam2014,
abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
archivePrefix = {arXiv},
arxivId = {1312.5419},
author = {Nam, Jinseok and Kim, Jungi and {Loza Menc{\'{i}}a}, Eneldo and Gurevych, Iryna and F{\"{u}}rnkranz, Johannes},
doi = {10.1007/978-3-662-44851-9_28},
eprint = {1312.5419},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam et al. - 2014 - Large-scale multi-label text classification - Revisiting neural networks(2).pdf:pdf},
isbn = {9783662448502},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Report/Multi-label},
number = {PART 2},
pages = {437--452},
title = {{Large-scale multi-label text classification - Revisiting neural networks}},
volume = {8725 LNAI},
year = {2014}
}
@article{Narayanan2018,
abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
archivePrefix = {arXiv},
arxivId = {1802.00682},
author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
eprint = {1802.00682},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine Learning Systems An Evaluation of the Human-Interpretabilit.pdf:pdf},
pages = {1--21},
title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
url = {http://arxiv.org/abs/1802.00682},
year = {2018}
}
@article{Narayanan2018,
abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
archivePrefix = {arXiv},
arxivId = {1802.00682},
author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
eprint = {1802.00682},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine Learning Systems An Evaluation of the Human-Interpretabilit.pdf:pdf},
mendeley-groups = {!Paper 3,11Thesis/Interpretability,11Thesis/Interpretability/Explanation,11Thesis/Interpretability/General},
pages = {1--21},
title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
url = {http://arxiv.org/abs/1802.00682},
year = {2018}
}
@article{Nogueira2011,
author = {Nogueira, T.M. and Camargo, H.a. and Rezende, S.O.},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Camargo, Rezende - 2011 - Fuzzy Rules for Document Classification to Improve Information Retrieval.pdf:pdf},
journal = {Mirlabs.Org},
keywords = {fuzzy clustering,imprecision,information retrieval,text categorization,text mining,uncertainty},
mendeley-groups = {Annotated/Decision Trees},
pages = {210--217},
title = {{Fuzzy Rules for Document Classification to Improve Information Retrieval}},
url = {http://www.mirlabs.org/ijcisim/regular{\_}papers{\_}2011/Paper25.pdf},
volume = {3},
year = {2011}
}
@article{NorouziM2009,
abstract = {In this thesis, we present a method for learning problem-specific hierarchical features specialized for vision applications. Recently, a greedy layerwise learning mechanism has been proposed for tuning parameters of fully connected hierarchical networks. This approach views layers of a network as Restricted Boltzmann Machines (RBM), and trains them separately from the bottom layer upwards. We develop Convolutional RBM (CRBM), an extension of the RBM model in which connections are local and scikit-learns are shared to respect the spatial structure of images. We switch between the CRBM and down-sampling layers and stack them on top of each other to build a multilayer hierarchy of alternating filtering and pooling. This framework learns generic features such as oriented edges at the bottom levels and features specific to an object class such as object parts in the top layers. Afterward, we feed the extracted features into a discriminative classifier for recognition. It is experimentally demonstrated that the features automatically learned by our algorithm are effective for object detection, by using them to obtain performance comparable to the state-of-the-art on handwritten digit classification and pedestrian detection.},
author = {{Norouzi M}},
doi = {10.1109/CVPR.2009.5206577},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi M - 2009 - Convolutional restricted Boltzmann machines for feature learning.pdf:pdf},
isbn = {9781424439911},
journal = {School of Computing Science-Simon Fraser University},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
pages = {2735--2742},
title = {{Convolutional restricted Boltzmann machines for feature learning}},
year = {2009}
}
@article{Number2016,
author = {Number, N I and Code, Post and Code, Sort and Account, Bank and Name, Society and Branch, Society and Roll, Building Society and Contact, Emergency and Number, Student},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Number et al. - 2016 - ENGAGEMENT OF A POST GRADUATE STUDENT DEMONSTRATOR.pdf:pdf},
number = {August},
pages = {1--7},
title = {{ENGAGEMENT OF A POST GRADUATE STUDENT DEMONSTRATOR}},
year = {2016}
}
@article{OldenD.A.2002a,
author = {{Olden  D. A.}, J D Y Jackson},
doi = {10.1016/S0304-3800(02)00064-9},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olden D. A. - 2002 - Illuminating the black box a ramdomization approach for understanding variable contributions in artificial neuronal.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {connection scikit-learns, sensitivity analysis, neural i},
mendeley-groups = {Literature Review},
pages = {135--150},
title = {{Illuminating the "black box": a ramdomization approach for understanding variable contributions in artificial neuronal networks.}},
volume = {154},
year = {2002}
}
@article{Oquab2015,
author = {Oquab, Maxime},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab - 2015 - Is object localization for free - Weakly-supervised learning with convolutional neural networks To cite this version Is.pdf:pdf},
isbn = {9781467369640},
mendeley-groups = {Progress Report},
number = {iii},
title = {{Is object localization for free ? - Weakly-supervised learning with convolutional neural networks To cite this version : Is object localization for free ? - Weakly-supervised learning with convolutional neural networks}},
year = {2015}
}
@article{Oquab2014,
abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large-scale visual recognition challenge (ILSVRC2012). The suc-cess of CNNs is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification meth-ods. Learning CNNs, however, amounts to estimating mil-lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi-ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep-resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
doi = {10.1109/CVPR.2014.222},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab et al. - 2014 - Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
mendeley-groups = {Progress Report,Interim Review},
pages = {1717--1724},
title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
year = {2014}
}
@article{Ou2007,
abstract = {Multi-class pattern classification has many applications including text document classification, speech recognition, object recognition, etc. Multi-class pattern classification using neural networks is not a trivial extension from two-class neural networks. This paper presents a comprehensive and competitive study in multi-class neural learning with focuses on issues including neural network architecture, encoding schemes, training methodology and training time complexity. Our study includes multi-class pattern classification using either a system of multiple neural networks or a single neural network, and modeling pattern classes using one-against-all, one-against-one, one-against-higher-order, and P-against-Q. We also discuss implementations of these approaches and analyze training time complexity associated with each approach. We evaluate six different neural network system architectures for multi-class pattern classification along the dimensions of imbalanced data, large number of pattern classes, large vs. small training data through experiments conducted on well-known benchmark data. ?? 2006 Pattern Recognition Society.},
author = {Ou, Guobin and Murphey, Yi Lu},
doi = {10.1016/j.patcog.2006.04.041},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ou, Murphey - 2007 - Multi-class pattern classification using neural networks.pdf:pdf},
isbn = {0769521282},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Machine learning,Multi-class classification,Neural networks,Pattern recognition},
number = {1},
pages = {4--18},
title = {{Multi-class pattern classification using neural networks}},
volume = {40},
year = {2007}
}
@article{Palangi2017,
abstract = {We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In our application of TPRN, internal representations-learned by end-to-end optimization in a deep neural network performing a textual question-answering (QA) task-can be interpreted using basic concepts from linguistic theory. No performance penalty need be paid for this increased interpretability: the proposed model performs comparably to a state-of-the-art system on the SQuAD QA task. The internal representation which is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Fine-grained analysis reveals specific correspondences between the learned roles and parts of speech as assigned by a standard tagger (Toutanova et al. 2003), and finds several discrepancies in the model's favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means to build representations using symbols and roles, with an inductive bias favoring use of these in an approximately discrete manner.},
archivePrefix = {arXiv},
arxivId = {1705.08432},
author = {Palangi, Hamid and Smolensky, Paul and He, Xiaodong and Deng, Li},
eprint = {1705.08432},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palangi et al. - 2017 - Question-Answering with Grammatically-Interpretable Representations.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
pages = {5350--5357},
title = {{Question-answering with grammatically-interpretable representations}},
url = {http://arxiv.org/abs/1705.08432},
year = {2018}
}


@article{Panchenko2016,
abstract = {Word sense embeddings represent a word sense as a low-dimensional numeric vector. While this representation is potentially useful for NLP applications, its interpretability is inherently limited. We propose a simple technique that improves interpretability of sense vectors by mapping them to synsets of a lexical resource. Our experiments with AdaGram sense embeddings and BabelNet synsets showthat it is possible to retrieve synsets that correspond to automatically learned sense vectors with Precision of 0.87, Recall of 0.42 andAUC of 0.78.},
author = {Panchenko, Alexander},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Panchenko - 2016 - Best of Both Worlds Making Word Sense Embeddings Interpretable.pdf:pdf},
journal = {the 10th edition of the Language Resources and Evaluation Conference (LREC 2016)},
keywords = {adagram,babelnet,lexical semantics,sense matching,word sense embeddings,wordnet},
mendeley-groups = {Annotated/Word Vectors},
pages = {2649--2655},
title = {{Best of Both Worlds: Making Word Sense Embeddings Interpretable}},
url = {http://www.lrec-conf.org/proceedings/lrec2016/pdf/625{\_}Paper.pdf},
year = {2016}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1211.5063},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {1211.5063},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - 2012 - On the difficulty of training Recurrent Neural Networks.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
mendeley-groups = {!Paper 3/Training LSTMs},
pmid = {18267787},
title = {{On the difficulty of training Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1211.5063},
year = {2012}
}
@article{Pazzani2000,
author = {Pazzani, Michael J},
doi = {10.1109/5254.850821},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pazzani - 2000 - Knowledge discovery from data.pdf:pdf},
issn = {1094-7167},
journal = {Intelligent systems and their applications, IEEE},
mendeley-groups = {Annotated/Overarching Interpretability},
number = {2},
pages = {10--12},
title = {{Knowledge discovery from data?}},
volume = {15},
year = {2000}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedregosa et al. - 2012 - Scikit-learn Machine Learning in Python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
mendeley-groups = {Annotated/Software},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Peng2015a,
abstract = {Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information.},
archivePrefix = {arXiv},
arxivId = {1506.07220},
author = {Peng, Yangtuo and Jiang, Hui},
eprint = {1506.07220},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Jiang - 2015 - Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks.pdf:pdf},
mendeley-groups = {Literature Review,Interim Review},
pages = {5},
title = {{Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks}},
url = {http://arxiv.org/abs/1506.07220},
year = {2015}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {Progress Report,Interim Review},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Peterson2016,
abstract = {Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reach-ing or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representa-tions learned by these networks and human psychological rep-resentations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these fea-tures do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human sim-ilarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological exper-iments.},
archivePrefix = {arXiv},
arxivId = {1608.02164},
author = {Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L},
eprint = {1608.02164},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peterson, Abbott, Griffiths - 2016 - Adapting Deep Network Features to Capture Psychological Representations.pdf:pdf},
keywords = {deep learning,neural networks,psychological,representations,similarity},
mendeley-groups = {Report/Features,Progress Report},
pages = {2363--2368},
title = {{Adapting Deep Network Features to Capture Psychological Representations}},
year = {2016}
}
@article{Pfenning2001,
abstract = {We reconsider the foundations of modal logic, following Martin-L{\"{o}}f's methodology of distinguishing judgments from propositions. We give constructive meaning explanations for necessity and possibility, which yields a simple and uniform system of natural deduction for intuitionistic modal logic that does not exhibit anomalies found in other proposals. We also give a new presentation of lax logic and find that the lax modality is already expressible using possibility and necessity. Through a computational interpretation of proofs in modal logic we further obtain a new formulation of Moggi's monadic metalanguage.$\backslash$n},
author = {Pfenning, Frank and Davies, Rowan},
doi = {10.1017/S0960129501003322},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfenning, Davies - 2001 - A judgmental reconstruction of modal logic.pdf:pdf},
isbn = {0960129501003},
issn = {0960-1295},
journal = {Mathematical Structures in Computer Science},
mendeley-groups = {Progress Report},
number = {4},
pages = {511--540},
title = {{A judgmental reconstruction of modal logic}},
url = {http://www.journals.cambridge.org/abstract{\_}S0960129501003322},
volume = {11},
year = {2001}
}
@article{Plikynas2004a,
author = {Plikynas, Darius},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plikynas - 2004 - Decision rules extraction from neural network A modified pedagogical approach.pdf:pdf},
journal = {Information Technology and Control},
keywords = {decisions reasonong,information extraction,neural networks},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
number = {31},
pages = {53--59},
title = {{Decision rules extraction from neural network: A modified pedagogical approach}},
url = {http://itc.ktu.lt/itc31/Plikyn31.pdf},
volume = {2},
year = {2004}
}
@article{Po,
author = {Po, Huang and Teaching, Zen and Po, Huang},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Po, Teaching, Po - Unknown - “ Up to now , you have refuted everything which has been said . You have done nothing to point out the true.pdf:pdf},
title = {{“ Up to now , you have refuted everything which has been said . You have done nothing to point out the true Dharma to us .”}}
}
@article{Poursabzi-sangdeh,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.07810v2},
author = {Poursabzi-sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Vaughan, Jennifer Wortman and Wallach, Hanna},
eprint = {arXiv:1802.07810v2},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.07810.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/General},
pages = {1--20},
title = {{Manipulating and Measuring Model Interpretability}}
}
@article{Predic2010a,
author = {Predic, N Y P and June, Meetup},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Predic, June - 2010 - Introduction to.pdf:pdf},
isbn = {1441923349},
pages = {1--8},
title = {{Introduction to}},
year = {2010}
}
@article{Press2016,
abstract = {We study the topmost scikit-learn matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that scikit-learn tying can reduce the size of neural translation models to less than half of their original size without harming their performance.},
archivePrefix = {arXiv},
arxivId = {1608.05859},
author = {Press, Ofir and Wolf, Lior},
eprint = {1608.05859},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Press, Wolf - 2016 - Using the Output Embedding to Improve Language Models.pdf:pdf},
isbn = {9781510838604},
mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs,!Paper 3/task/Sentiment treebank},
title = {{Using the Output Embedding to Improve Language Models}},
url = {http://arxiv.org/abs/1608.05859},
year = {2016}
}
@article{Prior2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.11571v2},
author = {Prior, Human-in-the-loop Interpretability and Ross, Andrew Slavin and Kim, Been and Gershman, Samuel J},
eprint = {arXiv:1805.11571v2},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1805.11571.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
number = {1},
pages = {1--13},
title = {{Human-in-the-Loop Interpretability Prior}},
year = {2018}
}
@article{Radford2017,
abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
archivePrefix = {arXiv},
arxivId = {1704.01444},
author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
eprint = {1704.01444},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford, Jozefowicz, Sutskever - 2017 - Learning to Generate Reviews and Discovering Sentiment.pdf:pdf},
mendeley-groups = {!Paper 3/task,!Paper 3/Interpretable LSTMs,!Paper 3/task/Large Movie Review,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank,!Paper 3/Understanding LSTMs},
title = {{Learning to Generate Reviews and Discovering Sentiment}},
url = {http://arxiv.org/abs/1704.01444},
year = {2017}
}
@article{Rankings,
author = {Rankings, Phrase Direction and Score, Ndcg and Rankings, Cluster and Tree, Decision and Validation, Using Cross},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rankings et al. - Unknown - Cross - validation Process.pdf:pdf},
pages = {7--8},
title = {{Cross - validation Process}}
}
@article{Read2010,
author = {Read, Jesse},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2010 - Scalable Multi-label Classification.pdf:pdf},
mendeley-groups = {!Paper 3/task/newsgroups},
title = {{Scalable Multi-label Classification}},
volume = {1994},
year = {2010}
}
@article{Read2013,
abstract = {abstract Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs compara-tive experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
author = {Read, Jesse},
doi = {10.4018/978-1-60566-058-5.ch021},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2013 - Multi-label Classification.pdf:pdf},
isbn = {978-1-4244-1065-1},
issn = {1548-3924},
mendeley-groups = {Report/Multi-label},
pages = {2002--2004},
title = {{Multi-label Classification}},
year = {2013}
}
@article{Read2010,
author = {Read, Jesse},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2010 - Scalable Multi-label Classification(2).pdf:pdf},
title = {{Scalable Multi-label Classification}},
year = {2010}
}
@article{Read2015a,
abstract = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
archivePrefix = {arXiv},
arxivId = {1503.09022},
author = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
eprint = {1503.09022},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Hollm{\'{e}}n - 2015 - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
keywords = {meta-labels,multi-label classification,neural net-,problem transformation},
mendeley-groups = {Interim Review},
pages = {1--23},
title = {{Multi-label Classification using Labels as Hidden Nodes}},
url = {http://arxiv.org/abs/1503.09022},
year = {2015}
}
@article{Read2015,
abstract = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
archivePrefix = {arXiv},
arxivId = {1503.09022},
author = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
eprint = {1503.09022},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Hollm{\'{e}}n - 2015 - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
keywords = {meta-labels,multi-label classification,neural net-,problem transformation},
pages = {1--23},
title = {{Multi-label Classification using Labels as Hidden Nodes}},
url = {http://arxiv.org/abs/1503.09022},
year = {2015}
}
@article{Read2014a,
abstract = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
archivePrefix = {arXiv},
arxivId = {1502.05988},
author = {Read, Jesse and Perez-Cruz, Fernando},
eprint = {1502.05988},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Perez-Cruz - 2014 - Deep Learning for Multi-label Classification.pdf:pdf},
pages = {1--8},
title = {{Deep Learning for Multi-label Classification}},
url = {http://arxiv.org/abs/1502.05988},
year = {2014}
}
@article{Reading2008,
author = {Reading, Additional},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reading - 2008 - Reproduced with permission of the copyright owner . Further reproduction prohibited without permission .pdf:pdf},
journal = {Pediatric Infectious Disease},
number = {3},
title = {{Reproduced with permission of the copyright owner . Further reproduction prohibited without permission .}},
volume = {34},
year = {2008}
}
@article{Rezende2014,
abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
archivePrefix = {arXiv},
arxivId = {1401.4082},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
doi = {10.1051/0004-6361/201527329},
eprint = {1401.4082},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende, Mohamed, Wierstra - 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
mendeley-groups = {!Paper 3/Bayesian Networks},
pmid = {23459267},
title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
url = {http://arxiv.org/abs/1401.4082},
year = {2014}
}

@article{Ribeiro,
abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, " sufficient " conditions for predic-tions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by ex-plaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - Unknown - Anchors High-Precision Model-Agnostic Explanations.pdf:pdf},
mendeley-groups = {!Paper 3},
title = {{Anchors: High-Precision Model-Agnostic Explanations}}
}
@article{Richards2001,
abstract = {This paper describes the analysis of a database of diabetic patients' clinical records and death certificates. The objective of the study was to find rules that describe associations between observations made of patients at their first visit to the hospital and early mortality. Pre-processing was carried out and a knowledge discovery in databases (KDD) package, developed by the Lanner Group and the University of East Anglia, was used for rule induction using simulated annealing. The most significant discovered rules describe an association that was not generally known or accepted by the medical community, however, recent independent studies confirm their validity. ?? 2001 Elsevier Science B.V.},
author = {Richards, G. and Rayward-Smith, V. J. and S??nksen, P. H. and Carey, S. and Weng, C.},
doi = {10.1016/S0933-3657(00)00110-X},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Richards et al. - 2001 - Data mining for indicators of early mortality in a database of clinical records.pdf:pdf},
isbn = {0933-3657},
issn = {09333657},
journal = {Artificial Intelligence in Medicine},
keywords = {Data mining,Diabetes,Neuropathy,Rule induction},
mendeley-groups = {Report/Medical domain},
number = {3},
pages = {215--231},
pmid = {11377148},
title = {{Data mining for indicators of early mortality in a database of clinical records}},
volume = {22},
year = {2001}
}
@article{Rifai2011,
abstract = {Although the structure and composition of plant communities is known to influence the functioning of ecosystems, there is as yet no agreement as to how these should be described from a functional perspective. We tested the biomass ratio hypothesis, which postulates that ecosystem properties should depend on species traits and on species contribution to the total biomass of the community, in a successional sere following vineyard abandonment in the Mediterranean region of France. Ecosystem-specific net primary productivity, litter decomposition rate, and total soil carbon and nitrogen varied significantly with field age, and correlated with community-aggregated (i.e., weighed according to the relative abundance of species) functional leaf traits. The three easily measurable traits tested, specific leaf area, leaf dry matter content, and nitrogen concentration, provide a simple means to scale up from organ to ecosystem functioning in complex plant communities. We propose that they be called {\&}8220;functional markers,{\&}8221; and be used to assess the impacts of community changes on ecosystem properties induced, in particular, by global change drivers.},
author = {Rifai, Salah and Muller, Xavier},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rifai, Muller - 2011 - Contractive Auto-Encoders Explicit Invariance During Feature Extraction.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Icml},
number = {1},
pages = {833--840},
title = {{Contractive Auto-Encoders : Explicit Invariance During Feature Extraction}},
url = {http://www.icml-2011.org/papers/455{\_}icmlpaper.pdf},
volume = {85},
year = {2011}
}
@article{Rivest1987,
abstract = {This paper introduces a new representation for Boolean functions, called decision lists,  and shows that they are eciently learnable from examples. More precisely, this result  is established for $\backslash$k-DL" {\{} the set of decision lists with conjunctive clauses of size k at  each decision. Since k-DL properly includes other well-known techniques for representing  Boolean functions such as k-CNF (formulae in conjunctive normal form with at most k  literals per clause), k-DNF (formulae in disjunctive normal form with at most k literals  per term), and decision trees of depth k, our result strictly increases the set of functions  which are known to be polynomially learnable, in the sense of Valiant (1984). Our proof is  constructive: we present an algorithm which can eciently construct an element of k-DL  consistent with a given set of examples, if one exists.}},
author = {Rivest, Ronald L.},
doi = {10.1023/A:1022607331053},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivest - 1987 - Learning Decision Lists.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Boolean formulae,Learning from examples,decision lists,polynomial-time identification},
mendeley-groups = {Annotated/Interpretable Classifiers},
number = {3},
pages = {229--246},
title = {{Learning Decision Lists}},
volume = {2},
year = {1987}
}
@article{Rocktaschel2015,
abstract = {While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention scikit-learns produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.},
archivePrefix = {arXiv},
arxivId = {1509.06664},
author = {Rockt{\"{a}}schel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Blunsom, Phil},
doi = {10.1017/CBO9781107415324.004},
eprint = {1509.06664},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rockt{\"{a}}schel et al. - 2015 - Reasoning about Entailment with Neural Attention.pdf:pdf},
isbn = {9781941643723},
issn = {10450823},
mendeley-groups = {!Paper 3/LSTM Types},
number = {2015},
pages = {1--9},
pmid = {9377276},
title = {{Reasoning about Entailment with Neural Attention}},
url = {http://arxiv.org/abs/1509.06664},
year = {2015}
}
@article{Rocktaschel2015,
author = {Rockt{\"{a}}schel, Tim and Singh, Sameer and Riedel, Sebastian},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rockt{\"{a}}schel, Singh, Riedel - 2015 - Injecting logical background knowledge into embeddings for relation extraction.pdf:pdf},
journal = {Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
title = {{Injecting logical background knowledge into embeddings for relation extraction}},
year = {2015}
}
@article{Rolfe2013,
abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
archivePrefix = {arXiv},
arxivId = {1301.3775},
author = {Rolfe, Jason Tyler and LeCun, Yan},
eprint = {1301.3775},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rolfe, LeCun - 2013 - Discriminative Recurrent Sparse Auto-Encoders.pdf:pdf},
journal = {CoRR},
pages = {15},
title = {{Discriminative Recurrent Sparse Auto-Encoders}},
url = {http://arxiv.org/abs/1301.3775},
year = {2013}
}
@article{Ross2017,
abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
archivePrefix = {arXiv},
arxivId = {1703.03717},
author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
eprint = {1703.03717},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
mendeley-groups = {!Paper 3/task/newsgroups},
title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
url = {http://arxiv.org/abs/1703.03717},
year = {2017}
}
@article{Ross2017,
abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
archivePrefix = {arXiv},
arxivId = {1703.03717},
author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
eprint = {1703.03717},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
mendeley-groups = {Report/Explaining predictions,Annotated/Explanations,!Paper 3/task/newsgroups},
title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
url = {http://arxiv.org/abs/1703.03717},
year = {2017}
}
@article{Ross2017a,
abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
archivePrefix = {arXiv},
arxivId = {1703.03717},
author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
eprint = {1703.03717},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
mendeley-groups = {!Paper 3/task/newsgroups},
title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
url = {http://arxiv.org/abs/1703.03717},
year = {2017}
}
@article{Rubin2012,
abstract = {Machine learning approaches to multi-label document classification have to date largely relied on discriminative modeling techniques such as support vector machines. A drawback of these approaches is that performance rapidly drops off as the total number of labels and the number of labels per document increase. This problem is amplified when the label frequencies exhibit the type of highly skewed distributions that are often observed in real-world datasets. In this paper we investigate a class of generative statistical topic models for multi-label documents that associate individual word tokens with different labels. We investigate the advantages of this approach relative to discriminative models, particularly with respect to classification problems involving large numbers of relatively rare labels. We compare the performance of generative and discriminative approaches on document labeling tasks ranging from datasets with several thousand labels to datasets with tens of labels. The experimental results indicate that probabilistic generative models can achieve competitive multi-label classification performance compared to discriminative methods, and have advantages for datasets with many labels and skewed label frequencies.},
archivePrefix = {arXiv},
arxivId = {1107.2462},
author = {Rubin, Timothy N. and Chambers, America and Smyth, Padhraic and Steyvers, Mark},
doi = {10.1007/s10994-011-5272-5},
eprint = {1107.2462},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubin et al. - 2012 - Statistical topic models for multi-label document classification.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Dependency-LDA,Document modeling,Graphical models,LDA,Multi-label classification,Probabilistic generative models,Text classification,Topic models},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
number = {1-2},
pages = {157--208},
title = {{Statistical topic models for multi-label document classification}},
volume = {88},
year = {2012}
}
@article{Rudinger2017,
abstract = {We introduce the notion of a multi-vector sentence representation based on a " one vector per proposition " philosophy, which we term skip-prop vectors. By representing each predicate-argument structure in a complex sentence as an individual vector, skip-prop is (1) a response to empirical evidence that single-vector sentence representations degrade with sentence length, and (2) a repre-sentation that maintains a semantically useful level of granularity. We demonstrate the feasibility of training skip-prop vectors, introducing a method adapted from skip-thought vectors, and compare skip-prop with " one vector per sentence " and " one vector per token " approaches.},
author = {Rudinger, Rachel and Duh, Kevin and {Van Durme}, Benjamin},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudinger, Duh, Van Durme - 2017 - Skip-Prop Representing Sentences with One Vector Per Proposition.pdf:pdf},
journal = {Iwcs},
mendeley-groups = {!Paper 3/Training LSTMs},
title = {{Skip-Prop: Representing Sentences with One Vector Per Proposition}},
year = {2017}
}
@article{Ruggieri2009,
author = {Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/tkdd.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability/Discrimination},
title = {{Data Mining for Discrimination Discovery}},
volume = {V},
year = {2009}
}
@article{Saad2007b,
abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., {\&} Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373-389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e. calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity-complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Saad, Emad W. and Wunsch, Donald C.},
doi = {10.1016/j.neunet.2006.07.005},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saad, Wunsch - 2007 - Neural network explanation using inversion.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Evolutionary algorithm,Explanation capability of neural networks,Hyperplanes,Inversion,Neural network explanation,Pedagogical,Rule extraction},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
number = {1},
pages = {78--93},
pmid = {17029713},
title = {{Neural network explanation using inversion}},
volume = {20},
year = {2007}
}
@article{Saad2007a,
abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., {\&} Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373-389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e. calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity-complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Saad, Emad W. and Wunsch, Donald C.},
doi = {10.1016/j.neunet.2006.07.005},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Evolutionary algorithm,Explanation capability of neural networks,Hyperplanes,Inversion,Neural network explanation,Pedagogical,Rule extraction},
number = {1},
pages = {78--93},
pmid = {17029713},
title = {{Neural network explanation using inversion}},
volume = {20},
year = {2007}
}
@article{Saaty2003,
abstract = {In 1956, Miller [1] conjectured that there is an upper limit on our capacity to process information on simultaneously interacting elements with reliable accuracy and with validity. This limit is seven plus or minus two elements. He noted that the number 7 occurs in many aspects of life, from the seven wonders of the world to the seven seas and seven deadly sins. We demonstrate in this paper that in making preference judgments on pairs of elements in a group, as we do in the analytic hierarchy process (AHP), the number of elements in the group should be no more than seven. The reason is founded in the consistency of information derived from relations among the elements. When the number of elements increases past seven, the resulting increase in inconsistency is too small for the mind to single out the element that causes the greatest inconsistency to scrutinize and correct its relation to the other elements, and the result is confusion to the mind from the existing information. The AHP as a theory of measurement has a basic way to obtain a measure of inconsistency for any such set of pairwise judgments. When the number of elements is seven or less the inconsistency measurement is relatively large with respect to the number of elements involved; when the number is more it is relatively small. The most inconsistent judgment is easily determined in the first case and the individual providing the judgments can change it in an effort to improve the overall inconsistency. In the second case, as the inconsistency measurement is relatively small, improving inconsistency requires only small perturbations and the judge would be hard put to determine what that change should be, and how such a small change could be justified for improving the validity of the outcome. The mind is sufficiently sensitive to improve large inconsistencies but not small ones. And the implication of this is that the number of elements in a set should be limited to seven plus or minus two.},
author = {Saaty, T.L. and Ozdemir, M.S.},
doi = {10.1016/S0895-7177(03)90083-5},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saaty, Ozdemir - 2003 - Why the magic number seven plus or minus two.pdf:pdf},
isbn = {0895-7177},
issn = {08957177},
journal = {Mathematical and Computer Modelling},
mendeley-groups = {Annotated/Psychology},
number = {3},
pages = {233--244},
pmid = {8022966},
title = {{Why the magic number seven plus or minus two}},
volume = {38},
year = {2003}
}
@article{Salton1975,
author = {Salton, G and Wong, A and Yang, C S},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Wong, Yang - 1975 - AVector Space Model for Automatic Indexing.pdf:pdf},
keywords = {and phrases,automatic indexing,automatic information,content analysis,document,retrieval},
mendeley-groups = {Report/Features},
number = {11},
title = {{AVector Space Model for Automatic Indexing}},
volume = {18},
year = {1975}
}
@article{Samanta2003,
abstract = {A study is presented to compare the performance of bearing fault detection using two different classifiers, namely, artificial neural networks (ANNs) and support vector machines (SMVs). The time-domain vibration signals of a rotating machine with normal and defective bearings are processed for feature extraction. The extracted features from original and preprocessed signals are used as inputs to the classifiers for two-class (normal or fault) recognition. The classifier parameters, e.g., the number of nodes in the hidden layer in case of ANNs and the radial basis function kernel parameter (width) in case of SVMs along with the selection of input features are optimized using genetic algorithms. The classifiers are trained with a subset of the experimental data for known machine conditions and are tested using the remaining set of data. The procedure is illustrated using the experimental vibration data of a rotating machine. The roles of different vibration signals and signal preprocessing techniques are investigated. The results show the effectiveness of the features and the classifiers in detection of machine condition. ?? 2003 Elsevier Ltd. All rights reserved.},
author = {Samanta, B. and Al-Balushi, K. R. and Al-Araimi, S. A.},
doi = {10.1016/j.engappai.2003.09.006},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samanta, Al-Balushi, Al-Araimi - 2003 - Artificial neural networks and support vector machines with genetic algorithm for bearing fault.pdf:pdf},
isbn = {09521976 (ISSN)},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Bearing faults,Condition monitoring,Feature selection,Genetic algorithm,Neural network,Rotating machines,Signal processing,Support vector machines},
number = {7-8},
pages = {657--665},
title = {{Artificial neural networks and support vector machines with genetic algorithm for bearing fault detection}},
volume = {16},
year = {2003}
}
@article{Samek,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.07979v1},
author = {Samek, Wojciech},
eprint = {arXiv:1706.07979v1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek - Unknown - Methods for Interpreting and Understanding Deep Neural Networks.pdf:pdf},
keywords = {activation maximization,deep neural networks,layer-wise,relevance propagation,sensitivity analysis,taylor decomposition},
mendeley-groups = {Annotated/Overarching Interpretability},
title = {{Methods for Interpreting and Understanding Deep Neural Networks}}
}
@article{Santos2015,
abstract = {Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06580v2},
author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
eprint = {arXiv:1504.06580v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos, Xiang, Zhou - 2015 - Classifying Relations by Ranking with Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
journal = {Acl-2015},
keywords = {Bing Xiang,Bowen Zhou,Cicero Nogueira dos Santos},
mendeley-groups = {Progress Report},
number = {3},
pages = {626--634},
title = {{Classifying Relations by Ranking with Convolutional Neural Networks}},
url = {http://arxiv.org/pdf/1504.06580.pdf},
year = {2015}
}
@article{Santos2015a,
abstract = {Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06580v2},
author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
eprint = {arXiv:1504.06580v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos, Xiang, Zhou - 2015 - Classifying Relations by Ranking with Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
journal = {Acl-2015},
keywords = {Bing Xiang,Bowen Zhou,Cicero Nogueira dos Santos},
mendeley-groups = {Progress Report},
number = {3},
pages = {626--634},
title = {{Classifying Relations by Ranking with Convolutional Neural Networks}},
url = {http://arxiv.org/pdf/1504.06580.pdf},
year = {2015}
}
@article{Sarikaya2014a,
abstract = {Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.},
author = {Sarikaya, Ruhi and Hinton, Geoffrey E. and Deoras, Anoop},
doi = {10.1109/TASLP.2014.2303296},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarikaya, Hinton, Deoras - 2014 - Application of deep belief networks for natural language understanding.pdf:pdf},
isbn = {2329-9290 VO  - 22},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Call-routing,DBN,Deep learning,Deep neural nets,Natural language understanding,RBM},
number = {4},
pages = {778--784},
title = {{Application of deep belief networks for natural language understanding}},
volume = {22},
year = {2014}
}
@article{Schaul2010,
author = {Schaul, Tom and Bayer, Justin and Wierstra, Daan and Sun, Yi},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaul et al. - 2010 - PyBrain.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine {\ldots}},
keywords = {neural networks,optimization,python,reinforcement learning},
pages = {743--746},
title = {{PyBrain}},
url = {http://dl.acm.org/citation.cfm?id=1756030},
volume = {11},
year = {2010}
}
@article{Schnabel2015,
abstract = {We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations ofwords from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.},
author = {Schnabel, Tobias and Labutov, Igor and Mimno, David and Joachims, Thorsten},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schnabel et al. - 2015 - Evaluation methods for unsupervised word embeddings.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
keywords = {Distributional semantics,Evaluation techniques},
mendeley-groups = {Report/Features},
number = {September},
pages = {298--307},
pmid = {1847047},
title = {{Evaluation methods for unsupervised word embeddings}},
year = {2015}
}
@article{Schockaert2015,
author = {Schockaert, S and Derrac, J},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert, Derrac - 2015 - Commonsense Reasoning Based on Betweenness and Direction in Distributional Models.pdf:pdf},
journal = {2015 AAAI Spring Symposium Series},
mendeley-groups = {Categories/Commonsense Reasoning},
number = {April},
pages = {3--6},
title = {{Commonsense Reasoning Based on Betweenness and Direction in Distributional Models}},
url = {http://users.cs.cf.ac.uk/S.Schockaert/Publications{\_}files/AAAI-SS2015.pdf},
year = {2015}
}
@article{Schockaert,
author = {Schockaert, Steven},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert - Unknown - Lexical inference as a spatial reasoning problem.pdf:pdf},
title = {{Lexical inference as a spatial reasoning problem}}
}
@article{Schockaert2015a,
author = {Schockaert, Steven and Lee, Jae Hee},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert, Lee - 2015 - Qualitative reasoning about directions in semantic spaces.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {3207--3213},
title = {{Qualitative reasoning about directions in semantic spaces}},
volume = {2015-Janua},
year = {2015}
}
@article{Schuhmacher2015,
author = {Schuhmacher, Michael and Dietz, Laura and Ponzetto, Simone Paolo},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schuhmacher, Dietz, Ponzetto - 2015 - Ranking Entities for Web Queries Through Text and Knowledge.pdf:pdf},
isbn = {9781450337946},
keywords = {entities,information retrieval,knowledge bases},
mendeley-groups = {Report/Rankings},
pages = {1461--1470},
title = {{Ranking Entities for Web Queries Through Text and Knowledge}},
year = {2015}
}
@article{Sculley,
author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Dennison, Dan},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/5656-hidden-technical-debt-in-machine-learning-systems.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability/Technical Debt},
pages = {1--9},
title = {{Hidden Technical Debt in Machine Learning Systems}}
}
@article{Seera2014a,
abstract = {In this paper, a hybrid intelligent system that consists of the Fuzzy Min-Max neural network, the Classification and Regression Tree, and the Random Forest model is proposed, and its efficacy as a decision support tool for medical data classification is examined. The hybrid intelligent system aims to exploit the advantages of the constituent models and, at the same time, alleviate their limitations. It is able to learn incrementally from data samples (owing to Fuzzy Min-Max neural network), explain its predicted outputs (owing to the Classification and Regression Tree), and achieve high classification performances (owing to Random Forest). To evaluate the effectiveness of the hybrid intelligent system, three benchmark medical data sets, viz., Breast Cancer Wisconsin, Pima Indians Diabetes, and Liver Disorders from the UCI Repository of Machine Learning, are used for evaluation. A number of useful performance metrics in medical applications which include accuracy, sensitivity, specificity, as well as the area under the Receiver Operating Characteristic curve are computed. The results are analyzed and compared with those from other methods published in the literature. The experimental outcomes positively demonstrate that the hybrid intelligent system is effective in undertaking medical data classification tasks. More importantly, the hybrid intelligent system not only is able to produce good results but also to elucidate its knowledge base with a decision tree. As a result, domain users (i.e., medical practitioners) are able to comprehend the prediction given by the hybrid intelligent system; hence accepting its role as a useful medical decision support tool. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Seera, Manjeevan and Lim, Chee Peng},
doi = {10.1016/j.eswa.2013.09.022},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seera, Lim - 2014 - A hybrid intelligent system for medical data classification.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification and regression tree,Fuzzy Min-Max neural network,Hybrid intelligent systems,Medical decision support,Random forest},
mendeley-groups = {Annotated/Decision Trees},
number = {5},
pages = {2239--2249},
publisher = {Elsevier Ltd},
title = {{A hybrid intelligent system for medical data classification}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.09.022},
volume = {41},
year = {2014}
}
@article{Sellam2018,
author = {Sellam, Thibault and Lin, Kevin and Huang, Ian Yiran and Vondrick, Carl and Research, Google and Wu, Eugene},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sellam et al. - 2018 - {\&}quot I Like the Way You Think! {\&}quot Inspecting the Internal Logic of Recurrent Neural Networks.pdf:pdf},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
pages = {1--3},
title = {{" I Like the Way You Think! " Inspecting the Internal Logic of Recurrent Neural Networks}},
url = {http://sellam.me/assets/papers/sellam-sysML.pdf},
year = {2018}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-scikit-learned Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
mendeley-groups = {Report/Explaining predictions,Annotated/Explanations},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Semeniuta2016,
abstract = {This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to $\backslash$textit{\{}forward{\}} connections of feed-forward architectures or RNNs, we propose to drop neurons directly in $\backslash$textit{\{}recurrent{\}} connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for Long Short-Term Memory network, the most popular type of RNN cells. Our experiments on NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.},
archivePrefix = {arXiv},
arxivId = {1603.05118},
author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
eprint = {1603.05118},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Semeniuta, Severyn, Barth - 2016 - Recurrent Dropout without Memory Loss.pdf:pdf},
mendeley-groups = {!Paper 3/Training LSTMs},
title = {{Recurrent Dropout without Memory Loss}},
url = {http://arxiv.org/abs/1603.05118},
year = {2016}
}
@article{Senin2013,
author = {Senin, Pavel and Malinchik, Sergey},
doi = {10.1109/ICDM.2013.52},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Senin, Malinchik - 2013 - SAX-VSM Interpretable time series classification using sax and vector space model.pdf:pdf},
isbn = {978-0-7695-5108-1},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {classification algorithms,time series analysis},
mendeley-groups = {Report},
number = {0704},
pages = {1175--1180},
title = {{SAX-VSM: Interpretable time series classification using sax and vector space model}},
volume = {298},
year = {2013}
}
@article{Setiono2000,
abstract = {An effective algorithm for extracting M-of-N rules from trained feedforward neural networks is proposed. Two components of the algorithm distinguish our method from previously proposed algorithms which extract symbolic rules from neural networks. First, we train a network where each input of the data can only have one of the two possible values, -1 or one. Second, we apply the hyperbolic tangent function to each connection from the input layer to the hidden layer of the network. By applying this squashing function, the activation values at the hidden units are effectively computed as the hyperbolic tangent (or the sigmoid) of the scikit-learned inputs, where the scikit-learns have magnitudes that are equal one. By restricting the inputs and the scikit-learns to binary values either -1 or one, the extraction of M-of-N rules from the networks becomes trivial. We demonstrate the effectiveness of the proposed algorithm on several widely tested datasets. For datasets consisting of thousands of patterns with many attributes, the rules extracted by the algorithm are surprisingly simple and accurate.},
author = {Setiono, Rudy},
doi = {10.1109/72.839020},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono - 2000 - Extracting M-of-N rules from trained neural networks.pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {512--519},
pmid = {18249780},
title = {{Extracting M-of-N rules from trained neural networks}},
volume = {11},
year = {2000}
}
@article{Setionoa,
author = {Setiono, Rudy},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono - Unknown - by pruning and hidden-unit splitting 1 Introduction.pdf:pdf},
number = {1},
pages = {1--34},
title = {{by pruning and hidden-unit splitting 1 Introduction}},
volume = {9}
}
@article{Setiono2008a,
abstract = {In this paper, we present a recursive algorithm for extracting classification rules from feedforward neural networks (NNs) that have been trained on data sets having both discrete and continuous attributes. The novelty of this algorithm lies in the conditions of the extracted rules: the rule conditions involving discrete attributes are disjoint from those involving continuous attributes. The algorithm starts by first generating rules with discrete attributes only to explain the classification process of the NN. If the accuracy of a rule with only discrete attributes is not satisfactory, the algorithm refines this rule by recursively generating more rules with discrete attributes not already present in the rule condition, or by generating a hyperplane involving only the continuous attributes. We show that for three real-life credit scoring data sets, the algorithm generates rules that are not only more accurate but also more comprehensible than those generated by other NN rule extraction methods.},
author = {Setiono, Rudy and Baesens, Bart and Mues, Christophe},
doi = {10.1109/TNN.2007.908641},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Baesens, Mues - 2008 - Recursive neural network rule extraction for data with mixed attributes.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Continuous attributes,Credit scoring,Discrete attributes,Rule extraction},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
number = {2},
pages = {299--307},
pmid = {18269960},
title = {{Recursive neural network rule extraction for data with mixed attributes}},
volume = {19},
year = {2008}
}
@article{Setiono2009,
abstract = {We address an important issue in knowledge discovery using neural networks that has been left out in a recent article "Knowledge discovery using a neural network simultaneous optimization algorithm on a real world classification problem" by Sexton et al. [R.S. Sexton, S. McMurtrey, D.J. Cleavenger, Knowledge discovery using a neural network simultaneous optimization algorithm on a real world classification problem, European Journal of Operational Research 168 (2006) 1009-1018]. This important issue is the generation of comprehensible rule sets from trained neural networks. In this note, we present our neural network rule extraction algorithm that is very effective in discovering knowledge embedded in a neural network. This algorithm is particularly appropriate in applications where comprehensibility as well as accuracy are required. For the same data sets used by Sexton et al. our algorithm produces accurate rule sets that are concise and comprehensible, and hence helps validate the claim that neural networks could be viable alternatives to other data mining tools for knowledge discovery. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Setiono, Rudy and Baesens, Bart and Mues, Christophe},
doi = {10.1016/j.ejor.2007.09.022},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Baesens, Mues - 2009 - A note on knowledge discovery using neural networks and its application to credit card screening.pdf:pdf},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Credit screening,Knowledge discovery,Neural networks,Rule extraction},
mendeley-groups = {Papers/Paper 1,Progress Report,Report},
number = {1},
pages = {326--332},
title = {{A note on knowledge discovery using neural networks and its application to credit card screening}},
volume = {192},
year = {2009}
}
@article{Setiono1997b,
abstract = {We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.},
author = {Setiono, Rudy and Liu, Huan},
doi = {10.1016/S0925-2312(97)00038-6},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Liu - 1997 - Neurolinear From neural networks to oblique decision rules(2).pdf:pdf},
isbn = {3-540-62858-4},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Discretization,Oblique-rule,Pruning,Rule extraction},
number = {1},
pages = {1--24},
title = {{Neurolinear: From neural networks to oblique decision rules}},
volume = {17},
year = {1997}
}
@article{Setiono1997c,
abstract = {We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.},
author = {Setiono, Rudy and Liu, Huan},
doi = {10.1016/S0925-2312(97)00038-6},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Liu - 1997 - Neurolinear From neural networks to oblique decision rules.pdf:pdf},
isbn = {3-540-62858-4},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Discretization,Oblique-rule,Pruning,Rule extraction},
mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
number = {1},
pages = {1--24},
title = {{Neurolinear: From neural networks to oblique decision rules}},
volume = {17},
year = {1997}
}
@article{Severyn2015,
abstract = {This paper describes our deep learning system for sentiment anal-ysis of tweets. The main contribution of this work is a new model for initializing the parameter scikit-learns of the convolutional neural network, which is crucial to train an accurate model while avoid-ing the need to inject any additional features. Briefly, we use an unsupervised neural language model to train initial word embed-dings that are further tuned by our deep learning model on a distant supervised corpus. At a final stage, the pre-trained parameters of the network are used to initialize the model. We train the latter on the supervised training data recently made available by the official system evaluation campaign on Twitter Sentiment Analysis orga-nized by Semeval-2015. A comparison between the results of our approach and the systems participating in the challenge on the of-ficial test sets, suggests that our model could be ranked in the first two positions in both the phrase-level subtask A (among 11 teams) and on the message-level subtask B (among 40 teams). This is an important evidence on the practical value of our solution.},
author = {Severyn, Aliaksei and Moschitti, Alessandro},
doi = {10.1145/2766462.2767830},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Severyn, Moschitti - 2015 - Twitter Sentiment Analysis with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781450336215},
journal = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '15},
keywords = {convolutional neural networks,twitter sentiment analysis},
mendeley-groups = {Progress Report},
pages = {959--962},
title = {{Twitter Sentiment Analysis with Deep Convolutional Neural Networks}},
url = {http://dl.acm.org/citation.cfm?doid=2766462.2767830},
year = {2015}
}
@article{Shi2017,
abstract = {Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such "two-step" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.},
archivePrefix = {arXiv},
arxivId = {1706.07276},
author = {Shi, Bei and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Lai, Kwun Ping},
doi = {10.1145/3077136.3080806},
eprint = {1706.07276},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2017 - Jointly Learning Word Embeddings and Latent Topics.pdf:pdf},
isbn = {9781450350228},
keywords = {china,document modeling,e work described in,grant council of the,hong kong special administrative,project code,region,supported by grants from,the research,this paper is substantially,topic model,word embedding},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
title = {{Jointly Learning Word Embeddings and Latent Topics}},
url = {http://arxiv.org/abs/1706.07276{\%}0Ahttp://dx.doi.org/10.1145/3077136.3080806},
year = {2017}
}
@article{Shrikumar2017,
abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
archivePrefix = {arXiv},
arxivId = {1704.02685},
author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
eprint = {1704.02685},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf},
mendeley-groups = {Report},
title = {{Learning Important Features Through Propagating Activation Differences}},
url = {http://arxiv.org/abs/1704.02685},
year = {2017}
}
@article{Shrikumar2017,
abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
archivePrefix = {arXiv},
arxivId = {1704.02685},
author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
eprint = {1704.02685},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences(2).pdf:pdf},
mendeley-groups = {Report/Explaining predictions},
title = {{Learning Important Features Through Propagating Activation Differences}},
url = {http://arxiv.org/abs/1704.02685},
volume = {1},
year = {2017}
}
@article{Siddharth2016,
abstract = {We develop a framework for incorporating structured graphical models in the $\backslash$emph{\{}encoders{\}} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes.},
archivePrefix = {arXiv},
arxivId = {1611.07492},
author = {Siddharth, N. and Paige, Brooks and Desmaison, Alban and {Van de Meent}, Jan-Willem and Wood, Frank and Goodman, Noah D. and Kohli, Pushmeet and Torr, Philip H. S.},
eprint = {1611.07492},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siddharth et al. - 2016 - Inducing Interpretable Representations with Variational Autoencoders.pdf:pdf},
journal = {arXiv preprint},
mendeley-groups = {Annotated/Generative Adversarial Nets},
number = {Nips 2016},
title = {{Inducing Interpretable Representations with Variational Autoencoders}},
url = {http://arxiv.org/abs/1611.07492},
year = {2016}
}
@article{Simonyan2013,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
mendeley-groups = {Report/Explaining predictions},
pages = {1--8},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034},
year = {2013}
}
@article{Skirpan2017,
abstract = {In this paper, we argue for the adoption of a normative definition of fairness within the machine learning community. After characterizing this definition, we review the current literature of Fair ML in light of its implications. We end by suggesting ways to incorporate a broader community and generate further debate around how to decide what is fair in ML.},
archivePrefix = {arXiv},
arxivId = {1706.09976},
author = {Skirpan, Michael and Gorelick, Micha},
eprint = {1706.09976},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skirpan, Gorelick - 2017 - The Authority of Fair in Machine Learning.pdf:pdf},
mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
title = {{The Authority of "Fair" in Machine Learning}},
url = {http://arxiv.org/abs/1706.09976},
year = {2017}
}
@article{Smilkov2016,
abstract = {Embeddings are ubiquitous in machine learning, appearing in recommender systems, NLP, and many other applications. Researchers and developers often need to explore the properties of a specific embedding, and one way to analyze embeddings is to visualize them. We present the Embedding Projector, a tool for interactive visualization and interpretation of embeddings.},
archivePrefix = {arXiv},
arxivId = {1611.05469},
author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Vi{\'{e}}gas, Fernanda B. and Wattenberg, Martin},
eprint = {1611.05469},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smilkov et al. - 2016 - Embedding Projector Interactive Visualization and Interpretation of Embeddings.pdf:pdf},
mendeley-groups = {Report},
number = {Nips},
title = {{Embedding Projector: Interactive Visualization and Interpretation of Embeddings}},
url = {http://arxiv.org/abs/1611.05469},
year = {2016}
}
@article{Socher2013a,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the {\ldots}},
pages = {1631--1642},
title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
year = {2013}
}
@article{Socher2013,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
doi = {10.1371/journal.pone.0073791},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank(2).pdf:pdf},
isbn = {9781937284978},
issn = {1932-6203},
journal = {Proceedings of the {\ldots}},
mendeley-groups = {!Paper 3/task/Sentiment treebank},
number = {October},
pages = {1631--1642},
pmid = {24086296},
title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
year = {2013}
}
@article{Soneson2014,
abstract = {BACKGROUND: With the large amount of biological data that is currently publicly available, many investigators combine multiple data sets to increase the sample size and potentially also the power of their analyses. However, technical differences ("batch effects") as well as differences in sample composition between the data sets may significantly affect the ability to draw generalizable conclusions from such studies.$\backslash$n$\backslash$nFOCUS: The current study focuses on the construction of classifiers, and the use of cross-validation to estimate their performance. In particular, we investigate the impact of batch effects and differences in sample composition between batches on the accuracy of the classification performance estimate obtained via cross-validation. The focus on estimation bias is a main difference compared to previous studies, which have mostly focused on the predictive performance and how it relates to the presence of batch effects.$\backslash$n$\backslash$nDATA: We work on simulated data sets. To have realistic intensity distributions, we use real gene expression data as the basis for our simulation. Random samples from this expression matrix are selected and assigned to group 1 (e.g., 'control') or group 2 (e.g., 'treated'). We introduce batch effects and select some features to be differentially expressed between the two groups. We consider several scenarios for our study, most importantly different levels of confounding between groups and batch effects.$\backslash$n$\backslash$nMETHODS: We focus on well-known classifiers: logistic regression, Support Vector Machines (SVM), k-nearest neighbors (kNN) and Random Forests (RF). Feature selection is performed with the Wilcoxon test or the lasso. Parameter tuning and feature selection, as well as the estimation of the prediction performance of each classifier, is performed within a nested cross-validation scheme. The estimated classification performance is then compared to what is obtained when applying the classifier to independent data.},
author = {Soneson, Charlotte and Gerster, Sarah and Delorenzi, Mauro},
doi = {10.1371/journal.pone.0100335},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Soneson, Gerster, Delorenzi - 2014 - Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {Annotated/Artifacts in the data},
number = {6},
pmid = {24967636},
title = {{Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation}},
volume = {9},
year = {2014}
}
@article{Sourek2015,
abstract = {We propose a method combining relational-logic representations with deep neural network learning. Domain-specific knowledge is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structure of given training or testing examples. Different networks corresponding to different examples share their scikit-learns, which co-evolve during training by stochastic gradient descend algorithm. Notable relational concepts can be discovered by interpreting shared hidden layer scikit-learns corresponding to the rules. Experiments on 78 relational learning benchmarks demonstrate the favorable performance of the method.},
archivePrefix = {arXiv},
arxivId = {1508.05128},
author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
eprint = {1508.05128},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sourek et al. - 2015 - Lifted Relational Neural Networks.pdf:pdf},
journal = {CoRR},
keywords = {lifted models,neural networks,relational learning},
mendeley-groups = {Progress Report},
pages = {1--21},
title = {{Lifted Relational Neural Networks}},
url = {http://arxiv.org/abs/1508.05128},
volume = {abs/1508.0},
year = {2015}
}
@article{Sourek2015a,
abstract = {We propose a method combining relational-logic representations with deep neural network learning. Domain-specific knowledge is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structure of given training or testing examples. Different networks corresponding to different examples share their scikit-learns, which co-evolve during training by stochastic gradient descend algorithm. Notable relational concepts can be discovered by interpreting shared hidden layer scikit-learns corresponding to the rules. Experiments on 78 relational learning benchmarks demonstrate the favorable performance of the method.},
archivePrefix = {arXiv},
arxivId = {1508.05128},
author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
eprint = {1508.05128},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sourek et al. - 2015 - Lifted Relational Neural Networks.pdf:pdf},
journal = {CoRR},
keywords = {lifted models,neural networks,relational learning},
pages = {1--21},
title = {{Lifted Relational Neural Networks}},
url = {http://arxiv.org/abs/1508.05128},
volume = {abs/1508.0},
year = {2015}
}
@article{Speer,
abstract = {Machine learning about language can be improved by sup-plying it with specific knowledge and sources of external in-formation. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embed-dings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowl-edge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a pur-pose. It is designed to represent the general knowledge in-volved in understanding language, improving natural lan-guage applications by allowing the application to better un-derstand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improve-ments on applications of word vectors, including solving SAT-style analogies.},
author = {Speer, Robert and Chin, Joshua and Havasi, Catherine},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Speer, Chin, Havasi - Unknown - ConceptNet 5.5 An Open Multilingual Graph of General Knowledge.pdf:pdf},
title = {{ConceptNet 5.5: An Open Multilingual Graph of General Knowledge}}
}
@article{Srivastava2014a,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller scikit-learns. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@misc{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller scikit-learns. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research (JMLR)},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Srivastava2013a,
abstract = {We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.},
archivePrefix = {arXiv},
arxivId = {abs/1309.6865},
author = {Srivastava, Nitish and Salakhutdinov, Rr and Hinton, Ge},
eprint = {1309.6865},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava, Salakhutdinov, Hinton - 2013 - Modeling Documents with Deep Boltzmann Machines.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
mendeley-groups = {!Paper 3/task/newsgroups},
primaryClass = {abs},
title = {{Modeling Documents with Deep Boltzmann Machines}},
url = {http://arxiv.org/abs/1309.6865{\%}5Cnhttp://www.arxiv.org/pdf/1309.6865.pdf},
year = {2013}
}
@article{Strobelt2018,
abstract = {Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.},
archivePrefix = {arXiv},
arxivId = {1606.07461},
author = {Strobelt, Hendrik and Gehrmann, Sebastian and Pfister, Hanspeter and Rush, Alexander M.},
doi = {10.1109/TVCG.2017.2744158},
eprint = {1606.07461},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobelt et al. - 2018 - LSTMVis A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks.pdf:pdf},
isbn = {1077-2626 VO - PP},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {LSTM,Machine Learning,Recurrent Neural Networks,Visualization},
mendeley-groups = {!Paper 3,!Paper 3/Understanding LSTMs},
number = {1},
pages = {667--676},
pmid = {28866526},
title = {{LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks}},
volume = {24},
year = {2018}
}
@article{Stubbs2007,
abstract = {The use of robots, especially autonomous mobile robots, to support work is expected to increase over the next few decades. However, little empirical research examines how users form mental models of robots, how they collaborate with them, and what factors contribute to the success or failure of human-robot collaboration. A two-year observational study of a collaborative human-robot system suggests that the factors disrupting the creation of common ground for interactive communication change at different levels of robot autonomy. Our observations of users collaborating with the remote robot showed differences in how the users reached common ground with the robot in terms of an accurate, shared understanding of the robot's context, planning, and actions - a process called grounding. We focus on how the types and levels of robot autonomy affect grounding. We also examine the challenges a highly autonomous system presents to people's ability to maintain a shared mental model of the robot},
author = {Stubbs, Kristen and Wettergreen, David and Hinds, Pamela J.},
doi = {10.1109/MIS.2007.21},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stubbs, Wettergreen, Hinds - 2007 - Autonomy and common ground in human-robot interaction A field study.pdf:pdf},
isbn = {1541-1672 VO  - 22},
issn = {15411672},
journal = {IEEE Intelligent Systems},
mendeley-groups = {Annotated/Overarching Interpretability},
number = {2},
pages = {42--50},
title = {{Autonomy and common ground in human-robot interaction: A field study}},
volume = {22},
year = {2007}
}
@article{Sun2016,
abstract = {Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-the-art document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.},
archivePrefix = {arXiv},
arxivId = {1603.07603},
author = {Sun, Fei and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Cheng, Xueqi},
eprint = {1603.07603},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2016 - Semantic Regularities in Document Representations.pdf:pdf},
mendeley-groups = {Annotated/Interpretable representations},
title = {{Semantic Regularities in Document Representations}},
url = {http://arxiv.org/abs/1603.07603},
year = {2016}
}
@article{Sun2006,
author = {Sun, Hongmao},
doi = {10.1002/cmdc.200500047},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun - 2006 - An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability.pdf:pdf},
mendeley-groups = {Annotated/Applications/Scientific Discovery},
pages = {315--322},
title = {{An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability}},
volume = {07110},
year = {2006}
}
@article{Sun1998,
abstract = {This paper presents a novel learning model CLARION, which is a hybrid model based on the two-level approach proposed by Sun. The model integrates neural, reinforcement, and symbolic learning methods to perform on-line, bottom-up learning (i.e., learning that goes from neural to symbolic representations). The model utilizes both procedural and declarative knowledge (in neural and symbolic representations, respectively), tapping into the synergy of the two types of processes. It was applied to deal with sequential decision tasks. Experiments and analyzes in various ways are reported that shed light on the advantages of the model.},
author = {Sun, Ron and Peterson, Todd},
doi = {10.1109/72.728364},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Peterson - 1998 - Autonomous learning of sequential tasks Experiments and analyzes.pdf:pdf},
isbn = {0364-0213},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Hybrid models,Markov decision process,Multistrategy learning,Navigation,Reinforcement learning,Rule extraction,Sequential decision making},
number = {6},
pages = {1217--1234},
pmid = {18255804},
title = {{Autonomous learning of sequential tasks: Experiments and analyzes}},
volume = {9},
year = {1998}
}
@article{Sundararajan2017,
abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
archivePrefix = {arXiv},
arxivId = {1703.01365},
author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
eprint = {1703.01365},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundararajan, Taly, Yan - 2017 - Axiomatic Attribution for Deep Networks.pdf:pdf},
issn = {1938-7228},
mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3},
title = {{Axiomatic Attribution for Deep Networks}},
url = {http://arxiv.org/abs/1703.01365},
year = {2017}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc},
eprint = {1409.3215},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}
@article{Systems,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.01256v2},
author = {Systems, Cyber-physical and Products, Data},
eprint = {arXiv:1610.01256v2},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1610.01256.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability/Safety},
pages = {1--20},
title = {{On the Safety of Machine Learning : Cyber-Physical Systems , Decision Sciences , and Data Products}}
}
@article{Szafron2004,
abstract = {Proteome Analyst (PA) (http://www.cs.ualberta.ca/{\~{}}bioinfo/PA/) is a publicly available, high-throughput, web-based system for predicting various properties of each protein in an entire proteome. Using machine-learned classifiers, PA can predict, for example, the GeneQuiz general function and Gene Ontology (GO) molecular function of a protein. In addition, PA is currently the most accurate and most comprehensive system for predicting subcellular localization, the location within a cell where a protein performs its main function. Two other capabilities of PA are notable. First, PA can create a custom classifier to predict a new property, without requiring any programming, based on labeled training data (i.e. a set of examples, each with the correct classification label) provided by a user. PA has been used to create custom classifiers for potassium-ion channel proteins and other general function ontologies. Second, PA provides a sophisticated explanation feature that shows why one prediction is chosen over another. The PA system produces a Na{\"{i}}ve Bayes classifier, which is amenable to a graphical and interactive approach to explanations for its predictions; transparent predictions increase the user's confidence in, and understanding of, PA.},
author = {Szafron, Duane and Lu, Paul and Greiner, Russell and Wishart, David S. and Poulin, Brett and Eisner, Roman and Lu, Zhiyong and Anvik, John and Macdonell, Cam and Fyshe, Alona and Meeuwis, David},
doi = {10.1093/nar/gkh485},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szafron et al. - 2004 - Proteome Analyst Custom predictions with explanations in a web-based tool for high-throughput proteome annotatio.pdf:pdf},
isbn = {1362-4962 (Electronic)},
issn = {03051048},
journal = {Nucleic Acids Research},
mendeley-groups = {Report/Biologicla domain},
number = {WEB SERVER ISS.},
pages = {365--371},
pmid = {15215412},
title = {{Proteome Analyst: Custom predictions with explanations in a web-based tool for high-throughput proteome annotations}},
volume = {32},
year = {2004}
}
@article{Taddy2015,
abstract = {There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.},
archivePrefix = {arXiv},
arxivId = {1504.07295},
author = {Taddy, Matt},
eprint = {1504.07295},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taddy - 2015 - Document Classification by Inversion of Distributed Language Representations.pdf:pdf},
isbn = {9781941643730},
journal = {Proceedings of the 53rd meeting of the Association for Computational Linquistics (ACL'15)},
mendeley-groups = {Annotated/Word Vectors},
pages = {45--49},
title = {{Document Classification by Inversion of Distributed Language Representations}},
url = {http://arxiv.org/abs/1504.07295},
year = {2015}
}
@article{Tai2015,
abstract = {Because of their superior ability to pre-serve sequence information over time, Long Short-Term Memory (LSTM) net-works, a type of recurrent neural net-work with a more complex computational unit, have obtained strong results on a va-riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn-tactic properties that would naturally com-bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti-ment Treebank).},
archivePrefix = {arXiv},
arxivId = {1503.0075},
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
doi = {10.1515/popets-2015-0023},
eprint = {1503.0075},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tai, Socher, Manning - 2015 - Improved semantic representations from tree-structured long short-term memory networks.pdf:pdf},
isbn = {9781941643723},
issn = {9781941643723},
journal = {Proceedings of ACL},
pages = {1556--1566},
pmid = {18267787},
title = {{Improved semantic representations from tree-structured long short-term memory networks}},
year = {2015}
}
@article{Tai2015a,
abstract = {Because of their superior ability to pre-serve sequence information over time, Long Short-Term Memory (LSTM) net-works, a type of recurrent neural net-work with a more complex computational unit, have obtained strong results on a va-riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn-tactic properties that would naturally com-bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti-ment Treebank).},
archivePrefix = {arXiv},
arxivId = {1503.0075},
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
doi = {10.1515/popets-2015-0023},
eprint = {1503.0075},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tai, Socher, Manning - 2015 - Improved semantic representations from tree-structured long short-term memory networks.pdf:pdf},
isbn = {9781941643723},
issn = {9781941643723},
journal = {Proceedings of ACL},
mendeley-groups = {Progress Report,!Paper 3/Interpretable LSTMs,!Paper 3/task/Sentiment treebank},
pages = {1556--1566},
pmid = {18267787},
title = {{Improved semantic representations from tree-structured long short-term memory networks}},
year = {2015}
}
@article{Tallec2017,
abstract = {Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Rescikit-learned Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT.},
archivePrefix = {arXiv},
arxivId = {1705.08209},
author = {Tallec, Corentin and Ollivier, Yann},
eprint = {1705.08209},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tallec, Ollivier - 2017 - Unbiasing Truncated Backpropagation Through Time.pdf:pdf},
mendeley-groups = {!Paper 3/Training LSTMs},
pages = {1--13},
title = {{Unbiasing Truncated Backpropagation Through Time}},
url = {http://arxiv.org/abs/1705.08209},
year = {2017}
}
@article{Talley2011,
author = {Talley, Edmund M and Newman, David and Mimno, David and Herr, Bruce W and Wallach, Hanna M and Burns, Gully A P C and Leenders, A G Miriam and Mccallum, Andrew},
doi = {10.1038/nmeth.1619},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Talley et al. - 2011 - correspondEnce Database of NIH grants using machine-learned categories and graphical clustering Predicting protei.pdf:pdf},
isbn = {2712009002},
issn = {1548-7091},
journal = {Nature Publishing Group},
mendeley-groups = {Report/Clustering},
number = {6},
pages = {443--444},
publisher = {Nature Publishing Group},
title = {{correspondEnce Database of NIH grants using machine-learned categories and graphical clustering Predicting protein associations with long noncoding RNAs}},
url = {http://dx.doi.org/10.1038/nmeth.1619},
volume = {8},
year = {2011}
}
@article{Tang2015,
abstract = {Document level sentiment classification remains a challenge: encoding the intrin- sic relations between sentences in the se- mantic meaning of a document. To ad- dress this, we introduce a neural network model to learn vector-based document rep- resentation in a unified, bottom-up fash- ion. The model first learns sentence rep- resentation with convolutional neural net- work or long short-term memory. After- wards, semantics of sentences and their relations are adaptively encoded in docu- ment representation with gated recurren- t neural network. We conduct documen- t level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimen- tal results show that: (1) our neural mod- el shows superior performances over sev- eral state-of-the-art algorithms; (2) gat- ed recurrent neural network dramatically outperforms standard recurrent neural net- work in document modeling for sentiment classification},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Tang, Duyu and Qin, Bing and Liu, Ting},
doi = {10.18653/v1/D15-1167},
eprint = {1508.04025},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:pdf},
isbn = {9781941643327},
issn = {10495258},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {Annotated/Document representation,!Paper 3/task,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank},
number = {September},
pages = {1422--1432},
title = {{Document Modeling with Gated Recurrent Neural Network for Sentiment Classification}},
url = {http://aclweb.org/anthology/D15-1167},
year = {2015}
}
@article{Tang2014,
abstract = {We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
doi = {10.3115/1220575.1220648},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2014 - Learning Sentiment-Specific Word Embedding.pdf:pdf},
isbn = {9781937284725},
issn = {03029743},
journal = {Acl},
mendeley-groups = {Progress Report},
pages = {1555--1565},
pmid = {18487783},
title = {{Learning Sentiment-Specific Word Embedding}},
year = {2014}
}
@article{Than,
author = {Than, Khoat and Ho, Tu Bao},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Than, Ho - Unknown - Fully Sparse Topic ModelsPKDD 2012.pdf.pdf:pdf},
mendeley-groups = {Annotated/Interpretable representations,Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
pages = {1--16},
title = {{Fully Sparse Topic Models[PKDD 2012].pdf}}
}
@article{Thiagarajan2016,
abstract = {With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior [1]. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted.},
archivePrefix = {arXiv},
arxivId = {1611.07429},
author = {Thiagarajan, Jayaraman J. and Kailkhura, Bhavya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan},
eprint = {1611.07429},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiagarajan et al. - 2016 - TreeView Peeking into Deep Neural Networks Via Feature-Space Partitioning.pdf:pdf},
journal = {30th Conference on Neural Information Processing Systems (NIPS 2016)},
mendeley-groups = {Annotated/Explanations,Annotated/Decision Trees},
number = {Nips},
title = {{TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning}},
url = {http://arxiv.org/abs/1611.07429},
year = {2016}
}
@article{Vincent2010,
abstract = {This paper presents the findings from a small-scale experiment investigating the presentation of a synchronous remote electronic examination. It discusses the students' experiences of taking such an examination. The study confirms that the majority of participants found the experience at least as good as a conventional written examination. In addition, typing answers does not prevent students from producing answers in the time available. However, the pressure of time continues to be a major cause of anxiety for students. The paper discusses technical issues, particularly those related to the loss of communications during the 3-hour duration of the exam. Although software processes were available to save and restore students' answers throughout the examination, problems still occurred and more robust software is required.},
author = {Thomas, Pete and Price, Blaine and Paine, Carina and Richards, Michael},
doi = {10.1111/1467-8535.00290},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas et al. - 2002 - Remote electronic examinations student experiences.pdf:pdf},
isbn = {0007-1013},
issn = {0007-1013},
journal = {Journal of Machine Learning Research},
number = {3},
pages = {3371--3408},
title = {{Remote electronic examinations: student experiences}},
url = {http://oro.open.ac.uk/2572/},
volume = {11},
year = {2002}
}
@article{Thomas2018,
author = {Thomas, Student},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas - 2018 - Interim Report Student Details October 2015 October 2018 Section A Student Self-Assessment A . 1 Thesis Title and Hypot.pdf:pdf},
title = {{Interim Report Student Details October 2015 October 2018 Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis Fine-tuning vector spaces by improving directions A . 2 Overall Progress}},
year = {2018}
}
@article{Thrun1995,
abstract = {Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neural networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illustrate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.},
author = {Thrun, S.},
doi = {10.1007/BFb0100473},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun - 1995 - Extracting rules from artificial neural networks with distributed representations.pdf:pdf},
isbn = {1049-5258},
issn = {16113349},
journal = {Advances in Neural Information Processing Systems},
pages = {505--512},
title = {{Extracting rules from artificial neural networks with distributed representations}},
year = {1995}
}
@article{Thrun1995,
abstract = {Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neu-ral networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illus-trate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.},
author = {Thrun, Sebastian},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun - 1995 - Extracting Rules from Artificial Neural Networks with Distributed Representations(2).pdf:pdf},
isbn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 7},
mendeley-groups = {Progress Report},
title = {{Extracting Rules from Artificial Neural Networks with Distributed Representations}},
year = {1995}
}
@article{Tian2011,
abstract = {We use the term "index predictor" to denote a score that consists of K binary rules such as "age {\textgreater} 60" or "blood pressure {\textgreater} 120 mm Hg." The index predictor is the sum of these binary scores, yielding a value from 0 to K. Such indices as often used in clinical studies to stratify population risk: They are usually derived from subject area considerations. In this paper, we propose a fast data-driven procedure for automatically constructing such indices for linear, logistic, and Cox regression models. We also extend the procedure to create indices for detecting treatment-marker interactions. The methods are illustrated on a study with protein biomarkers as well as a large microarray gene expression study.},
author = {Tian, Lu and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxq047},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Tibshirani - 2011 - Adaptive index models for marker-based risk stratification.pdf:pdf},
isbn = {1468-4357 (Electronic)$\backslash$r1465-4644 (Linking)},
issn = {14654644},
journal = {Biostatistics},
keywords = {Degree of freedom,Index predictor,International prognostic index},
mendeley-groups = {Annotated/Applications/Medical},
number = {1},
pages = {68--86},
pmid = {20663850},
title = {{Adaptive index models for marker-based risk stratification}},
volume = {12},
year = {2011}
}
@article{Ticklea,
author = {Tickle, Alan B and Andrews, Robert and Golea, Mostefa and Diederich, Joachim},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tickle et al. - Unknown - The truth is in there directions and challenges in extracting rules from trained artificial neural networks.pdf:pdf},
journal = {Neural Networks},
mendeley-groups = {Progress Report},
title = {{The truth is in there : directions and challenges in extracting rules from trained artificial neural networks}}
}
@article{Tickle,
author = {Tickle, Alan B and Diederich, Joachim},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tickle, Diederich - Unknown - From Trained Artificial Neural Networks.pdf:pdf},
mendeley-groups = {Progress Report},
title = {{From Trained Artificial Neural Networks}}
}
@article{TomasMikolovWen-tauYih2013,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer scikit-learns. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {{Tomas Mikolov , Wen-tau Yih}, Geoffrey Zweig},
doi = {10.3109/10826089109058901},
eprint = {1301.3781},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/rvecs.pdf:pdf},
isbn = {9781937284473},
issn = {9781937284473},
journal = {Hlt-Naacl},
mendeley-groups = {11Thesis/Word Vectors},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic Regularities in Continuous Space Word Representations}},
url = {http://anthology.aclweb.org/N/N13/N13-1.pdf{\#}page=655},
year = {2013}
}
@article{Towell1993,
author = {Towell, G and Shavlik, J W},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik - 1993 - The Extraction of Refined Rules From Knowledge Based Neural Networks.pdf:pdf},
journal = {Machine Learning},
keywords = {integrated learning},
number = {1},
pages = {71--101},
title = {{The Extraction of Refined Rules From Knowledge Based Neural Networks}},
volume = {13},
year = {1993}
}
@article{Towell1990,
author = {Towell, Geoffrey G and Shavlik, Jude W and Noordeweir, Michiel O},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik, Noordeweir - 1990 - Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks.pdf:pdf},
journal = {Proceedings of the Eighth National Conference on Artificial Intelligence},
pages = {861--866},
title = {{Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks}},
year = {1990}
}
@article{Towell1990a,
author = {Towell, Geoffrey G and Shavlik, Jude W and Noordeweir, Michiel O},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik, Noordeweir - 1990 - Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks.pdf:pdf},
journal = {Proceedings of the Eighth National Conference on Artificial Intelligence},
pages = {861--866},
title = {{Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks}},
year = {1990}
}
@article{Turney2010,
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
archivePrefix = {arXiv},
arxivId = {1003.1141},
author = {Turney, Peter D. and Pantel, Patrick},
doi = {10.1613/jair.2934},
eprint = {1003.1141},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turney, Pantel - 2010 - From frequency to meaning Vector space models of semantics.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
mendeley-groups = {Report/Explaining predictions,Report/Clustering/properties},
pages = {141--188},
title = {{From frequency to meaning: Vector space models of semantics}},
volume = {37},
year = {2010}
}
@article{Twomey1998,
author = {Twomey, Janet M. and Smith, Alice E.},
doi = {10.1109/5326.704579},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Twomey, Smith - 1998 - Bias and variance of validation methods for function approximation neural networks under conditions of sparse dat.pdf:pdf},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
keywords = {Function approximation,Neural network,Resampling,Validation},
number = {3},
pages = {417--430},
title = {{Bias and variance of validation methods for function approximation neural networks under conditions of sparse data}},
volume = {28},
year = {1998}
}
@article{Ustun2014,
abstract = {We present an integer programming framework to build accurate and interpretable discrete linear classification models. Unlike existing approaches, our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice. To this end, our framework can produce models that are fully optimized for accuracy, by minimizing the 0--1 classification loss, and that address multiple aspects of interpretability, by incorporating a range of discrete constraints and penalty functions. We use our framework to produce models that are difficult to create with existing methods, such as scoring systems and M-of-N rule tables. In addition, we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction. We show that discrete linear classifiers can attain the training accuracy of any other linear classifier, and provide an Occam's Razor type argument as to why the use of small discrete coefficients can provide better generalization. We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis.},
archivePrefix = {arXiv},
arxivId = {1405.4047},
author = {Ustun, Berk and Rudin, Cynthia},
eprint = {1405.4047},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ustun, Rudin - 2014 - Methods and Models for Interpretable Linear Classification.pdf:pdf},
journal = {arXiv},
mendeley-groups = {Annotated/Interpretable Classifiers,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
pages = {1--57},
title = {{Methods and Models for Interpretable Linear Classification}},
url = {http://arxiv.org/abs/1405.4047},
year = {2014}
}
@article{Utgoff2002,
abstract = {We explore incremental assimilation of new knowledge by sequential learning. Of particular interest is how a network of many knowledge layers can be constructed in an on-line manner, such that the learned units represent building blocks of knowledge that serve to compress the overall representation and facilitate transfer. We motivate the need for many layers of knowledge, and we advocate sequential learning as an avenue for promoting the construction of layered knowledge structures. Finally, our novel STL algorithm demonstrates a method for simultaneously acquiring and organizing a collection of concepts and functions as a network from a stream of unstructured information.},
author = {Utgoff, Paul E and Stracuzzi, David J},
doi = {10.1109/DEVLRN.2002.1011824},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Utgoff, Stracuzzi - 2002 - Many-layered learning.pdf:pdf},
isbn = {0-7695-1459-6},
issn = {0899-7667},
journal = {Neural computation},
number = {10},
pages = {2497--2529},
pmid = {12396572},
title = {{Many-layered learning.}},
volume = {14},
year = {2002}
}
@article{Valizadegan2009,
abstract = {Learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels. The ranking algorithms are often evaluated using information retrieval measures, such as Normalized Discounted Cumulative Gain (NDCG) [1] and Mean Average Precision (MAP) [2]. Until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures. The main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function. We propose a probabilistic framework that addresses this challenge by optimizing the expectation of NDCG over all the possible permutations of documents. A relaxation strategy is used to approximate the average of NDCG over the space of permutation, and a bound optimization approach is proposed to make the computation efficient. Extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets. 1},
author = {Valizadegan, Hamed and Jin, R},
doi = {10.1561/1500000016},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Valizadegan, Jin - 2009 - Learning to rank by optimizing ndcg measure.pdf:pdf},
isbn = {9781615679119},
issn = {1554-0669},
journal = {Advances in neural {\ldots}},
mendeley-groups = {Annotated/Ranking},
pages = {1--9},
title = {{Learning to rank by optimizing ndcg measure}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2009{\_}0344.pdf},
year = {2009}
}
@article{VanAssche2008,
abstract = {The purpose of this column is to stimulate discussion among nurses regarding the importance of nursing theory-guided practice. The use of metaphor may shed light on defining nursing by its own terms. The time has come for nursing to recognize its worth as an autonomous discipline and own its contributions.},
author = {{Van Assche}, Anneleen and Blockeel, Hendrik},
doi = {10.1007/978-3-540-78469-2_26},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Assche, Blockeel - 2008 - Seeing the forest through the trees learning a comprehensible model from a first order ensemble.pdf:pdf},
isbn = {3540784683},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Comprehensibility,Ensembles,First order decision trees},
mendeley-groups = {Report/Biologicla domain,Report/Medical domain},
pages = {269--279},
pmid = {22228521},
title = {{Seeing the forest through the trees learning a comprehensible model from a first order ensemble}},
volume = {4894 LNAI},
year = {2008}
}
@article{VanLinh2017,
abstract = {{\textcopyright} 2016, Springer-Verlag London.As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises-Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental results showed the advantages of our approach in terms of separability, interpretability and effectiveness in classification task of datasets with high dimension and complex distribution. Our method is highly competitive with state-of-the-art approaches.},
author = {{Van Linh}, Ngo and Anh, Nguyen Kim and Than, Khoat and Dang, Chien Nguyen},
doi = {10.1007/s10115-016-0956-6},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Linh et al. - 2017 - An effective and interpretable method for document classification.pdf:pdf},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Bayesian nonparametrics,Classification,Variational inference,Von Mises-Fisher distribution},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
number = {3},
pages = {763--793},
title = {{An effective and interpretable method for document classification}},
volume = {50},
year = {2017}
}
@article{Vandewiele2016,
abstract = {Models obtained by decision tree induction techniques excel in being interpretable.However, they can be prone to overfitting, which results in a low predictive performance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial. To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques.},
archivePrefix = {arXiv},
arxivId = {1611.05722},
author = {Vandewiele, Gilles and Janssens, Olivier and Ongenae, Femke and {De Turck}, Filip and {Van Hoecke}, Sofie},
eprint = {1611.05722},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vandewiele et al. - 2016 - GENESIM genetic extraction of a single, interpretable model.pdf:pdf},
mendeley-groups = {Annotated/Decision Trees,Report},
number = {Nips},
title = {{GENESIM: genetic extraction of a single, interpretable model}},
url = {http://arxiv.org/abs/1611.05722},
year = {2016}
}
@article{Veale2017,
abstract = {Machine learning systems are increasingly used to sup-port public sector decision-making across a variety of sectors. Given concerns around accountability in these domains, and amidst accusations of intentional or unintentional bias, there have been increased calls for transparency of these technologies. Few, however, have considered how logics and practices concerning transparency have been understood by those involved in the machine learning systems already being piloted and deployed in public bodies today. This short pa-per distils insights about transparency on the ground from interviews with 27 such actors, largely public ser-vants and relevant contractors, across 5 OECD coun-tries. Considering transparency and opacity in relation to trust and buy-in, better decision-making, and the avoidance of gaming, it seeks to provide useful insights for those hoping to develop socio-technical approaches to transparency that might be useful to practitioners on-the-ground.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.09249v2},
author = {Veale, Michael},
eprint = {arXiv:1706.09249v2},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale - 2017 - Logics and practices of transparency and opacity in real-world applications of public sector machine learning.pdf:pdf},
keywords = {()},
mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
title = {{Logics and practices of transparency and opacity in real-world applications of public sector machine learning}},
year = {2017}
}
@article{Veale2018,
abstract = {Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions — like taxation, justice, and child protection —-are now commonplace. How might design-ers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regard-ing challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning — absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications. NOTE 04/10/2017: This is a document currently under review. We'd be happy if you wanted to cite this — so please get in touch if that is the case. You can also circulate it to colleagues, but please don't publish it publicly. An earlier, shorter version of the paper is available that you can link to publicly: Veale, M (2017) Logics and practices of opacity and transparency in real-world applications of public sector machine learning. Presented as a talk at the 4th Workshop on Fairness, Accountability and Trans-parency in Machine Learning (FAT/ML 2017), Halifax, Nova Scotia, Canada. Available at:},
archivePrefix = {arXiv},
arxivId = {1802.01029},
author = {Veale, Michael and Kleek, Max Van and Binns, Reuben},
doi = {10.1145/3173574.3174014},
eprint = {1802.01029},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale, Kleek, Binns - 2018 - Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Maki.pdf:pdf},
isbn = {9781450356206},
mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
title = {{Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making}},
url = {http://},
year = {2018}
}
@article{Vellido2012,
abstract = {This article investigates methods for the accurate and robust differentiation of metastases from glioblastomas on the basis of single-voxel (1)H MRS information. Single-voxel (1)H MR spectra from a total of 109 patients (78 glioblastomas and 31 metastases) from the multicenter, international INTERPRET database, plus a test set of 40 patients (30 glioblastomas and 10 metastases) from three different centers in the Barcelona (Spain) metropolitan area, were analyzed using a robust method for feature (spectral frequency) selection coupled with a linear-in-the-parameters single-layer perceptron classifier. For the test set, a parsimonious selection of five frequencies yielded an area under the receiver operating characteristic curve of 0.86, and an area under the convex hull of the receiver operating characteristic curve of 0.91. Moreover, these accurate results for the discrimination between glioblastomas and metastases were obtained using a small number of frequencies that are amenable to metabolic interpretation, which should ease their use as diagnostic markers. Importantly, the prediction can be expressed as a simple formula based on a linear combination of these frequencies. As a result, new cases could be straightforwardly predicted by integrating this formula into a computer-based medical decision support system. This work also shows that the combination of spectra acquired at different TEs (short TE, 20-32 ms; long TE, 135-144 ms) is key to the successful discrimination between glioblastomas and metastases from single-voxel (1)H MRS.},
author = {Vellido, A. and Romero, E. and Juli??-Sap??, M. and Maj??s, C. and Moreno-Torres, ?? and Pujol, J. and Ar??s, C.},
doi = {10.1002/nbm.1797},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vellido et al. - 2012 - Robust discrimination of glioblastomas from metastatic brain tumors on the basis of single-voxel 1H MRS.pdf:pdf},
isbn = {1099-1492 (Electronic)$\backslash$n0952-3480 (Linking)},
issn = {09523480},
journal = {NMR in Biomedicine},
keywords = {Feature selection,Glioblastomas,High-grade malignant tumors,Medical decision support system,Metastases,Pattern recognition,SV 1H MRS},
mendeley-groups = {Report},
number = {6},
pages = {819--828},
pmid = {22081447},
title = {{Robust discrimination of glioblastomas from metastatic brain tumors on the basis of single-voxel 1H MRS}},
volume = {25},
year = {2012}
}
@article{Yang2019,
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	archivePrefix = {arXiv},
	arxivId = {1906.08237},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	eprint = {1906.08237},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/Text classification top perfomer.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {11Thesis/State of the art},
	number = {NeurIPS},
	pages = {1--18},
	title = {{XLNet: Generalized autoregressive pretraining for language understanding}},
	volume = {32},
	year = {2019}
}

@article{Vellido2012a,
abstract = {Peer Reviewed},
author = {Vellido, Alfredo and Martin-Guerroro, J D and Lisboa, P},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vellido, Martin-Guerroro, Lisboa - 2012 - Making machine learning models interpretable.pdf:pdf},
isbn = {9782874190490},
journal = {20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
mendeley-groups = {Report},
number = {April},
pages = {163--172},
title = {{Making machine learning models interpretable}},
year = {2012}
}
@article{OToole1993,
	abstract = {Faces can be represented efficiently as a weighted linear combination of the eigenvectors of a covariance matrix of face images. It has also been shown [J. Opt. Soc. Am. 4, 519-524 (1987)] that identifiable faces can be made by using only a subset of the eigenvectors, i.e., those with the largest eigenvalues. This low-dimensional repre­sentation is optimal in that it minimizes the squared error between the representation of the face image and the original face image. The present study demonstrates that, whereas this low-dimensional representation is optimal for identifying the physical categories of face, like sex, it is not optimal for recognizing the faces (i.e., discriminating known from unknown faces). Various low-dimensional representations of the faces in the higher dimensions of the face space (i.e., the eigenvectors with smaller eigenvalues) provide better information for face recognition. {\textcopyright} 1993 Optical Society of America.},
	author = {O'Toole, A. J. and Deffenbacher, K. A. and Valentin, D. and Abdi, H.},
	doi = {10.1364/josaa.10.000405},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/Low-dimensional{\_}representation{\_}of{\_}faces{\_}in{\_}higher{\_} (1).pdf:pdf},
	issn = {1084-7529},
	journal = {Journal of the Optical Society of America A},
	number = {3},
	pages = {405},
	title = {{Low-dimensional representation of faces in higher dimensions of the face space}},
	volume = {10},
	year = {1993}
}

@article{Vembu2011,
abstract = {Label ranking is a complex prediction task where the goal is to map instances to a total order over a finite set of predefined labels. An interesting aspect of this problem is that it subsumes several supervised learning problems, such as multiclass prediction, multilabel classification, and hierarchical classification. Unsurprisingly, there exists a plethora of label ranking algorithms in the literature due, in part, to this versatile nature of the problem. In this paper, we survey these algorithms.},
author = {Vembu, Shankar and G{\"{a}}rtner, Thomas},
doi = {10.1007/978-3-642-14125-6_3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vembu, G{\"{a}}rtner - 2011 - Label ranking algorithms A survey.pdf:pdf},
isbn = {9783642141249},
journal = {Preference Learning},
pages = {45--64},
title = {{Label ranking algorithms: A survey}},
year = {2011}
}
@book{Bishop2006,
	author = {Bishop, Christopher M.},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	year = {2006},
	isbn = {0387310738},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg}
}
@book{manning2008introduction,
	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective.},
	added-at = {2010-12-20T06:06:48.000+0100},
	address = {Cambridge, UK},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	biburl = {https://www.bibsonomy.org/bibtex/29f4ab13e07b48b9723113aa74224be65/folke},
	interhash = {2e574e46b7668a7268e7f02b46f4d9bb},
	intrahash = {9f4ab13e07b48b9723113aa74224be65},
	isbn = {978-0-521-86571-5},
	keywords = {book information introduction ir retrieval},
	publisher = {Cambridge University Press},
	timestamp = {2010-12-20T06:06:48.000+0100},
	title = {Introduction to Information Retrieval},
	url = {http://nlp.stanford.edu/IR-book/information-retrieval-book.html},
	year = 2008
}

@article{Verbeke2011,
abstract = {Customer churn prediction models aim to detect customers with a high propensity to attrite. Predictive accuracy, comprehensibility, and justifiability are three key aspects of a churn prediction model. An accurate model permits to correctly target future churners in a retention marketing campaign, while a comprehensible and intuitive rule-set allows to identify the main drivers for customers to churn, and to develop an effective retention strategy in accordance with domain knowledge. This paper provides an extended overview of the literature on the use of data mining in customer churn prediction modeling. It is shown that only limited attention has been paid to the comprehensibility and the intuitiveness of churn prediction models. Therefore, two novel data mining techniques are applied to churn prediction modeling, and benchmarked to traditional rule induction techniques such as C4.5 and RIPPER. Both AntMiner+ and ALBA are shown to induce accurate as well as comprehensible classification rule-sets. AntMiner+ is a high performing data mining technique based on the principles of Ant Colony Optimization that allows to include domain knowledge by imposing monotonicity constraints on the final rule-set. ALBA on the other hand combines the high predictive accuracy of a non-linear support vector machine model with the comprehensibility of the rule-set format. The results of the benchmarking experiments show that ALBA improves learning of classification techniques, resulting in comprehensible models with increased performance. AntMiner+ results in accurate, comprehensible, but most importantly justifiable models, unlike the other modeling techniques included in this study. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
author = {Verbeke, Wouter and Martens, David and Mues, Christophe and Baesens, Bart},
doi = {10.1016/j.eswa.2010.08.023},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbeke et al. - 2011 - Building comprehensible customer churn prediction models with advanced rule induction techniques.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {ALBA,Ant Colony Optimization,Churn prediction,Classification,Comprehensible rule induction,Data mining},
mendeley-groups = {Annotated/Applications/Marketing},
number = {3},
pages = {2354--2364},
publisher = {Elsevier Ltd},
title = {{Building comprehensible customer churn prediction models with advanced rule induction techniques}},
url = {http://dx.doi.org/10.1016/j.eswa.2010.08.023},
volume = {38},
year = {2011}
}
@article{fano1961transmission,
	abstract = {A basic text on the theoretical foundations of information theory, for graduate students and engineers interested in electrical communications and for others seeking a general introduction to the field, with some important new material on tilting probability distributions and coding for discrete channels.},
	author = {Fano, Robert M},
	journal = {American Journal of Physics},
	keywords = {SML-LIB-BIBLIO,lang:ENG},
	mendeley-tags = {SML-LIB-BIBLIO,lang:ENG},
	pages = {793--794},
	title = {{Transmission of information: A statistical theory of communications}},
	volume = {29},
	year = {1961}
}
@article{Vilnis2015,
abstract = {Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.},
archivePrefix = {arXiv},
arxivId = {1412.6623},
author = {Vilnis, Luke and McCallum, Andrew},
eprint = {1412.6623},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vilnis, McCallum - 2015 - Word Representations via Gaussian Embedding.pdf:pdf},
journal = {Iclr},
mendeley-groups = {Progress Report},
pages = {12},
title = {{Word Representations via Gaussian Embedding}},
url = {http://arxiv.org/abs/1412.6623},
year = {2015}
}
@article{Vincent2008a,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1145/1390156.1390294},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent et al. - 2008 - Extracting and composing robust features with denoising autoencoders.pdf:pdf},
isbn = {9781605582054},
issn = {1605582050},
journal = {Proceedings of the 25th international conference on Machine learning},
mendeley-groups = {Papers/Paper 1,Report/Features,Report},
pages = {1096--1103},
title = {{Extracting and composing robust features with denoising autoencoders}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.149.8111{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1390156.1390294},
year = {2008}
}
@article{Wachter2017,
abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
archivePrefix = {arXiv},
arxivId = {1711.00399},
author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
doi = {10.2139/ssrn.3063289},
eprint = {1711.00399},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wachter, Mittelstadt, Russell - 2017 - Counterfactual Explanations without Opening the Black Box Automated Decisions and the GDPR.pdf:pdf},
issn = {1556-5068},
mendeley-groups = {!Paper 3/Justifying Interpretability},
number = {1},
pages = {1--47},
title = {{Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR}},
url = {http://arxiv.org/abs/1711.00399},
year = {2017}
}
@article{Wainer2017,
author = {Wainer, Jacques and Cawley, Gavin},
file = {:D$\backslash$:/PhD/PGR/16-174.pdf:pdf},
keywords = {bootstrap,cross-validation,hyperparameters,k-fold,resampling,svm},
pages = {1--35},
title = {{Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters}},
volume = {18},
year = {2017}
}
@article{Wallach2008,
abstract = {This thesis introduces new methods for statistically modelling text using topic models. Topic models have seen many successes in recent years, and are used in a variety of applications, including analysis of news articles, topic-based search interfaces and navigation tools for digital libraries. Despite these recent successes, the field of topic modelling is still relatively new and there remains much to be explored. One notice- able absence from most of the previous work on topic modelling is consideration of language and document structurefrom low-level structures, including word order and syntax, to higher-level structures, such as relationships between documents. The focus of this thesis is therefore structured topic modelsmodels that combine latent topics with information about document structure, ranging from local sen- tence structure to inter-document relationships. These models draw on techniques from Bayesian statistics, including hierarchical Dirichlet distributions and processes, Pitman-Yor processes, and Markov chain Monte Carlo methods. Several methods for estimating the parameters of Dirichlet-multinomial distributions are also compared. The main contribution of this thesis is the introduction of three structured topic mod- els. The first is a topic-based language model. This model captures both word order and latent topics by extending a Bayesian topic model to incorporate n-gram statistics. A bigram version of the new model does better at predicting future words than either a topic model or a trigram language model. It also provides interpretable topics. The second model arises from a Bayesian reinterpretation of a classic generative de- pendency parsing model. The new model demonstrates that parsing performance can be substantially improved by a careful choice of prior and by sampling hyperparame- ters. Additionally, the generative nature of the model facilitates the inclusion of latent state variables, which act as specialised part-of-speech tags or syntactic topics. The third is a model that captures high-level relationships between documents. This model uses nonparametric Bayesian priors and Markov chain Monte Carlo methods to infer topic-based document clusters. The model assigns a higher probability to un- seen test documents than either a clustering model without topics or a Bayesian topic model without document clusters. The model can be extended to incorporate author information, resulting in finer-grained clusters and better predictive performance.},
author = {Wallach, Hanna M},
file = {:D$\backslash$:/Downloads/Work/1eefe0e5c69d6e14840e2d2b30f62a09a28b.pdf:pdf},
journal = {Doctor},
mendeley-groups = {Annotated/Topic models/Unsupervised Topic Models},
number = {2001},
pages = {136},
title = {{Structured Topic Models for Language}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.2537{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@article{Wang,
abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn sim-ilarity metric directly from images. It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sam-pling algorithm is proposed to learn the model with dis-tributed asynchronized stochastic gradient. Extensive ex-periments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
archivePrefix = {arXiv},
arxivId = {1404.4661},
author = {Wang, Jiang and Song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jingbin and Philbin, James and Chen, Bo and Wu, Ying},
doi = {10.1109/CVPR.2014.180},
eprint = {1404.4661},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Learning Fine-grained Image Similarity with Deep Ranking.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
mendeley-groups = {Report/Features,Progress Report},
title = {{Learning Fine-grained Image Similarity with Deep Ranking}}
}
@article{WenminLi,
abstract = {Previous studies propose that associative classification has high$\backslash$nclassification accuracy and strong flexibility at handling unstructured$\backslash$ndata. However, it still suffers from the huge set of mined rules and$\backslash$nsometimes biased classification or overfitting since the classification$\backslash$nis based on only a single high-confidence rule. The authors propose a$\backslash$nnew associative classification method, CMAR, i.e., Classification based$\backslash$non Multiple Association Rules. The method extends an efficient frequent$\backslash$npattern mining method, FP-growth, constructs a class$\backslash$ndistribution-associated FP-tree, and mines large databases efficiently.$\backslash$nMoreover, it applies a CR-tree structure to store and retrieve mined$\backslash$nassociation rules efficiently, and prunes rules effectively based on$\backslash$nconfidence, correlation and database coverage. The classification is$\backslash$nperformed based on a scikit-learned {\&}chi;2 analysis using multiple$\backslash$nstrong association rules. Our extensive experiments on 26 databases from$\backslash$nthe UCI machine learning database repository show that CMAR is$\backslash$nconsistent, highly effective at classification of various kinds of$\backslash$ndatabases and has better average classification accuracy in comparison$\backslash$nwith CBA and C4.5. Moreover, our performance study shows that the method$\backslash$nis highly efficient and scalable in comparison with other reported$\backslash$nassociative classification methods},
author = {{Wenmin Li} and {Jiawei Han} and {Jian Pei}},
doi = {10.1109/ICDM.2001.989541},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wenmin Li, Jiawei Han, Jian Pei - Unknown - CMAR accurate and efficient classification based on multiple class-association rules.pdf:pdf},
isbn = {0-7695-1119-8},
issn = {15504786},
journal = {Proceedings 2001 IEEE International Conference on Data Mining},
mendeley-groups = {Report},
pages = {369--376},
title = {{CMAR: accurate and efficient classification based on multiple class-association rules}},
url = {http://ieeexplore.ieee.org/document/989541/}
}
@article{Williamson2010,
abstract = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric mixed membership model - each data point is modeled with a collection of components of different proportions. Though powerful, the HDP makes an assumption that the probability of a component being exhibited by a data point is positively correlated with its proportion within that data point. This might be an undesirable assumption. For example, in topic modeling, a topic (component) might be rare throughout the corpus but dominant within those documents (data points) where it occurs. We develop the IBP compound Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data prevalence and within-data proportion in a mixed membership model. The ICD combines properties from the HDP and the Indian buffet process (IBP), a Bayesian nonparametric prior on binary matrices. The ICD assigns a subset of the shared mixture components to each data point. This subset, the data point's ``focus'', is determined independently from the amount that each of its components contribute. We develop an ICD mixture model for text, the focused topic model (FTM), and show superior performance over the HDP-based topic model.},
author = {Williamson, Sinead and Wang, Chong and Heller, Katherine A},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williamson, Wang, Heller - 2010 - The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling.pdf:pdf},
isbn = {9781605589077},
journal = {Icml},
mendeley-groups = {Annotated/Interpretable representations,!Paper 3/task/newsgroups},
pages = {1151--1158},
title = {{The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling}},
url = {http://www.icml2010.org/papers/397.pdf},
year = {2010}
}
@article{Windows2014,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Windows, Microsoft and Os, Mac and When, Considerable Points and Wei, Yanhao and Yildirim, Pinar and den Bulte, Christophe and Dellarocas, Chris and Weekly, The and Weekly, I C T Issues and {W. E. Henley} and Vyas, Shilpan Dineshkumar and Uk, The and Trend, Security and Trend, Finance Security and Technology, Banker and Insights, Financial and Longtop, About and Technologies, Financial and Tan, Arhnue and Darby, Sime and Corp, I O I and Loong, Kim and SW공학센터 and Skan, Julian and Lumb, Richard and Masood, Samad and Conway, Sean K. and Shue, Kelly and Service, Social Networking and Server, Web and Module, Server and Security, Financial and Scheme, Lending and Sample, Char and Schaffer, Kim and Report, Survey and Report, Industry Issue and Pos, Mobile and Permissions, App and Experience, Web and Links, App and Payments, Mobile and Support, Fingerprint and Store, Play and Paper, Working and Online, Fast Identity and Okten, Cagla and Osili, Una Okonkwo and November, Security Focus and Name, Last and Name, First and Training, Online and Training, Practical and Darin, C and Training, Rank Online and Kimberly, M and Deepa, G and Board, Ethics and Principal, Enter and Primary, Investigator and Systems, Food and Study, Emu Behaviour and Co-investigator, New and Mohamad, Rosli and Building, Accountancy and Ismail, Noor Azizi and March, Security Focus and Lin, Mingfeng and Prabhala, N.R. and Viswanathan, Siva and Lee, Kwanghoe and Park, Sean and Lee, Jihye Jenna and Park, Sean and Law, Fintech and Straight, R Jason and Vice, Senior and Privacy, Chief and Straight, Unitedlex and Douglass, Duncan B and Avery, B Y Christopher and Fanger, Gwen and Douglass, Duncan B and KPMG and Kempe, David and Kleinberg, Jon and Tardos, {\'{E}}va and Karma, Credit and Issues, Special and Issue, Market and Internet, Secure and Service, Payment and Insight, L G Business and Indicators, Hot and Huang, Cheng-Lung and Chen, Mu-Chen and Wang, Chieh-Jen and Group, Alibaba and Go, Rnaseq P F and Go, Rnaseq P F and Go, David Down and Go, David Down and Go, Rnaseq P F and From, Industry Overview and Freedman, Seth and Jin, Ginger and Forgot, Email Password and Foust, By Dean and February, Aaron Pressman and Corp, Fair Isaac and Fair, Bill and Isaac, Earl and Fellowes, Matt and Isaac, Fair and Fico, The and Isaac, Fair and Sanders, Anthony B and Bank, Deutsche and York, New and Street, Wall and Fico, The and For, Everything and Home, T H E and Finance, Segye and February, Security Focus and {Eroglu S., Toprak S., Urgan O, MD, Ozge E. Onur, MD, Arzu Denizbasi, MD, Haldun Akoglu, MD, Cigdem Ozpolat, MD, Ebru Akoglu}, Md and {Ernst {\&} Young} and Economics, Size South Mountain and Duarte, Jefferson and Siegel, Stephan and Young, Lance and Debnath, Souvik and Ganguly, Niloy and Mitra, Pabitra and Data, Big and Dapp, Thomas S. and Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R. A. and Holmberg, S. D. and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, SVD Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise and Contributor, Tom Groenfeldt and Conference, Mobile Security and Call, Secure Monitor and Compass, Gyro and Cho, Byungchul and Park, Jong-man and Chavan, Jayshree and Chat, We and Channel, Omni and Challenge, Business and Brief, Keri and Brief, Keri and Block, Bitcoin and Economics, Size South Mountain and Big, Using and Based, Data and Analysis, Network and Bankitx, A and Bank, Challenger and Bank, Challenger and Bank, Challenger and Bankers, British and Street, High and Bank, British Business and Authority, Financial Conduct and Accenture and Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Windows et al. - 2014 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Uma {\'{e}}tica para quantos?},
keywords = {12,2007,3,Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\&} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cornway,Corporate Finance,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,Industrial Organization,J.,JUVENTUD,Lumb,MODIFICACIONES CORPORALES,Male,Masood,Motivation,Movement,Public,R.,Risk-Taking,S.,S.K.,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Skan,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\&} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,advantages,aesthetics,and e-banking,and on cor-,anomaly detection,as none were found,authentication,autoinjury and health,body,business model,candidate,classification,collaboration,competition,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,credit access,credit financing,credit score,credit scoring,critical success factors,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,customer satisfaction,customer scoring,data mining,decision tree,department of economics at,e-,e- banking,e-banking,e-commerce,e-payment,e-trading,electronic communication and computation,emergency,endogenous tie,epidural,esth{\'{e}}tique,est{\'{e}}tica,feature sim-,finance includes e-payment,financial fervices technology,financial services innovation,find any reports of,fintech,fintech analysis,fintech start-ups,functions,genetic programming,global fintech comparison,high resolution images,if neuraxial anes-,in practice,indonesia,information technology,ing with neuraxial anesthesia,internet bank,internet primary bank,jarunee wonglimpiyarat,jeunesse,jibc december 2007,juvenile cultures,juventud,limitations,luation of non-urgent visits,m-commerce,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,multimodal biometric,needle through a,nes corporales,network security,networks,neural networks,no,patents analysis,perforaci{\'{o}}n corporal,piel,professor of marketing,professor of marketing at,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,recommender system,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,smart cards,social network analysis,social networks,social status,spinal,strategic,strategy,support vector machine,sustainable reconstruction,sydney fintech,sydney start-ups,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,the university of pennsylvania,the wharton school of,to a busy urban,traditional banking services,unimodal biometric,university of pennsylvania,vol,was reviewed to see,youth},
mendeley-groups = {!Paper 3/Training LSTMs,!Paper 3/Training Feedforward/CNN},
number = {2},
pages = {81--87},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167{\%}5Cnhttp://www.americanbanker.com/issues/179{\_}124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/15003161{\%}5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1},
volume = {XXXIII},
year = {2014}
}
@article{Wisdom2016,
abstract = {Recurrent neural networks (RNNs) are powerful and effective for processing sequential data. However, RNNs are usually considered "black box" models whose internal structure and learned parameters are not interpretable. In this paper, we propose an interpretable RNN based on the sequential iterative soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery problem, which models a sequence of correlated observations with a sequence of sparse latent vectors. The architecture of the resulting SISTA-RNN is implicitly defined by the computational structure of SISTA, which results in a novel stacked RNN architecture. Furthermore, the scikit-learns of the SISTA-RNN are perfectly interpretable as the parameters of a principled statistical model, which in this case include a sparsifying dictionary, iterative step size, and regularization parameters. In addition, on a particular sequential compressive sensing task, the SISTA-RNN trains faster and achieves better performance than conventional state-of-the-art black box RNNs, including long-short term memory (LSTM) RNNs.},
archivePrefix = {arXiv},
arxivId = {1611.07252},
author = {Wisdom, Scott and Powers, Thomas and Pitton, James and Atlas, Les},
eprint = {1611.07252},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wisdom et al. - 2016 - Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery.pdf:pdf},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
number = {Nips},
pages = {1--8},
title = {{Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery}},
url = {http://arxiv.org/abs/1611.07252},
year = {2016}
}
@article{Wood,
author = {Wood, Matthew and Rana, Omer and C, Eran Peer and C, Nathan Melly and C, Ryan Codrai and C, Matt Wood and C, Dom Routley and C, Mahima Dalal and C, Jamie Harkins and C, Jeremy Yee and C, Zara Siddique},
pages = {1--4},
title = {{Role in team}}
}
@article{Words2002,
author = {Words, Additional Key},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Words - 2002 - Cumulated Gain-based Evaluation of IR Techniques.pdf:pdf},
mendeley-groups = {Report/Clustering/properties},
number = {4},
pages = {422--446},
title = {{Cumulated Gain-based Evaluation of IR Techniques}},
volume = {20},
year = {2002}
}
@article{Wu2013,
abstract = {Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker's training data for synthesizing the converted speech. Both objective and subjective evaluations indicate that our proposed method outperforms JD-GMM and conventional unit selection methods. Copyright {\textcopyright} 2013 ISCA.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Wu, Zhizheng and Virtanen, Tuomas and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou},
doi = {10.1063/1.4906785},
eprint = {1502.01710},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2013 - character-level-convolutional-networks-for-text-classification.pdf:pdf},
isbn = {0123456789},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Multi-frame exemplar,Temporal information,Unit selection,Voice conversion},
mendeley-groups = {!Paper 3/task,!Paper 3/task/Yelp},
pages = {3057--3061},
pmid = {25246403},
title = {character-level-convolutional-networks-for-text-classification},
year = {2013}
}
@article{Xie2013,
abstract = {Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets demonstrate the effectiveness of our model.},
archivePrefix = {arXiv},
arxivId = {1309.6874},
author = {Xie, Pengtao and Xing, Eric P},
eprint = {1309.6874},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Xing - 2013 - Integrating Document Clustering and Topic Modeling.pdf:pdf},
journal = {Proceedings of the 29th conference on uncertainty in artificial intelligence},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
pages = {694--703},
title = {{Integrating Document Clustering and Topic Modeling}},
url = {http://www.cs.cmu.edu/{~}pengtaox/papers/uai2013paper.pdf},
year = {2013}
}
@article{XinYao1999a,
abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection scikit-learns, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
archivePrefix = {arXiv},
arxivId = {1108.1530},
author = {{Xin Yao}},
doi = {10.1109/5.784219},
eprint = {1108.1530},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xin Yao - 1999 - Evolving artificial neural networks.pdf:pdf},
isbn = {9780470287194},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {evolutionary computation,intelligent systems,neu-},
number = {9},
pages = {1423--1447},
pmid = {9821520},
title = {{Evolving artificial neural networks}},
volume = {87},
year = {1999}
}
@article{Xu2003,
abstract = {In this paper, we propose a novel document clustering method based on the non-negative factorization of the term- document matrix of the given document corpus. In the la- tent semantic space derived by the non-negative matrix fac- torization (NMF), each axis captures the base topic of a par- ticular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value. Our experimental evalua- tions show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clus- tering methods not only in the easy and reliable derivation of document clustering results, but also in document clus- tering accuracies.},
archivePrefix = {arXiv},
arxivId = {1410.0993},
author = {Xu, Wei and Liu, Xin and Gong, Yihong},
doi = {10.1145/860484.860485},
eprint = {1410.0993},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Liu, Gong - 2003 - Document clustering based on non-negative matrix factorization.pdf:pdf},
isbn = {1581136463},
issn = {01635840},
journal = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval - SIGIR '03},
keywords = {document clustering,non-negative matrix factorization},
mendeley-groups = {Annotated/NMF},
pages = {267},
title = {{Document clustering based on non-negative matrix factorization}},
url = {http://portal.acm.org/citation.cfm?doid=860435.860485},
year = {2003}
}
@article{Y.2015a,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Y., Lecun and Y., Bengio and G., Hinton},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Y., Y., G. - 2015 - Deep learning.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930630277{\&}partnerID=40{\&}md5=befeefa64ddca265c713cf81f4e2fc54},
volume = {521},
year = {2015}
}
@article{Yang2014,
abstract = {Abstract Genetic algorithms are among the most popular evolutionary algorithms in terms of the diversity of their applications. A vast majority of well-known optimization problems have been solved using genetic algorithms. In addition, genetic algorithms are population-based, and many modern evolutionary algorithms are directly based on genetic algorithms or have some strong similarities to them.},
author = {Yang, Xin-She},
doi = {10.1016/B978-0-12-416743-8.00005-1},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Man, Tang, Kwong - 1999 - Genetic Algorithms.pdf:pdf},
isbn = {978-0-12-416743-8},
issn = {0036-8733},
journal = {Nature-Inspired Optimization Algorithms},
keywords = {Evolutionary algorithm,Genetic algorithms,Genetic operators,Optimization},
pages = {77--87},
title = {{Genetic Algorithms}},
url = {http://www.sciencedirect.com/science/article/pii/B9780124167438000051{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/B9780124167438000051},
year = {2014}
}
@article{Yang2017,
abstract = {We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.},
archivePrefix = {arXiv},
arxivId = {1711.03953},
author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
eprint = {1711.03953},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2017 - Breaking the Softmax Bottleneck A High-Rank RNN Language Model.pdf:pdf},
mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
pages = {1--18},
title = {{Breaking the Softmax Bottleneck: A High-Rank RNN Language Model}},
url = {http://arxiv.org/abs/1711.03953},
year = {2017}
}
@article{Yang2016,
abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
archivePrefix = {arXiv},
arxivId = {1606.02393},
author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
doi = {10.18653/v1/N16-1174},
eprint = {1606.02393},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2016 - Hierarchical Attention Networks for Document Classification.pdf:pdf},
isbn = {9781941643914},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
mendeley-groups = {Annotated/Document representation,!Paper 3/Structured LSTMs,!Paper 3/task/Yelp},
pages = {1480--1489},
title = {{Hierarchical Attention Networks for Document Classification}},
url = {http://aclweb.org/anthology/N16-1174},
year = {2016}
}
@article{Yeh2016,
author = {Yeh, Chih-kuan and Wu, Wei-chieh and Ko, Wei-jen and Wang, Yu-chiang Frank},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeh et al. - 2016 - Learning Deep Latent Spaces for Multi-Label Classification.pdf:pdf},
mendeley-groups = {Progress Report,Interim Review},
title = {{Learning Deep Latent Spaces for Multi-Label Classification}},
year = {2016}
}
@article{Yin2017,
abstract = {Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
archivePrefix = {arXiv},
arxivId = {1702.01923},
author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"{u}}tze, Hinrich},
eprint = {1702.01923},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin et al. - 2017 - Comparative Study of CNN and RNN for Natural Language Processing.pdf:pdf},
mendeley-groups = {!Paper 3/Training LSTMs},
title = {{Comparative Study of CNN and RNN for Natural Language Processing}},
url = {http://arxiv.org/abs/1702.01923},
year = {2017}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - How transferable are features in deep neural networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27 (Proceedings of NIPS)},
mendeley-groups = {Progress Report,Interim Review,Report},
pages = {1--9},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
volume = {27},
year = {2014}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visualization(2).pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Zafar2017,
abstract = {The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness--given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design convex margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.},
archivePrefix = {arXiv},
arxivId = {1707.00010},
author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P and Weller, Adrian},
eprint = {1707.00010},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf:pdf},
journal = {arXiv:1707.00010 [cs, stat]},
keywords = {Algorithmic fairness,Machine learning},
mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
title = {{From Parity to Preference-based Notions of Fairness in Classification}},
url = {http://arxiv.org/abs/1707.00010},
year = {2017}
}
@article{Zaidan2007,
abstract = {We propose a new framework for supervised machine learning. Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with “rationales.” When annotating an example, the human teacher will also highlight evidence supporting this annotation—thereby teaching the machine learner why the example belongs to the category. We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance signi?cantly on a sample task, namely sentiment classi?cation of movie reviews. We hypothesize that in some situations, providing rationales is a more fruitful use of an annotator's time than annotating more examples.},
author = {Zaidan, O. and Zaidan, O. and Eisner, J. and Eisner, J. and Piatko, C. and Piatko, C.},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaidan et al. - 2007 - Using “annotator rationales” to improve machine learning for text categorization.pdf:pdf},
isbn = {9781932432657},
journal = {Proceedings of NAACL-HLT},
mendeley-groups = {Report/Explaining predictions,Annotated},
number = {April},
pages = {260--267},
title = {{Using “annotator rationales” to improve machine learning for text categorization}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Using+Annotator+Rationales+to+Improve+Machine+Learning+for+Text+Categorization{\#}0},
volume = {260},
year = {2007}
}
@article{Zaremba2014,
abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99{\%} accuracy.},
archivePrefix = {arXiv},
arxivId = {1410.4615},
author = {Zaremba, Wojciech and Sutskever, Ilya},
doi = {10.1016/S0893-6080(96)00073-1},
eprint = {1410.4615},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever - 2014 - Learning to Execute.pdf:pdf},
isbn = {1410.4615},
issn = {08936080},
mendeley-groups = {!Paper 3/task/Sentiment treebank},
pages = {1--25},
title = {{Learning to Execute}},
url = {http://arxiv.org/abs/1410.4615},
year = {2014}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks arXiv1311.2901v3 cs.CV 28 Nov 2013.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Vision-ECCV 2014},
mendeley-groups = {Progress Report,Interim Review},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and Understanding Convolutional Networks arXiv:1311.2901v3 [cs.CV] 28 Nov 2013}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53{\%}5Cnhttp://arxiv.org/abs/1311.2901{\%}5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
volume = {8689},
year = {2014}
}
@article{Zhai2016,
abstract = {In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the scikit-learns learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the scikit-learns of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.},
archivePrefix = {arXiv},
arxivId = {1512.04466},
author = {Zhai, Shuangfei and Zhang, Zhongfei},
eprint = {1512.04466},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhai, Zhang - 2016 - Semisupervised Autoencoder for Sentiment Analysis.pdf:pdf},
isbn = {9781577357605},
journal = {Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016)},
keywords = {Technical Papers: Machine Learning Applications},
mendeley-groups = {Progress Report,Interim Review},
pages = {1394--1400},
title = {{Semisupervised Autoencoder for Sentiment Analysis}},
url = {http://arxiv.org/abs/1512.04466},
volume = {13902},
year = {2016}
}
@article{Zhang2014,
abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
author = {Zhang, Min Ling and Zhou, Zhi Hua},
doi = {10.1109/TKDE.2013.39},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - 2014 - A review on multi-label learning algorithms.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Multi-label learning,algorithm adaptation,label correlations-problem transformation},
mendeley-groups = {Progress Report},
number = {8},
pages = {1819--1837},
title = {{A review on multi-label learning algorithms}},
volume = {26},
year = {2014}
}
@article{Zhang2007a,
abstract = {Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms. ?? 2007 Pattern Recognition Society.},
author = {Zhang, Min Ling and Zhou, Zhi Hua},
doi = {10.1016/j.patcog.2006.12.019},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - 2007 - ML-KNN A lazy learning approach to multi-label learning.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Functional genomics,K-nearest neighbor,Lazy learning,Machine learning,Multi-label learning,Natural scene classification,Text categorization},
number = {7},
pages = {2038--2048},
title = {{ML-KNN: A lazy learning approach to multi-label learning}},
volume = {40},
year = {2007}
}
@article{Zhang2010a,
abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
author = {Zhang, Min-Ling and Zhang, Kun},
doi = {10.1145/1835804.1835930},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
isbn = {9781450300551},
issn = {9781577355687},
journal = {Kdd},
mendeley-groups = {Progress Report},
pages = {999--1007},
title = {{Multi-label learning by exploiting label dependency}},
url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
year = {2010}
}
@article{Zhang2010,
abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
author = {Zhang, Min-Ling and Zhang, Kun},
doi = {10.1145/1835804.1835930},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
isbn = {9781450300551},
issn = {9781577355687},
journal = {Kdd},
mendeley-groups = {Progress Report},
pages = {999--1007},
title = {{Multi-label learning by exploiting label dependency}},
url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
year = {2010}
}
@article{Zhang,
abstract = {Multilabel learning is an extension of standard binary classi cation where the goal is to predict a set of labels (we call an individual label a tag) for each input example. The recent probabilistic classi er chain (PCC) method learns a series of probabilistic models that capture tag correlations. In this paper, we show how the PCC model may be viewed as a neural network with connections between output nodes. We then show that using a hidden layer in the neural network, instead of connections between output nodes, brings advantages that include tractable test-time inference and removing the need to select a xed tag ordering. Moreover, the hidden units capture nonlinear latent structure, which improves classi cation accuracy, and allows correlations between tags to be visualized explicitly. Compared to previous neural network methods for multilabel learning, we explain several design decisions that lead to a notable decrease in training time and a notable increase in accuracy. Empirical results show that the new method outperforms existing MLL methods on benchmark datasets. A nal contribution of the paper is to introduce a new multilabel dataset of movies where the tags are genres. Experimentally, the new method performs best on this dataset also.},
author = {Zhang, Min-ling and Zhou, Zhi-hua},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - Unknown - Neural Networks for Multi-Label Learning.pdf:pdf},
journal = {Performance Evaluation},
keywords = {backpropagation,machine learning,multi-label learning,neural net-,text categorization,works},
pages = {1--22},
title = {{Neural Networks for Multi-Label Learning}}
}
@article{Zhang2015,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135-1144},
numpages = {10},
keywords = {explaining machine learning, interpretability, black box classifier, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{Zhang2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.00614v2},
author = {Zhang, Quanshi and Zhu, Song-chun},
eprint = {arXiv:1802.00614v2},
file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.00614.pdf:pdf},
mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Visual},
title = {{Visual Interpretability for Deep Learning: a Survey}},
year = {2017}
}
@article{Zhang2016,
abstract = {We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their component sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions.},
archivePrefix = {arXiv},
arxivId = {1605.04469},
author = {Zhang, Ye and Marshall, Iain and Wallace, Byron C.},
eprint = {1605.04469},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Marshall, Wallace - 2016 - Rationale-Augmented Convolutional Neural Networks for Text Classification.pdf:pdf},
mendeley-groups = {Report/Explaining predictions},
title = {{Rationale-Augmented Convolutional Neural Networks for Text Classification}},
url = {http://arxiv.org/abs/1605.04469},
year = {2016}
}
@article{Zhang2016,
abstract = {We present a novel subset scan method to detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup, and identify the characteristics of this subgroup. This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias or regions of poor classifier fit. This allows consideration of not just subgroups of a priori interest or small dimensions, but the space of all possible subgroups of features. To address the difficulty of considering these exponentially many possible subgroups, we use subset scan and parametric bootstrap-based methods. Extending this method, we can penalize the complexity of the detected subgroup and also identify subgroups with high classification errors. We demonstrate these methods and find interesting results on the COMPAS crime recidivism and credit delinquency data.},
archivePrefix = {arXiv},
arxivId = {1611.08292},
author = {Zhang, Zhe and Neill, Daniel B.},
eprint = {1611.08292},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Neill - 2016 - Identifying Significant Predictive Bias in Classifiers.pdf:pdf},
mendeley-groups = {Annotated/Fairness},
number = {June},
pages = {1--5},
title = {{Identifying Significant Predictive Bias in Classifiers}},
url = {http://arxiv.org/abs/1611.08292},
year = {2016}
}
@article{Zhang2017a,
abstract = {The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet to establish a direct multimodal mapping between medical images and diagnostic reports that can read images, generate diagnostic reports, retrieve images by symptom descriptions, and visualize attention, to provide justifications of the network diagnosis process. MDNet includes an image model and a language model. The image model is proposed to enhance multi-scale feature ensembles and utilization efficiency. The language model, integrated with our improved attention mechanism, aims to read and explore discriminative image feature descriptions from reports to learn a direct mapping from sentence words to image pixels. The overall network is trained end-to-end by using our developed optimization strategy. Based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we conduct sufficient experiments to demonstrate that MDNet outperforms comparative baselines. The proposed image model obtains state-of-the-art performance on two CIFAR datasets as well.},
archivePrefix = {arXiv},
arxivId = {1707.02485},
author = {Zhang, Zizhao and Xie, Yuanpu and Xing, Fuyong and McGough, Mason and Yang, Lin},
doi = {10.1109/CVPR.2017.378},
eprint = {1707.02485},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - MDNet A Semantically and Visually Interpretable Medical Image Diagnosis Network.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
mendeley-groups = {!Paper 3/Interpretable LSTMs},
title = {{MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network}},
url = {http://arxiv.org/abs/1707.02485},
year = {2017}
}
@article{Zhao2017,
author = {Zhao, Rui and Mao, Kezhi},
doi = {10.1109/TFUZZ.2017.2690222},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Mao - 2017 - Fuzzy Bag-of-Words Model for Document Representation.pdf:pdf},
issn = {1063-6706},
journal = {IEEE Transactions on Fuzzy Systems},
mendeley-groups = {Annotated/Document representation,!Paper 3/task/newsgroups},
number = {8},
pages = {1--1},
title = {{Fuzzy Bag-of-Words Model for Document Representation}},
url = {http://ieeexplore.ieee.org/document/7891009/},
volume = {14},
year = {2017}
}
@article{Zhou2004,
abstract = {In the research of rule extraction from neural networks, fidelity describes how well the rules mimic the behavior of a neural network while accuracy describes how well the rules can be generalized. This paper identifies the fidelity-accuracy dilemma. It argues to distinguish rule extraction using neural networks and rule extraction for neural networks according to their differing goals, where fidelity and accuracy should be excluded from the rule quality evaluation framework, respectively.},
author = {Zhou, Zhi-Hua},
doi = {10.1007/BF02944803},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou - 2004 - Rule Extraction Using Neural Networks or for Neural Networks ×.pdf:pdf},
issn = {1000-9000},
journal = {J. Comput. Sci. {\&} Technol.},
keywords = {accuracy,fidelity,machine learning,neural networks,rule extraction},
pages = {249--253},
title = {{Rule Extraction: Using Neural Networks or for Neural Networks? ×}},
url = {http://ac.els-cdn.com/S1877750313000185/1-s2.0-S1877750313000185-main.pdf?{\_}tid=f08c3f6c-e35d-11e4-9abc-00000aacb35f{\&}acdnat=1429095513{\_}7903f6ce12ea46c9f359f585fc91609a},
volume = {19},
year = {2004}
}
@article{Zhu2012,
abstract = {A supervised topic model can use side information such as ratings or labels associated with doc- uments or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective func- tions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet alloca- tion (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) un- der a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or re- gression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, espe- cially for classification.},
archivePrefix = {arXiv},
arxivId = {0912.5507},
author = {Zhu, Jun and Ahmed, a and Xing, Ep},
doi = {10.1145/1553374.1553535},
eprint = {0912.5507},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Ahmed, Xing - 2012 - MedLDA Maximum Margin Supervised Topic Models.pdf:pdf},
isbn = {978-1-60558-516-1},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {latent dirichlet allocation,max-margin learning,maximum entropy discrimination,supervised topic models,support vector machines},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,!Paper 3/task/newsgroups},
pages = {2237--2278},
title = {{MedLDA: Maximum Margin Supervised Topic Models}},
url = {http://jmlr.csail.mit.edu/papers/volume13/zhu12a/zhu12a.pdf},
volume = {13},
year = {2012}
}
@article{Jameel,
	author = {Jameel, Shoaib and Bouraoui, Zied and Schockaert, Steven},
	file = {:E$\backslash$:/MEmbER{\_}{\_}{\_}SIGIR{\_}2017-10.pdf:pdf},
	isbn = {9781450350228},
	keywords = {entity embedding,entity rank-,list completion,maximum margin},
	mendeley-groups = {11Thesis/Applications},
	title = {{MEmbER : Max-Margin Based Embeddings for Entity Retrieval}}
}

@article{Zhu2014,
abstract = {Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max- margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the “augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time efficiency. The classification performance is also improved over competitors on binary, multi- class and multi-label classification tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1310.2816v1},
author = {Zhu, Jun and Chen, Ning and Perkins, Hugh and Zhang, Bo},
eprint = {arXiv:1310.2816v1},
file = {:D$\backslash$:/Downloads/Work/zhu14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Gibbs classifiers,max-margin learning,regularized Bayesian inference,supervised topic models,support vector machines},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
pages = {1073--1110},
title = {{Gibbs max-margin topic models with data augmentation}},
volume = {15},
year = {2014}
}
@article{Zhu2016,
abstract = {Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference.},
archivePrefix = {arXiv},
arxivId = {1602.07428},
author = {Zhu, Jun and Song, Jiaming and Chen, Bei},
doi = {10.1.1.160.2072},
eprint = {1602.07428},
file = {:D$\backslash$:/Downloads/Work/1602.07428.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {13669516},
mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
number = {1},
title = {{Max-Margin Nonparametric Latent Feature Models for Link Prediction}},
url = {http://arxiv.org/abs/1602.07428},
volume = {6},
year = {2016}
}
@article{Zhu2015,
abstract = {The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we pro-pose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hier-archies, e.g., language or image parse structures. We leverage the models for semantic composi-tion to understand the meaning of text, a funda-mental problem in natural language understand-ing, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that with-out considering the structures.},
author = {Zhu, Xiaodan and Sobhani, Parinaz and Guo, Hongyu},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Sobhani, Guo - 2015 - Long Short-Term Memory Over Recursive Structures.pdf:pdf},
isbn = {9781510810587},
journal = {International Conference on Machine Learning (ICML)},
mendeley-groups = {!Paper 3/Interpretable LSTMs,!Paper 3/task/Sentiment treebank},
pages = {1604--1612},
title = {{Long Short-Term Memory Over Recursive Structures}},
volume = {37},
year = {2015}
}
@article{Zhang2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.00614v2},
	author = {Zhang, Quanshi and Zhu, Song-chun},
	eprint = {arXiv:1802.00614v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.00614.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Visual},
	title = {{Visual Interpretability for Deep Learning: a Survey}},
	year = {2017}
}

@article{Rothe2016,
	author = {Rothe, Sascha and Processing, Language},
	file = {:C$\backslash$:/Users/Workk/Documents/P16-2083.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	pages = {512--517},
	title = {{Word Embedding Calculus in Meaningful Ultradense Subspaces}},
	year = {2016}
}


@article{kim2013deriving,
	author = {Kim, Joo-kyung},
	file = {:C$\backslash$:/Users/Workk/Documents/D13-1169.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	number = {October},
	pages = {1625--1630},
	title = {{Deriving adjectival scales from continuous space word representations}},
	year = {2013}
}

@article{Yang2016,
	abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	archivePrefix = {arXiv},
	arxivId = {1606.02393},
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	doi = {10.18653/v1/N16-1174},
	eprint = {1606.02393},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2016 - Hierarchical Attention Networks for Document Classification.pdf:pdf},
	isbn = {9781941643914},
	journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Document representation,!Paper 3/Structured LSTMs,!Paper 3/task/Yelp,11Thesis/Document Representations},
	pages = {1480--1489},
	title = {{Hierarchical Attention Networks for Document Classification}},
	url = {http://aclweb.org/anthology/N16-1174},
	year = {2016}
}
@article{Tang2015,
	abstract = {Document level sentiment classification remains a challenge: encoding the intrin- sic relations between sentences in the se- mantic meaning of a document. To ad- dress this, we introduce a neural network model to learn vector-based document rep- resentation in a unified, bottom-up fash- ion. The model first learns sentence rep- resentation with convolutional neural net- work or long short-term memory. After- wards, semantics of sentences and their relations are adaptively encoded in docu- ment representation with gated recurren- t neural network. We conduct documen- t level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimen- tal results show that: (1) our neural mod- el shows superior performances over sev- eral state-of-the-art algorithms; (2) gat- ed recurrent neural network dramatically outperforms standard recurrent neural net- work in document modeling for sentiment classification},
	archivePrefix = {arXiv},
	arxivId = {1508.04025},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	doi = {10.18653/v1/D15-1167},
	eprint = {1508.04025},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:pdf},
	isbn = {9781941643327},
	issn = {10495258},
	journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	mendeley-groups = {Annotated/Document representation,!Paper 3/task,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank,11Thesis/Document Representations},
	number = {September},
	pages = {1422--1432},
	title = {{Document Modeling with Gated Recurrent Neural Network for Sentiment Classification}},
	url = {http://aclweb.org/anthology/D15-1167},
	year = {2015}
}
@article{Bau2017,
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
	archivePrefix = {arXiv},
	arxivId = {1704.05796},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	doi = {10.1109/CVPR.2017.354},
	eprint = {1704.05796},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1704.05796.pdf:pdf},
	isbn = {9781538604571},
	issn = {1530-1567},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Representations},
	pages = {3319--3327},
	pmid = {12882599},
	title = {{Network dissection: Quantifying interpretability of deep visual representations}},
	volume = {2017-Janua},
	year = {2017}
}

@incollection{Zhang2012,
title = {Large-Scale Sparse Principal Component Analysis with Application to Text Data},
author = {Youwei Zhang and Laurent E. Ghaoui},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {532--539},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4337-large-scale-sparse-principal-component-analysis-with-application-to-text-data.pdf}
}
@article{TomasMikolovWen-tauYih2013,
	
	archivePrefix = {arXiv},
	arxivId = {1301.3781},
	doi = {10.3109/10826089109058901},
	eprint = {1301.3781},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/rvecs.pdf:pdf},
	isbn = {9781937284473},
	issn = {9781937284473},
	journal = {Hlt-Naacl},
	mendeley-groups = {11Thesis},
	number = {June},
	pages = {746--751},
	pmid = {1938007},
	title = {{Linguistic Regularities in Continuous Space Word Representations}},
	url = {http://anthology.aclweb.org/N/N13/N13-1.pdf{\#}page=655},
	year = {2013}
}
@article{Lipton2016,
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	archivePrefix = {arXiv},
	arxivId = {1606.03490},
	author = {Lipton, Zachary C.},
	eprint = {1606.03490},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
	mendeley-groups = {Annotated/Overarching Interpretability,Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {Whi},
	title = {{The Mythos of Model Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	year = {2016}
}
@article{Fyshe2015,
	abstract = {Vector Space Models (VSMs) of Semantics are useful tools for exploring the semantics of single words, and the composition of words to make phrasal meaning. While many meth- ods can estimate the meaning (i.e. vector) of a phrase, few do so in an interpretable way. We introduce a new method (CNNSE) that al- lows word and phrase vectors to adapt to the notion of composition. Our method learns a VSM that is both tailored to support a chosen semantic composition operation, and whose resulting features have an intuitive interpreta- tion. Interpretability allows for the exploration of phrasal semantics, which we leverage to an- alyze performance on a behavioral task. 1},
	author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - 2015 - A compositional and interpretable semantic space.pdf:pdf},
	isbn = {9781941643495},
	journal = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015)},
	mendeley-groups = {Progress Report,Interim Review,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
	pages = {32--41},
	title = {{A compositional and interpretable semantic space}},
	year = {2015}
}
@article{Ruggieri2009,
	author = {Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/tkdd.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	title = {{Data Mining for Discrimination Discovery}},
	volume = {V},
	year = {2009}
}
@article{Blodgett2017,
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
	archivePrefix = {arXiv},
	arxivId = {1707.00061},
	author = {Blodgett, Su Lin and O'Connor, Brendan},
	eprint = {1707.00061},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blodgett, O'Connor - 2017 - Racial Disparity in Natural Language Processing A Case Study of Social Media African-American English.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English}},
	url = {http://arxiv.org/abs/1707.00061},
	year = {2017}
}
@article{Lau2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1607.05368v1},
	author = {Lau, Jey Han and Baldwin, Timothy},
	eprint = {arXiv:1607.05368v1},
	file = {:C$\backslash$:/Users/Workk/Documents/1607.05368.pdf:pdf},
	mendeley-groups = {11Thesis},
	title = {{Practical Insights into Document Embedding Generation}},
	year = {2014}
}
@article{Gupta2015,
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1505.05192v1},
	author = {Gupta, Abhinav and Efros, Alexei a},
	doi = {10.1109/ICCV.2015.167},
	eprint = {arXiv:1505.05192v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta, Efros - 2015 - Unsupervised Visual Representation Learning by Context Prediction.pdf:pdf},
	isbn = {978-1-4673-8391-2},
	issn = {978-1-4673-8391-2},
	journal = {arXiv preprint},
	mendeley-groups = {Report/Features,11Thesis},
	pages = {1422--1430},
	pmid = {903},
	title = {{Unsupervised Visual Representation Learning by Context Prediction}},
	year = {2015}
}
@article{Ustun2014,
	abstract = {We present an integer programming framework to build accurate and interpretable discrete linear classification models. Unlike existing approaches, our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice. To this end, our framework can produce models that are fully optimized for accuracy, by minimizing the 0--1 classification loss, and that address multiple aspects of interpretability, by incorporating a range of discrete constraints and penalty functions. We use our framework to produce models that are difficult to create with existing methods, such as scoring systems and M-of-N rule tables. In addition, we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction. We show that discrete linear classifiers can attain the training accuracy of any other linear classifier, and provide an Occam's Razor type argument as to why the use of small discrete coefficients can provide better generalization. We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis.},
	archivePrefix = {arXiv},
	arxivId = {1405.4047},
	author = {Ustun, Berk and Rudin, Cynthia},
	eprint = {1405.4047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ustun, Rudin - 2014 - Methods and Models for Interpretable Linear Classification.pdf:pdf},
	journal = {arXiv},
	mendeley-groups = {Annotated/Interpretable Classifiers,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	pages = {1--57},
	title = {{Methods and Models for Interpretable Linear Classification}},
	url = {http://arxiv.org/abs/1405.4047},
	year = {2014}
}
@article{Systems,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1610.01256v2},
	author = {Systems, Cyber-physical and Products, Data},
	eprint = {arXiv:1610.01256v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1610.01256.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Safety},
	pages = {1--20},
	title = {{On the Safety of Machine Learning : Cyber-Physical Systems , Decision Sciences , and Data Products}}
}
@article{Lundberg2016,
	abstract = {Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.},
	archivePrefix = {arXiv},
	arxivId = {1611.07478},
	author = {Lundberg, Scott and Lee, Su-In},
	eprint = {1611.07478},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Lee - 2016 - An unexpected unity among methods for interpreting model predictions.pdf:pdf},
	mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	number = {Nips},
	pages = {1--6},
	title = {{An unexpected unity among methods for interpreting model predictions}},
	url = {http://arxiv.org/abs/1611.07478},
	year = {2016}
}
@article{VanLinh2017,
	
	author = {{Van Linh}, Ngo and Anh, Nguyen Kim and Than, Khoat and Dang, Chien Nguyen},
	doi = {10.1007/s10115-016-0956-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Linh et al. - 2017 - An effective and interpretable method for document classification.pdf:pdf},
	issn = {02193116},
	journal = {Knowledge and Information Systems},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
	number = {3},
	pages = {763--793},
	title = {{An effective and interpretable method for document classification}},
	volume = {50},
	year = {2017}
}
@article{Veale2018,
	
	archivePrefix = {arXiv},
	arxivId = {1802.01029},
	author = {Veale, Michael and Kleek, Max Van and Binns, Reuben},
	doi = {10.1145/3173574.3174014},
	eprint = {1802.01029},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale, Kleek, Binns - 2018 - Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Maki.pdf:pdf},
	isbn = {9781450356206},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
	title = {{Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making}},
	url = {http://},
	year = {2018}
}
@article{Barocas2016,
	
	author = {Barocas, Solon and Selbst, Andrew},
	doi = {http://dx.doi.org/10.15779/Z38BG31},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barocas, Selbst - 2016 - Big Data ' s Disparate Impact.pdf:pdf},
	issn = {9780262327343},
	journal = {California law review},
	mendeley-groups = {Annotated/Overarching Interpretability,11Thesis/Interpretability/Discrimination},
	number = {1},
	pages = {671--729},
	title = {{Big Data ' s Disparate Impact}},
	url = {https://ssrn.com/abstract=2477899},
	volume = {104},
	year = {2016}
}
@article{Miller2017a,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.07269v1},
	author = {Miller, Tim},
	eprint = {arXiv:1706.07269v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 2017 - Explanation in Artificial Intelligence Insights from the Social Sciences.pdf:pdf},
	keywords = {explainability,explainable ai,explanation,interpretability,transparency},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
	title = {{Explanation in Artificial Intelligence : Insights from the Social Sciences}},
	year = {2017}
}
@article{Kim2013,
	author = {Kim, Joo-kyung},
	file = {:C$\backslash$:/Users/Workk/Documents/D13-1169.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	number = {October},
	pages = {1625--1630},
	title = {{Deriving adjectival scales from continuous space word representations}},
	year = {2013}
}
@article{Dosilovic2018,
	author = {Do{\v{s}}ilovi{\'{c}}, Filip Karlo and Br{\v{c}}i{\'{c}}, Mario and Hlupi{\'{c}}, Nikica},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/dsdc{\_}11{\_}4754.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Explanation},
	pages = {232--237},
	title = {{232 Mipro 2018/Ds-Dc}},
	year = {2018}
}
@article{Gardenfors2014,
	author = {G{\"{a}}rdenfors, Peter},
	doi = {10.1007/978-1-4020-9877-2},
	file = {:C$\backslash$:/Users/Workk/Documents/Conceptual{\_}Spaces.pdf:pdf},
	isbn = {9781402098772},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {September},
	title = {{Conceptual spaces}},
	year = {2014}
}
@article{Bolukbasi2016,
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	archivePrefix = {arXiv},
	arxivId = {1607.06520},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	eprint = {1607.06520},
	file = {:D$\backslash$:/Downloads/Play/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf:pdf},
	isbn = {9781510838819},
	issn = {10495258},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {Nips},
	pages = {1--9},
	title = {{Debiasing Word Embedding}},
	url = {https://code.google.com/archive/p/word2vec/},
	year = {2016}
}
@book{Moewes,
	author = {Moewes, Christian and N{\"{u}}rnberger, Andreas},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/The safe and interpretable machine learning part.pdf:pdf},
	isbn = {9783642323775},
	mendeley-groups = {11Thesis/Interpretability/Safety},
	title = {{in Intelligent Data Analysis}}
}
@article{Prior2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.11571v2},
	author = {Prior, Human-in-the-loop Interpretability and Ross, Andrew Slavin and Kim, Been and Gershman, Samuel J},
	eprint = {arXiv:1805.11571v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1805.11571.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	number = {1},
	pages = {1--13},
	title = {{Human-in-the-Loop Interpretability Prior}},
	year = {2018}
}
@inproceedings{Gladkova2016,
	author = {Gladkova, Anna and Drozd, Aleksandr},
	booktitle = {Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP},
	doi = {10.18653/v1/W16-2507},
	isbn = {9781945626142},
	mendeley-groups = {Report/Features,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	pages = {36--42},
	title = {{Intrinsic Evaluations of Word Embeddings: What Can We Do Better?}},
	year = {2016}
}
@article{Zhao2018,
	abstract = {Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.},
	archivePrefix = {arXiv},
	arxivId = {1809.01496},
	author = {Zhao, Jieyu and Zhou, Yichao and Li, Zeyu and Wang, Wei and Chang, Kai-Wei},
	eprint = {1809.01496},
	file = {:D$\backslash$:/Downloads/Play/1809.01496.pdf:pdf},
	issn = {0029-2303 (Print)},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis/Interpretability/Discrimination},
	pmid = {3270028},
	title = {{Learning Gender-Neutral Word Embeddings}},
	url = {http://arxiv.org/abs/1809.01496},
	year = {2018}
}
@article{Hara2016,
	abstract = {Tree ensembles such as random forests and boosted trees are renowned for their high prediction performance; however, their interpretability is critically limited. One way of interpreting a complex tree ensemble is to obtain its simplified representation, which is formalized as a model selection problem: Given a complex tree ensemble, we want to obtain the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm. Our approach has three appealing features: the prediction performance is maintained, the coverage is sufficiently large, and the computation is reasonably feasible. Our synthetic data experiment and real world data applications show that complicated tree ensembles are approximated reasonably as interpretable.},
	archivePrefix = {arXiv},
	arxivId = {1606.09066},
	author = {Hara, Satoshi and Hayashi, Kohei},
	eprint = {1606.09066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara, Hayashi - 2016 - Making Tree Ensembles Interpretable A Bayesian Model Selection Approach.pdf:pdf},
	mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	number = {Whi},
	title = {{Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach}},
	url = {http://arxiv.org/abs/1606.09066},
	year = {2016}
}
@article{Rothe2016,
	author = {Rothe, Sascha and Processing, Language},
	file = {:C$\backslash$:/Users/Workk/Documents/P16-2083.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	pages = {512--517},
	title = {{Word Embedding Calculus in Meaningful Ultradense Subspaces}},
	year = {2016}
}
@article{Zafar2017,
	abstract = {The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness--given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design convex margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.},
	archivePrefix = {arXiv},
	arxivId = {1707.00010},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1707.00010},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf:pdf},
	journal = {arXiv:1707.00010 [cs, stat]},
	keywords = {Algorithmic fairness,Machine learning},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{From Parity to Preference-based Notions of Fairness in Classification}},
	url = {http://arxiv.org/abs/1707.00010},
	year = {2017}
}
@article{Martinc,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1902.00438v2},
	author = {Martinc, Matej and Kralj, Jan and Pollak, Senja},
	eprint = {arXiv:1902.00438v2},
	file = {:E$\backslash$:/1902.00438.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Representations,11Thesis},
	title = {{tax2vec : Constructing Interpretable Features from Taxonomies for Short Text Classification}}
}
@article{Lakkaraju2017,
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	archivePrefix = {arXiv},
	arxivId = {1707.01154},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	eprint = {1707.01154},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju et al. - 2017 - Interpretable {\&} Explorable Approximations of Black Box Models.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	title = {{Interpretable {\&} Explorable Approximations of Black Box Models}},
	url = {http://arxiv.org/abs/1707.01154},
	year = {2017}
}
@article{Edunov2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1808.09381v2},
	author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David and Park, Menlo and Brain, Google and View, Mountain},
	eprint = {arXiv:1808.09381v2},
	file = {:E$\backslash$:/1808.09381.pdf:pdf},
	mendeley-groups = {11Thesis/State of the art},
	title = {{Understanding Back-Translation at Scale}},
	year = {2018}
}
@article{Bechavod2017,
	abstract = {We present a regularization-inspired approach for reducing bias in learned classifiers. In particular, we focus on binary classification tasks over individuals from two populations, where, as our criterion for fairness, we wish to achieve similar false positive rates in both populations, and similar false negative rates in both populations. As a proof of concept, we implement our approach and empirically evaluate its ability to achieve both fairness and accuracy, using the COMPAS scores data for prediction of recidivism.},
	archivePrefix = {arXiv},
	arxivId = {1707.00044},
	author = {Bechavod, Yahav and Ligett, Katrina},
	eprint = {1707.00044},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechavod, Ligett - 2017 - Learning Fair Classifiers A Regularization-Inspired Approach.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	pages = {6--10},
	title = {{Learning Fair Classifiers: A Regularization-Inspired Approach}},
	url = {http://arxiv.org/abs/1707.00044},
	year = {2017}
}
@article{Kim2017,
	abstract = {Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate scikit-learning scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others.},
	author = {Kim, Han Kyul and Kim, Hyunjoong and Cho, Sungzoon},
	doi = {10.1016/j.neucom.2017.05.046},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kim, Cho - 2017 - Bag-of-concepts Comprehending document representation through clustering words in distributed representation.pdf:pdf},
	issn = {18728286},
	journal = {Neurocomputing},
	keywords = {Bag-of-concepts,Interpretable document representation,Word2vec clustering},
	mendeley-groups = {Annotated/Word Vectors,11Thesis/Interpretability,11Thesis/Interpretability/Representations,11Thesis},
	pages = {336--352},
	title = {{Bag-of-concepts: Comprehending document representation through clustering words in distributed representation}},
	volume = {266},
	year = {2017}
}
@article{Amato2009,
	author = {Amato, Claudia and Fanizzi, Nicola and Fazzinga, Bettina},
	file = {:C$\backslash$:/Users/Workk/Documents/Combining{\_}Semantic{\_}Web{\_}Search{\_}with{\_}the{\_}Power{\_}of{\_}In.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {June 2014},
	title = {{Combining Semantic Web Search with the Power of Inductive Reasoning . Combining Semantic Web Search with the Power of Inductive Reasoning}},
	year = {2009}
}
@article{Johnson2015,
	abstract = {Convolutional neural network (CNN) is a neu-ral network that can make use of the inter-nal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embed-ding of small text regions for use in classifi-cation. In addition to a straightforward adap-tation of CNN from image to text, a sim-ple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.},
	archivePrefix = {arXiv},
	arxivId = {1412.1058},
	author = {Johnson, Rie and Zhang, Tong},
	eprint = {1412.1058},
	file = {:C$\backslash$:/Users/Workk/Documents/1412.1058.pdf:pdf},
	isbn = {9781941643495},
	journal = {Naacl},
	mendeley-groups = {11Thesis},
	number = {2011},
	pages = {103--112},
	title = {{Effective Use of Word Order for Text Categorization with Convolutional Neural Networks}},
	url = {http://arxiv.org/abs/1412.1058{\%}5Cnhttp://arxiv.org/abs/1412.1058v1},
	year = {2015}
}
@article{Hardt2016,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1610.02413},
	author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
	eprint = {arXiv:1610.02413},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/6374-equality-of-opportunity-in-supervised-learning.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {Nips},
	title = {{Equality of Opportunity in Supervised Learning}},
	year = {2016}
}
@article{Gilpin,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.00069v2},
	author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	eprint = {arXiv:1806.00069v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1806.00069.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	title = {{Explaining Explanations : An Approach to Evaluating Interpretability of Machine Learning}}
}
@article{Kitaev2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.01052v1},
	author = {Kitaev, Nikita and Klein, Dan},
	eprint = {arXiv:1805.01052v1},
	file = {:E$\backslash$:/1805.01052.pdf:pdf},
	mendeley-groups = {11Thesis},
	title = {{Constituency Parsing with a Self-Attentive Encoder}},
	year = {2017}
}
@article{Curry,
	author = {Curry, Edward and Buitelaar, Paul},
	file = {:C$\backslash$:/Users/Workk/Documents/preprint{\_}nldb{\_}{\_}commonsense{\_}2014.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	pages = {1--12},
	title = {{A Distributional Semantics Approach for Selective Reasoning on Commonsense Graph Knowledge Bases}}
}
@article{Huysmans2011,
	abstract = {An important objective of data mining is the development of predictive models. Based on a number of observations, a model is constructed that allows the analysts to provide classifications or predictions for new observations. Currently, most research focuses on improving the accuracy or precision of these models and comparatively little research has been undertaken to increase their comprehensibility to the analyst or end-user. This is mainly due to the subjective nature of 'comprehensibility', which depends on many factors outside the model, such as the user's experience and his/her prior knowledge. Despite this influence of the observer, some representation formats are generally considered to be more easily interpretable than others. In this paper, an empirical study is presented which investigates the suitability of a number of alternative representation formats for classification when interpretability is a key requirement. The formats under consideration are decision tables, (binary) decision trees, propositional rules, and oblique rules. An end-user experiment was designed to test the accuracy, response time, and answer confidence for a set of problem-solving tasks involving the former representations. Analysis of the results reveals that decision tables perform significantly better on all three criteria, while post-test voting also reveals a clear preference of users for decision tables in terms of ease of use. ?? 2010 Elsevier B.V. All rights reserved.},
	author = {Huysmans, Johan and Dejaeger, Karel and Mues, Christophe and Vanthienen, Jan and Baesens, Bart},
	doi = {10.1016/j.dss.2010.12.003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huysmans et al. - 2011 - An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models.pdf:pdf},
	isbn = {0167-9236},
	issn = {01679236},
	journal = {Decision Support Systems},
	keywords = {Classification,Comprehensibility,Data mining,Decision tables,Knowledge representation},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	number = {1},
	pages = {141--154},
	publisher = {Elsevier B.V.},
	title = {{An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models}},
	url = {http://dx.doi.org/10.1016/j.dss.2010.12.003},
	volume = {51},
	year = {2011}
}
@article{Ethayarajh2018,
	abstract = {A surprising property of word vectors is that vector algebra can often be used to solve word analogies. However, it is unclear why - and when - linear operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a rigorous explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has often conjectured that linear structures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel theoretical justification for the addition of SGNS word vectors by showing that it automatically down-scikit-learns the more frequent word, as scikit-learning schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, providing rigorous justification for its use in capturing word dissimilarity.},
	archivePrefix = {arXiv},
	arxivId = {1810.04882},
	author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
	eprint = {1810.04882},
	file = {:D$\backslash$:/Downloads/Play/1810.04882.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	title = {{Towards Understanding Linear Word Analogies}},
	url = {http://arxiv.org/abs/1810.04882},
	year = {2018}
}
@article{Sculley,
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Dennison, Dan},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/5656-hidden-technical-debt-in-machine-learning-systems.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Technical Debt},
	pages = {1--9},
	title = {{Hidden Technical Debt in Machine Learning Systems}}
}
@article{Ekstrand2014,
	author = {Ekstrand, Michael D and Harper, F Maxwell and Willemsen, Martijn C and Konstan, Joseph A},
	file = {:D$\backslash$:/PhD/Papedrs/listcmp.pdf:pdf},
	isbn = {9781450326681},
	keywords = {12,21,algorithms with comparable accuracy,movie recommendation domain,of,recommender systems,those differences in the,to map out some,user study,we present},
	mendeley-groups = {11Thesis/Interpretability/General},
	title = {{User Perception of Differences in Recommender Algorithms}},
	year = {2014}
}
@article{Mitchell2015,
	abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that per-formance on this task can be related to orthogonality within the space. Explic-itly designing such structure into a neu-ral network model results in represen-tations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within En-glish Wikipedia text to enable this de-composition can produce substantial im-provements on semantic-similarity, pos-induction and word-analogy tasks.},
	author = {Mitchell, Jeff and Steedman, Mark},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/semsynacl2015{\_}final.pdf:pdf},
	isbn = {9781941643723},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis/Interpretability/Visual},
	pages = {1301--1310},
	title = {{Orthogonality of Syntax and Semantics within Distributional Spaces}},
	url = {http://www.aclweb.org/anthology/P15-1126},
	year = {2015}
}
@article{Chang2009,
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
	doi = {10.1.1.100.1089},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Models.pdf:pdf},
	isbn = {9781615679119},
	issn = {1098-6596},
	journal = {Advances in Neural Information Processing Systems 22},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	pages = {288----296},
	pmid = {25246403},
	title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
	url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
	year = {2009}
}
@article{H.~Zou2006,
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
	archivePrefix = {arXiv},
	arxivId = {1205.0121v2},
	author = {H.{\~{}}Zou and T.{\~{}}Hastie and R.{\~{}}Tibshirani and Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	doi = {10.1198/106186006X113430},
	eprint = {1205.0121v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H.{\~{}}Zou et al. - 2006 - Sparse principal component analysis.pdf:pdf},
	isbn = {106186006X},
	issn = {1061-8600},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {arrays,ca 94305,composition,d student in the,department of statistics at,edu,elastic net,email,gene expression,gene expression arrays,hui zou is a,hzou,lasso,multivariate analysis,ph,singular,singular value de-,stanford,stanford university,stat,thresholding,value decomposition},
	mendeley-groups = {Annotated/NMF,11Thesis,!Paper 3},
	number = {2},
	pages = {265--286},
	pmid = {21811560},
	title = {{Sparse principal component analysis}},
	volume = {15},
	year = {2006}
}
@article{Bostrom2011,
	author = {Bostrom, Nick},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/artificialintelligence.pdf:pdf},
	keywords = {artificial intelligence, ethics},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	pages = {1--20},
	title = {{The Ethics of Artificial Intelligence}},
	year = {2011}
}
@article{Chen2016,
	abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	archivePrefix = {arXiv},
	arxivId = {1606.03657},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	eprint = {1606.03657},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
	issn = {10495258},
	mendeley-groups = {Annotated/Generative Adversarial Nets,Report,11Thesis},
	title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	url = {http://arxiv.org/abs/1606.03657},
	year = {2016}
}
@article{Beltagy2013,
	author = {Beltagy, Islam and Chau, Cuong and Boleda, Gemma and Garrette, Dan and Erk, Katrin},
	file = {:C$\backslash$:/Users/Workk/Documents/S13-1002.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	pages = {11--21},
	title = {{Montague Meets Markov : Deep Semantics with Probabilistic Logical Form}},
	volume = {1},
	year = {2013}
}
@article{Goodman2016,
	abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
	archivePrefix = {arXiv},
	arxivId = {1606.08813},
	author = {Goodman, Bryce and Flaxman, Seth},
	eprint = {1606.08813},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
	isbn = {978-0-674-36827-9},
	journal = {2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)},
	keywords = {machine learning},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	number = {Whi},
	pages = {26--30},
	title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
	url = {http://arxiv.org/abs/1606.08813},
	year = {2016}
}
@article{FenTan2016,
	abstract = {Ensembles of decision trees have good prediction accuracy but suffer from a lack of interpretability. We propose a new approach for interpreting tree ensembles by finding prototypes in tree space, utilizing the naturally-learned similarity measure from the tree ensemble. Demonstrating the method on random forests, we show that the method benefits from two unique aspects of tree ensembles by leveraging tree structure to sequentially find prototypes, and utilizing the naturally-learned similarity measure from the tree ensemble. The method provides good prediction accuracy when found prototypes are used in nearest-prototype classifiers, while us-ing fewer prototypes than competitor methods. We are investigating the sensitivity of the method to different prototype-finding procedures and demonstrating it on higher-dimensional data.},
	archivePrefix = {arXiv},
	arxivId = {1611.07115},
	author = {{Fen Tan}, Hui and Hooker, Giles J and Wells, Martin T},
	eprint = {1611.07115},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fen Tan, Hooker, Wells - 2016 - Tree Space Prototypes Another Look at Making Tree Ensembles Interpretable.pdf:pdf},
	journal = {Nips},
	mendeley-groups = {Annotated/Decision Trees,!Paper 3/task/newsgroups,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	title = {{Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable}},
	year = {2016}
}
@article{Freitas2013,
	abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
	author = {Freitas, Alex A.},
	doi = {10.1145/2594473.2594475},
	isbn = {1931-0145},
	issn = {19310145},
	journal = {ACM SIGKDD Explorations Newsletter},
	keywords = {bayesian network classifiers,decision table,decision tree,monotonicity constraint,nearest neighbors,rule induction},
	mendeley-groups = {Report/Just about interpretability,Annotated/Overarching Interpretability,Report,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {1},
	pages = {1--10},
	url = {http://dl.acm.org.miman.bib.bth.se/citation.cfm?id=2594475},
	volume = {15},
	year = {2013}
}

@INPROCEEDINGS{Arthur,
    author = {David Arthur and Sergei Vassilvitskii},
    title = {K-means++: the advantages of careful seeding},
    booktitle = {In Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms},
    year = {2007}
}
@article{Ananny2016,
	
	author = {Ananny, Mike and Crawford, Kate},
	doi = {10.1177/1461444816676645},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ananny, Crawford - 2016 - Seeing without knowing Limitations of the transparency ideal and its application to algorithmic accountability.pdf:pdf},
	issn = {1461-4448},
	journal = {New Media {\&} Society},
	keywords = {accountability,algorithms,critical infrastructure studies,platform governance},
	mendeley-groups = {!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
	pages = {146144481667664},
	title = {{Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability}},
	url = {http://journals.sagepub.com/doi/10.1177/1461444816676645},
	year = {2016}
}
@article{Kim2018,
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/kim18d.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	title = {{Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCAV )}},
	year = {2018}
}
@article{Doshi-Velez2017,
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	archivePrefix = {arXiv},
	arxivId = {1702.08608},
	author = {Doshi-Velez, Finale and Kim, Been},
	eprint = {1702.08608},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {Ml},
	pages = {1--13},
	title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	year = {2017}
}
@article{Le2014,
	abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
	archivePrefix = {arXiv},
	arxivId = {1405.4053},
	author = {Le, Qv and Mikolov, Tomas},
	doi = {10.1145/2740908.2742760},
	eprint = {1405.4053},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
	isbn = {9781634393973},
	issn = {10495258},
	journal = {International Conference on Machine Learning - ICML 2014},
	mendeley-groups = {Progress Report,Interim Review,11Thesis},
	pages = {1188--1196},
	pmid = {9377276},
	title = {{Distributed Representations of Sentences and Documents}},
	url = {http://arxiv.org/abs/1405.4053},
	volume = {32},
	year = {2014}
}
@article{Grgic-Hlaca2017,
	abstract = {Consider a binary decision making process where a single machine learning classifier replaces a multitude of humans. We raise questions about the resulting loss of diversity in the decision making process. We study the potential benefits of using random classifier ensembles instead of a single classifier in the context of fairness-aware learning and demonstrate various attractive properties: (i) an ensemble of fair classifiers is guaranteed to be fair, for several different measures of fairness, (ii) an ensemble of unfair classifiers can still achieve fair outcomes, and (iii) an ensemble of classifiers can achieve better accuracy-fairness trade-offs than a single classifier. Finally, we introduce notions of distributional fairness to characterize further potential benefits of random classifier ensembles.},
	archivePrefix = {arXiv},
	arxivId = {1706.10208},
	author = {Grgi{\'{c}}-Hla{\v{c}}a, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1706.10208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grgi{\'{c}}-Hla{\v{c}}a et al. - 2017 - On Fairness, Diversity and Randomness in Algorithmic Decision Making.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{On Fairness, Diversity and Randomness in Algorithmic Decision Making}},
	url = {http://arxiv.org/abs/1706.10208},
	year = {2017}
}
@article{Veale2017,
	abstract = {Machine learning systems are increasingly used to sup-port public sector decision-making across a variety of sectors. Given concerns around accountability in these domains, and amidst accusations of intentional or unintentional bias, there have been increased calls for transparency of these technologies. Few, however, have considered how logics and practices concerning transparency have been understood by those involved in the machine learning systems already being piloted and deployed in public bodies today. This short pa-per distils insights about transparency on the ground from interviews with 27 such actors, largely public ser-vants and relevant contractors, across 5 OECD coun-tries. Considering transparency and opacity in relation to trust and buy-in, better decision-making, and the avoidance of gaming, it seeks to provide useful insights for those hoping to develop socio-technical approaches to transparency that might be useful to practitioners on-the-ground.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.09249v2},
	author = {Veale, Michael},
	eprint = {arXiv:1706.09249v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale - 2017 - Logics and practices of transparency and opacity in real-world applications of public sector machine learning.pdf:pdf},
	keywords = {()},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{Logics and practices of transparency and opacity in real-world applications of public sector machine learning}},
	year = {2017}
}
@article{Gupta2015,
	author = {Gupta, Abhijeet and Boleda, Gemma and Baroni, Marco and Pad, Sebastian},
	file = {:C$\backslash$:/Users/Workk/Documents/EMNLP002.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {September},
	pages = {12--21},
	title = {{Distributional vectors encode referential attributes}},
	year = {2015}
}
@article{Bengio2012,
	archivePrefix = {arXiv},
	arxivId = {1206.5538},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	doi = {10.1109/TPAMI.2013.50},
	eprint = {1206.5538},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2012 - Representation Learning A Review and New Perspectives.pdf:pdf},
	isbn = {0162-8828 VO - 35},
	issn = {1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	mendeley-groups = {Annotated/Representation Learning,Report/Features,11Thesis},
	number = {8},
	pages = {1798--1828},
	pmid = {23787338},
	title = {{Representation Learning: A Review and New Perspectives}},
	volume = {35},
	year = {2012}
}
@article{Binns2017,
	abstract = {The internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and are subject to the idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic content moderation systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants.},
	archivePrefix = {arXiv},
	arxivId = {1707.01477},
	author = {Binns, Reuben and Veale, Michael and {Van Kleek}, Max and Shadbolt, Nigel},
	eprint = {1707.01477},
	isbn = {9783319672557},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Algorithmic accountability,Discussion platforms,Freedom of speech,Machine learning,Online abuse},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
	pages = {405--415},
	title = {{Like trainer, like bot? Inheritance of bias in algorithmic content moderation}},
	volume = {10540 LNCS},
	year = {2017}
}
@article{Sculley2010,
	author = {Sculley, D},
	file = {:E$\backslash$:/Downloads/Work/fastkmeans.pdf:pdf},
	isbn = {9781605587998},
	mendeley-groups = {11Thesis},
	pages = {4--5},
	title = {{Web-Scale K-Means Clustering}},
	year = {2010}
}
@article{Galloway1982,
	abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic similarity, pos-induction and word-analogy tasks.},
	author = {Galloway, Patricia},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/P15-1126.pdf:pdf},
	journal = {ALLC Journal},
	keywords = {*Diachronic Linguistics (di1),*French (fr2),*Poetry (pl2),*Statistical Analysis of Style (st3),5710: poetics/literary theory; poetics,article,cluster analysis, typology, Lai de l'Ombre manuscr},
	mendeley-groups = {11Thesis/Interpretability/Visual},
	number = {1},
	pages = {1--8},
	title = {{Clustering Variants in the Lai de l'Ombre Manuscripts: Techniques and Principles}},
	url = {http://search.proquest.com/docview/85463650?accountid=8330{\%}5Cnhttp://library.anu.edu.au:4550/resserv?genre=article{\&}issn={\&}title=ALLC+Journal{\&}volume=3{\&}issue=1{\&}date=1982-04-01{\&}atitle=Clustering+Variants+in+the+Lai+de+l'Ombre+Manuscripts:+Techniques+and+Princ},
	volume = {3},
	year = {1982}
}
@inproceedings{DBLP:conf/naacl/HillCK16,
	author    = {Felix Hill and
	Kyunghyun Cho and
	Anna Korhonen},
	title     = {Learning Distributed Representations of Sentences from Unlabelled
	Data},
	booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	pages     = {1367--1377},
	year      = {2016}
}

@inproceedings{DBLP:conf/icml/LeM14,
	author    = {Quoc V. Le and
	Tomas Mikolov},
	title     = {Distributed Representations of Sentences and Documents},
	booktitle = {Proceedings of the 31th International Conference on Machine Learning},
	pages     = {1188--1196},
	year      = {2014}
}

@inproceedings{labutov2013re,
	title={Re-embedding words},
	author={Labutov, Igor and Lipson, Hod},
	booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},
	pages={489--493},
	year={2013}
}

@inproceedings{yu2017refining,
	title={Refining word embeddings for sentiment analysis},
	author={Yu, Liang-Chih and Wang, Jin and Lai, K Robert and Zhang, Xuejie},
	booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	pages={534--539},
	year={2017}
}

@article{tang2016sentiment,
	title={Sentiment embeddings with applications to sentiment analysis},
	author={Tang, Duyu and Wei, Furu and Qin, Bing and Yang, Nan and Liu, Ting and Zhou, Ming},
	journal={IEEE Transactions on Knowledge and Data Engineering},
	volume={28},
	pages={496--509},
	year={2016}
}

@article{DBLP:conf/conll/HashimotoSMT15,
	added-at = {2015-04-09T00:00:00.000+0200},
	author = {Hashimoto, Kazuma and Stenetorp, Pontus and Miwa, Makoto and Tsuruoka, Yoshimasa},
	biburl = {https://puma.ub.uni-stuttgart.de/bibtex/24e6cf47cc7934f807c28ae9abf26712f/dblp},
	ee = {http://arxiv.org/abs/1503.00095},
	interhash = {4710a8b43af4ccaf4c909be5d31401c6},
	intrahash = {4e6cf47cc7934f807c28ae9abf26712f},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2016-02-02T09:48:16.000+0100},
	title = {Task-Oriented Learning of Word Embeddings for Semantic Relation Classification.},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1503.html#HashimotoSMT15},
	volume = {abs/1503.00095},
	year = 2015
}


@inproceedings{faruqui2015retrofitting,
	title={Retrofitting Word Vectors to Semantic Lexicons},
	author={Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay Kumar and Dyer, Chris and Hovy, Eduard and Smith, Noah A},
	booktitle={Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	pages={1606--1615},
	year={2015}
}

@article{HAHN1998197,
	author = "Ulrike Hahn and Nick Chater",
	title = "Similarity and rules: distinct? exhaustive? empirically distinguishable?",
	journal = "Cognition",
	volume = "65",
	pages = "197 - 230",
	year = "1998"
}

@book{gardenfors2004conceptual,
	title={Conceptual Spaces: The Geometry of Thought},
	author={G{\"a}rdenfors, Peter},
	year={2004},
	publisher={MIT press}
}

@article{tversky1977features,
	title={Features of similarity.},
	author={Tversky, Amos},
	journal={Psychological review},
	volume={84},
	pages={327-352},
	year={1977}
}

@inproceedings{kovashka2012whittlesearch,
abstract = {We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query “black shoes”, the user might state, “Show me shoe images like these, but sportier.” Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently “whittle away” irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient—both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.},
archivePrefix = {arXiv},
arxivId = {1505.04141},
author = {Kovashka, Adriana and Parikh, Devi and Grauman, Kristen},
doi = {10.1007/s11263-015-0814-0},
eprint = {1505.04141},
file = {:E$\backslash$:/WhittleSearch{\_}Image{\_}search{\_}with{\_}relative.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Active selection,Content-based image search,Interactive image search,Relative attributes},
mendeley-groups = {11Thesis/Applications},
number = {2},
pages = {185--210},
title = {{WhittleSearch: Interactive Image Search with Relative Attribute Feedback}},
volume = {115},
year = {2015}
}

@article{Vig:2012:TGE:2362394.2362395,
	Author = {Vig, Jesse and Sen, Shilad and Riedl, John},
	Date-Added = {2014-02-19 20:16:26 +0000},
	Date-Modified = {2014-02-19 20:17:08 +0000},
	Journal = {ACM Transactions on Interactive Intelligent Systems},
	Number = {3},
	Pages = {13:1--13:44},
	Title = {The Tag Genome: Encoding Community Knowledge to Support Novel Interaction},
	Volume = {2},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2362394.2362395},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/2362394.2362395}}


@article{viappiani2006preference,
abstract = {We consider interactive tools that help users search for their most preferred item in a large collection of options. In particular, we examine example-critiquing, a technique for enabling users to incrementally construct preference models by critiquing example options that are presented to them. We present novel techniques for improving the example-critiquing technology by adding suggestions to its displayed options. Such suggestions are calculated based on an analysis of users' current preference model and their potential hidden preferences. We evaluate the performance of our model-based suggestion techniques with both synthetic and real users. Results show that such suggestions are highly attractive to users and can stimulate them to express more preferences to improve the chance of identifying their most preferred item by up to 78{\%}.},
author = {Viappiani, P. and Faltings, B. and Pu, P.},
doi = {10.1613/jair.2075},
file = {:E$\backslash$:/Downloads/Work/10477-Article Text-19459-1-10-20180216.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
mendeley-groups = {11Thesis/Applications},
pages = {465--503},
title = {{Preference-based Search using Example-Critiquing with Suggestions}},
volume = {27},
year = {2006}
}

@article{derracAIJ,
	Author = {Derrac, J. and Schockaert, S.},
	journal = {Artificial Intelligence},
	Title = {Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning},
	Year = {2015},
	Pages = {74--105}}

@inproceedings{van2017structural,
	title={Structural Regularities in Text-based Entity Vector Spaces},
	author={Van Gysel, Christophe and de Rijke, Maarten and Kanoulas, Evangelos},
	booktitle={Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
	pages={3--10},
	year={2017}
}

@inproceedings{DBLP:conf/iccv/DemirelCI17,
	author    = {Berkan Demirel and
	Ramazan Gokberk Cinbis and
	Nazli Ikizler{-}Cinbis},
	title     = {Attributes2Classname: {A} Discriminative Model for Attribute-Based
	Unsupervised Zero-Shot Learning},
	booktitle = {{IEEE} International Conference on Computer Vision},
	pages     = {1241--1250},
	year      = {2017}
}


@inproceedings{DBLP:conf/sigir/JameelBS17,
           month = {August},
          author = {Shoaib Jameel and Zied Bouraoui and Steven Schockaert},
       booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '17},
           title = {MEmbER: Max-Margin Based Embeddings for Entity Retrieval},
       publisher = {ACM},
         journal = {SIGIR '17 Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
           pages = {783--792},
            year = {2017},
             url = {https://kar.kent.ac.uk/69598/},
        abstract = {We propose a new class of methods for learning vector space embeddings of entities. While most existing methods focus on modelling similarity, our primary aim is to learn embeddings that are interpretable, in the sense that query terms have a direct geometric representation in the vector space. Intuitively, we want all entities that have some property (i.e. for which a given term is relevant) to be located in some well-defined region of the space. This is achieved by imposing max-margin constraints that are derived from a bag-of-words representation of the entities. The resulting vector spaces provide us with a natural vehicle for identifying entities that have a given property (or ranking them according to how much they have the property), and conversely, to describe what a given set of entities have in common. As we show in our experiments, our models lead to a substantially better performance in a range of entity-oriented search tasks, such as list completion and entity ranking.}
}


@inproceedings{van2016learning,
	title={Learning latent vector spaces for product search},
	author={Van Gysel, Christophe and de Rijke, Maarten and Kanoulas, Evangelos},
	booktitle={Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	pages={165--174},
	year={2016}
}

@inproceedings{liang2016factorization,
	title={Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence},
	author={Liang, Dawen and Altosaar, Jaan and Charlin, Laurent and Blei, David M},
	booktitle={Proceedings of the 10th ACM Conference on Recommender Systems},
	pages={59--66},
	year={2016}
}

@inproceedings{Vasile:2016:MPE:2959100.2959160,
	author = {Vasile, Flavian and Smirnova, Elena and Conneau, Alexis},
	title = {Meta-Prod2Vec: Product Embeddings Using Side-Information for Recommendation},
	booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
	year = {2016},
	pages = {225--232},
} 

@INPROCEEDINGS{Bordes_translatingembeddings,
	author = {Antoine Bordes and Nicolas Usunier and Jason Weston and Oksana Yakhnenko},
	title = {Translating embeddings for modeling multi-relational data},
	booktitle = {In Advances in Neural Information Processing Systems 26. Curran Associates, Inc},
	year = {2013},
	pages = {2787--2795}
}

@inproceedings{gupta2015distributional,
	added-at = {2016-09-26T11:39:52.000+0200},
	author = {Gupta, Abhijeet and Boleda, Gemma and Baroni, Marco and Padó, Sebastian},
	biburl = {https://puma.ub.uni-stuttgart.de/bibtex/29875d27269f388b7e24e967bf27dbb2f/unibiblio},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	eventtitle = {EMNLP},
	interhash = {7830d4dbb8765dfeed0435a425df8c72},
	intrahash = {9875d27269f388b7e24e967bf27dbb2f},
	keywords = {hp unibibliografie},
	language = {eng},
	timestamp = {2016-09-26T11:39:52.000+0200},
	title = {Distributional vectors encode referential attributes},
	url = {http://www.aclweb.org/anthology/D/D15/D15-1002},
	venue = {Lisbon, Portugal},
	year = 2015
}

@inproceedings{kim2013deriving,
	added-at = {2013-11-22T00:00:00.000+0100},
	author = {Kim, Joo-Kyung and de Marneffe, Marie-Catherine},
	biburl = {https://puma.ub.uni-stuttgart.de/bibtex/230a9610d85156a8b47e30b6d6b168247/dblp},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	ee = {http://aclweb.org/anthology/D/D13/D13-1169.pdf},
	interhash = {f6049412c91665a1b4d15edbfe7230dd},
	intrahash = {30a9610d85156a8b47e30b6d6b168247},
	isbn = {978-1-937284-97-8},
	keywords = {dblp},
	pages = {1625-1630},
	publisher = {ACL},
	timestamp = {2016-02-02T17:20:48.000+0100},
	title = {Deriving Adjectival Scales from Continuous Space Word Representations.},
	url = {http://dblp.uni-trier.de/db/conf/emnlp/emnlp2013.html#KimM13},
	year = 2013
}


@inproceedings{DBLP:conf/acl/RotheS16,
	added-at = {2016-08-16T00:00:00.000+0200},
	author = {Rothe, Sascha and Sch\"utze, Hinrich},
	biburl = {https://puma.ub.uni-stuttgart.de/bibtex/26442130d76c70c7fdc924bd6700ba30a/dblp},
	booktitle = {ACL (2)},
	ee = {http://aclweb.org/anthology/P/P16/P16-2083.pdf},
	interhash = {5424545f5014a9b1fc3b7cf0561deba3},
	intrahash = {6442130d76c70c7fdc924bd6700ba30a},
	isbn = {978-1-945626-01-2},
	keywords = {dblp},
	publisher = {The Association for Computer Linguistics},
	timestamp = {2016-08-17T11:32:58.000+0200},
	title = {Word Embedding Calculus in Meaningful Ultradense Subspaces.},
	url = {http://dblp.uni-trier.de/db/conf/acl/acl2016-2.html#RotheS16},
	year = 2016
}

@inproceedings{teh2005sharing,
	author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
	title = {Sharing Clusters Among Related Groups: Hierarchical Dirichlet Processes},
	booktitle = {Proceedings of the 17th International Conference on Neural Information Processing Systems},
	series = {NIPS'04},
	year = {2004},
	location = {Vancouver, British Columbia, Canada},
	pages = {1385--1392},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=2976040.2976214},
	acmid = {2976214},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 



@inproceedings{wang2006topics,
	added-at = {2006-10-05T00:00:00.000+0200},
	author = {Wang, Xuerui and McCallum, Andrew},
	biburl = {https://puma.ub.uni-stuttgart.de/bibtex/28d46d3498d6ca7f7dc578cfc06785980/dblp},
	booktitle = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
	editor = {Eliassi-Rad, Tina and Ungar, Lyle H. and Craven, Mark and Gunopulos, Dimitrios},
	ee = {http://doi.acm.org/10.1145/1150402.1150450},
	interhash = {f8c70bdbc78681d8db009ab7247e4d3a},
	intrahash = {8d46d3498d6ca7f7dc578cfc06785980},
	isbn = {1-59593-339-5},
	keywords = {dblp},
	pages = {424-433},
	publisher = {ACM},
	timestamp = {2016-02-02T15:32:27.000+0100},
	title = {Topics over time: a non-Markov continuous-time model of topical trends.},
	url = {http://dblp.uni-trier.de/db/conf/kdd/kdd2006.html#WangM06},
	year = 2006
}




@inproceedings{rosen2004author,
	author = {Rosen-Zvi, Michal and Griffiths, Thomas and Steyvers, Mark and Smyth, Padhraic},
	title = {The Author-topic Model for Authors and Documents},
	booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
	series = {UAI '04},
	year = {2004},
	isbn = {0-9749039-0-6},
	location = {Banff, Canada},
	pages = {487--494},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=1036843.1036902},
	acmid = {1036902},
	publisher = {AUAI Press},
	address = {Arlington, Virginia, United States},
} 




@inproceedings{Blei2006,
	added-at = {2014-12-11T00:00:00.000+0100},
	author = {Blei, David M. and Lafferty, John D.},
	biburl = {https://puma.ub.uni-stuttgart.de/bibtex/21e1eff4f1d78abe9e3197e8c2ed10af0/dblp},
	booktitle = {Advances in Neural Information Processing Systems 18},
	ee = {http://papers.nips.cc/paper/2906-correlated-topic-models},
	interhash = {fe7d3a6937627b6d6d6a0da0d4c12cd2},
	intrahash = {1e1eff4f1d78abe9e3197e8c2ed10af0},
	keywords = {dblp},
	pages = {147-154},
	timestamp = {2016-02-02T16:02:29.000+0100},
	title = {Correlated Topic Models.},
	url = {http://dblp.uni-trier.de/db/conf/nips/nips2005.html#BleiL05},
	year = 2005
}


@article{ASI:ASI1,
	Author = {Deerwester, S. and Dumais, S. T. and Furnas, G. W. and Landauer, T. K. and Harshman, R.},    
	Journal = {Journal of the American Society for Information Science},
	Number = {6},
	Pages = {391--407},	
	Title = {Indexing by latent semantic analysis},	
	Volume = {41},
	Year = {1990}}


@InProceedings{glove2014,
	author = 	"Pennington, Jeffrey
	and Socher, Richard
	and Manning, Christopher",
	title = 	"Glove: Global Vectors for Word Representation",
	booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
	year = 	"2014",
	publisher = 	"Association for Computational Linguistics",
	pages = 	"1532--1543",
	location = 	"Doha, Qatar",
	doi = 	"10.3115/v1/D14-1162",
	url = 	"http://www.aclweb.org/anthology/D14-1162"
}




@inproceedings{DBLP:conf/nips/MikolovSCCD13,
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	title = {Distributed Representations of Words and Phrases and Their Compositionality},
	booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
	series = {NIPS'13},
	year = {2013},
	location = {Lake Tahoe, Nevada},
	pages = {3111--3119},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
	acmid = {2999959},
	publisher = {Curran Associates Inc.},
	address = {USA},
} 




@inproceedings{DBLP:conf/acl/DasZD15,
	author    = {Rajarshi Das and
	Manzil Zaheer and
	Chris Dyer},
	title     = {Gaussian {LDA} for Topic Models with Word Embeddings},
	booktitle = {Proc.\ ACL},
	pages     = {795--804},
	year      = {2015}
}

@inproceedings{DBLP:conf/aaai/LiuLCS15,
	author    = {Yang Liu and
	Zhiyuan Liu and
	Tat{-}Seng Chua and
	Maosong Sun},
	title     = {Topical Word Embeddings},
	booktitle = {Proc.\ AAAI},
	pages     = {2418--2424},
	year      = {2015}
}

@article{vrehuuvrek2011gensim,
	title={Gensim—statistical semantics in python},
	author={{\v{R}}eh{\uu}{\v{r}}ek, Radim and Sojka, Petr and others},
	journal={Retrieved from genism. org},
	year={2011}
}

@article{DBLP:journals/corr/DaiOL15,
	author    = {Andrew M. Dai and
	Christopher Olah and
	Quoc V. Le},
	title     = {Document Embedding with Paragraph Vectors},
	journal   = {CoRR},
	volume    = {abs/1507.07998},
	year      = {2015}
}

@article{jarvelin2002cumulated,
	title={Cumulated gain-based evaluation of {IR} techniques},
	author={J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
	journal={ACM Transactions on Information Systems},
	volume={20},
	number={4},
	pages={422--446},
	year={2002}
}


@inproceedings{DBLP:conf/ijcai/SchockaertL15,
	added-at = {2016-04-11T00:00:00.000+0200},
	author = {Schockaert, Steven and Lee, Jae Hee},
	biburl = {https://puma.ub.uni-stuttgart.de/bibtex/2e23fb1c6a6b2147fb24528a9d6527826/dblp},
	booktitle = {Proceedings of the Twenty-Fourth International Joint Conference on
	Artificial Intelligence},
	editor = {Yang, Qiang and Wooldridge, Michael},
	ee = {http://ijcai.org/Abstract/15/452},
	interhash = {442f102b1ff3b0a172e6b20c4bffa881},
	intrahash = {e23fb1c6a6b2147fb24528a9d6527826},
	isbn = {978-1-57735-738-4},
	keywords = {dblp},
	pages = {3207-3213},
	publisher = {AAAI Press},
	timestamp = {2016-07-21T11:34:20.000+0200},
	title = {Qualitative Reasoning about Directions in Semantic Spaces.},
	url = {http://dblp.uni-trier.de/db/conf/ijcai/ijcai2015.html#SchockaertL15},
	year = 2015
}



COPY+PASTING ALL REFERENCES AS A QUICK FIX TO MISSING ENTRIES:

@article{Kayande2009,
	abstract = {Model-based decision support systems (DSS) improve performance in many contexts that are data-rich, uncertain, and require repetitive decisions. But such DSS are often not designed to help users understand and internalize the underlying factors driving DSS recommendations. Users then feel uncertain about DSS recommendations, leading them to possibly avoid using the system. We argue that a DSS must be designed to induce an alignment of a decision maker's mental model with the decision model embedded in the DSS. Such an alignment requires effort from the decision maker and guidance from the DSS. We experimentally evaluate two DSS design characteristics that facilitate such alignment: (i) feedback on the upside potential for performance improvement and (ii) feedback on corrective actions to improve decisions. We show that, in tandem, these two types of DSS feedback induce decision makers to align their mental models with the decision model, a process we call deep learning, whereas individually these two types of feedback have little effect on deep learning. We also show that deep learning, in turn, improves user evaluations of the DSS. We discuss how our findings could lead to DSS design improvements and better returns on DSS investments. [ABSTRACT FROM AUTHOR] Copyright of Information Systems Research is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	author = {Kayande, Ujwal and {De Bruyn}, Arnaud and Lilien, Gary L. and Rangaswamy, Arvind and van Bruggen, Gerrit H.},
	doi = {10.1287/isre.1080.0198},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kayande et al. - 2009 - How incorporating feedback mechanisms in a DSS affects DSS evaluations.pdf:pdf},
	isbn = {10477047},
	issn = {10477047},
	journal = {Information Systems Research},
	keywords = {DSS design,Decision support systems,Evaluations,Feedback,Learning,Mental models},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {4},
	pages = {527--546},
	title = {{How incorporating feedback mechanisms in a DSS affects DSS evaluations}},
	volume = {20},
	year = {2009}
}

@article{Joulin2016,
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
	archivePrefix = {arXiv},
	arxivId = {1607.01759},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	doi = {1511.09249v1},
	eprint = {1607.01759},
	file = {:D$\backslash$:/Downloads/Work/1607.01759.pdf:pdf},
	isbn = {9781577357384},
	issn = {10450823},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	pmid = {1000303116},
	title = {{Bag of Tricks for Efficient Text Classification}},
	url = {http://arxiv.org/abs/1607.01759},
	year = {2016}
}
@article{Wallach2008,
	abstract = {This thesis introduces new methods for statistically modelling text using topic models. Topic models have seen many successes in recent years, and are used in a variety of applications, including analysis of news articles, topic-based search interfaces and navigation tools for digital libraries. Despite these recent successes, the field of topic modelling is still relatively new and there remains much to be explored. One notice- able absence from most of the previous work on topic modelling is consideration of language and document structurefrom low-level structures, including word order and syntax, to higher-level structures, such as relationships between documents. The focus of this thesis is therefore structured topic modelsmodels that combine latent topics with information about document structure, ranging from local sen- tence structure to inter-document relationships. These models draw on techniques from Bayesian statistics, including hierarchical Dirichlet distributions and processes, Pitman-Yor processes, and Markov chain Monte Carlo methods. Several methods for estimating the parameters of Dirichlet-multinomial distributions are also compared. The main contribution of this thesis is the introduction of three structured topic mod- els. The first is a topic-based language model. This model captures both word order and latent topics by extending a Bayesian topic model to incorporate n-gram statistics. A bigram version of the new model does better at predicting future words than either a topic model or a trigram language model. It also provides interpretable topics. The second model arises from a Bayesian reinterpretation of a classic generative de- pendency parsing model. The new model demonstrates that parsing performance can be substantially improved by a careful choice of prior and by sampling hyperparame- ters. Additionally, the generative nature of the model facilitates the inclusion of latent state variables, which act as specialised part-of-speech tags or syntactic topics. The third is a model that captures high-level relationships between documents. This model uses nonparametric Bayesian priors and Markov chain Monte Carlo methods to infer topic-based document clusters. The model assigns a higher probability to un- seen test documents than either a clustering model without topics or a Bayesian topic model without document clusters. The model can be extended to incorporate author information, resulting in finer-grained clusters and better predictive performance.},
	author = {Wallach, Hanna M},
	file = {:D$\backslash$:/Downloads/Work/1eefe0e5c69d6e14840e2d2b30f62a09a28b.pdf:pdf},
	journal = {Doctor},
	mendeley-groups = {Annotated/Topic models/Unsupervised Topic Models},
	number = {2001},
	pages = {136},
	title = {{Structured Topic Models for Language}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.2537{\&}rep=rep1{\&}type=pdf},
	year = {2008}
}
@article{Zhu2016,
	abstract = {Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference.},
	archivePrefix = {arXiv},
	arxivId = {1602.07428},
	author = {Zhu, Jun and Song, Jiaming and Chen, Bei},
	doi = {10.1.1.160.2072},
	eprint = {1602.07428},
	file = {:D$\backslash$:/Downloads/Work/1602.07428.pdf:pdf},
	isbn = {978-1-4503-1285-1},
	issn = {13669516},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	number = {1},
	title = {{Max-Margin Nonparametric Latent Feature Models for Link Prediction}},
	url = {http://arxiv.org/abs/1602.07428},
	volume = {6},
	year = {2016}
}
@article{Zhu2014,
	abstract = {Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max- margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the “augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time efficiency. The classification performance is also improved over competitors on binary, multi- class and multi-label classification tasks.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1310.2816v1},
	author = {Zhu, Jun and Chen, Ning and Perkins, Hugh and Zhang, Bo},
	eprint = {arXiv:1310.2816v1},
	file = {:D$\backslash$:/Downloads/Work/zhu14a.pdf:pdf},
	isbn = {1532-4435},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {Gibbs classifiers,max-margin learning,regularized Bayesian inference,supervised topic models,support vector machines},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {1073--1110},
	title = {{Gibbs max-margin topic models with data augmentation}},
	volume = {15},
	year = {2014}
}
@article{Blei2010,
	abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
	archivePrefix = {arXiv},
	arxivId = {1003.0783},
	author = {Blei, David M. and McAuliffe, Jon D.},
	doi = {10.1002/asmb.540},
	eprint = {1003.0783},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, McAuliffe - 2010 - Supervised Topic Models.pdf:pdf},
	isbn = {160560352X},
	issn = {15241904},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {1--8},
	title = {{Supervised Topic Models}},
	url = {http://arxiv.org/abs/1003.0783},
	year = {2010}
}
@article{Zhu2012,
	abstract = {A supervised topic model can use side information such as ratings or labels associated with doc- uments or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective func- tions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet alloca- tion (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) un- der a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or re- gression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, espe- cially for classification.},
	archivePrefix = {arXiv},
	arxivId = {0912.5507},
	author = {Zhu, Jun and Ahmed, a and Xing, Ep},
	doi = {10.1145/1553374.1553535},
	eprint = {0912.5507},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Ahmed, Xing - 2012 - MedLDA Maximum Margin Supervised Topic Models.pdf:pdf},
	isbn = {978-1-60558-516-1},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {latent dirichlet allocation,max-margin learning,maximum entropy discrimination,supervised topic models,support vector machines},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {2237--2278},
	title = {{MedLDA: Maximum Margin Supervised Topic Models}},
	url = {http://jmlr.csail.mit.edu/papers/volume13/zhu12a/zhu12a.pdf},
	volume = {13},
	year = {2012}
}
@article{Lacoste-Julien2008,
	abstract = {Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in finding  a reduced dimensionality representation.  Specifically, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood.  By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification task and show how our model can identify shared topics across classes as well as class-dependent topics.},
	author = {Lacoste-Julien, Simon and Sha, Fei and Jordan, Michael I.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lacoste-Julien, Sha, Jordan - 2008 - DiscLDA Discriminative Learning for Dimensionality Reduction and Classification.pdf:pdf},
	isbn = {9781605609492},
	journal = {Proc.\ NIPS},
	keywords = {Computational, Information-Theoretic Learning with,Information Retrieval {\&} Textual Information Access,Learning/Statistics {\&} Optimisation},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {1--8},
	title = {{DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification}},
	url = {http://eprints.pascal-network.org/archive/00004292/},
	volume = {21},
	year = {2008}
}
@article{Mcauley2013,
	author = {Mcauley, Julian},
	isbn = {9781450324090},
	journal = {RecSys '13 Proceedings of the 7th ACM conference on Recommender systems},
	keywords = {recommender systems,topic models},
	mendeley-groups = {Annotated/Datasets},
	pages = {165--172},
	title = {{Hidden Factors and Hidden Topics : Understanding Rating Dimensions with Review Text}},
	year = {2013}
}
@article{Maas2011,
	abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
	author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	doi = {978-1-932432-87-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:pdf},
	isbn = {9781932432879},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Datasets,!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {142--150},
	title = {{Learning Word Vectors for Sentiment Analysis}},
	year = {2011}
}
@article{Duchi2011,
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1103.4296v1},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	doi = {10.1109/CDC.2012.6426698},
	eprint = {arXiv:1103.4296v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
	isbn = {9780982252925},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
	mendeley-groups = {Annotated/Software},
	pages = {2121--2159},
	pmid = {2868127},
	title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	volume = {12},
	year = {2011}
}
@article{Valizadegan2009,
	abstract = {Learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels. The ranking algorithms are often evaluated using information retrieval measures, such as Normalized Discounted Cumulative Gain (NDCG) [1] and Mean Average Precision (MAP) [2]. Until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures. The main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function. We propose a probabilistic framework that addresses this challenge by optimizing the expectation of NDCG over all the possible permutations of documents. A relaxation strategy is used to approximate the average of NDCG over the space of permutation, and a bound optimization approach is proposed to make the computation efficient. Extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets. 1},
	author = {Valizadegan, Hamed and Jin, R},
	doi = {10.1561/1500000016},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Valizadegan, Jin - 2009 - Learning to rank by optimizing ndcg measure.pdf:pdf},
	isbn = {9781615679119},
	issn = {1554-0669},
	journal = {Advances in neural {\ldots}},
	mendeley-groups = {Annotated/Ranking},
	pages = {1--9},
	title = {{Learning to rank by optimizing ndcg measure}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2009{\_}0344.pdf},
	year = {2009}
}
@article{Pedregosa2012,
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	archivePrefix = {arXiv},
	arxivId = {1201.0490},
	author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
	doi = {10.1007/s13398-014-0173-7.2},
	eprint = {1201.0490},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedregosa et al. - 2012 - Scikit-learn Machine Learning in Python.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	mendeley-groups = {Annotated/Software},
	pages = {2825--2830},
	pmid = {1000044560},
	title = {{Scikit-learn: Machine Learning in Python}},
	url = {http://arxiv.org/abs/1201.0490},
	volume = {12},
	year = {2012}
}
@article{Lee1999,
	abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1408.1149},
	author = {Lee, D D and Seung, H S},
	doi = {10.1038/44565},
	eprint = {arXiv:1408.1149},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Seung - 1999 - Learning the parts of objects by non-negative matrix factorization.pdf:pdf},
	isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
	issn = {0028-0836},
	journal = {Nature},
	keywords = {Algorithms,Face,Humans,Learning,Models, Neurological,Perception,Perception: physiology,Semantics},
	mendeley-groups = {Annotated/NMF},
	number = {6755},
	pages = {788--91},
	pmid = {10548103},
	title = {{Learning the parts of objects by non-negative matrix factorization.}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10548103},
	volume = {401},
	year = {1999}
}
@article{Hoyer2004,
	abstract = {Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of `sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.},
	archivePrefix = {arXiv},
	arxivId = {cs/0408058},
	author = {Hoyer, Patrik O.},
	doi = {10.1109/ICMLC.2011.6016966},
	eprint = {0408058},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoyer - 2004 - Non-negative matrix factorization with sparseness constraints.pdf:pdf},
	isbn = {0780395174},
	issn = {1532-4435},
	keywords = {data-adaptive representations,non-negative matrix factorization,sparseness},
	mendeley-groups = {Annotated/NMF},
	pages = {1457--1469},
	pmid = {1000253614},
	primaryClass = {cs},
	title = {{Non-negative matrix factorization with sparseness constraints}},
	url = {http://arxiv.org/abs/cs/0408058},
	volume = {5},
	year = {2004}
}
@article{Xu2003,
	abstract = {In this paper, we propose a novel document clustering method based on the non-negative factorization of the term- document matrix of the given document corpus. In the la- tent semantic space derived by the non-negative matrix fac- torization (NMF), each axis captures the base topic of a par- ticular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value. Our experimental evalua- tions show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clus- tering methods not only in the easy and reliable derivation of document clustering results, but also in document clus- tering accuracies.},
	archivePrefix = {arXiv},
	arxivId = {1410.0993},
	author = {Xu, Wei and Liu, Xin and Gong, Yihong},
	doi = {10.1145/860484.860485},
	eprint = {1410.0993},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Liu, Gong - 2003 - Document clustering based on non-negative matrix factorization.pdf:pdf},
	isbn = {1581136463},
	issn = {01635840},
	journal = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval - SIGIR '03},
	keywords = {document clustering,non-negative matrix factorization},
	mendeley-groups = {Annotated/NMF},
	pages = {267},
	title = {{Document clustering based on non-negative matrix factorization}},
	url = {http://portal.acm.org/citation.cfm?doid=860435.860485},
	year = {2003}
}
@article{Donoho2004,
	abstract = {We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that un- der certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sam- pling. For such databases there is a generative model in terms of “parts” and NMF correctly identifies the ”parts”. We show that our theoretical results are predictive of the performance of publishedNMFcode, by run- ning the published algorithms on one of our synthetic image articulation databases.},
	archivePrefix = {arXiv},
	arxivId = {1512.00567},
	author = {Donoho, Dl and Stodden, Vc},
	doi = {10.1.1.85.8157},
	eprint = {1512.00567},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho, Stodden - 2004 - When does non-negative matrix factorization give a correct decomposition into parts.pdf:pdf},
	isbn = {9780262201520},
	issn = {09501991},
	journal = {Proc. Advances in Neural Information Processing Systems 16},
	mendeley-groups = {Annotated/NMF},
	pages = {1141--1148},
	pmid = {11585793},
	title = {{When does non-negative matrix factorization give a correct decomposition into parts?}},
	url = {http://academiccommons.columbia.edu/catalog/ac:140175},
	year = {2004}
}
@article{Zhao2017,
	author = {Zhao, Rui and Mao, Kezhi},
	doi = {10.1109/TFUZZ.2017.2690222},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Mao - 2017 - Fuzzy Bag-of-Words Model for Document Representation.pdf:pdf},
	issn = {1063-6706},
	journal = {IEEE Transactions on Fuzzy Systems},
	mendeley-groups = {Annotated/Document representation},
	number = {8},
	pages = {1--1},
	title = {{Fuzzy Bag-of-Words Model for Document Representation}},
	url = {http://ieeexplore.ieee.org/document/7891009/},
	volume = {14},
	year = {2017}
}
@article{Cao2015,
	author = {Cao, Tru and Lim, Ee Peng and Zhou, Zhi Hua and Ho, Tu Bao and Cheung, David and Motoda, Hiroshi},
	doi = {10.1007/978-3-319-18038-0},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2015 - Advances in knowledge discovery and data mining 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam,.pdf:pdf},
	isbn = {9783319180373},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Annotated/Document representation},
	pages = {212--225},
	title = {{Advances in knowledge discovery and data mining: 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam, May 19-22, 2015 proceedings, part I}},
	volume = {9077},
	year = {2015}
}
@article{Kamkarhaghighi2017,
	author = {Kamkarhaghighi, Mehran and Makrehchi, Masoud},
	doi = {10.1016/j.eswa.2017.08.021},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamkarhaghighi, Makrehchi - 2017 - Content Tree Word Embedding for document representation.pdf:pdf},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Word embedding,Content tree,Word2Vec,GloVe,Sentime},
	mendeley-groups = {Annotated/Document representation},
	pages = {241--249},
	publisher = {Elsevier Ltd},
	title = {{Content Tree Word Embedding for document representation}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417417305596},
	volume = {90},
	year = {2017}
}
@article{Tang2015,
	abstract = {Document level sentiment classification remains a challenge: encoding the intrin- sic relations between sentences in the se- mantic meaning of a document. To ad- dress this, we introduce a neural network model to learn vector-based document rep- resentation in a unified, bottom-up fash- ion. The model first learns sentence rep- resentation with convolutional neural net- work or long short-term memory. After- wards, semantics of sentences and their relations are adaptively encoded in docu- ment representation with gated recurren- t neural network. We conduct documen- t level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimen- tal results show that: (1) our neural mod- el shows superior performances over sev- eral state-of-the-art algorithms; (2) gat- ed recurrent neural network dramatically outperforms standard recurrent neural net- work in document modeling for sentiment classification},
	archivePrefix = {arXiv},
	arxivId = {1508.04025},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	doi = {10.18653/v1/D15-1167},
	eprint = {1508.04025},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:pdf},
	isbn = {9781941643327},
	issn = {10495258},
	journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	mendeley-groups = {Annotated/Document representation},
	number = {September},
	pages = {1422--1432},
	title = {{Document Modeling with Gated Recurrent Neural Network for Sentiment Classification}},
	url = {http://aclweb.org/anthology/D15-1167},
	year = {2015}
}
@article{Yang2016,
	abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	archivePrefix = {arXiv},
	arxivId = {1606.02393},
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	doi = {10.18653/v1/N16-1174},
	eprint = {1606.02393},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2016 - Hierarchical Attention Networks for Document Classification.pdf:pdf},
	isbn = {9781941643914},
	journal = {Proc.\ NAACL-HLT},
	mendeley-groups = {Annotated/Document representation},
	pages = {1480--1489},
	title = {{Hierarchical Attention Networks for Document Classification}},
	url = {http://aclweb.org/anthology/N16-1174},
	year = {2016}
}

@article{Sun2016,
	archivePrefix = {arXiv},
	arxivId = {1603.07603},
	author = {Sun, Fei and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Cheng, Xueqi},
	eprint = {1603.07603},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2016 - Semantic Regularities in Document Representations.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations},
	title = {{Semantic Regularities in Document Representations}},
	url = {http://arxiv.org/abs/1603.07603},
	year = {2016}
}

@article{Kiros2014,
	abstract = {In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2710v1},
	author = {Kiros, Ryan and Zemel, Rs and Salakhutdinov, Ruslan},
	eprint = {arXiv:1406.2710v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2014 - A Multiplicative Model for Learning Distributed Text-Based Attribute Representations.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {1--11},
	title = {{A Multiplicative Model for Learning Distributed Text-Based Attribute Representations}},
	url = {http://arxiv.org/abs/1406.2710},
	year = {2014}
}
@article{Arras2016,
	abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
	archivePrefix = {arXiv},
	arxivId = {1612.07843},
	author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
	eprint = {1612.07843},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2016 - {\&}quotWhat is Relevant in a Text Document{\&}quot An Interpretable Machine Learning Approach.pdf:pdf},
	isbn = {1111111111},
	mendeley-groups = {Annotated/Explanations},
	pages = {1--23},
	title = {{"What is Relevant in a Text Document?": An Interpretable Machine Learning Approach}},
	url = {http://arxiv.org/abs/1612.07843},
	year = {2016}
}
@article{Li2014,
	abstract = {Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the '' forced topic'' problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.},
	author = {Li, Ximing and Ouyang, Jihong and Lu, You and Zhou, Xiaotang and Tian, Tian},
	doi = {10.1007/s10791-014-9244-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - Group topic model organizing topics into groups.pdf:pdf},
	issn = {15737659},
	journal = {Information Retrieval},
	keywords = {Document clustering,Group,Latent Dirichlet allocation,Online learning,Topic modeling,Variational inference},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	number = {1},
	pages = {1--25},
	title = {{Group topic model: organizing topics into groups}},
	volume = {18},
	year = {2014}
}
@article{Xie2013,
	abstract = {Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets demonstrate the effectiveness of our model.},
	archivePrefix = {arXiv},
	arxivId = {1309.6874},
	author = {Xie, Pengtao and Xing, Eric P},
	eprint = {1309.6874},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Xing - 2013 - Integrating Document Clustering and Topic Modeling.pdf:pdf},
	journal = {Proceedings of the 29th conference on uncertainty in artificial intelligence},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {694--703},
	title = {{Integrating Document Clustering and Topic Modeling}},
	url = {http://www.cs.cmu.edu/{~}pengtaox/papers/uai2013paper.pdf},
	year = {2013}
}
@article{Shi2017,
	abstract = {Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such "two-step" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.},
	archivePrefix = {arXiv},
	arxivId = {1706.07276},
	author = {Shi, Bei and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Lai, Kwun Ping},
	doi = {10.1145/3077136.3080806},
	eprint = {1706.07276},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2017 - Jointly Learning Word Embeddings and Latent Topics.pdf:pdf},
	isbn = {9781450350228},
	keywords = {china,document modeling,e work described in,grant council of the,hong kong special administrative,project code,region,supported by grants from,the research,this paper is substantially,topic model,word embedding},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	title = {{Jointly Learning Word Embeddings and Latent Topics}},
	url = {http://arxiv.org/abs/1706.07276{\%}0Ahttp://dx.doi.org/10.1145/3077136.3080806},
	year = {2017}
}
@article{Bowman2015,
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	archivePrefix = {arXiv},
	arxivId = {1511.06349},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	eprint = {1511.06349},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:pdf},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	title = {{Generating Sentences from a Continuous Space}},
	url = {http://arxiv.org/abs/1511.06349},
	year = {2015}
}
@article{Arjovsky2017,
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	archivePrefix = {arXiv},
	arxivId = {1701.04862},
	author = {Arjovsky, Martin and Bottou, L{\'{e}}on},
	doi = {10.2507/daaam.scibook.2010.27},
	eprint = {1701.04862},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arjovsky, Bottou - 2017 - Towards Principled Methods for Training Generative Adversarial Networks.pdf:pdf},
	isbn = {1584880309},
	issn = {17269687},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	title = {{Towards Principled Methods for Training Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1701.04862},
	year = {2017}
}
@article{Hamilton2014,
	abstract = {The rise in prevalence of algorithmically curated feeds in online news and social media sites raises a new question for designers, critics, and scholars of media: how aware are users of the role of algorithms and filters in their news sources? This paper situates this problem within the history of design for interaction, with an emphasis on the contemporary challenges of studying, and designing for, the algorithmic "curation" of feeds. Such a problem presents particular challenges when, as is common, neither the user nor the researcher has access to the actual proprietary algorithms at work. Author},
	author = {Hamilton, Kevin and Karahalios, Karrie and Sandvig, Christian and Eslami, Motahhare},
	doi = {10.1145/2559206.2578883},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamilton et al. - 2014 - A path to understanding the effects of algorithm awareness.pdf:pdf},
	isbn = {9781450324748},
	journal = {Proceedings of the extended abstracts of the 32nd annual ACM conference on Human factors in computing systems - CHI EA '14},
	mendeley-groups = {Annotated/Psychology},
	pages = {631--642},
	title = {{A path to understanding the effects of algorithm awareness}},
	url = {http://dl.acm.org/citation.cfm?doid=2559206.2578883},
	year = {2014}
}
@book{Edwards2017,
	abstract = {ABSTRACT This article reflects the kinds of situations and spaces where people and algorithms meet. In what situations do people become aware of algorithms? How do they experience and make sense of these algorithms, given their often hidden and invisible nature? To what extent does an awareness of algorithms affect people's use of these platforms, if at all? To help answer these questions, this article examines people's personal stories about the Facebook algorithm through tweets and interviews with 25 ordinary users. To understand the spaces where people and algorithms meet, this article develops the notion of the algorithmic imaginary. It is argued that the algorithmic imaginary - ways of thinking about what algorithms are, what they should be and how they function - is not just productive of different moods and sensations but plays a generative role in moulding the Facebook algorithm itself. Examining how algorithms make people feel, then, seems crucial if we want to understand their social power.},
	author = {Edwards, Lilian and Veale, Michael},
	booktitle = {SSRN Electronic Journal},
	doi = {10.2139/ssrn.2972855},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards, Veale - 2017 - Slave to the Algorithm Why a Right to Explanationn is Probably Not the Remedy You are Looking for.pdf:pdf},
	isbn = {3540445668},
	issn = {1556-5068},
	mendeley-groups = {Annotated/Overarching Interpretability},
	pages = {1--65},
	title = {{Slave to the Algorithm? Why a Right to Explanationn is Probably Not the Remedy You are Looking for}},
	url = {https://www.ssrn.com/abstract=2972855},
	volume = {2017},
	year = {2017}
}
@article{Bastani2017,
	abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
	archivePrefix = {arXiv},
	arxivId = {1706.09773},
	author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
	eprint = {1706.09773},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim, Bastani - 2017 - Interpretability via Model Extraction.pdf:pdf},
	keywords = {()},
	mendeley-groups = {Annotated/Fairness},
	title = {{Interpretability via Model Extraction}},
	url = {http://arxiv.org/abs/1706.09773},
	year = {2017}
}
@article{Skirpan2017,
	abstract = {In this paper, we argue for the adoption of a normative definition of fairness within the machine learning community. After characterizing this definition, we review the current literature of Fair ML in light of its implications. We end by suggesting ways to incorporate a broader community and generate further debate around how to decide what is fair in ML.},
	archivePrefix = {arXiv},
	arxivId = {1706.09976},
	author = {Skirpan, Michael and Gorelick, Micha},
	eprint = {1706.09976},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skirpan, Gorelick - 2017 - The Authority of Fair in Machine Learning.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	title = {{The Authority of "Fair" in Machine Learning}},
	url = {http://arxiv.org/abs/1706.09976},
	year = {2017}
}
@article{Zafar2017,
	abstract = {The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness--given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design convex margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.},
	archivePrefix = {arXiv},
	arxivId = {1707.00010},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1707.00010},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf:pdf},
	journal = {arXiv:1707.00010 [cs, stat]},
	keywords = {Algorithmic fairness,Machine learning},
	mendeley-groups = {Annotated/Fairness},
	title = {{From Parity to Preference-based Notions of Fairness in Classification}},
	url = {http://arxiv.org/abs/1707.00010},
	year = {2017}
}
@article{Soneson2014,
	abstract = {BACKGROUND: With the large amount of biological data that is currently publicly available, many investigators combine multiple data sets to increase the sample size and potentially also the power of their analyses. However, technical differences ("batch effects") as well as differences in sample composition between the data sets may significantly affect the ability to draw generalizable conclusions from such studies.$\backslash$n$\backslash$nFOCUS: The current study focuses on the construction of classifiers, and the use of cross-validation to estimate their performance. In particular, we investigate the impact of batch effects and differences in sample composition between batches on the accuracy of the classification performance estimate obtained via cross-validation. The focus on estimation bias is a main difference compared to previous studies, which have mostly focused on the predictive performance and how it relates to the presence of batch effects.$\backslash$n$\backslash$nDATA: We work on simulated data sets. To have realistic intensity distributions, we use real gene expression data as the basis for our simulation. Random samples from this expression matrix are selected and assigned to group 1 (e.g., 'control') or group 2 (e.g., 'treated'). We introduce batch effects and select some features to be differentially expressed between the two groups. We consider several scenarios for our study, most importantly different levels of confounding between groups and batch effects.$\backslash$n$\backslash$nMETHODS: We focus on well-known classifiers: logistic regression, Support Vector Machines (SVM), k-nearest neighbors (kNN) and Random Forests (RF). Feature selection is performed with the Wilcoxon test or the lasso. Parameter tuning and feature selection, as well as the estimation of the prediction performance of each classifier, is performed within a nested cross-validation scheme. The estimated classification performance is then compared to what is obtained when applying the classifier to independent data.},
	author = {Soneson, Charlotte and Gerster, Sarah and Delorenzi, Mauro},
	doi = {10.1371/journal.pone.0100335},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Soneson, Gerster, Delorenzi - 2014 - Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation.pdf:pdf},
	isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Artifacts in the data},
	number = {6},
	pmid = {24967636},
	title = {{Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation}},
	volume = {9},
	year = {2014}
}
@article{Bach2015a,
	abstract = {Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally considered `black box' predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher Vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification, (3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data. All experiments are performed on the PASCAL VOC 2007 data set.},
	archivePrefix = {arXiv},
	arxivId = {1512.00172},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
	doi = {10.1109/CVPR.2016.318},
	eprint = {1512.00172},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - Analyzing Classifiers Fisher Vectors and Deep Neural Networks.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {10636919},
	mendeley-groups = {Annotated/Artifacts in the data},
	title = {{Analyzing Classifiers: Fisher Vectors and Deep Neural Networks}},
	url = {http://arxiv.org/abs/1512.00172},
	year = {2015}
}
@article{Zhang2016,
	abstract = {We present a novel subset scan method to detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup, and identify the characteristics of this subgroup. This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias or regions of poor classifier fit. This allows consideration of not just subgroups of a priori interest or small dimensions, but the space of all possible subgroups of features. To address the difficulty of considering these exponentially many possible subgroups, we use subset scan and parametric bootstrap-based methods. Extending this method, we can penalize the complexity of the detected subgroup and also identify subgroups with high classification errors. We demonstrate these methods and find interesting results on the COMPAS crime recidivism and credit delinquency data.},
	archivePrefix = {arXiv},
	arxivId = {1611.08292},
	author = {Zhang, Zhe and Neill, Daniel B.},
	eprint = {1611.08292},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Neill - 2016 - Identifying Significant Predictive Bias in Classifiers.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	number = {June},
	pages = {1--5},
	title = {{Identifying Significant Predictive Bias in Classifiers}},
	url = {http://arxiv.org/abs/1611.08292},
	year = {2016}
}
@article{Leek2014,
	author = {Leek, Jeffrey T},
	doi = {10.1038/nrg2825.Tackling},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leek - 2014 - NIH Public Access.pdf:pdf},
	mendeley-groups = {Annotated/Artifacts in the data},
	number = {10},
	pages = {1--15},
	title = {{NIH Public Access}},
	volume = {11},
	year = {2014}
}
@article{Blodgett2017,
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
	archivePrefix = {arXiv},
	arxivId = {1707.00061},
	author = {Blodgett, Su Lin and O'Connor, Brendan},
	eprint = {1707.00061},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blodgett, O'Connor - 2017 - Racial Disparity in Natural Language Processing A Case Study of Social Media African-American English.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	title = {{Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English}},
	url = {http://arxiv.org/abs/1707.00061},
	year = {2017}
}
@article{Bechavod2017,
	abstract = {We present a regularization-inspired approach for reducing bias in learned classifiers. In particular, we focus on binary classification tasks over individuals from two populations, where, as our criterion for fairness, we wish to achieve similar false positive rates in both populations, and similar false negative rates in both populations. As a proof of concept, we implement our approach and empirically evaluate its ability to achieve both fairness and accuracy, using the COMPAS scores data for prediction of recidivism.},
	archivePrefix = {arXiv},
	arxivId = {1707.00044},
	author = {Bechavod, Yahav and Ligett, Katrina},
	eprint = {1707.00044},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechavod, Ligett - 2017 - Learning Fair Classifiers A Regularization-Inspired Approach.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	pages = {6--10},
	title = {{Learning Fair Classifiers: A Regularization-Inspired Approach}},
	url = {http://arxiv.org/abs/1707.00044},
	year = {2017}
}
@article{Grgic-Hlaca2017,
	abstract = {Consider a binary decision making process where a single machine learning classifier replaces a multitude of humans. We raise questions about the resulting loss of diversity in the decision making process. We study the potential benefits of using random classifier ensembles instead of a single classifier in the context of fairness-aware learning and demonstrate various attractive properties: (i) an ensemble of fair classifiers is guaranteed to be fair, for several different measures of fairness, (ii) an ensemble of unfair classifiers can still achieve fair outcomes, and (iii) an ensemble of classifiers can achieve better accuracy-fairness trade-offs than a single classifier. Finally, we introduce notions of distributional fairness to characterize further potential benefits of random classifier ensembles.},
	archivePrefix = {arXiv},
	arxivId = {1706.10208},
	author = {Grgi{\'{c}}-Hla{\v{c}}a, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1706.10208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grgi{\'{c}}-Hla{\v{c}}a et al. - 2017 - On Fairness, Diversity and Randomness in Algorithmic Decision Making.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	title = {{On Fairness, Diversity and Randomness in Algorithmic Decision Making}},
	url = {http://arxiv.org/abs/1706.10208},
	year = {2017}
}
@article{Lakkaraju2017,
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	archivePrefix = {arXiv},
	arxivId = {1707.01154},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	eprint = {1707.01154},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju et al. - 2017 - Interpretable {\&} Explorable Approximations of Black Box Models.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	title = {{Interpretable {\&} Explorable Approximations of Black Box Models}},
	url = {http://arxiv.org/abs/1707.01154},
	year = {2017}
}

@article{Veale2017,
	author    = {Michael Veale},
	title     = {Logics and practices of transparency and opacity in real-world applications
	of public sector machine learning},
	journal   = {CoRR},
	volume    = {abs/1706.09249},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.09249},
	archivePrefix = {arXiv},
	eprint    = {1706.09249},
	timestamp = {Mon, 06 Nov 2017 12:14:00 +0100},
	biburl    = {http://dblp.org/rec/bib/journals/corr/Veale17},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{Than,
	author = {Than, Khoat and Ho, Tu Bao},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Than, Ho - Unknown - Fully Sparse Topic ModelsPKDD 2012.pdf.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations,Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {1--16},
	title = {{Fully Sparse Topic Models[PKDD 2012].pdf}}
}
@article{H.~Zou2006,
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
	archivePrefix = {arXiv},
	arxivId = {1205.0121v2},
	author = {H.{\~{}}Zou and T.{\~{}}Hastie and R.{\~{}}Tibshirani and Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	doi = {10.1198/106186006X113430},
	eprint = {1205.0121v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H.{\~{}}Zou et al. - 2006 - Sparse principal component analysis.pdf:pdf},
	isbn = {106186006X},
	issn = {1061-8600},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {arrays,ca 94305,composition,d student in the,department of statistics at,edu,elastic net,email,gene expression,gene expression arrays,hui zou is a,hzou,lasso,multivariate analysis,ph,singular,singular value de-,stanford,stanford university,stat,thresholding,value decomposition},
	mendeley-groups = {Annotated/NMF},
	number = {2},
	pages = {265--286},
	pmid = {21811560},
	title = {{Sparse principal component analysis}},
	volume = {15},
	year = {2006}
}
@article{Williamson2010,
	abstract = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric mixed membership model - each data point is modeled with a collection of components of different proportions. Though powerful, the HDP makes an assumption that the probability of a component being exhibited by a data point is positively correlated with its proportion within that data point. This might be an undesirable assumption. For example, in topic modeling, a topic (component) might be rare throughout the corpus but dominant within those documents (data points) where it occurs. We develop the IBP compound Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data prevalence and within-data proportion in a mixed membership model. The ICD combines properties from the HDP and the Indian buffet process (IBP), a Bayesian nonparametric prior on binary matrices. The ICD assigns a subset of the shared mixture components to each data point. This subset, the data point's ``focus'', is determined independently from the amount that each of its components contribute. We develop an ICD mixture model for text, the focused topic model (FTM), and show superior performance over the HDP-based topic model.},
	author = {Williamson, Sinead and Wang, Chong and Heller, Katherine A},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williamson, Wang, Heller - 2010 - The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling.pdf:pdf},
	isbn = {9781605589077},
	journal = {Icml},
	mendeley-groups = {Annotated/Interpretable representations},
	pages = {1151--1158},
	title = {{The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling}},
	url = {http://www.icml2010.org/papers/397.pdf},
	year = {2010}
}
@article{Greff2016,
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	archivePrefix = {arXiv},
	arxivId = {1503.04069},
	author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
	doi = {10.1109/TNNLS.2016.2582924},
	eprint = {1503.04069},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2016 - LSTM A Search Space Odyssey.pdf:pdf},
	isbn = {9788578110796},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	mendeley-groups = {Annotated/Representation Learning},
	pmid = {25246403},
	title = {{LSTM: A Search Space Odyssey}},
	year = {2016}
}
@article{Saaty2003,
	abstract = {In 1956, Miller [1] conjectured that there is an upper limit on our capacity to process information on simultaneously interacting elements with reliable accuracy and with validity. This limit is seven plus or minus two elements. He noted that the number 7 occurs in many aspects of life, from the seven wonders of the world to the seven seas and seven deadly sins. We demonstrate in this paper that in making preference judgments on pairs of elements in a group, as we do in the analytic hierarchy process (AHP), the number of elements in the group should be no more than seven. The reason is founded in the consistency of information derived from relations among the elements. When the number of elements increases past seven, the resulting increase in inconsistency is too small for the mind to single out the element that causes the greatest inconsistency to scrutinize and correct its relation to the other elements, and the result is confusion to the mind from the existing information. The AHP as a theory of measurement has a basic way to obtain a measure of inconsistency for any such set of pairwise judgments. When the number of elements is seven or less the inconsistency measurement is relatively large with respect to the number of elements involved; when the number is more it is relatively small. The most inconsistent judgment is easily determined in the first case and the individual providing the judgments can change it in an effort to improve the overall inconsistency. In the second case, as the inconsistency measurement is relatively small, improving inconsistency requires only small perturbations and the judge would be hard put to determine what that change should be, and how such a small change could be justified for improving the validity of the outcome. The mind is sufficiently sensitive to improve large inconsistencies but not small ones. And the implication of this is that the number of elements in a set should be limited to seven plus or minus two.},
	author = {Saaty, T.L. and Ozdemir, M.S.},
	doi = {10.1016/S0895-7177(03)90083-5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saaty, Ozdemir - 2003 - Why the magic number seven plus or minus two.pdf:pdf},
	isbn = {0895-7177},
	issn = {08957177},
	journal = {Mathematical and Computer Modelling},
	mendeley-groups = {Annotated/Psychology},
	number = {3},
	pages = {233--244},
	pmid = {8022966},
	title = {{Why the magic number seven plus or minus two}},
	volume = {38},
	year = {2003}
}
@article{Miller1956,
	abstract = {First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Miller, George A.},
	doi = {10.1037/h0043158},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1956 - The magical number seven, plus or minus two some limits on our capacity for processing information.pdf:pdf},
	isbn = {0198568770;},
	issn = {1939-1471},
	journal = {Psychological Review},
	mendeley-groups = {Annotated/Psychology},
	number = {2},
	pages = {81--97},
	pmid = {8022966},
	title = {{The magical number seven, plus or minus two: some limits on our capacity for processing information.}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0043158},
	volume = {63},
	year = {1956}
}
@article{Jiang2016,
	abstract = {This paper involves deriving high quality information from unstructured text data through the integration of rich document representations to improve machine learning text classification problems. Previous research has applied Neural Network Language Models (NNLMs) to document classification performance, and word vector representations have been used to measure semantics among text. Never have they been combined together and shown to have improved text classification performance. Our belief is that the inference and clustering abilities of word vectors coupled with the power of a neural network can create more accurate classification predictions. The first phase our work focused on word vector representations for classification purposes. This approach included analyzing two distinct text sources with pre-marked binary outcomes for classification, creating a benchmark metric, and comparing against word vector representations within the feature space as a classifier. The results showed promise, obtaining an area under the curve of 0.95 utilizing word vectors, relative to the benchmark case of 0.93. The second phase of the project focused on utilizing an extension of the neural network model used in phase one to represent a document in its entirety as opposed to being represented word by word. Preliminary results indicated a slight improvement over the baseline model of approximately 2-3 percent.},
	author = {Jiang, Suqi and Lewris, Jason and Voltmer, Michael and Wang, Hongning},
	doi = {10.1109/SIEDS.2016.7489319},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2016 - Integrating rich document representations for text classification.pdf:pdf},
	isbn = {9781509009701},
	journal = {2016 IEEE Systems and Information Engineering Design Symposium, SIEDS 2016},
	keywords = {Natural Language Processing,Text Classification,Text Mining,Word2vec},
	mendeley-groups = {Annotated/Document representation},
	pages = {303--308},
	title = {{Integrating rich document representations for text classification}},
	year = {2016}
}
@article{Arras2017,
	abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
	archivePrefix = {arXiv},
	arxivId = {1612.07843},
	author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
	doi = {10.1371/journal.pone.0181142},
	eprint = {1612.07843},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2017 - What is relevant in a text document An interpretable machine learning approach.pdf:pdf},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Explanations},
	number = {8},
	pages = {1--19},
	title = {{"What is relevant in a text document?": An interpretable machine learning approach}},
	volume = {12},
	year = {2017}
}
@article{Kim2017,
	abstract = {Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate scikit-learning scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others.},
	author = {Kim, Han Kyul and Kim, Hyunjoong and Cho, Sungzoon},
	doi = {10.1016/j.neucom.2017.05.046},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kim, Cho - 2017 - Bag-of-concepts Comprehending document representation through clustering words in distributed representation.pdf:pdf},
	issn = {18728286},
	journal = {Neurocomputing},
	keywords = {Bag-of-concepts,Interpretable document representation,Word2vec clustering},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {336--352},
	title = {{Bag-of-concepts: Comprehending document representation through clustering words in distributed representation}},
	volume = {266},
	year = {2017}
}
@article{Panchenko2016,
	abstract = {Word sense embeddings represent a word sense as a low-dimensional numeric vector. While this representation is potentially useful for NLP applications, its interpretability is inherently limited. We propose a simple technique that improves interpretability of sense vectors by mapping them to synsets of a lexical resource. Our experiments with AdaGram sense embeddings and BabelNet synsets showthat it is possible to retrieve synsets that correspond to automatically learned sense vectors with Precision of 0.87, Recall of 0.42 andAUC of 0.78.},
	author = {Panchenko, Alexander},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Panchenko - 2016 - Best of Both Worlds Making Word Sense Embeddings Interpretable.pdf:pdf},
	journal = {the 10th edition of the Language Resources and Evaluation Conference (LREC 2016)},
	keywords = {adagram,babelnet,lexical semantics,sense matching,word sense embeddings,wordnet},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {2649--2655},
	title = {{Best of Both Worlds: Making Word Sense Embeddings Interpretable}},
	url = {http://www.lrec-conf.org/proceedings/lrec2016/pdf/625{\_}Paper.pdf},
	year = {2016}
}
@article{Dou2013,
	abstract = {Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system--HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.},
	author = {Dou, Wenwen and Yu, Li and Wang, Xiaoyu and Ma, Zhiqiang and Ribarsky, William},
	doi = {10.1109/TVCG.2013.162},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dou et al. - 2013 - HierarchicalTopics Visually exploring large text collections using topic hierarchies.pdf:pdf},
	isbn = {1077-2626},
	issn = {10772626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Hierarchical topic representation,rose tree,topic modeling,visual analytics},
	mendeley-groups = {Annotated/Explanations},
	number = {12},
	pages = {2002--2011},
	pmid = {24051766},
	title = {{HierarchicalTopics: Visually exploring large text collections using topic hierarchies}},
	volume = {19},
	year = {2013}
}
@article{VanLinh2017,
	abstract = {{\textcopyright} 2016, Springer-Verlag London.As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises-Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental results showed the advantages of our approach in terms of separability, interpretability and effectiveness in classification task of datasets with high dimension and complex distribution. Our method is highly competitive with state-of-the-art approaches.},
	author = {{Van Linh}, Ngo and Anh, Nguyen Kim and Than, Khoat and Dang, Chien Nguyen},
	doi = {10.1007/s10115-016-0956-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Linh et al. - 2017 - An effective and interpretable method for document classification.pdf:pdf},
	issn = {02193116},
	journal = {Knowledge and Information Systems},
	keywords = {Bayesian nonparametrics,Classification,Variational inference,Von Mises-Fisher distribution},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	number = {3},
	pages = {763--793},
	title = {{An effective and interpretable method for document classification}},
	volume = {50},
	year = {2017}
}
@ARTICLE{Blei03latentdirichlet,
	author = {David M. Blei and Andrew Y. Ng and Michael I. Jordan and John Lafferty},
	title = {Latent dirichlet allocation},
	journal = {Journal of Machine Learning Research},
	year = {2003},
	volume = {3},
	pages = {2003}
}
@article{Miller2017,
	abstract = {In his seminal book The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity [2004, Sams Indianapolis, IN, USA], Alan Cooper ar-gues that a major reason why software is of-ten poorly designed (from a user perspective) is that programmers are in charge of design de-cisions, rather than interaction designers. As a result, programmers design software for them-selves, rather than for their target audience; a phenomenon he refers to as the 'inmates run-ning the asylum'. This paper argues that ex-plainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But ex-plainable AI is more likely to succeed if re-searchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science; and if evalu-ation of these models is focused more on people than on technology. From a light scan of litera-ture, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
	author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller, Howe, Sonenberg - 2017 - Explainable AI Beware of Inmates Running the Asylum.pdf:pdf},
	journal = {IJCAI - Workshop on Explainable AI},
	mendeley-groups = {Annotated/Overarching Interpretability},
	title = {{Explainable AI: Beware of Inmates Running the Asylum}},
	year = {2017}
}
@article{Hayes2017,
	abstract = {Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.},
	author = {Hayes, Bradley and Shah, Julie A.},
	doi = {10.1145/2909824.3020233},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hayes, Shah - 2017 - Improving Robot Controller Transparency Through Autonomous Policy Explanation.pdf:pdf},
	isbn = {9781450343367},
	issn = {21672148},
	journal = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17},
	mendeley-groups = {Annotated/Overarching Interpretability},
	pages = {303--312},
	title = {{Improving Robot Controller Transparency Through Autonomous Policy Explanation}},
	url = {http://dl.acm.org/citation.cfm?doid=2909824.3020233},
	year = {2017}
}
@article{Linegang2006,
	abstract = {The US Navy is funding the development of advanced automation systems to plan and execute unmanned vehicles missions, pushing towards a higher level of autonomy for automated planning systems. With effective systems, the human could play a role of mission manager and automation systems could perform mission planning and execution tasks with limited human involvement. Evaluations of the automation systems currently under development are identifying critical conflicts between human operator expectations and automated planning results. This paper presents a model of this human-automation interaction system and summarizes the resulting system design effort. This model provides a theory explaining the source of conflict between human and automation, and predicts that an ecological approach to display design would reduce that conflict. Based on that prediction, the paper describes initial results of an ecological approach to system analysis and design, intended to improve human-automation interaction for these types of advanced automation systems.},
	author = {Linegang, M. P. and Stoner, H. a. and Patterson, M. J. and Seppelt, B. D. and Hoffman, J. D. and Crittendon, Z. B. and Lee, John D.},
	doi = {10.1177/154193120605002304},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Linegang et al. - 2006 - Human-Automation Collaboration in Dynamic Mission Planning A Challenge Requiring an Ecological Approach.pdf:pdf},
	isbn = {10711813 (ISSN); 9780945289296 (ISBN)},
	issn = {1071-1813},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {23},
	pages = {2482--2486},
	title = {{Human-Automation Collaboration in Dynamic Mission Planning: A Challenge Requiring an Ecological Approach}},
	volume = {50},
	year = {2006}
}
@article{Chen2014c,
	abstract = {We have provided a model and framework as a foundation for transparent interfaces via our Situation Awareness-based Agent Transparency (SAT) model. In this report we discuss the implications of agent transparency for operator trust and workload; we also review potential user interface designs (information visualization and displaying uncertainty information) to support agent transparency. Finally, we provide examples of transparent interface design efforts currently ongoing at the U.S. Army Research Laboratory's Human Research and Engineering Directorate under the Autonomy Research Pilot Initiative.},
	author = {Chen, Jessie Y. C. and Procci, Katelyn and Boyce, Michael and Wright, Julia and Garcia, Andre and Barnes, Michael J.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Situation Awareness-Based Agent Transparency.pdf:pdf},
	isbn = {ARL-TR-6905},
	journal = {US Army Research Laboratory},
	keywords = {autonomous systems,human-robot interaction,situation awareness (SA),transparency,trust},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {April},
	pages = {1--29},
	title = {{Situation Awareness-Based Agent Transparency}},
	year = {2014}
}
@article{Stubbs2007,
	abstract = {The use of robots, especially autonomous mobile robots, to support work is expected to increase over the next few decades. However, little empirical research examines how users form mental models of robots, how they collaborate with them, and what factors contribute to the success or failure of human-robot collaboration. A two-year observational study of a collaborative human-robot system suggests that the factors disrupting the creation of common ground for interactive communication change at different levels of robot autonomy. Our observations of users collaborating with the remote robot showed differences in how the users reached common ground with the robot in terms of an accurate, shared understanding of the robot's context, planning, and actions - a process called grounding. We focus on how the types and levels of robot autonomy affect grounding. We also examine the challenges a highly autonomous system presents to people's ability to maintain a shared mental model of the robot},
	author = {Stubbs, Kristen and Wettergreen, David and Hinds, Pamela J.},
	doi = {10.1109/MIS.2007.21},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stubbs, Wettergreen, Hinds - 2007 - Autonomy and common ground in human-robot interaction A field study.pdf:pdf},
	isbn = {1541-1672 VO  - 22},
	issn = {15411672},
	journal = {IEEE Intelligent Systems},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {2},
	pages = {42--50},
	title = {{Autonomy and common ground in human-robot interaction: A field study}},
	volume = {22},
	year = {2007}
}
@article{Mercado2016,
	abstract = {OBJECTIVE We investigated the effects of level of agent transparency on operator performance, trust, and workload in a context of human-agent teaming for multirobot management. BACKGROUND Participants played the role of a heterogeneous unmanned vehicle (UxV) operator and were instructed to complete various missions by giving orders to UxVs through a computer interface. An intelligent agent (IA) assisted the participant by recommending two plans-a top recommendation and a secondary recommendation-for every mission. METHOD A within-subjects design with three levels of agent transparency was employed in the present experiment. There were eight missions in each of three experimental blocks, grouped by level of transparency. During each experimental block, the IA was incorrect three out of eight times due to external information (e.g., commander's intent and intelligence). Operator performance, trust, workload, and usability data were collected. RESULTS Results indicate that operator performance, trust, and perceived usability increased as a function of transparency level. Subjective and objective workload data indicate that participants' workload did not increase as a function of transparency. Furthermore, response time did not increase as a function of transparency. CONCLUSION Unlike previous research, which showed that increased transparency resulted in increased performance and trust calibration at the cost of greater workload and longer response time, our results support the benefits of transparency for performance effectiveness without additional costs. APPLICATION The current results will facilitate the implementation of IAs in military settings and will provide useful data to the design of heterogeneous UxV teams.},
	author = {Mercado, Joseph E. and Rupp, Michael A. and Chen, Jessie Y. C. and Barnes, Michael J. and Barber, Daniel and Procci, Katelyn},
	doi = {10.1177/0018720815621206},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mercado et al. - 2016 - Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management.pdf:pdf},
	issn = {0018-7208},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	keywords = {address correspondence to joseph,agent teaming,army,e,human,human research and engineering,intelligent agent transparency,mercado,multi-uxv management,research laboratory,s,u},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {3},
	pages = {401--415},
	pmid = {26867556},
	title = {{Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management}},
	url = {http://journals.sagepub.com/doi/10.1177/0018720815621206},
	volume = {58},
	year = {2016}
}
@article{Martens2011,
	abstract = {This paper proposes a complete framework to assess the overall performance of classification models from a user perspective in terms of accuracy, comprehensibility, and justifiability. A review is provided of accuracy and comprehensibility measures, and a novel metric is introduced that allows one to measure the justifiability of classification models. Furthermore, taxonomy of domain constraints is introduced, and an overview of the existing approaches to impose constraints and include domain knowledge in data mining techniques is presented. Finally, justifiability metric is applied to a credit scoring and customer churn prediction case. ?? 2011 Elsevier B.V. All rights reserved.},
	author = {Martens, David and Vanthienen, Jan and Verbeke, Wouter and Baesens, Bart},
	doi = {10.1016/j.dss.2011.01.013},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2011 - Performance of classification models from a user perspective.pdf:pdf},
	isbn = {0167-9236},
	issn = {01679236},
	journal = {Decision Support Systems},
	keywords = {Classification,Comprehensibility,Data mining,Justifiability,Metrics},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {4},
	pages = {782--793},
	publisher = {Elsevier B.V.},
	title = {{Performance of classification models from a user perspective}},
	url = {http://dx.doi.org/10.1016/j.dss.2011.01.013},
	volume = {51},
	year = {2011}
}
@article{Martens,
	author = {Martens, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens - Unknown - Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary.pdf:pdf},
	journal = {Knowledge Creation Diffusion Utilization},
	mendeley-groups = {Annotated/Applications/Financial Engineering},
	pages = {1--2},
	title = {{Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary}}
}
@article{Malioutov2013,
	abstract = {We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean 'sensing' matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting. We show competitive classification accuracy using the proposed approach. Copyright 2013 by the author(s).},
	author = {Malioutov, D M and Varshney, K R},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malioutov, Varshney - 2013 - Exact rule learning via Boolean compressed sensing.pdf:pdf},
	journal = {30th International Conference on Machine Learning, ICML 2013},
	keywords = {Classification accuracy; Exact recoveries; Group,Learning systems; Signal reconstruction,Recovery},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	number = {PART 3},
	pages = {1802--1810},
	title = {{Exact rule learning via Boolean compressed sensing}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84897531301{\&}partnerID=40{\&}md5=896c81fac3b62d2c5b6e9f3cf3a5a96d},
	year = {2013}
}
@article{Pazzani2000,
	author = {Pazzani, Michael J},
	doi = {10.1109/5254.850821},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pazzani - 2000 - Knowledge discovery from data.pdf:pdf},
	issn = {1094-7167},
	journal = {Intelligent systems and their applications, IEEE},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {2},
	pages = {10--12},
	title = {{Knowledge discovery from data?}},
	volume = {15},
	year = {2000}
}
@article{Haury2011,
	abstract = {Biomarker discovery from high-dimensional data is a crucial problem with enormous applications in biology and medicine. It is also extremely challenging from a statistical viewpoint, but surprisingly few studies have investigated the relative strengths and weaknesses of the plethora of existing feature selection methods. In this study we compare feature selection methods on public gene expression datasets for breast cancer prognosis, in terms of predictive performance, stability and functional interpretability of the signatures they produce. We observe that the feature selection method has a significant influence on the accuracy, stability and interpretability of signatures. Surprisingly, complex wrapper and embedded methods generally do not outperform simple univariate feature selection methods, and ensemble feature selection has generally no positive effect. Overall a simple Student's t-test seems to provide the best results.},
	archivePrefix = {arXiv},
	arxivId = {1101.5008},
	author = {Haury, Anne Claire and Gestraud, Pierre and Vert, Jean Philippe},
	doi = {10.1371/journal.pone.0028210},
	eprint = {1101.5008},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haury, Gestraud, Vert - 2011 - The influence of feature selection methods on accuracy, stability and interpretability of molecular signa.pdf:pdf},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Applications/Scientific Discovery},
	number = {12},
	pages = {1--12},
	pmid = {22205940},
	title = {{The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures}},
	volume = {6},
	year = {2011}
}
@article{Rivest1987,
	abstract = {This paper introduces a new representation for Boolean functions, called decision lists,  and shows that they are eciently learnable from examples. More precisely, this result  is established for $\backslash$k-DL" {\{} the set of decision lists with conjunctive clauses of size k at  each decision. Since k-DL properly includes other well-known techniques for representing  Boolean functions such as k-CNF (formulae in conjunctive normal form with at most k  literals per clause), k-DNF (formulae in disjunctive normal form with at most k literals  per term), and decision trees of depth k, our result strictly increases the set of functions  which are known to be polynomially learnable, in the sense of Valiant (1984). Our proof is  constructive: we present an algorithm which can eciently construct an element of k-DL  consistent with a given set of examples, if one exists.}},
	author = {Rivest, Ronald L.},
	doi = {10.1023/A:1022607331053},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivest - 1987 - Learning Decision Lists.pdf:pdf},
	issn = {15730565},
	journal = {Machine Learning},
	keywords = {Boolean formulae,Learning from examples,decision lists,polynomial-time identification},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	number = {3},
	pages = {229--246},
	title = {{Learning Decision Lists}},
	volume = {2},
	year = {1987}
}
@article{Tian2011,
	abstract = {We use the term "index predictor" to denote a score that consists of K binary rules such as "age {\textgreater} 60" or "blood pressure {\textgreater} 120 mm Hg." The index predictor is the sum of these binary scores, yielding a value from 0 to K. Such indices as often used in clinical studies to stratify population risk: They are usually derived from subject area considerations. In this paper, we propose a fast data-driven procedure for automatically constructing such indices for linear, logistic, and Cox regression models. We also extend the procedure to create indices for detecting treatment-marker interactions. The methods are illustrated on a study with protein biomarkers as well as a large microarray gene expression study.},
	author = {Tian, Lu and Tibshirani, Robert},
	doi = {10.1093/biostatistics/kxq047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Tibshirani - 2011 - Adaptive index models for marker-based risk stratification.pdf:pdf},
	isbn = {1468-4357 (Electronic)$\backslash$r1465-4644 (Linking)},
	issn = {14654644},
	journal = {Biostatistics},
	keywords = {Degree of freedom,Index predictor,International prognostic index},
	mendeley-groups = {Annotated/Applications/Medical},
	number = {1},
	pages = {68--86},
	pmid = {20663850},
	title = {{Adaptive index models for marker-based risk stratification}},
	volume = {12},
	year = {2011}
}
@article{Sun2006,
	author = {Sun, Hongmao},
	doi = {10.1002/cmdc.200500047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun - 2006 - An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability.pdf:pdf},
	mendeley-groups = {Annotated/Applications/Scientific Discovery},
	pages = {315--322},
	title = {{An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability}},
	volume = {07110},
	year = {2006}
}
@article{Verbeke2011,
	abstract = {Customer churn prediction models aim to detect customers with a high propensity to attrite. Predictive accuracy, comprehensibility, and justifiability are three key aspects of a churn prediction model. An accurate model permits to correctly target future churners in a retention marketing campaign, while a comprehensible and intuitive rule-set allows to identify the main drivers for customers to churn, and to develop an effective retention strategy in accordance with domain knowledge. This paper provides an extended overview of the literature on the use of data mining in customer churn prediction modeling. It is shown that only limited attention has been paid to the comprehensibility and the intuitiveness of churn prediction models. Therefore, two novel data mining techniques are applied to churn prediction modeling, and benchmarked to traditional rule induction techniques such as C4.5 and RIPPER. Both AntMiner+ and ALBA are shown to induce accurate as well as comprehensible classification rule-sets. AntMiner+ is a high performing data mining technique based on the principles of Ant Colony Optimization that allows to include domain knowledge by imposing monotonicity constraints on the final rule-set. ALBA on the other hand combines the high predictive accuracy of a non-linear support vector machine model with the comprehensibility of the rule-set format. The results of the benchmarking experiments show that ALBA improves learning of classification techniques, resulting in comprehensible models with increased performance. AntMiner+ results in accurate, comprehensible, but most importantly justifiable models, unlike the other modeling techniques included in this study. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
	author = {Verbeke, Wouter and Martens, David and Mues, Christophe and Baesens, Bart},
	doi = {10.1016/j.eswa.2010.08.023},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbeke et al. - 2011 - Building comprehensible customer churn prediction models with advanced rule induction techniques.pdf:pdf},
	isbn = {0957-4174},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {ALBA,Ant Colony Optimization,Churn prediction,Classification,Comprehensible rule induction,Data mining},
	mendeley-groups = {Annotated/Applications/Marketing},
	number = {3},
	pages = {2354--2364},
	publisher = {Elsevier Ltd},
	title = {{Building comprehensible customer churn prediction models with advanced rule induction techniques}},
	url = {http://dx.doi.org/10.1016/j.eswa.2010.08.023},
	volume = {38},
	year = {2011}
}
@article{Hauser2010,
	abstract = {The authors test methods, based on cognitively simple decision rules, that predict which products consumers select for their consideration sets. Drawing on qualitative research, the authors propose disjunctions-of- conjunctions (DOC) decision rules that generalize well-studied decision models, such as disjunctive, conjunctive, lexicographic, and subset conjunctive rules. They propose two machine-learning methods to estimate cognitively simple DOC rules. They observe consumers' consideration sets for global positioning systems for both calibration and validation data.They compare the proposed methods with both machine- learning and hierarchical Bayes methods, each based on five extant compensatory and noncompensatory rules. For the validation data, the cognitively simple DOC-based methods predict better than the ten benchmark methods on an information theoretic measure and on hit rates. The results are robust with respect to format by which consideration is measured, sample, and presentation of profiles. The article closes with an illustration of how DOC-based rules can affect managerial decisions.},
	author = {Hauser, John R and Toubia, Olivier and Evgeniou, Theodoros and Befurt, Rene and Dzyabura, Daria},
	doi = {10.1509/jmkr.47.3.485},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hauser et al. - 2010 - Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets.pdf:pdf},
	isbn = {00222437},
	issn = {0022-2437},
	journal = {Journal of Marketing Research},
	keywords = {cognitive,conjoint analysis,consideration sets,consumer,decision theory,disjunctions of conjunctions,heuristics,lexicography,machine learning,noncompensatory decisions,simplicity},
	mendeley-groups = {Annotated/Applications/Marketing},
	number = {3},
	pages = {485--496},
	pmid = {50522113},
	title = {{Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets}},
	volume = {47},
	year = {2010}
}
@article{Martens2007,
	abstract = {In recent years, support vector machines (SVMs) were successfully applied to a wide range of applications. However, since the classifier is described as a complex mathematical function, it is rather incomprehensible for humans. This opacity property prevents them from being used in many real-life applications where both accuracy and comprehensibility are required, such as medical diagnosis and credit risk evaluation. To overcome this limitation, rules can be extracted from the trained SVM that are interpretable by humans and keep as much of the accuracy of the SVM as possible. In this paper, we will provide an overview of the recently proposed rule extraction techniques for SVMs and introduce two others taken from the artificial neural networks domain, being Trepan and G-REX. The described techniques are compared using publicly available datasets, such as Ripley's synthetic dataset and the multi-class iris dataset. We will also look at medical diagnosis and credit scoring where comprehensibility is a key requirement and even a regulatory recommendation. Our experiments show that the SVM rule extraction techniques lose only a small percentage in performance compared to SVMs and therefore rank at the top of comprehensible classification techniques. ?? 2006 Elsevier B.V. All rights reserved.},
	author = {Martens, David and Baesens, Bart and {Van Gestel}, Tony and Vanthienen, Jan},
	doi = {10.1016/j.ejor.2006.04.051},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2007 - Comprehensible credit scoring models using rule extraction from support vector machines.pdf:pdf},
	isbn = {0377-2217},
	issn = {03772217},
	journal = {European Journal of Operational Research},
	keywords = {Classification,Credit scoring,Rule extraction,Support vector machine},
	mendeley-groups = {Annotated/Applications/Credit scoring},
	number = {3},
	pages = {1466--1476},
	title = {{Comprehensible credit scoring models using rule extraction from support vector machines}},
	volume = {183},
	year = {2007}
}
@article{Nogueira2011,
	author = {Nogueira, T.M. and Camargo, H.a. and Rezende, S.O.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Camargo, Rezende - 2011 - Fuzzy Rules for Document Classification to Improve Information Retrieval.pdf:pdf},
	journal = {Mirlabs.Org},
	keywords = {fuzzy clustering,imprecision,information retrieval,text categorization,text mining,uncertainty},
	mendeley-groups = {Annotated/Decision Trees},
	pages = {210--217},
	title = {{Fuzzy Rules for Document Classification to Improve Information Retrieval}},
	url = {http://www.mirlabs.org/ijcisim/regular{\_}papers{\_}2011/Paper25.pdf},
	volume = {3},
	year = {2011}
}
@article{Goodfellow2014,
	abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2661v1},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	doi = {10.1017/CBO9781139058452},
	eprint = {arXiv:1406.2661v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
	isbn = {1406.2661},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems 27},
	mendeley-groups = {Annotated/Representation Learning,Annotated/Generative Adversarial Nets},
	pages = {2672--2680},
	pmid = {1000183096},
	title = {{Generative Adversarial Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	year = {2014}
}
@article{Korde2012,
	abstract = {As most information (over 80{\%}) is stored as text, text mining is believed to have a high commercial potential value. knowledge may be discovered from many sources of information; yet, unstructured texts remain the largest readily available source of knowledge .Text classification which classifies the documents according to predefined categories .In this paper we are tried to give the introduction of text classification, process of text classification as well as the overview of the classifiers and tried to compare the some existing classifier on basis of few criteria like time complexity, principal and performance.},
	author = {Korde, Vandana and Mahender, C Namrata},
	doi = {10.5121/ijaia.2012.3208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korde, Mahender - 2012 - Text Classification and Classifiers A Survey.pdf:pdf},
	issn = {09762191},
	journal = {International Journal of Artificial Intelligence {\&} Applications},
	keywords = {classifiers,text classification,text representation},
	mendeley-groups = {Annotated/Decision Trees},
	number = {2},
	pages = {85--99},
	title = {{Text Classification and Classifiers: A Survey}},
	url = {http://www.airccse.org/journal/ijaia/papers/3212ijaia08.pdf},
	volume = {3},
	year = {2012}
}
@article{Apte1994,
	abstract = {We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67{\%} recall/precision breakeven point to 80.5{\%}. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.},
	author = {Apt{\'{e}}, Chidanand and Damerau, Fred and Weiss, Sholom M.},
	doi = {10.1145/183422.183423},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Apt{\'{e}}, Damerau, Weiss - 1994 - Automated learning of decision rules for text categorization.pdf:pdf},
	isbn = {1046-8188},
	issn = {1046-8188},
	journal = {ACM Trans. Inf. Syst.},
	mendeley-groups = {Annotated/Decision Trees},
	number = {3},
	pages = {233--251},
	title = {{Automated learning of decision rules for text categorization}},
	url = {http://portal.acm.org/citation.cfm?id=183423{\&}dl=},
	volume = {12},
	year = {1994}
}
@article{Siddharth2016,
	abstract = {We develop a framework for incorporating structured graphical models in the $\backslash$emph{\{}encoders{\}} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes.},
	archivePrefix = {arXiv},
	arxivId = {1611.07492},
	author = {Siddharth, N. and Paige, Brooks and Desmaison, Alban and {Van de Meent}, Jan-Willem and Wood, Frank and Goodman, Noah D. and Kohli, Pushmeet and Torr, Philip H. S.},
	eprint = {1611.07492},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siddharth et al. - 2016 - Inducing Interpretable Representations with Variational Autoencoders.pdf:pdf},
	journal = {arXiv preprint},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	number = {Nips 2016},
	title = {{Inducing Interpretable Representations with Variational Autoencoders}},
	url = {http://arxiv.org/abs/1611.07492},
	year = {2016}
}
@article{FenTan2016,
	abstract = {Ensembles of decision trees have good prediction accuracy but suffer from a lack of interpretability. We propose a new approach for interpreting tree ensembles by finding prototypes in tree space, utilizing the naturally-learned similarity measure from the tree ensemble. Demonstrating the method on random forests, we show that the method benefits from two unique aspects of tree ensembles by leveraging tree structure to sequentially find prototypes, and utilizing the naturally-learned similarity measure from the tree ensemble. The method provides good prediction accuracy when found prototypes are used in nearest-prototype classifiers, while us-ing fewer prototypes than competitor methods. We are investigating the sensitivity of the method to different prototype-finding procedures and demonstrating it on higher-dimensional data.},
	archivePrefix = {arXiv},
	arxivId = {1611.07115},
	author = {{Fen Tan}, Hui and Hooker, Giles J and Wells, Martin T},
	eprint = {1611.07115},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fen Tan, Hooker, Wells - 2016 - Tree Space Prototypes Another Look at Making Tree Ensembles Interpretable.pdf:pdf},
	journal = {Nips},
	mendeley-groups = {Annotated/Decision Trees},
	title = {{Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable}},
	year = {2016}
}
@article{Thiagarajan2016,
	abstract = {With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior [1]. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted.},
	archivePrefix = {arXiv},
	arxivId = {1611.07429},
	author = {Thiagarajan, Jayaraman J. and Kailkhura, Bhavya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan},
	eprint = {1611.07429},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiagarajan et al. - 2016 - TreeView Peeking into Deep Neural Networks Via Feature-Space Partitioning.pdf:pdf},
	journal = {30th Conference on Neural Information Processing Systems (NIPS 2016)},
	mendeley-groups = {Annotated/Explanations,Annotated/Decision Trees},
	number = {Nips},
	title = {{TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning}},
	url = {http://arxiv.org/abs/1611.07429},
	year = {2016}
}
@article{Kumar2015,
	abstract = {{\textless}p{\textgreater} A framework for automated detection and classification of cancer from microscopic biopsy images using clinically significant and biologically interpretable features is proposed and examined. The various stages involved in the proposed methodology include enhancement of microscopic images, segmentation of background cells, features extraction, and finally the classification. An appropriate and efficient method is employed in each of the design steps of the proposed framework after making a comparative analysis of commonly used method in each category. For highlighting the details of the tissue and structures, the contrast limited adaptive histogram equalization approach is used. For the segmentation of background cells, {\textless}math id="M1"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}k{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -means segmentation algorithm is used because it performs better in comparison to other commonly used segmentation methods. In feature extraction phase, it is proposed to extract various biologically interpretable and clinically significant shapes as well as morphology based features from the segmented images. These include gray level texture features, color based features, color gray level texture features, Law's Texture Energy based features, Tamura's features, and wavelet features. Finally, the {\textless}math id="M2"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}K{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -nearest neighborhood method is used for classification of images into normal and cancerous categories because it is performing better in comparison to other commonly used methods for this application. The performance of the proposed framework is evaluated using well-known parameters for four fundamental tissues (connective, epithelial, muscular, and nervous) of randomly selected 1000 microscopic biopsy images. {\textless}/p{\textgreater}},
	author = {Kumar, Rajesh and Srivastava, Rajeev and Srivastava, Subodh},
	doi = {10.1155/2015/457906},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Srivastava, Srivastava - 2015 - Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significan.pdf:pdf},
	issn = {2314-5129, 2314-5137},
	journal = {Journal of Medical Engineering},
	mendeley-groups = {Annotated/Applications/Medical},
	number = {2015},
	pages = {1--14},
	pmid = {21329180},
	title = {{Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significant and Biologically Interpretable Features}},
	url = {http://www.hindawi.com/journals/jme/2015/457906/},
	volume = {2015},
	year = {2015}
}
@article{Ustun2014,
	abstract = {We present an integer programming framework to build accurate and interpretable discrete linear classification models. Unlike existing approaches, our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice. To this end, our framework can produce models that are fully optimized for accuracy, by minimizing the 0--1 classification loss, and that address multiple aspects of interpretability, by incorporating a range of discrete constraints and penalty functions. We use our framework to produce models that are difficult to create with existing methods, such as scoring systems and M-of-N rule tables. In addition, we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction. We show that discrete linear classifiers can attain the training accuracy of any other linear classifier, and provide an Occam's Razor type argument as to why the use of small discrete coefficients can provide better generalization. We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis.},
	archivePrefix = {arXiv},
	arxivId = {1405.4047},
	author = {Ustun, Berk and Rudin, Cynthia},
	eprint = {1405.4047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ustun, Rudin - 2014 - Methods and Models for Interpretable Linear Classification.pdf:pdf},
	journal = {arXiv},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	pages = {1--57},
	title = {{Methods and Models for Interpretable Linear Classification}},
	url = {http://arxiv.org/abs/1405.4047},
	year = {2014}
}
@article{Goodman2016,
	abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
	archivePrefix = {arXiv},
	arxivId = {1606.08813},
	author = {Goodman, Bryce and Flaxman, Seth},
	eprint = {1606.08813},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
	isbn = {978-0-674-36827-9},
	journal = {Proc.\ Workshop on Human Interpretability in Machine Learning},
	keywords = {machine learning},
	mendeley-groups = {Annotated/Overarching Interpretability},
	pages = {26--30},
	title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
	url = {http://arxiv.org/abs/1606.08813},
	year = {2016}
}
@article{Barocas2016,
	abstract = {Big data claims to be neutral. It isn't.Advocates of algorithmic techniques like data mining argue that they eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data mining can inherit the prejudices of prior decision-makers or reflect the widespread biases that persist in society at large. Often, the “patterns” it discovers are simply preexisting societal patterns of inequality and exclusion. Unthinking reliance on data mining can deny members of vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm's use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Article examines these concerns through the lens of American anti-discrimination law — more particularly, through Title VII's prohibition on discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining's victims would seem to lie in disparate impact doctrine. Case law and the EEOC's Uniform Guidelines, though, hold that a practice can be justified as a business necessity where its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. As a result, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others' discrimination against members of vulnerable groups, or flaws in the underlying data.Addressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, where the discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying anti-discrimination law: nondiscrimination and anti-subordination. Finding a solution to big data's disparate impact will require more than best efforts to stamp out prejudice and bias; it will require wholesale reexamination of the meanings of “discrimination” and “fairness.”},
	author = {Barocas, Solon and Selbst, Andrew},
	doi = {http://dx.doi.org/10.15779/Z38BG31},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barocas, Selbst - 2016 - Big Data ' s Disparate Impact.pdf:pdf},
	issn = {9780262327343},
	journal = {California law review},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {1},
	pages = {671--729},
	title = {{Big Data ' s Disparate Impact}},
	url = {https://ssrn.com/abstract=2477899},
	volume = {104},
	year = {2016}
}
@article{Letham2010,
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. We introduce a generative model called the Bayesian List Machine for fitting decision lists, a type of interpretable classifier, to data. We use the model to predict stroke in atrial fibrillation patients, and produce predictive models that are simple enough to be understood by patients yet significantly outperform the medical scoring systems currently in use.},
	author = {Letham, Benjamin and Rudin, Cynthia and Mccormick, Tyler H and Madigan, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2010 - An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis.pdf:pdf},
	isbn = {9781577356288},
	journal = {AAAI Technical Report WS-13-17},
	keywords = {AAAI Technical Report WS-13-17},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {609},
	pages = {65--67},
	title = {{An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis}},
	year = {2010}
}
@article{Grosenick2008,
	abstract = {{\textless}para{\textgreater} Despite growing interest in applying machine learning to neuroimaging analyses, few studies have gone beyond classifying sensory input to directly predicting behavioral output. With spatial resolution on the order of millimeters and temporal r...},
	author = {Grosenick, Logan and Greer, Stephanie and Knutson, Brian},
	doi = {10.1109/TNSRE.2008.926701},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grosenick, Greer, Knutson - 2008 - Interpretable Classifiers for fMRI Improve Prediction of Purchases.pdf:pdf},
	isbn = {1558-0210 (Electronic)},
	issn = {15580210},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	keywords = {Accumbens,classification,discriminant,elastic net,frontal,functional magnetic resonance imaging (fMRI),human,insula,lasso,penalized discriminant analysis (PDA),prediction,purchasing,single-trial,sparse,spatiotemporal,support vector machine (SVM)},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {6},
	pages = {539--548},
	pmid = {19144586},
	title = {{Interpretable Classifiers for fMRI Improve Prediction of Purchases}},
	volume = {16},
	year = {2008}
}
@article{Garc??a2009,
	abstract = {Classification in imbalanced domains is a recent challenge in data mining. We refer to imbalanced classification when data presents many examples from one class and few from the other class, and the less representative class is the one which has more interest from the point of view of the learning task. One of the most used techniques to tackle this problem consists in preprocessing the data previously to the learning process. This preprocessing could be done through under-sampling; removing examples, mainly belonging to the majority class; and over-sampling, by means of replicating or generating new minority examples. In this paper, we propose an under-sampling procedure guided by evolutionary algorithms to perform a training set selection for enhancing the decision trees obtained by the C4.5 algorithm and the rule sets obtained by PART rule induction algorithm. The proposal has been compared with other under-sampling and over-sampling techniques and the results indicate that the new approach is very competitive in terms of accuracy when comparing with over-sampling and it outperforms standard under-sampling. Moreover, the obtained models are smaller in terms of number of leaves or rules generated and they can considered more interpretable. The results have been contrasted through non-parametric statistical tests over multiple data sets. Crown Copyright ?? 2009.},
	author = {Garc??a, Salvador and Fern??ndez, Alberto and Herrera, Francisco},
	doi = {10.1016/j.asoc.2009.04.004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garca, Fernndez, Herrera - 2009 - Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with.pdf:pdf},
	isbn = {15684946},
	issn = {15684946},
	journal = {Applied Soft Computing Journal},
	keywords = {Data reduction,Decision trees,Evolutionary algorithms,Imbalanced classification,Rule induction,Training set selection},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {4},
	pages = {1304--1314},
	title = {{Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with evolutionary training set selection over imbalanced problems}},
	volume = {9},
	year = {2009}
}
@article{Taddy2015,
	abstract = {There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.},
	archivePrefix = {arXiv},
	arxivId = {1504.07295},
	author = {Taddy, Matt},
	eprint = {1504.07295},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taddy - 2015 - Document Classification by Inversion of Distributed Language Representations.pdf:pdf},
	isbn = {9781941643730},
	journal = {Proceedings of the 53rd meeting of the Association for Computational Linquistics (ACL'15)},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {45--49},
	title = {{Document Classification by Inversion of Distributed Language Representations}},
	url = {http://arxiv.org/abs/1504.07295},
	year = {2015}
}
@article{Kim2015,
	abstract = {We present the Mind the Gap Model (MGM), an approach for interpretable fea- ture extraction and selection. By placing interpretability criteria directly into the model, we allowfor the model to both optimize parameters related to interpretabil- ity and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and dis- ease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.},
	author = {Kim, Been and Shah, Julie and Doshi-Velez, Finale},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Shah, Doshi-Velez - 2015 - Mind the Gap A Generative Approach to Interpretable Feature Selection and Extraction.pdf:pdf},
	issn = {10495258},
	journal = {Nips},
	mendeley-groups = {Annotated/Interpretable representations},
	pages = {1--9},
	title = {{Mind the Gap : A Generative Approach to Interpretable Feature Selection and Extraction}},
	year = {2015}
}
@article{Li2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1503.00185v5},
	author = {Li, Jiwei and Luong, Minh-thang and Jurafsky, Dan and Hovy, Eduard},
	eprint = {arXiv:1503.00185v5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - When Are Tree Structures Necessary for Deep Learning of.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	title = {{When Are Tree Structures Necessary for Deep Learning of}},
	year = {2014}
}
@article{Ai2016,
	author = {Ai, Qingyao and Yang, Liu and Guo, Jiafeng and Croft, W Bruce},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ai et al. - 2016 - Analysis of the Paragraph Vector Model for Information Retrieval.pdf:pdf},
	isbn = {9781450344975},
	keywords = {language model,paragraph vector},
	mendeley-groups = {Annotated/Document representation},
	title = {{Analysis of the Paragraph Vector Model for Information Retrieval}},
	year = {2016}
}
@article{Boz,
	author = {Boz, Olcay and Ave, Laurel},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boz, Ave - Unknown - Decision Tree DecText - Decision Tree Extractor.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees},
	pages = {1--7},
	title = {{Decision Tree DecText - Decision Tree Extractor}}
}
@article{Bastani,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1705.08504v1},
	author = {Bastani, Osbert and Kim, Carolyn},
	eprint = {arXiv:1705.08504v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim - Unknown - Interpreting Blackbox Models via Model Extraction.pdf:pdf},
	mendeley-groups = {Annotated},
	title = {{Interpreting Blackbox Models via Model Extraction}}
}
@article{Craven,
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - Unknown - Extracting Thee-Structured Representations of Thained Networks.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees},
	title = {{Extracting Thee-Structured Representations of Thained Networks}}
}
@article{Martensa,
	author = {Martens, David and Provost, Foster},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens, Provost - Unknown - Explaining Data-Driven Document Classifications.pdf:pdf},
	keywords = {comprehensibility,document classification,instance level explanation,text mining},
	mendeley-groups = {Annotated/Explanations},
	title = {{Explaining Data-Driven Document Classifications *}}
}
@article{Fyshe,
	author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - Unknown - A Compositional and Interpretable Semantic Space.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations,Annotated/Word Vectors},
	title = {{A Compositional and Interpretable Semantic Space}}
}

@article{Luo2015,
abstract = {Word embeddings encode semantic meanings of words into low-dimension word vectors. In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors. Nonnegative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http://github.com/skTim/OIWE.},
author = {Luo, Hongyin and Luan, Zhiyuan and Huanbo, Liuv and Sun, Maosong},
doi = {10.18653/v1/d15-1196},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2015 - Online Learning of Interpretable Word Embeddings.pdf:pdf},
isbn = {9781941643327},
journal = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {Annotated/Word Vectors,11Thesis/Interpretability/Word Vectors/Constraints},
number = {September},
pages = {1687--1692},
title = {{Online learning of interpretable word embeddings}},
year = {2015}
}

@article{Bechberger,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.04825v1},
	author = {Bechberger, Lucas},
	eprint = {arXiv:1706.04825v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechberger - Unknown - Neural Representations.pdf:pdf},
	mendeley-groups = {Annotated/Conceptual Spaces and Neural Networks,Annotated/Generative Adversarial Nets},
	title = {{Neural Representations}}
}
@article{Miller2017a,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.07269v1},
	author = {Miller, Tim},
	eprint = {arXiv:1706.07269v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 2017 - Explanation in Artificial Intelligence Insights from the Social Sciences.pdf:pdf},
	keywords = {explainability,explainable ai,explanation,interpretability,transparency},
	mendeley-groups = {Annotated/Overarching Interpretability},
	title = {{Explanation in Artificial Intelligence : Insights from the Social Sciences}},
	year = {2017}
}
@article{Samek,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.07979v1},
	author = {Samek, Wojciech},
	eprint = {arXiv:1706.07979v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek - Unknown - Methods for Interpreting and Understanding Deep Neural Networks.pdf:pdf},
	keywords = {activation maximization,deep neural networks,layer-wise,relevance propagation,sensitivity analysis,taylor decomposition},
	mendeley-groups = {Annotated/Overarching Interpretability},
	title = {{Methods for Interpreting and Understanding Deep Neural Networks}}
}
@article{Chen,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1301.3618v2},
	author = {Chen, Danqi and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
	eprint = {arXiv:1301.3618v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - Neural Tensor Networks and Semantic Word Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	pages = {1--4},
	title = {{Neural Tensor Networks and Semantic Word Vectors}}
}

@article{Dai,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1507.07998v1},
	author = {Dai, Andrew M and Olah, Christopher},
	eprint = {arXiv:1507.07998v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Olah - Unknown - Document Embedding with Paragraph Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	pages = {1--8},
	title = {{Document Embedding with Paragraph Vectors}}
}
@article{Kiros,
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros et al. - Unknown - Skip-Thought Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	number = {786},
	pages = {1--9},
	title = {{Skip-Thought Vectors}}
}
@article{Liu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1705.09207v2},
	author = {Liu, Yang and Lapata, Mirella},
	eprint = {arXiv:1705.09207v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lapata - 2017 - Learning Structured Text Representations.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations},
	title = {{Learning Structured Text Representations}},
	year = {2017}
}
@article{Lai1990,
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 1990 - Recurrent Convolutional Neural Networks for Text Classification.pdf:pdf},
	keywords = {NLP and Machine Learning Track},
	mendeley-groups = {Annotated/Representation Learning},
	pages = {2267--2273},
	title = {{Recurrent Convolutional Neural Networks for Text Classification}},
	year = {1990}
}
@article{Zaidan2007,
	abstract = {We propose a new framework for supervised machine learning. Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with “rationales.” When annotating an example, the human teacher will also highlight evidence supporting this annotation—thereby teaching the machine learner why the example belongs to the category. We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance signi?cantly on a sample task, namely sentiment classi?cation of movie reviews. We hypothesize that in some situations, providing rationales is a more fruitful use of an annotator's time than annotating more examples.},
	author = {Zaidan, O. and Zaidan, O. and Eisner, J. and Eisner, J. and Piatko, C. and Piatko, C.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaidan et al. - 2007 - Using “annotator rationales” to improve machine learning for text categorization.pdf:pdf},
	isbn = {9781932432657},
	journal = {Proceedings of NAACL-HLT},
	mendeley-groups = {Report/Explaining predictions,Annotated},
	number = {April},
	pages = {260--267},
	title = {{Using “annotator rationales” to improve machine learning for text categorization}},
	url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Using+Annotator+Rationales+to+Improve+Machine+Learning+for+Text+Categorization{\#}0},
	volume = {260},
	year = {2007}
}
@article{Donahue2011,
	abstract = {Traditional supervised visual learning simply asks annotators {\&}{\#}x201C;what{\&}{\#}x201D; label an image should have. We propose an approach for image classification problems requiring subjective judgment that also asks {\&}{\#}x201C;why{\&}{\#}x201D;, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the first, the annotator highlights the spatial region of interest he found most influential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to refine the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices.},
	author = {Donahue, Jeff and Grauman, Kristen},
	doi = {10.1109/ICCV.2011.6126394},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue, Grauman - 2011 - Annotator rationales for visual recognition.pdf:pdf},
	isbn = {9781457711015},
	issn = {1550-5499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	mendeley-groups = {Report/Explaining predictions,Annotated},
	number = {Iccv},
	pages = {1395--1402},
	title = {{Annotator rationales for visual recognition}},
	year = {2011}
}
@article{Ross2017,
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	archivePrefix = {arXiv},
	arxivId = {1703.03717},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	eprint = {1703.03717},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Annotated/Explanations},
	title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
	url = {http://arxiv.org/abs/1703.03717},
	year = {2017}
}
@article{Selvaraju2016,
	abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-scikit-learned Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
	archivePrefix = {arXiv},
	arxivId = {1611.07450},
	author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
	eprint = {1611.07450},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Annotated/Explanations},
	pages = {1--4},
	title = {{Grad-CAM: Why did you say that?}},
	url = {http://arxiv.org/abs/1611.07450},
	year = {2016}
}
@article{Lipton2016,
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	archivePrefix = {arXiv},
	arxivId = {1606.03490},
	author = {Lipton, Zachary C.},
	eprint = {1606.03490},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
	mendeley-groups = {Annotated/Overarching Interpretability,Report/Explaining predictions},
	number = {Whi},
	title = {{The Mythos of Model Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	year = {2016}
}
@article{Freitas2010,
	author = {Freitas, AA and Wieser, DC and Apweiler, R},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas, Wieser, Apweiler - 2010 - On the importance of comprehensible classification models for protein function prediction.pdf:pdf},
	journal = {IEEE/ACM Transactions on},
	mendeley-groups = {Annotated/Applications/Scientific Discovery,Report/Biologicla domain},
	number = {1},
	pages = {172--182},
	title = {{On the importance of comprehensible classification models for protein function prediction}},
	url = {http://dl.acm.org/citation.cfm?id=1719290},
	volume = {7},
	year = {2010}
}

@article{Freitas2013,
	abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
	author = {Freitas, Alex A.},
	doi = {10.1145/2594473.2594475},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas - 2013 - Comprehensible Classification Models - a position paper.pdf:pdf},
	isbn = {1931-0145},
	issn = {19310145},
	journal = {ACM SIGKDD Explorations Newsletter},
	keywords = {bayesian network classifiers,decision table,decision tree,monotonicity constraint,nearest neighbors,rule induction},
	mendeley-groups = {Report/Just about interpretability,Annotated/Overarching Interpretability,Report},
	number = {1},
	pages = {1--10},
	title = {{Comprehensible Classification Models - a position paper}},
	url = {http://dl.acm.org.miman.bib.bth.se/citation.cfm?id=2594475},
	volume = {15},
	year = {2013}
}
@article{Chen2016,
	abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	archivePrefix = {arXiv},
	arxivId = {1606.03657},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	eprint = {1606.03657},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
	issn = {10495258},
	mendeley-groups = {Annotated/Generative Adversarial Nets,Report},
	title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	url = {http://arxiv.org/abs/1606.03657},
	year = {2016}
}
@article{Letham2015,
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if {\ldots} then. . . statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1511.01644v1},
	author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
	doi = {10.1214/15-AOAS848},
	eprint = {arXiv:1511.01644v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2015 - Interpretable classifiers using rules and bayesian analysis Building a better stroke prediction model.pdf:pdf},
	isbn = {9781577356288},
	issn = {19417330},
	journal = {Annals of Applied Statistics},
	keywords = {Bayesian analysis,Classification,Interpretability},
	mendeley-groups = {Annotated/Rule-based classiifers,Report},
	number = {3},
	pages = {1350--1371},
	title = {{Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model}},
	volume = {9},
	year = {2015}
}
@article{Vandewiele2016,
	abstract = {Models obtained by decision tree induction techniques excel in being interpretable.However, they can be prone to overfitting, which results in a low predictive performance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial. To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques.},
	archivePrefix = {arXiv},
	arxivId = {1611.05722},
	author = {Vandewiele, Gilles and Janssens, Olivier and Ongenae, Femke and {De Turck}, Filip and {Van Hoecke}, Sofie},
	eprint = {1611.05722},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vandewiele et al. - 2016 - GENESIM genetic extraction of a single, interpretable model.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees,Report},
	number = {Nips},
	title = {{GENESIM: genetic extraction of a single, interpretable model}},
	url = {http://arxiv.org/abs/1611.05722},
	year = {2016}
}
@article{Chang2009,
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
	doi = {10.1.1.100.1089},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Models.pdf:pdf},
	isbn = {9781615679119},
	issn = {1098-6596},
	journal = {Proc.\ NIPS},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {288--296},
	pmid = {25246403},
	title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
	url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
	year = {2009}
}
@article{Rubin2012,
	abstract = {Machine learning approaches to multi-label document classification have to date largely relied on discriminative modeling techniques such as support vector machines. A drawback of these approaches is that performance rapidly drops off as the total number of labels and the number of labels per document increase. This problem is amplified when the label frequencies exhibit the type of highly skewed distributions that are often observed in real-world datasets. In this paper we investigate a class of generative statistical topic models for multi-label documents that associate individual word tokens with different labels. We investigate the advantages of this approach relative to discriminative models, particularly with respect to classification problems involving large numbers of relatively rare labels. We compare the performance of generative and discriminative approaches on document labeling tasks ranging from datasets with several thousand labels to datasets with tens of labels. The experimental results indicate that probabilistic generative models can achieve competitive multi-label classification performance compared to discriminative methods, and have advantages for datasets with many labels and skewed label frequencies.},
	archivePrefix = {arXiv},
	arxivId = {1107.2462},
	author = {Rubin, Timothy N. and Chambers, America and Smyth, Padhraic and Steyvers, Mark},
	doi = {10.1007/s10994-011-5272-5},
	eprint = {1107.2462},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubin et al. - 2012 - Statistical topic models for multi-label document classification.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Dependency-LDA,Document modeling,Graphical models,LDA,Multi-label classification,Probabilistic generative models,Text classification,Topic models},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	number = {1-2},
	pages = {157--208},
	title = {{Statistical topic models for multi-label document classification}},
	volume = {88},
	year = {2012}
}
@article{Arora2012,
	abstract = {Topic Modeling is an approach used for automatic comprehension and classification of data in a variety of settings, and perhaps the canonical application is in uncovering thematic structure in a corpus of documents. A number of foundational works both in machine learning and in theory have suggested a probabilistic model for documents, whereby documents arise as a convex combination of (i.e. distribution on) a small number of topic vectors, each topic vector being a distribution on words (i.e. a vector of word-frequencies). Similar models have since been used in a variety of application areas; the Latent Dirichlet Allocation or LDA model of Blei et al. is especially popular. Theoretical studies of topic modeling focus on learning the model's parameters assuming the data is actually generated from it. Existing approaches for the most part rely on Singular Value Decomposition(SVD), and consequently have one of two limitations: these works need to either assume that each document contains only one topic, or else can only recover the span of the topic vectors instead of the topic vectors themselves. This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main tool in this context, which is an analog of SVD where all vectors are nonnegative. Using this tool we give the first polynomial-time algorithm for learning topic models without the above two limitations. The algorithm uses a fairly mild assumption about the underlying topic matrix called separability, which is usually found to hold in real-life data. A compelling feature of our algorithm is that it generalizes to models that incorporate topic-topic correlations, such as the Correlated Topic Model and the Pachinko Allocation Model. We hope that this paper will motivate further theoretical results that use NMF as a replacement for SVD - just as NMF has come to replace SVD in many applications.},
	archivePrefix = {arXiv},
	arxivId = {1204.1956},
	author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur},
	doi = {10.1109/FOCS.2012.49},
	eprint = {1204.1956},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Ge, Moitra - 2012 - Learning topic models - Going beyond SVD.pdf:pdf},
	isbn = {978-0-7695-4874-6},
	issn = {02725428},
	journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {1--10},
	title = {{Learning topic models - Going beyond SVD}},
	year = {2012}
}
@article{Hullermeier2008,
	abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison (RPC), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (scikit-learned) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare RPC to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that RPC is superior in terms of computational efficiency, and at least competitive in terms of accuracy. ?? 2008 Elsevier B.V. All rights reserved.},
	author = {H{\"{u}}llermeier, Eyke and F{\"{u}}rnkranz, Johannes and Cheng, Weiwei and Brinker, Klaus},
	doi = {10.1016/j.artint.2008.08.002},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{u}}llermeier et al. - 2008 - Label ranking by learning pairwise preferences.pdf:pdf},
	isbn = {0004-3702},
	issn = {00043702},
	journal = {Artificial Intelligence},
	keywords = {Constraint classification,Pairwise classification,Preference learning,Ranking},
	mendeley-groups = {Annotated/Ranking,Progress Report},
	number = {16-17},
	pages = {1897--1916},
	title = {{Label ranking by learning pairwise preferences}},
	volume = {172},
	year = {2008}
}
@article{Faruqui2015,
	abstract = {Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.},
	archivePrefix = {arXiv},
	arxivId = {1506.02004},
	author = {Faruqui, Manaal and Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah A.},
	eprint = {1506.02004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui et al. - 2015 - Sparse Overcomplete Word Vector Representations.pdf:pdf},
	isbn = {9781941643723},
	journal = {Acl-2015},
	mendeley-groups = {Annotated/Word Vectors,Progress Report,Interim Review,Annotated/NMF},
	pages = {1491--1500},
	title = {{Sparse Overcomplete Word Vector Representations}},
	url = {http://homes.cs.washington.edu/{~}nasmith/papers/faruqui+tsvetkov+yogatama+dyer+smith.acl15.pdf},
	year = {2015}
}
@article{Mikolov2013,
	abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lowerMikolov, T., Chen, K., Corrado, G., {\&} Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Arxiv, 1-12. http://doi.org/10.1162/153244303322533223 computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities. 1},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1301.3781v3},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	doi = {10.1162/153244303322533223},
	eprint = {arXiv:1301.3781v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {Arxiv},
	mendeley-groups = {Annotated/Word Vectors,Report/Features,Interim Review,Report},
	pages = {1--12},
	pmid = {18244602},
	title = {{Efficient Estimation of Word Representations in Vector Space}},
	url = {http://arxiv.org/abs/1301.3781},
	year = {2013}
}
@article{Agerb,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders(2).pdf:pdf},
	mendeley-groups = {Annotated/Past work,Progress Report},
	title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@misc{chollet2015keras,
	title={Keras},
	author={Chollet, Fran\c{c}ois and others},
	year={2015},
	publisher={GitHub},
	howpublished={\url{https://github.com/fchollet/keras}},
}
@inproceedings{Loper:2002:NNL:1118108.1118117,
	author = {Loper, Edward and Bird, Steven},
	title = {{NLTK}: The Natural Language Toolkit},
	booktitle = {Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics},
	year = {2002},
	pages = {63--70}
} 

@article{Hu2016,
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the scikit-learns of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	archivePrefix = {arXiv},
	arxivId = {1603.06318},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	eprint = {1603.06318},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules.pdf:pdf},
	journal = {arXiv preprint},
	mendeley-groups = {Papers/Paper 1,Literature Review,Annotated/Rule-based classiifers,Report},
	pages = {1--18},
	title = {{Harnessing Deep Neural Networks with Logic Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	year = {2016}
}
@article{Seera2014a,
	abstract = {In this paper, a hybrid intelligent system that consists of the Fuzzy Min-Max neural network, the Classification and Regression Tree, and the Random Forest model is proposed, and its efficacy as a decision support tool for medical data classification is examined. The hybrid intelligent system aims to exploit the advantages of the constituent models and, at the same time, alleviate their limitations. It is able to learn incrementally from data samples (owing to Fuzzy Min-Max neural network), explain its predicted outputs (owing to the Classification and Regression Tree), and achieve high classification performances (owing to Random Forest). To evaluate the effectiveness of the hybrid intelligent system, three benchmark medical data sets, viz., Breast Cancer Wisconsin, Pima Indians Diabetes, and Liver Disorders from the UCI Repository of Machine Learning, are used for evaluation. A number of useful performance metrics in medical applications which include accuracy, sensitivity, specificity, as well as the area under the Receiver Operating Characteristic curve are computed. The results are analyzed and compared with those from other methods published in the literature. The experimental outcomes positively demonstrate that the hybrid intelligent system is effective in undertaking medical data classification tasks. More importantly, the hybrid intelligent system not only is able to produce good results but also to elucidate its knowledge base with a decision tree. As a result, domain users (i.e., medical practitioners) are able to comprehend the prediction given by the hybrid intelligent system; hence accepting its role as a useful medical decision support tool. ?? 2013 Elsevier Ltd. All rights reserved.},
	author = {Seera, Manjeevan and Lim, Chee Peng},
	doi = {10.1016/j.eswa.2013.09.022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seera, Lim - 2014 - A hybrid intelligent system for medical data classification.pdf:pdf},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Classification and regression tree,Fuzzy Min-Max neural network,Hybrid intelligent systems,Medical decision support,Random forest},
	mendeley-groups = {Annotated/Decision Trees},
	number = {5},
	pages = {2239--2249},
	publisher = {Elsevier Ltd},
	title = {{A hybrid intelligent system for medical data classification}},
	url = {http://dx.doi.org/10.1016/j.eswa.2013.09.022},
	volume = {41},
	year = {2014}
}
@article{Chorowski2015a,
	abstract = {People can understand complex structures if they relate to more isolated yet understandable concepts. Despite this fact, popular pattern recognition tools, such as decision tree or production rule learners, produce only flat models which do not build intermediate data representations. On the other hand, neural networks typically learn hierarchical but opaque models. We show how constraining neurons' scikit-learns to be nonnegative improves the interpretability of a network's operation. We analyze the proposed method on large data sets: the MNIST digit recognition data and the Reuters text categorization data. The patterns learned by traditional and constrained network are contrasted to those learned with principal component analysis and nonnegative matrix factorization.},
	author = {Chorowski, Jan and Zurada, Jacek M.},
	doi = {10.1109/TNNLS.2014.2310059},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chorowski, Zurada - 2015 - Learning understandable neural networks with nonnegative scikit-learn constraints.pdf:pdf},
	isbn = {2162-237X VO - 26},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Multilayer perceptron,pattern analysis,supervised learning,white-box models.},
	mendeley-groups = {Annotated/Word Vectors},
	number = {1},
	pages = {62--69},
	title = {{Learning understandable neural networks with nonnegative scikit-learn constraints}},
	volume = {26},
	year = {2015}
}

@article{Mikolov2013,
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	archivePrefix = {arXiv},
	arxivId = {1310.4546},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	doi = {10.1162/jmlr.2003.3.4-5.951},
	eprint = {1310.4546},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
	isbn = {2150-8097},
	issn = {10495258},
	journal = {Nips},
	mendeley-groups = {Annotated/Word Vectors,Interim Review},
	pages = {1--9},
	pmid = {903},
	title = {{Distributed Representations of Words and Phrases and their Compositionality}},
	year = {2013}
}
@article{Bengio2012,
	archivePrefix = {arXiv},
	arxivId = {1206.5538},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	doi = {10.1109/TPAMI.2013.50},
	eprint = {1206.5538},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2012 - Representation Learning A Review and New Perspectives.pdf:pdf},
	isbn = {0162-8828 VO - 35},
	issn = {1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	mendeley-groups = {Annotated/Representation Learning,Report/Features},
	number = {8},
	pages = {1798--1828},
	pmid = {23787338},
	title = {{Representation Learning: A Review and New Perspectives}},
	volume = {35},
	year = {2012}
}
@article{Li2016a,
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
	archivePrefix = {arXiv},
	arxivId = {1506.01066},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	eprint = {1506.01066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Visualizing and Understanding Neural Models in NLP.pdf:pdf},
	journal = {Proc.\ NAACL-HLT},
	keywords = {Neural Network,Visualization},
	pages = {1--10},
	title = {{Visualizing and Understanding Neural Models in NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	year = {2016}
}

@article{Chaney2012,
	abstract = {Managing large collections of documents is an important$\backslash$nproblem for many areas of science, industry, and$\backslash$nculture. Probabilistic topic modeling offers a promising$\backslash$nsolution. Topic modeling is an unsupervised machine$\backslash$nlearning method that learns the underlying themes in$\backslash$na large collection of otherwise unorganized documents.$\backslash$nThis discovered structure summarizes and organizes the$\backslash$ndocuments. However, topic models are high-level statistical$\backslash$ntools—a user must scrutinize numerical distributions$\backslash$nto understand and explore their results. In this$\backslash$npaper, we present a method for visualizing topic models.$\backslash$nOur method creates a navigator of the documents,$\backslash$nallowing users to explore the hidden structure that a$\backslash$ntopic model discovers. These browsing interfaces reveal$\backslash$nmeaningful patterns in a collection, helping end-users$\backslash$nexplore and understand its contents in new ways. We$\backslash$nprovide open source software of our method.},
	author = {Chaney, Ajb and Blei, Dm},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaney, Blei - 2012 - Visualizing Topic Models.pdf:pdf},
	isbn = {9781577355564},
	journal = {Proc.\ ICWSM},
	pages = {419--422},
	title = {{Visualizing Topic Models.}},
	year = {2012}
}

@article{Maas2011,
	abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
	author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	doi = {978-1-932432-87-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:pdf},
	isbn = {9781932432879},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Datasets,!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {142--150},
	title = {{Learning Word Vectors for Sentiment Analysis}},
	year = {2011}
}


@article{Abadi2016,
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	archivePrefix = {arXiv},
	arxivId = {1603.04467},
	author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	doi = {10.1038/nn.3331},
	eprint = {1603.04467},
	file = {:C$\backslash$:/Users/Workk/Desktop/1603.04467.pdf:pdf},
	isbn = {0010-0277},
	issn = {0270-6474},
	pmid = {16411492},
	title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
	url = {http://arxiv.org/abs/1603.04467},
	year = {2016}
}

@article{Skirpan2017,
	abstract = {In this paper, we argue for the adoption of a normative definition of fairness within the machine learning community. After characterizing this definition, we review the current literature of Fair ML in light of its implications. We end by suggesting ways to incorporate a broader community and generate further debate around how to decide what is fair in ML.},
	archivePrefix = {arXiv},
	arxivId = {1706.09976},
	author = {Skirpan, Michael and Gorelick, Micha},
	eprint = {1706.09976},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skirpan, Gorelick - 2017 - The Authority of Fair in Machine Learning.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{The Authority of "Fair" in Machine Learning}},
	url = {http://arxiv.org/abs/1706.09976},
	year = {2017}
}
@article{Version2017,
	author = {Version, Document},
	file = {:E$\backslash$:/Member-SIGIR.pdf:pdf},
	isbn = {9781450350228},
	keywords = {entity embedding,entity rank-,list completion,maximum margin},
	mendeley-groups = {11Thesis/Applications},
	title = {{Kent Academic Repository}},
	year = {2017}
}
@article{Kovashka,
abstract = {We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query “black shoes”, the user might state, “Show me shoe images like these, but sportier.” Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently “whittle away” irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient—both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.},
archivePrefix = {arXiv},
arxivId = {1505.04141},
author = {Kovashka, Adriana and Parikh, Devi and Grauman, Kristen},
doi = {10.1007/s11263-015-0814-0},
eprint = {1505.04141},
file = {:E$\backslash$:/WhittleSearch{\_}Image{\_}search{\_}with{\_}relative.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Active selection,Content-based image search,Interactive image search,Relative attributes},
mendeley-groups = {11Thesis/Applications},
number = {2},
pages = {185--210},
title = {{WhittleSearch: Interactive Image Search with Relative Attribute Feedback}},
volume = {115},
year = {2015}
}

@article{Vig2014,
abstract = {This article introduces the tag genome, a data structure that extends the traditional tagging model to provide enhanced forms of user interaction. Just as a biological genome encodes an organism based on a sequence of genes, the tag genome encodes an item in an information space based on its relationship to a common set of tags. We present a machine learning approach for computing the tag genome, and we evaluate several learning models on a ground truth dataset provided by users. We describe an application of the tag genome called Movie Tuner which enables users to navigate from one item to nearby items along dimensions represented by tags. We present the results of a 7-week field trial of 2,531 users of Movie Tuner and a survey evaluating users subjective experience. Finally, we outline the broader space of applications of the tag genome. {\textcopyright} 2012 ACM.},
author = {Vig, Jesse and Sen, Shilad and Riedl, John},
doi = {10.1145/2362394.2362395},
file = {:E$\backslash$:/Downloads/Work/tag{\_}genome.pdf:pdf},
issn = {21606463},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {Tagging,conversational recommenders,data mining,information retrieval,machine learning,recommender systems},
number = {3},
title = {{The tag genome: Encoding community knowledge to support novel interaction}},
volume = {2},
year = {2012}
}




@article{Amodei2016,
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	archivePrefix = {arXiv},
	arxivId = {1606.06565},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
	doi = {1606.06565},
	eprint = {1606.06565},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
	isbn = {0387310738},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Safety},
	pages = {1--29},
	title = {{Concrete Problems in AI Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	year = {2016}
}
@article{H.Zou2006,
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
	archivePrefix = {arXiv},
	arxivId = {1205.0121v2},
	author = {H.{\~{}}Zou and T.{\~{}}Hastie and R.{\~{}}Tibshirani and Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	doi = {10.1198/106186006X113430},
	eprint = {1205.0121v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H.{\~{}}Zou et al. - 2006 - Sparse principal component analysis.pdf:pdf},
	isbn = {106186006X},
	issn = {1061-8600},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {arrays,ca 94305,composition,d student in the,department of statistics at,edu,elastic net,email,gene expression,gene expression arrays,hui zou is a,hzou,lasso,multivariate analysis,ph,singular,singular value de-,stanford,stanford university,stat,thresholding,value decomposition},
	mendeley-groups = {Annotated/NMF,11Thesis,!Paper 3},
	number = {2},
	pages = {265--286},
	pmid = {21811560},
	title = {{Sparse principal component analysis}},
	volume = {15},
	year = {2006}
}

@article{Garg2017,
	abstract = {Word embeddings use vectors to represent words such that the geometry between vectors captures semantic relationship between the words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding can be leveraged to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 years of text data with the U.S. Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures global social shifts -- e.g., the women's movement in the 1960s and Asian immigration into the U.S -- and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a powerful new intersection between machine learning and quantitative social science.},
	archivePrefix = {arXiv},
	arxivId = {1711.08412},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	doi = {10.1073/pnas.1720347115},
	eprint = {1711.08412},
	file = {:D$\backslash$:/Downloads/Play/E3635.full.pdf:pdf},
	isbn = {1720347115},
	issn = {0027-8424},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {16},
	pmid = {29615513},
	title = {{Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes}},
	url = {http://arxiv.org/abs/1711.08412{\%}0Ahttp://dx.doi.org/10.1073/pnas.1720347115},
	volume = {115},
	year = {2017}
}
@article{Poursabzi-sangdeh,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.07810v2},
	author = {Poursabzi-sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Vaughan, Jennifer Wortman and Wallach, Hanna},
	eprint = {arXiv:1802.07810v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.07810.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/General},
	pages = {1--20},
	title = {{Manipulating and Measuring Model Interpretability}}
}
@article{Narayanan2018,
	abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
	archivePrefix = {arXiv},
	arxivId = {1802.00682},
	author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
	eprint = {1802.00682},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine Learning Systems An Evaluation of the Human-Interpretabilit.pdf:pdf},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability,11Thesis/Interpretability/Explanation,11Thesis/Interpretability/General},
	pages = {1--21},
	title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
	url = {http://arxiv.org/abs/1802.00682},
	year = {2018}
}
@article{Gong2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1809.06858v1},
	author = {Gong, Chengyue},
	eprint = {arXiv:1809.06858v1},
	file = {:E$\backslash$:/1809.06858.pdf:pdf},
	mendeley-groups = {11Thesis/State of the art},
	number = {Nips},
	pages = {1--15},
	title = {{FRAGE : Frequency-Agnostic Word Representation}},
	volume = {1},
	year = {2018}
}

@article{Gong2018,
	abstract = {Machine learning systems are increasingly used to sup-port public sector decision-making across a variety of sectors. Given concerns around accountability in these domains, and amidst accusations of intentional or unintentional bias, there have been increased calls for transparency of these technologies. Few, however, have considered how logics and practices concerning transparency have been understood by those involved in the machine learning systems already being piloted and deployed in public bodies today. This short pa-per distils insights about transparency on the ground from interviews with 27 such actors, largely public ser-vants and relevant contractors, across 5 OECD countries. Considering transparency and opacity in relation to trust and buy-in, better decision-making, and the avoidance of gaming, it seeks to provide useful insights for those hoping to develop socio-technical approaches to transparency that might be useful to practitioners on-the-ground.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1809.06858v1},
	author = {Gong, Chengyue},
	eprint = {arXiv:1809.06858v1},
	file = {:E$\backslash$:/1809.06858.pdf:pdf},
	mendeley-groups = {11Thesis/State of the art},
	number = {Nips},
	pages = {1--15},
	title = {{FRAGE : Frequency-Agnostic Word Representation}},
	volume = {1},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/AAAI{\_}2020{\_}{\_}{\_}hierarchical{\_}feature{\_}directions.pdf:pdf},
	mendeley-groups = {11Thesis/Rana},
	title = {{Hierarchical Linear Disentanglement of Entity Embeddings}},
	year = {2015}
}
@article{VanLinh2017,
	abstract = {{\textcopyright} 2016, Springer-Verlag London.As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises-Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental results showed the advantages of our approach in terms of separability, interpretability and effectiveness in classification task of datasets with high dimension and complex distribution. Our method is highly competitive with state-of-the-art approaches.},
	author = {{Van Linh}, Ngo and Anh, Nguyen Kim and Than, Khoat and Dang, Chien Nguyen},
	doi = {10.1007/s10115-016-0956-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Linh et al. - 2017 - An effective and interpretable method for document classification.pdf:pdf},
	issn = {02193116},
	journal = {Knowledge and Information Systems},
	keywords = {Bayesian nonparametrics,Classification,Variational inference,Von Mises-Fisher distribution},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
	number = {3},
	pages = {763--793},
	title = {{An effective and interpretable method for document classification}},
	volume = {50},
	year = {2017}
}
@article{Chen2016,
	abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	archivePrefix = {arXiv},
	arxivId = {1606.03657},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	eprint = {1606.03657},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
	issn = {10495258},
	mendeley-groups = {Annotated/Generative Adversarial Nets,Report,11Thesis,11Thesis/Interpretability/GAN's and VAE},
	title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	url = {http://arxiv.org/abs/1606.03657},
	year = {2016}
}
@article{Galloway1982,
	abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic similarity, pos-induction and word-analogy tasks.},
	author = {Galloway, Patricia},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/P15-1126.pdf:pdf},
	journal = {ALLC Journal},
	keywords = {*Diachronic Linguistics (di1),*French (fr2),*Poetry (pl2),*Statistical Analysis of Style (st3),5710: poetics/literary theory; poetics,article,cluster analysis, typology, Lai de l'Ombre manuscr},
	mendeley-groups = {11Thesis/Interpretability/Visual},
	number = {1},
	pages = {1--8},
	title = {{Clustering Variants in the Lai de l'Ombre Manuscripts: Techniques and Principles}},
	url = {http://search.proquest.com/docview/85463650?accountid=8330{\%}5Cnhttp://library.anu.edu.au:4550/resserv?genre=article{\&}issn={\&}title=ALLC+Journal{\&}volume=3{\&}issue=1{\&}date=1982-04-01{\&}atitle=Clustering+Variants+in+the+Lai+de+l'Ombre+Manuscripts:+Techniques+and+Princ},
	volume = {3},
	year = {1982}
}
@article{Burel2018,
	author = {Burel, Gr{\'{e}}goire and Alani, Harith},
	file = {:E$\backslash$:/73af4bf3b35e194724dc64e784dabe30c689.pdf:pdf},
	keywords = {api,convolutional neural networks,deep learning,event detection,word embeddings},
	mendeley-groups = {11Thesis/Applications},
	number = {May},
	title = {{Crisis Event Extraction Service ( CREES ) - Automatic Detection and Classification of Crisis-related Content on Social Media}},
	year = {2018}
}
@article{Garg2017,
	abstract = {Word embeddings use vectors to represent words such that the geometry between vectors captures semantic relationship between the words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding can be leveraged to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 years of text data with the U.S. Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures global social shifts -- e.g., the women's movement in the 1960s and Asian immigration into the U.S -- and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a powerful new intersection between machine learning and quantitative social science.},
	archivePrefix = {arXiv},
	arxivId = {1711.08412},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	doi = {10.1073/pnas.1720347115},
	eprint = {1711.08412},
	file = {:D$\backslash$:/Downloads/Play/E3635.full.pdf:pdf},
	isbn = {1720347115},
	issn = {0027-8424},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {16},
	pmid = {29615513},
	title = {{Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes}},
	url = {http://arxiv.org/abs/1711.08412{\%}0Ahttp://dx.doi.org/10.1073/pnas.1720347115},
	volume = {115},
	year = {2017}
}

@article{Binns2017,
	abstract = {The internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and are subject to the idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic content moderation systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants.},
	archivePrefix = {arXiv},
	arxivId = {1707.01477},
	author = {Binns, Reuben and Veale, Michael and {Van Kleek}, Max and Shadbolt, Nigel},
	doi = {10.1007/978-3-319-67256-4_32},
	eprint = {1707.01477},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Binns et al. - 2017 - Like trainer, like bot Inheritance of bias in algorithmic content moderation.pdf:pdf},
	isbn = {9783319672557},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Algorithmic accountability,Discussion platforms,Freedom of speech,Machine learning,Online abuse},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
	pages = {405--415},
	title = {{Like trainer, like bot? Inheritance of bias in algorithmic content moderation}},
	volume = {10540 LNCS},
	year = {2017}
}
@article{Poursabzi-sangdeh,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.07810v2},
	author = {Poursabzi-sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Vaughan, Jennifer Wortman and Wallach, Hanna},
	eprint = {arXiv:1802.07810v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.07810.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/General},
	pages = {1--20},
	title = {{Manipulating and Measuring Model Interpretability}}
}
@article{Kim2013,
	author = {Kim, Joo-kyung},
	file = {:C$\backslash$:/Users/Workk/Documents/D13-1169.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	number = {October},
	pages = {1625--1630},
	title = {{Deriving adjectival scales from continuous space word representations}},
	year = {2013}
}
@article{Zhang,
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
	file = {:E$\backslash$:/PhD/Papedrs/Zhang{\_}StackGAN{\_}Text{\_}to{\_}ICCV{\_}2017{\_}paper.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {5907--5915},
	title = {{StackGAN : Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}}
}
@article{Bechavod2017,
	abstract = {We present a regularization-inspired approach for reducing bias in learned classifiers. In particular, we focus on binary classification tasks over individuals from two populations, where, as our criterion for fairness, we wish to achieve similar false positive rates in both populations, and similar false negative rates in both populations. As a proof of concept, we implement our approach and empirically evaluate its ability to achieve both fairness and accuracy, using the COMPAS scores data for prediction of recidivism.},
	archivePrefix = {arXiv},
	arxivId = {1707.00044},
	author = {Bechavod, Yahav and Ligett, Katrina},
	eprint = {1707.00044},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechavod, Ligett - 2017 - Learning Fair Classifiers A Regularization-Inspired Approach.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	pages = {6--10},
	title = {{Learning Fair Classifiers: A Regularization-Inspired Approach}},
	url = {http://arxiv.org/abs/1707.00044},
	year = {2017}
}
@article{Hu2017,
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	archivePrefix = {arXiv},
	arxivId = {1703.00955},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	eprint = {1703.00955},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2017 - Toward Controlled Generation of Text.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/GAN's and VAE},
	title = {{Toward Controlled Generation of Text}},
	url = {http://arxiv.org/abs/1703.00955},
	year = {2017}
}
@article{Gilpin,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.00069v2},
	author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	eprint = {arXiv:1806.00069v2},
	file = {:E$\backslash$:/PhD/Papedrs/4da8391a737273be4868613468b61d80d466.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted},
	title = {{Explaining Explanations : An Approach to Evaluating Interpretability of Machine Learning}}
}
@article{Mitchell2015,
	abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that per-formance on this task can be related to orthogonality within the space. Explic-itly designing such structure into a neu-ral network model results in represen-tations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within En-glish Wikipedia text to enable this de-composition can produce substantial im-provements on semantic-similarity, pos-induction and word-analogy tasks.},
	author = {Mitchell, Jeff and Steedman, Mark},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/semsynacl2015{\_}final.pdf:pdf},
	isbn = {9781941643723},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis/Interpretability/Visual},
	pages = {1301--1310},
	title = {{Orthogonality of Syntax and Semantics within Distributional Spaces}},
	url = {http://www.aclweb.org/anthology/P15-1126},
	year = {2015}
}
@article{Hara2016,
	abstract = {Tree ensembles such as random forests and boosted trees are renowned for their high prediction performance; however, their interpretability is critically limited. One way of interpreting a complex tree ensemble is to obtain its simplified representation, which is formalized as a model selection problem: Given a complex tree ensemble, we want to obtain the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm. Our approach has three appealing features: the prediction performance is maintained, the coverage is sufficiently large, and the computation is reasonably feasible. Our synthetic data experiment and real world data applications show that complicated tree ensembles are approximated reasonably as interpretable.},
	archivePrefix = {arXiv},
	arxivId = {1606.09066},
	author = {Hara, Satoshi and Hayashi, Kohei},
	eprint = {1606.09066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara, Hayashi - 2016 - Making Tree Ensembles Interpretable A Bayesian Model Selection Approach.pdf:pdf},
	mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	number = {Whi},
	title = {{Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach}},
	url = {http://arxiv.org/abs/1606.09066},
	year = {2016}
}
@article{Kovashka,
abstract = {We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query “black shoes”, the user might state, “Show me shoe images like these, but sportier.” Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently “whittle away” irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient—both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.},
archivePrefix = {arXiv},
arxivId = {1505.04141},
author = {Kovashka, Adriana and Parikh, Devi and Grauman, Kristen},
doi = {10.1007/s11263-015-0814-0},
eprint = {1505.04141},
file = {:E$\backslash$:/WhittleSearch{\_}Image{\_}search{\_}with{\_}relative.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Active selection,Content-based image search,Interactive image search,Relative attributes},
mendeley-groups = {11Thesis/Applications},
number = {2},
pages = {185--210},
title = {{WhittleSearch: Interactive Image Search with Relative Attribute Feedback}},
volume = {115},
year = {2015}
}
@article{Jameel,
	author = {Jameel, Shoaib and Bouraoui, Zied and Schockaert, Steven},
	file = {:E$\backslash$:/MEmbER{\_}{\_}{\_}SIGIR{\_}2017-10.pdf:pdf},
	isbn = {9781450350228},
	keywords = {entity embedding,entity rank-,list completion,maximum margin},
	mendeley-groups = {11Thesis/Applications},
	title = {{MEmbER : Max-Margin Based Embeddings for Entity Retrieval}}
}
@article{Reed2017,
	author = {Reed, S and Kalchbrenner, N and Bapst, V and Botvinick, M and Freitas, N De and Deepmind, Google},
	file = {:E$\backslash$:/PhD/Papedrs/Generating Interpretable Images with Controllable Structure.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {1--13},
	title = {{G ENERATING I NTERPRETABLE I MAGES WITH C ONTROLLABLE S TRUCTURE}},
	year = {2017}
}
@article{Zhao2018,
	abstract = {Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.},
	archivePrefix = {arXiv},
	arxivId = {1809.01496},
	author = {Zhao, Jieyu and Zhou, Yichao and Li, Zeyu and Wang, Wei and Chang, Kai-Wei},
	eprint = {1809.01496},
	file = {:D$\backslash$:/Downloads/Play/1809.01496.pdf:pdf},
	issn = {0029-2303 (Print)},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis/Interpretability/Discrimination},
	pmid = {3270028},
	title = {{Learning Gender-Neutral Word Embeddings}},
	url = {http://arxiv.org/abs/1809.01496},
	year = {2018}
}
@article{Ehrentraut2018,
	author = {Ehrentraut, Claudia and Ekholm, Markus and Dalianis, Hercules},
	doi = {10.1177/1460458216656471},
	file = {:E$\backslash$:/1460458216656471.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	title = {{Detecting hospital-acquired infections : A document classification approach using support vector machines and gradient tree boosting}},
	year = {2018}
}
@article{Trifonov,
  title={Learning and Evaluating Sparse Interpretable Sentence Embeddings},
  author={Valentin Trifonov and Octavian-Eugen Ganea and Anna Potapenko and Thomas Hofmann},
  booktitle={BlackboxNLP EMNLP},
  year={2018}
}

@article{Chen2016,
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	file = {:E$\backslash$:/PhD/Papedrs/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted},
	number = {Nips},
	title = {{InfoGAN : Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	year = {2016}
}
@article{Hsu2017,
	author = {Hsu, Wei-ning and Zhang, Yu and Glass, James},
	file = {:E$\backslash$:/PhD/Papedrs/6784-unsupervised-learning-of-disentangled-and-interpretable-representations-from-sequential-data.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	number = {Nips},
	title = {{Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data}},
	year = {2017}
}
@article{Luo2015,
	author = {Luo, Hongyin and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2015 - Online Learning of Interpretable Word Embeddings.pdf:pdf},
	mendeley-groups = {Annotated/Word Vectors,11Thesis/Interpretability/Word Vectors/Constraints},
	number = {September},
	pages = {1687--1692},
	title = {{Online Learning of Interpretable Word Embeddings}},
	year = {2015}
}
@article{Lei2017,
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	file = {:E$\backslash$:/PhD/Papedrs/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	number = {Nips},
	pages = {1--12},
	title = {{Style Transfer from Non-Parallel Text by}},
	year = {2017}
}
@article{Alshaikh2019,
	author = {Alshaikh, Rana and Schockaert, Steven},
	file = {:E$\backslash$:/PhD/Papedrs/K19-1013.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {131--139},
	title = {{Learning Conceptual Spaces with Disentangled Facets}},
	year = {2019}
}
@article{Emnlp-ijcnlp,
	author = {Emnlp-ijcnlp, Anonymous},
	file = {:E$\backslash$:/CoNLL{\_}facets-2.pdf:pdf},
	mendeley-groups = {11Thesis/Rana},
	pages = {1--9},
	title = {{No Title}}
}
@article{Beltagy2013,
	author = {Beltagy, Islam and Chau, Cuong and Boleda, Gemma and Garrette, Dan and Erk, Katrin},
	file = {:C$\backslash$:/Users/Workk/Documents/S13-1002.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	pages = {11--21},
	title = {{Montague Meets Markov : Deep Semantics with Probabilistic Logical Form}},
	volume = {1},
	year = {2013}
}
@article{Bostrom2011,
	author = {Bostrom, Nick},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/artificialintelligence.pdf:pdf},
	keywords = {artificial intelligence, ethics},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	pages = {1--20},
	title = {{The Ethics of Artificial Intelligence}},
	year = {2011}
}
@article{Zhang2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.03850v3},
	author = {Zhang, Yizhe and Gan, Zhe and Fan, Kai and Chen, Zhi and Henao, Ricardo and Shen, Dinghan and Carin, Lawrence},
	eprint = {arXiv:1706.03850v3},
	file = {:E$\backslash$:/PhD/Papedrs/1706.03850.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Adversarial Feature Matching for Text Generation}},
	year = {2017}
}
@article{Sculley2010,
	author = {Sculley, D},
	file = {:E$\backslash$:/Downloads/Work/fastkmeans.pdf:pdf},
	isbn = {9781605587998},
	mendeley-groups = {11Thesis},
	pages = {4--5},
	title = {{Web-Scale K-Means Clustering}},
	year = {2010}
}
@article{Zhang2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.00614v2},
	author = {Zhang, Quanshi and Zhu, Song-chun},
	eprint = {arXiv:1802.00614v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.00614.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Visual},
	title = {{Visual Interpretability for Deep Learning: a Survey}},
	year = {2017}
}
@article{Huysmans2011,
	abstract = {An important objective of data mining is the development of predictive models. Based on a number of observations, a model is constructed that allows the analysts to provide classifications or predictions for new observations. Currently, most research focuses on improving the accuracy or precision of these models and comparatively little research has been undertaken to increase their comprehensibility to the analyst or end-user. This is mainly due to the subjective nature of 'comprehensibility', which depends on many factors outside the model, such as the user's experience and his/her prior knowledge. Despite this influence of the observer, some representation formats are generally considered to be more easily interpretable than others. In this paper, an empirical study is presented which investigates the suitability of a number of alternative representation formats for classification when interpretability is a key requirement. The formats under consideration are decision tables, (binary) decision trees, propositional rules, and oblique rules. An end-user experiment was designed to test the accuracy, response time, and answer confidence for a set of problem-solving tasks involving the former representations. Analysis of the results reveals that decision tables perform significantly better on all three criteria, while post-test voting also reveals a clear preference of users for decision tables in terms of ease of use. ?? 2010 Elsevier B.V. All rights reserved.},
	author = {Huysmans, Johan and Dejaeger, Karel and Mues, Christophe and Vanthienen, Jan and Baesens, Bart},
	doi = {10.1016/j.dss.2010.12.003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huysmans et al. - 2011 - An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models.pdf:pdf},
	isbn = {0167-9236},
	issn = {01679236},
	journal = {Decision Support Systems},
	keywords = {Classification,Comprehensibility,Data mining,Decision tables,Knowledge representation},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	number = {1},
	pages = {141--154},
	publisher = {Elsevier B.V.},
	title = {{An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models}},
	url = {http://dx.doi.org/10.1016/j.dss.2010.12.003},
	volume = {51},
	year = {2011}
}
@article{Veale2017,
	abstract = {Machine learning systems are increasingly used to sup-port public sector decision-making across a variety of sectors. Given concerns around accountability in these domains, and amidst accusations of intentional or unintentional bias, there have been increased calls for transparency of these technologies. Few, however, have considered how logics and practices concerning transparency have been understood by those involved in the machine learning systems already being piloted and deployed in public bodies today. This short pa-per distils insights about transparency on the ground from interviews with 27 such actors, largely public ser-vants and relevant contractors, across 5 OECD coun-tries. Considering transparency and opacity in relation to trust and buy-in, better decision-making, and the avoidance of gaming, it seeks to provide useful insights for those hoping to develop socio-technical approaches to transparency that might be useful to practitioners on-the-ground.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.09249v2},
	author = {Veale, Michael},
	eprint = {arXiv:1706.09249v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale - 2017 - Logics and practices of transparency and opacity in real-world applications of public sector machine learning.pdf:pdf},
	keywords = {()},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{Logics and practices of transparency and opacity in real-world applications of public sector machine learning}},
	year = {2017}
}
@article{Bolukbasi2016,
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	archivePrefix = {arXiv},
	arxivId = {1607.06520},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	eprint = {1607.06520},
	file = {:D$\backslash$:/Downloads/Play/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf:pdf},
	isbn = {9781510838819},
	issn = {10495258},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {Nips},
	pages = {1--9},
	title = {{Debiasing Word Embedding}},
	url = {https://code.google.com/archive/p/word2vec/},
	year = {2016}
}
@article{Grgic-Hlaca2017,
	abstract = {Consider a binary decision making process where a single machine learning classifier replaces a multitude of humans. We raise questions about the resulting loss of diversity in the decision making process. We study the potential benefits of using random classifier ensembles instead of a single classifier in the context of fairness-aware learning and demonstrate various attractive properties: (i) an ensemble of fair classifiers is guaranteed to be fair, for several different measures of fairness, (ii) an ensemble of unfair classifiers can still achieve fair outcomes, and (iii) an ensemble of classifiers can achieve better accuracy-fairness trade-offs than a single classifier. Finally, we introduce notions of distributional fairness to characterize further potential benefits of random classifier ensembles.},
	archivePrefix = {arXiv},
	arxivId = {1706.10208},
	author = {Grgi{\'{c}}-Hla{\v{c}}a, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1706.10208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grgi{\'{c}}-Hla{\v{c}}a et al. - 2017 - On Fairness, Diversity and Randomness in Algorithmic Decision Making.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{On Fairness, Diversity and Randomness in Algorithmic Decision Making}},
	url = {http://arxiv.org/abs/1706.10208},
	year = {2017}
}
@article{Goodman2016,
	abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
	archivePrefix = {arXiv},
	arxivId = {1606.08813},
	author = {Goodman, Bryce and Flaxman, Seth},
	eprint = {1606.08813},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
	isbn = {978-0-674-36827-9},
	journal = {2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)},
	keywords = {machine learning},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	number = {Whi},
	pages = {26--30},
	title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
	url = {http://arxiv.org/abs/1606.08813},
	year = {2016}
}
@article{Skirpan2017,
	abstract = {In this paper, we argue for the adoption of a normative definition of fairness within the machine learning community. After characterizing this definition, we review the current literature of Fair ML in light of its implications. We end by suggesting ways to incorporate a broader community and generate further debate around how to decide what is fair in ML.},
	archivePrefix = {arXiv},
	arxivId = {1706.09976},
	author = {Skirpan, Michael and Gorelick, Micha},
	eprint = {1706.09976},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skirpan, Gorelick - 2017 - The Authority of Fair in Machine Learning.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{The Authority of "Fair" in Machine Learning}},
	url = {http://arxiv.org/abs/1706.09976},
	year = {2017}
}
@article{Nguyen2014,
	author = {Nguyen, Tien T and Maxwell, Pik-mai Hui F and Loren, Harper and Joseph, Terveen},
	file = {:E$\backslash$:/PhD/Papedrs/nej1.pdf:pdf},
	isbn = {9781450327442},
	keywords = {content diversity,experience,filter bubble,longitudinal data,offerings,recommender system,sonalized product and information,tag-genome,they play a,user},
	mendeley-groups = {11Thesis/Interpretability/GAN's and VAE},
	pages = {677--686},
	title = {{Exploring the Filter Bubble : The Effect of Using Recommender Systems on Content Diversity}},
	year = {2014}
}
@article{Johnson2015,
	abstract = {Convolutional neural network (CNN) is a neu-ral network that can make use of the inter-nal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embed-ding of small text regions for use in classifi-cation. In addition to a straightforward adap-tation of CNN from image to text, a sim-ple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.},
	archivePrefix = {arXiv},
	arxivId = {1412.1058},
	author = {Johnson, Rie and Zhang, Tong},
	eprint = {1412.1058},
	file = {:C$\backslash$:/Users/Workk/Documents/1412.1058.pdf:pdf},
	isbn = {9781941643495},
	journal = {Naacl},
	mendeley-groups = {11Thesis},
	number = {2011},
	pages = {103--112},
	title = {{Effective Use of Word Order for Text Categorization with Convolutional Neural Networks}},
	url = {http://arxiv.org/abs/1412.1058{\%}5Cnhttp://arxiv.org/abs/1412.1058v1},
	year = {2015}
}
@article{FenTan2016,
	abstract = {Ensembles of decision trees have good prediction accuracy but suffer from a lack of interpretability. We propose a new approach for interpreting tree ensembles by finding prototypes in tree space, utilizing the naturally-learned similarity measure from the tree ensemble. Demonstrating the method on random forests, we show that the method benefits from two unique aspects of tree ensembles by leveraging tree structure to sequentially find prototypes, and utilizing the naturally-learned similarity measure from the tree ensemble. The method provides good prediction accuracy when found prototypes are used in nearest-prototype classifiers, while us-ing fewer prototypes than competitor methods. We are investigating the sensitivity of the method to different prototype-finding procedures and demonstrating it on higher-dimensional data.},
	archivePrefix = {arXiv},
	arxivId = {1611.07115},
	author = {{Fen Tan}, Hui and Hooker, Giles J and Wells, Martin T},
	eprint = {1611.07115},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fen Tan, Hooker, Wells - 2016 - Tree Space Prototypes Another Look at Making Tree Ensembles Interpretable.pdf:pdf},
	journal = {Nips},
	mendeley-groups = {Annotated/Decision Trees,!Paper 3/task/newsgroups,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	title = {{Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable}},
	year = {2016}
}
@article{Doshi-Velez2017,
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	archivePrefix = {arXiv},
	arxivId = {1702.08608},
	author = {Doshi-Velez, Finale and Kim, Been},
	eprint = {1702.08608},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {Ml},
	pages = {1--13},
	title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	year = {2017}
}
@article{Kitaev2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.01052v1},
	author = {Kitaev, Nikita and Klein, Dan},
	eprint = {arXiv:1805.01052v1},
	file = {:E$\backslash$:/1805.01052.pdf:pdf},
	mendeley-groups = {11Thesis},
	title = {{Constituency Parsing with a Self-Attentive Encoder}},
	year = {2017}
}
@article{Blodgett2017,
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
	archivePrefix = {arXiv},
	arxivId = {1707.00061},
	author = {Blodgett, Su Lin and O'Connor, Brendan},
	eprint = {1707.00061},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blodgett, O'Connor - 2017 - Racial Disparity in Natural Language Processing A Case Study of Social Media African-American English.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English}},
	url = {http://arxiv.org/abs/1707.00061},
	year = {2017}
}
@article{Ekstrand,
	author = {Ekstrand, Michael D and Kluver, Daniel and Harper, F Maxwell and Konstan, Joseph A},
	file = {:E$\backslash$:/PhD/Papedrs/MultiRecs-Author.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/GAN's and VAE},
	title = {{Letting Users Choose Recommender Algorithms : An Experimental Study}}
}
@article{Ananny2016,
	abstract = {Models for understanding and holding systems accountable have long rested upon ideals and logics of transparency. Being able to see a system is sometimes equated with being able to know how it works and govern it—a pattern that recurs in recent work about transparency and computational systems. But can “black boxes' ever be opened, and if so, would that ever be sufficient? In this article, we critically interrogate the ideal of transparency, trace some of its roots in scientific and sociotechnical epistemological cultures, and present 10 limitations to its application. We specifically focus on the inadequacy of transparency for understanding and governing algorithmic systems and sketch an alternative typology of algorithmic accountability grounded in constructive engagements with the limitations of transparency ideals.},
	author = {Ananny, Mike and Crawford, Kate},
	doi = {10.1177/1461444816676645},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ananny, Crawford - 2016 - Seeing without knowing Limitations of the transparency ideal and its application to algorithmic accountability.pdf:pdf},
	issn = {1461-4448},
	journal = {New Media {\&} Society},
	keywords = {accountability,algorithms,critical infrastructure studies,platform governance},
	mendeley-groups = {!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
	pages = {146144481667664},
	title = {{Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability}},
	url = {http://journals.sagepub.com/doi/10.1177/1461444816676645},
	year = {2016}
}
@inproceedings{Gladkova2016,
	abstract = {This paper presents an analysis of exist- ing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evalua- tions is “interpretability” of word embed- dings: a “good” embedding produces re- sults that make sense in terms of tradi- tional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of dis- tributional meaning representations. We argue for a shift from abstract ratings of word embedding “quality” to exploration of their strengths and weaknesses},
	author = {Gladkova, Anna and Drozd, Aleksandr},
	booktitle = {Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP},
	doi = {10.18653/v1/W16-2507},
	isbn = {9781945626142},
	mendeley-groups = {Report/Features,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	pages = {36--42},
	title = {{Intrinsic Evaluations of Word Embeddings: What Can We Do Better?}},
	year = {2016}
}
@article{Gong2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1809.06858v1},
	author = {Gong, Chengyue},
	eprint = {arXiv:1809.06858v1},
	file = {:E$\backslash$:/1809.06858.pdf:pdf},
	mendeley-groups = {11Thesis/State of the art},
	number = {Nips},
	pages = {1--15},
	title = {{FRAGE : Frequency-Agnostic Word Representation}},
	volume = {1},
	year = {2018}
}
@article{Sculley,
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Dennison, Dan},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/5656-hidden-technical-debt-in-machine-learning-systems.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Technical Debt},
	pages = {1--9},
	title = {{Hidden Technical Debt in Machine Learning Systems}}
}
@article{Gupta2015,
	author = {Gupta, Abhijeet and Boleda, Gemma and Baroni, Marco and Pad, Sebastian},
	file = {:C$\backslash$:/Users/Workk/Documents/EMNLP002.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {September},
	pages = {12--21},
	title = {{Distributional vectors encode referential attributes}},
	year = {2015}
}
@article{Ekstrand2014,
	author = {Ekstrand, Michael D and Harper, F Maxwell and Willemsen, Martijn C and Konstan, Joseph A},
	file = {:E$\backslash$:/PhD/Papedrs/listcmp.pdf:pdf},
	isbn = {9781450326681},
	keywords = {12,21,algorithms with comparable accuracy,movie recommendation domain,of,recommender systems,those differences in the,to map out some,user study,we present},
	mendeley-groups = {11Thesis/Interpretability/General},
	title = {{User Perception of Differences in Recommender Algorithms}},
	year = {2014}
}
@article{Le2014,
	abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
	archivePrefix = {arXiv},
	arxivId = {1405.4053},
	author = {Le, Qv and Mikolov, Tomas},
	doi = {10.1145/2740908.2742760},
	eprint = {1405.4053},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
	isbn = {9781634393973},
	issn = {10495258},
	journal = {International Conference on Machine Learning - ICML 2014},
	mendeley-groups = {Progress Report,Interim Review,11Thesis},
	pages = {1188--1196},
	pmid = {9377276},
	title = {{Distributed Representations of Sentences and Documents}},
	url = {http://arxiv.org/abs/1405.4053},
	volume = {32},
	year = {2014}
}
@article{Lakkaraju2017,
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	archivePrefix = {arXiv},
	arxivId = {1707.01154},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	eprint = {1707.01154},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju et al. - 2017 - Interpretable {\&} Explorable Approximations of Black Box Models.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	title = {{Interpretable {\&} Explorable Approximations of Black Box Models}},
	url = {http://arxiv.org/abs/1707.01154},
	year = {2017}
}
@article{Bau2017,
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
	archivePrefix = {arXiv},
	arxivId = {1704.05796},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	doi = {10.1109/CVPR.2017.354},
	eprint = {1704.05796},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1704.05796.pdf:pdf},
	isbn = {9781538604571},
	issn = {1530-1567},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Representations},
	pages = {3319--3327},
	pmid = {12882599},
	title = {{Network dissection: Quantifying interpretability of deep visual representations}},
	volume = {2017-Janua},
	year = {2017}
}
@article{Barocas2016,
	abstract = {Big data claims to be neutral. It isn't.Advocates of algorithmic techniques like data mining argue that they eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data mining can inherit the prejudices of prior decision-makers or reflect the widespread biases that persist in society at large. Often, the “patterns” it discovers are simply preexisting societal patterns of inequality and exclusion. Unthinking reliance on data mining can deny members of vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm's use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Article examines these concerns through the lens of American anti-discrimination law — more particularly, through Title VII's prohibition on discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining's victims would seem to lie in disparate impact doctrine. Case law and the EEOC's Uniform Guidelines, though, hold that a practice can be justified as a business necessity where its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. As a result, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others' discrimination against members of vulnerable groups, or flaws in the underlying data.Addressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, where the discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying anti-discrimination law: nondiscrimination and anti-subordination. Finding a solution to big data's disparate impact will require more than best efforts to stamp out prejudice and bias; it will require wholesale reexamination of the meanings of “discrimination” and “fairness.”},
	author = {Barocas, Solon and Selbst, Andrew},
	doi = {http://dx.doi.org/10.15779/Z38BG31},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barocas, Selbst - 2016 - Big Data ' s Disparate Impact.pdf:pdf},
	issn = {9780262327343},
	journal = {California law review},
	mendeley-groups = {Annotated/Overarching Interpretability,11Thesis/Interpretability/Discrimination},
	number = {1},
	pages = {671--729},
	title = {{Big Data ' s Disparate Impact}},
	url = {https://ssrn.com/abstract=2477899},
	volume = {104},
	year = {2016}
}
@article{Amato2009,
	author = {Amato, Claudia and Fanizzi, Nicola and Fazzinga, Bettina},
	file = {:C$\backslash$:/Users/Workk/Documents/Combining{\_}Semantic{\_}Web{\_}Search{\_}with{\_}the{\_}Power{\_}of{\_}In.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {June 2014},
	title = {{Combining Semantic Web Search with the Power of Inductive Reasoning . Combining Semantic Web Search with the Power of Inductive Reasoning}},
	year = {2009}
}
@article{Systems,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1610.01256v2},
	author = {Systems, Cyber-physical and Products, Data},
	eprint = {arXiv:1610.01256v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1610.01256.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Safety},
	pages = {1--20},
	title = {{On the Safety of Machine Learning : Cyber-Physical Systems , Decision Sciences , and Data Products}}
}
@article{Hu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1703.00955v4},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
	eprint = {arXiv:1703.00955v4},
	file = {:E$\backslash$:/PhD/Papedrs/1703.00955.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted},
	title = {{Toward Controlled Generation of Text}},
	year = {2017}
}
@book{Moewes,
	author = {Moewes, Christian and N{\"{u}}rnberger, Andreas},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/The safe and interpretable machine learning part.pdf:pdf},
	isbn = {9783642323775},
	mendeley-groups = {11Thesis/Interpretability/Safety},
	title = {{in Intelligent Data Analysis}}
}



@incollection{Zhang2012,
title = {Large-Scale Sparse Principal Component Analysis with Application to Text Data},
author = {Youwei Zhang and Laurent E. Ghaoui},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {532--539},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4337-large-scale-sparse-principal-component-analysis-with-application-to-text-data.pdf}
}

@article{Miller2017a,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.07269v1},
	author = {Miller, Tim},
	eprint = {arXiv:1706.07269v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 2017 - Explanation in Artificial Intelligence Insights from the Social Sciences.pdf:pdf},
	keywords = {explainability,explainable ai,explanation,interpretability,transparency},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
	title = {{Explanation in Artificial Intelligence : Insights from the Social Sciences}},
	year = {2017}
}
@article{Yang2018,
	author = {Yang, Zichao and Hu, Zhiting and Dyer, Chris and Xing, Eric P and Berg-kirkpatrick, Taylor},
	file = {:E$\backslash$:/PhD/Papedrs/7959-unsupervised-text-style-transfer-using-language-models-as-discriminators.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	number = {NeurIPS},
	pages = {1--12},
	title = {{Unsupervised Text Style Transfer using Language Models as Discriminators}},
	year = {2018}
}
@article{Lakhotia2018,
	author = {Lakhotia, Suyash and Bresson, Xavier},
	doi = {10.1109/CW.2018.00022},
	file = {:E$\backslash$:/PhD/Papedrs/08590017.pdf:pdf},
	isbn = {9781538673157},
	journal = {2018 International Conference on Cyberworlds (CW)},
	keywords = {-text classification,artificial,convolutional,feedforward neural networks,graph convolutional neural networks,machine learning,neural networks},
	mendeley-groups = {11Thesis/Neural network multi-l;abe},
	pages = {58--65},
	publisher = {IEEE},
	title = {{An Experimental Comparison of Text Classification Techniques}},
	year = {2018}
}
@article{Ustun2014,
	abstract = {We present an integer programming framework to build accurate and interpretable discrete linear classification models. Unlike existing approaches, our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice. To this end, our framework can produce models that are fully optimized for accuracy, by minimizing the 0--1 classification loss, and that address multiple aspects of interpretability, by incorporating a range of discrete constraints and penalty functions. We use our framework to produce models that are difficult to create with existing methods, such as scoring systems and M-of-N rule tables. In addition, we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction. We show that discrete linear classifiers can attain the training accuracy of any other linear classifier, and provide an Occam's Razor type argument as to why the use of small discrete coefficients can provide better generalization. We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis.},
	archivePrefix = {arXiv},
	arxivId = {1405.4047},
	author = {Ustun, Berk and Rudin, Cynthia},
	eprint = {1405.4047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ustun, Rudin - 2014 - Methods and Models for Interpretable Linear Classification.pdf:pdf},
	journal = {arXiv},
	mendeley-groups = {Annotated/Interpretable Classifiers,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	pages = {1--57},
	title = {{Methods and Models for Interpretable Linear Classification}},
	url = {http://arxiv.org/abs/1405.4047},
	year = {2014}
}
@article{Lundberg2016,
	abstract = {Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.},
	archivePrefix = {arXiv},
	arxivId = {1611.07478},
	author = {Lundberg, Scott and Lee, Su-In},
	eprint = {1611.07478},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Lee - 2016 - An unexpected unity among methods for interpreting model predictions.pdf:pdf},
	mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	number = {Nips},
	pages = {1--6},
	title = {{An unexpected unity among methods for interpreting model predictions}},
	url = {http://arxiv.org/abs/1611.07478},
	year = {2016}
}
@article{Ethayarajh2018,
	abstract = {A surprising property of word vectors is that vector algebra can often be used to solve word analogies. However, it is unclear why - and when - linear operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a rigorous explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has often conjectured that linear structures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel theoretical justification for the addition of SGNS word vectors by showing that it automatically down-scikit-learns the more frequent word, as scikit-learning schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, providing rigorous justification for its use in capturing word dissimilarity.},
	archivePrefix = {arXiv},
	arxivId = {1810.04882},
	author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
	eprint = {1810.04882},
	file = {:D$\backslash$:/Downloads/Play/1810.04882.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	title = {{Towards Understanding Linear Word Analogies}},
	url = {http://arxiv.org/abs/1810.04882},
	year = {2018}
}
@article{TomasMikolovWen-tauYih2013,
	abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer scikit-learns. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
	archivePrefix = {arXiv},
	arxivId = {1301.3781},
	author = {{Tomas Mikolov∗ , Wen-tau Yih}, Geoffrey Zweig},
	doi = {10.3109/10826089109058901},
	eprint = {1301.3781},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/rvecs.pdf:pdf},
	isbn = {9781937284473},
	issn = {9781937284473},
	journal = {Hlt-Naacl},
	mendeley-groups = {11Thesis},
	number = {June},
	pages = {746--751},
	pmid = {1938007},
	title = {{Linguistic Regularities in Continuous Space Word Representations}},
	url = {http://anthology.aclweb.org/N/N13/N13-1.pdf{\#}page=655},
	year = {2013}
}
@article{Viappiani2006,
abstract = {We consider interactive tools that help users search for their most preferred item in a large collection of options. In particular, we examine example-critiquing, a technique for enabling users to incrementally construct preference models by critiquing example options that are presented to them. We present novel techniques for improving the example-critiquing technology by adding suggestions to its displayed options. Such suggestions are calculated based on an analysis of users' current preference model and their potential hidden preferences. We evaluate the performance of our model-based suggestion techniques with both synthetic and real users. Results show that such suggestions are highly attractive to users and can stimulate them to express more preferences to improve the chance of identifying their most preferred item by up to 78{\%}.},
author = {Viappiani, P. and Faltings, B. and Pu, P.},
doi = {10.1613/jair.2075},
file = {:E$\backslash$:/Downloads/Work/10477-Article Text-19459-1-10-20180216.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
mendeley-groups = {11Thesis/Applications},
pages = {465--503},
title = {{Preference-based Search using Example-Critiquing with Suggestions}},
volume = {27},
year = {2006}
}

\cite{Viappiani2006}
@article{Curry,
	author = {Curry, Edward and Buitelaar, Paul},
	file = {:C$\backslash$:/Users/Workk/Documents/preprint{\_}nldb{\_}{\_}commonsense{\_}2014.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	pages = {1--12},
	title = {{A Distributional Semantics Approach for Selective Reasoning on Commonsense Graph Knowledge Bases}}
}
@article{Zhanga,
	author = {Zhang, Lei and Corporation, Linkedin},
	file = {:E$\backslash$:/1801.07883.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	title = {{Deep Learning for Sentiment Analysis : A Survey}}
}
@article{Rothe2016,
	author = {Rothe, Sascha and Processing, Language},
	file = {:C$\backslash$:/Users/Workk/Documents/P16-2083.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	pages = {512--517},
	title = {{Word Embedding Calculus in Meaningful Ultradense Subspaces}},
	year = {2016}
}
@article{Hardt2016,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1610.02413},
	author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
	eprint = {arXiv:1610.02413},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/6374-equality-of-opportunity-in-supervised-learning.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {Nips},
	title = {{Equality of Opportunity in Supervised Learning}},
	year = {2016}
}
@article{Gupta2015,
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1505.05192v1},
	author = {Gupta, Abhinav and Efros, Alexei a},
	doi = {10.1109/ICCV.2015.167},
	eprint = {arXiv:1505.05192v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta, Efros - 2015 - Unsupervised Visual Representation Learning by Context Prediction.pdf:pdf},
	isbn = {978-1-4673-8391-2},
	issn = {978-1-4673-8391-2},
	journal = {arXiv preprint},
	mendeley-groups = {Report/Features,11Thesis},
	pages = {1422--1430},
	pmid = {903},
	title = {{Unsupervised Visual Representation Learning by Context Prediction}},
	year = {2015}
}
@article{Kim2017,
	abstract = {Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate scikit-learning scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others.},
	author = {Kim, Han Kyul and Kim, Hyunjoong and Cho, Sungzoon},
	doi = {10.1016/j.neucom.2017.05.046},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kim, Cho - 2017 - Bag-of-concepts Comprehending document representation through clustering words in distributed representation.pdf:pdf},
	issn = {18728286},
	journal = {Neurocomputing},
	keywords = {Bag-of-concepts,Interpretable document representation,Word2vec clustering},
	mendeley-groups = {Annotated/Word Vectors,11Thesis/Interpretability,11Thesis/Interpretability/Representations,11Thesis},
	pages = {336--352},
	title = {{Bag-of-concepts: Comprehending document representation through clustering words in distributed representation}},
	volume = {266},
	year = {2017}
}
@article{Narayanan2018,
	abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
	archivePrefix = {arXiv},
	arxivId = {1802.00682},
	author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
	eprint = {1802.00682},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine Learning Systems An Evaluation of the Human-Interpretabilit.pdf:pdf},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability,11Thesis/Interpretability/Explanation,11Thesis/Interpretability/General},
	pages = {1--21},
	title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
	url = {http://arxiv.org/abs/1802.00682},
	year = {2018}
}
@article{Martinc,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1902.00438v2},
	author = {Martinc, Matej and Kralj, Jan and Pollak, Senja},
	eprint = {arXiv:1902.00438v2},
	file = {:E$\backslash$:/1902.00438.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Representations,11Thesis},
	title = {{tax2vec : Constructing Interpretable Features from Taxonomies for Short Text Classification}}
}
@article{Edunov2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1808.09381v2},
	author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David and Park, Menlo and Brain, Google and View, Mountain},
	eprint = {arXiv:1808.09381v2},
	file = {:E$\backslash$:/1808.09381.pdf:pdf},
	mendeley-groups = {11Thesis/State of the art},
	title = {{Understanding Back-Translation at Scale}},
	year = {2018}
}
@article{Lau2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1607.05368v1},
	author = {Lau, Jey Han and Baldwin, Timothy},
	eprint = {arXiv:1607.05368v1},
	file = {:C$\backslash$:/Users/Workk/Documents/1607.05368.pdf:pdf},
	mendeley-groups = {11Thesis},
	title = {{Practical Insights into Document Embedding Generation}},
	year = {2014}
}
@article{H.~Zou2006,
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
	archivePrefix = {arXiv},
	arxivId = {1205.0121v2},
	author = {H.{\~{}}Zou and T.{\~{}}Hastie and R.{\~{}}Tibshirani and Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	doi = {10.1198/106186006X113430},
	eprint = {1205.0121v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H.{\~{}}Zou et al. - 2006 - Sparse principal component analysis.pdf:pdf},
	isbn = {106186006X},
	issn = {1061-8600},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {arrays,ca 94305,composition,d student in the,department of statistics at,edu,elastic net,email,gene expression,gene expression arrays,hui zou is a,hzou,lasso,multivariate analysis,ph,singular,singular value de-,stanford,stanford university,stat,thresholding,value decomposition},
	mendeley-groups = {Annotated/NMF,11Thesis,!Paper 3},
	number = {2},
	pages = {265--286},
	pmid = {21811560},
	title = {{Sparse principal component analysis}},
	volume = {15},
	year = {2006}
}
@article{Freitas2013,
	abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
	author = {Freitas, Alex A.},
	doi = {10.1145/2594473.2594475},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas - 2013 - Comprehensible Classification Models - a position paper.pdf:pdf},
	isbn = {1931-0145},
	issn = {19310145},
	journal = {ACM SIGKDD Explorations Newsletter},
	keywords = {bayesian network classifiers,decision table,decision tree,monotonicity constraint,nearest neighbors,rule induction},
	mendeley-groups = {Report/Just about interpretability,Annotated/Overarching Interpretability,Report,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {1},
	pages = {1--10},
	title = {{Comprehensible Classification Models - a position paper}},
	url = {http://dl.acm.org.miman.bib.bth.se/citation.cfm?id=2594475},
	volume = {15},
	year = {2013}
}
@article{Dosilovic2018,
	author = {Do{\v{s}}ilovi{\'{c}}, Filip Karlo and Br{\v{c}}i{\'{c}}, Mario and Hlupi{\'{c}}, Nikica},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/dsdc{\_}11{\_}4754.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Explanation},
	pages = {232--237},
	title = {{232 Mipro 2018/Ds-Dc}},
	year = {2018}
}
@article{Kim2018,
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/kim18d.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	title = {{Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCAV )}},
	year = {2018}
}
@article{Prior2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.11571v2},
	author = {Prior, Human-in-the-loop Interpretability and Ross, Andrew Slavin and Kim, Been and Gershman, Samuel J},
	eprint = {arXiv:1805.11571v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1805.11571.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	number = {1},
	pages = {1--13},
	title = {{Human-in-the-Loop Interpretability Prior}},
	year = {2018}
}
@article{Gilpin,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.00069v2},
	author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	eprint = {arXiv:1806.00069v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1806.00069.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	title = {{Explaining Explanations : An Approach to Evaluating Interpretability of Machine Learning}}
}

@article{Xu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.01345v3},
	author = {Xu, Jingjing and Ren, Xuancheng and Lin, Junyang and Sun, Xu},
	eprint = {arXiv:1802.01345v3},
	file = {:E$\backslash$:/PhD/Papedrs/1802.01345.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation}},
	year = {2017}
}
@article{Zafar2017,
	abstract = {The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness--given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design convex margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.},
	archivePrefix = {arXiv},
	arxivId = {1707.00010},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1707.00010},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf:pdf},
	journal = {arXiv:1707.00010 [cs, stat]},
	keywords = {Algorithmic fairness,Machine learning},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{From Parity to Preference-based Notions of Fairness in Classification}},
	url = {http://arxiv.org/abs/1707.00010},
	year = {2017}
}
@article{Veale2018,
	abstract = {Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions — like taxation, justice, and child protection —-are now commonplace. How might design-ers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regard-ing challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning — absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications. NOTE 04/10/2017: This is a document currently under review. We'd be happy if you wanted to cite this — so please get in touch if that is the case. You can also circulate it to colleagues, but please don't publish it publicly. An earlier, shorter version of the paper is available that you can link to publicly: Veale, M (2017) Logics and practices of opacity and transparency in real-world applications of public sector machine learning. Presented as a talk at the 4th Workshop on Fairness, Accountability and Trans-parency in Machine Learning (FAT/ML 2017), Halifax, Nova Scotia, Canada. Available at:},
	archivePrefix = {arXiv},
	arxivId = {1802.01029},
	author = {Veale, Michael and Kleek, Max Van and Binns, Reuben},
	doi = {10.1145/3173574.3174014},
	eprint = {1802.01029},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale, Kleek, Binns - 2018 - Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Maki.pdf:pdf},
	isbn = {9781450356206},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
	title = {{Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making}},
	url = {http://},
	year = {2018}
}
@article{Gardenfors2014,
	author = {G{\"{a}}rdenfors, Peter},
	doi = {10.1007/978-1-4020-9877-2},
	file = {:C$\backslash$:/Users/Workk/Documents/Conceptual{\_}Spaces.pdf:pdf},
	isbn = {9781402098772},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {September},
	title = {{Conceptual spaces}},
	year = {2014}
}
@article{Lipton2016,
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	archivePrefix = {arXiv},
	arxivId = {1606.03490},
	author = {Lipton, Zachary C.},
	eprint = {1606.03490},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
	mendeley-groups = {Annotated/Overarching Interpretability,Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {Whi},
	title = {{The Mythos of Model Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	year = {2016}
}
@article{Chang2009,
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
	doi = {10.1.1.100.1089},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Models.pdf:pdf},
	isbn = {9781615679119},
	issn = {1098-6596},
	journal = {Advances in Neural Information Processing Systems 22},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	pages = {288----296},
	pmid = {25246403},
	title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
	url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
	year = {2009}
}
@article{Zhao,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1906.06719v1},
	author = {Zhao, Shenjian},
	eprint = {arXiv:1906.06719v1},
	file = {:E$\backslash$:/PhD/Papedrs/1906.06719.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Fixing Gaussian Mixture VAEs for Interpretable Text Generation}}
}
@article{Amodei2016,
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	archivePrefix = {arXiv},
	arxivId = {1606.06565},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
	doi = {1606.06565},
	eprint = {1606.06565},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
	isbn = {0387310738},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Safety},
	pages = {1--29},
	title = {{Concrete Problems in AI Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	year = {2016}
}
@article{Ruggieri2009,
	author = {Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/tkdd.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	title = {{Data Mining for Discrimination Discovery}},
	volume = {V},
	year = {2009}
}
@article{Wu,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1609.08144v2},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	eprint = {arXiv:1609.08144v2},
	file = {:E$\backslash$:/1609.08144.pdf (7.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	pages = {1--23},
	title = {{Google's Neural Machine Translation System : Bridging the Gap between Human and Machine Translation}}
}
@article{Pazzani2007,
	author = {Pazzani, Michael J and Billsus, Daniel},
	file = {:E$\backslash$:/10.1.1.130.8327.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	pages = {325--341},
	title = {{Content-Based Recommendation Systems}},
	year = {2007}
}

@article{Fisch2016,
abstract = {This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
archivePrefix = {arXiv},
arxivId = {1704.00051},
author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
doi = {10.18653/v1/P17-1171},
eprint = {1704.00051},
file = {:E$\backslash$:/1704.00051.pdf:pdf},
isbn = {9781945626753},
journal = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
mendeley-groups = {11Thesis/Applications},
pages = {1870--1879},
title = {{Reading Wikipedia to answer open-domain questions}},
volume = {1},
year = {2017}
}

\cite{Fisch2016}
@article{Rifai2011,
	abstract = {Although the structure and composition of plant communities is known to influence the functioning of ecosystems, there is as yet no agreement as to how these should be described from a functional perspective. We tested the biomass ratio hypothesis, which postulates that ecosystem properties should depend on species traits and on species contribution to the total biomass of the community, in a successional sere following vineyard abandonment in the Mediterranean region of France. Ecosystem-specific net primary productivity, litter decomposition rate, and total soil carbon and nitrogen varied significantly with field age, and correlated with community-aggregated (i.e., weighed according to the relative abundance of species) functional leaf traits. The three easily measurable traits tested, specific leaf area, leaf dry matter content, and nitrogen concentration, provide a simple means to scale up from organ to ecosystem functioning in complex plant communities. We propose that they be called {\&}8220;functional markers,{\&}8221; and be used to assess the impacts of community changes on ecosystem properties induced, in particular, by global change drivers.},
	author = {Rifai, Salah and Muller, Xavier},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rifai, Muller - 2011 - Contractive Auto-Encoders Explicit Invariance During Feature Extraction.pdf:pdf},
	isbn = {978-1-4503-0619-5},
	journal = {Icml},
	number = {1},
	pages = {833--840},
	title = {{Contractive Auto-Encoders : Explicit Invariance During Feature Extraction}},
	url = {http://www.icml-2011.org/papers/455{\_}icmlpaper.pdf},
	volume = {85},
	year = {2011}
}
@book{Carroll1976,
	abstract = {The authors' general objective was to demonstrate that three affective dimensions of meaning -- Evaluation, Potency, and Activity (E-P-A) -- are in fact pancultural. the study was done by 80 researchers in 20 different countries.},
	author = {Carroll, John B},
	booktitle = {The American Journal of Psychology},
	isbn = {9780252004261},
	keywords = {ACTIVITY,Dimension,affect,evaluation,meaning,potency,semantics,space,universals,word class},
	number = {1},
	pages = {172--178},
	title = {{Cross-Cultural Universals of Affective Meaning}},
	volume = {89},
	year = {1976}
}
@article{Hayes2017,
	abstract = {Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.},
	author = {Hayes, Bradley and Shah, Julie A.},
	doi = {10.1145/2909824.3020233},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hayes, Shah - 2017 - Improving Robot Controller Transparency Through Autonomous Policy Explanation.pdf:pdf},
	isbn = {9781450343367},
	issn = {21672148},
	journal = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17},
	mendeley-groups = {Annotated/Overarching Interpretability},
	pages = {303--312},
	title = {{Improving Robot Controller Transparency Through Autonomous Policy Explanation}},
	url = {http://dl.acm.org/citation.cfm?doid=2909824.3020233},
	year = {2017}
}
@article{Caruana2015,
	abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
	author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
	doi = {10.1145/2783258.2788613},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caruana et al. - 2015 - Intelligible Models for HealthCare.pdf:pdf},
	isbn = {9781450336642},
	journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
	keywords = {additive models,classification,healthcare,intelligibility,interaction detection,logistic regression,risk prediction},
	mendeley-groups = {Report/Explaining predictions},
	pages = {1721--1730},
	title = {{Intelligible Models for HealthCare}},
	url = {http://dl.acm.org/citation.cfm?id=2783258.2788613},
	year = {2015}
}
@article{Utgoff2002,
	abstract = {We explore incremental assimilation of new knowledge by sequential learning. Of particular interest is how a network of many knowledge layers can be constructed in an on-line manner, such that the learned units represent building blocks of knowledge that serve to compress the overall representation and facilitate transfer. We motivate the need for many layers of knowledge, and we advocate sequential learning as an avenue for promoting the construction of layered knowledge structures. Finally, our novel STL algorithm demonstrates a method for simultaneously acquiring and organizing a collection of concepts and functions as a network from a stream of unstructured information.},
	author = {Utgoff, Paul E and Stracuzzi, David J},
	doi = {10.1109/DEVLRN.2002.1011824},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Utgoff, Stracuzzi - 2002 - Many-layered learning.pdf:pdf},
	isbn = {0-7695-1459-6},
	issn = {0899-7667},
	journal = {Neural computation},
	number = {10},
	pages = {2497--2529},
	pmid = {12396572},
	title = {{Many-layered learning.}},
	volume = {14},
	year = {2002}
}
@article{Model2018,
	author = {Model, Autoregressive Exogenous and Lu, Yao},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Model, Lu - 2018 - INTERPRETABLE LSTM NEURAL NETWORK FOR.pdf:pdf},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	pages = {1--7},
	title = {{INTERPRETABLE LSTM NEURAL NETWORK FOR}},
	year = {2018}
}
@article{Karaletsos2015,
	abstract = {Representation learning systems typically rely on massive amounts of labeled data in order to be trained effectively. Recently, high-dimensional parametric models like convolutional neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Human-in-the-loop systems like crowdsourcing are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. We propose to combine generative unsupervised feature learning with learning from similarity orderings in order to learn models which take advantage of privileged information coming from the crowd. We use a fast variational algorithm to learn the model on standard datasets and demonstrate applicability to two image datasets, where classification is drastically improved. We show how triplet-samples of the crowd can supplement labels as a source of information to shape latent spaces with rich semantic information.},
	archivePrefix = {arXiv},
	arxivId = {1506.05011},
	author = {Karaletsos, Theofanis and Belongie, Serge and R{\"{a}}tsch, Gunnar},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1506.05011},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karaletsos, Belongie, R{\"{a}}tsch - 2015 - Bayesian representation learning with oracle constraints.pdf:pdf},
	journal = {Iclr},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Bayesian representation learning with oracle constraints}},
	url = {http://arxiv.org/abs/1506.05011},
	year = {2015}
}
@article{Towell1990,
	author = {Towell, Geoffrey G and Shavlik, Jude W and Noordeweir, Michiel O},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik, Noordeweir - 1990 - Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks.pdf:pdf},
	journal = {Proceedings of the Eighth National Conference on Artificial Intelligence},
	pages = {861--866},
	title = {{Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks}},
	year = {1990}
}
@article{Zhang2010a,
	abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
	author = {Zhang, Min-Ling and Zhang, Kun},
	doi = {10.1145/1835804.1835930},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
	isbn = {9781450300551},
	issn = {9781577355687},
	journal = {Kdd},
	mendeley-groups = {Progress Report},
	pages = {999--1007},
	title = {{Multi-label learning by exploiting label dependency}},
	url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
	year = {2010}
}
@article{Burges1998a,
	abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
	archivePrefix = {arXiv},
	arxivId = {1111.6189v1},
	author = {Burges, C.J.C. J Christopher J C},
	doi = {10.1023/A:1009715923555},
	eprint = {1111.6189v1},
	isbn = {0818672404},
	issn = {13845810},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
	number = {2},
	pages = {121--167},
	pmid = {5207842081938259593},
	title = {{A tutorial on support vector machines for pattern recognition}},
	url = {http://www.springerlink.com/index/Q87856173126771Q.pdf},
	volume = {2},
	year = {1998}
}
@article{Verbeke2011,
	abstract = {Customer churn prediction models aim to detect customers with a high propensity to attrite. Predictive accuracy, comprehensibility, and justifiability are three key aspects of a churn prediction model. An accurate model permits to correctly target future churners in a retention marketing campaign, while a comprehensible and intuitive rule-set allows to identify the main drivers for customers to churn, and to develop an effective retention strategy in accordance with domain knowledge. This paper provides an extended overview of the literature on the use of data mining in customer churn prediction modeling. It is shown that only limited attention has been paid to the comprehensibility and the intuitiveness of churn prediction models. Therefore, two novel data mining techniques are applied to churn prediction modeling, and benchmarked to traditional rule induction techniques such as C4.5 and RIPPER. Both AntMiner+ and ALBA are shown to induce accurate as well as comprehensible classification rule-sets. AntMiner+ is a high performing data mining technique based on the principles of Ant Colony Optimization that allows to include domain knowledge by imposing monotonicity constraints on the final rule-set. ALBA on the other hand combines the high predictive accuracy of a non-linear support vector machine model with the comprehensibility of the rule-set format. The results of the benchmarking experiments show that ALBA improves learning of classification techniques, resulting in comprehensible models with increased performance. AntMiner+ results in accurate, comprehensible, but most importantly justifiable models, unlike the other modeling techniques included in this study. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
	author = {Verbeke, Wouter and Martens, David and Mues, Christophe and Baesens, Bart},
	doi = {10.1016/j.eswa.2010.08.023},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbeke et al. - 2011 - Building comprehensible customer churn prediction models with advanced rule induction techniques.pdf:pdf},
	isbn = {0957-4174},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {ALBA,Ant Colony Optimization,Churn prediction,Classification,Comprehensible rule induction,Data mining},
	mendeley-groups = {Annotated/Applications/Marketing},
	number = {3},
	pages = {2354--2364},
	publisher = {Elsevier Ltd},
	title = {{Building comprehensible customer churn prediction models with advanced rule induction techniques}},
	url = {http://dx.doi.org/10.1016/j.eswa.2010.08.023},
	volume = {38},
	year = {2011}
}
@article{Herlocker2000,
	abstract = {Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.},
	archivePrefix = {arXiv},
	arxivId = {48},
	author = {Herlocker, Jonathan L and Konstan, Joseph a and Riedl, John},
	doi = {10.1145/358916.358995},
	eprint = {48},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herlocker, Konstan, Riedl - 2000 - Explaining collaborative filtering recommendations.pdf:pdf},
	isbn = {1581132220},
	issn = {00318655},
	journal = {Proceedings of the 2000 ACM conference on Computer supported cooperative work},
	keywords = {Explanations,GroupLens,MovieLens,collaborative filtering,recommender systems},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {241--250},
	pmid = {8234466},
	title = {{Explaining collaborative filtering recommendations}},
	url = {http://dl.acm.org/citation.cfm?id=358995},
	year = {2000}
}
@article{Ding2014,
	author = {Ding, Shifei and Jia, Hongjie and Chen, Jinrong and Jin, Fengxiang},
	doi = {10.1007/s10462-012-9313-7},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2014 - Granular neural networks.pdf:pdf},
	isbn = {0269-2821},
	issn = {02692821},
	journal = {Artificial Intelligence Review},
	keywords = {Fuzzy neural networks,Granular neural networks,Rough neural networks},
	number = {3},
	pages = {373--384},
	title = {{Granular neural networks}},
	volume = {41},
	year = {2014}
}
@article{Bechberger,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.04825v1},
	author = {Bechberger, Lucas},
	eprint = {arXiv:1706.04825v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechberger - Unknown - Neural Representations.pdf:pdf},
	mendeley-groups = {Annotated/Conceptual Spaces and Neural Networks,Annotated/Generative Adversarial Nets},
	title = {{Neural Representations}}
}
@article{Peterson2016,
	abstract = {Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reach-ing or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representa-tions learned by these networks and human psychological rep-resentations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these fea-tures do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human sim-ilarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological exper-iments.},
	archivePrefix = {arXiv},
	arxivId = {1608.02164},
	author = {Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L},
	eprint = {1608.02164},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peterson, Abbott, Griffiths - 2016 - Adapting Deep Network Features to Capture Psychological Representations.pdf:pdf},
	keywords = {deep learning,neural networks,psychological,representations,similarity},
	mendeley-groups = {Report/Features,Progress Report},
	pages = {2363--2368},
	title = {{Adapting Deep Network Features to Capture Psychological Representations}},
	year = {2016}
}
@article{Vincent2010,
	abstract = {This paper presents the findings from a small-scale experiment investigating the presentation of a synchronous remote electronic examination. It discusses the students' experiences of taking such an examination. The study confirms that the majority of participants found the experience at least as good as a conventional written examination. In addition, typing answers does not prevent students from producing answers in the time available. However, the pressure of time continues to be a major cause of anxiety for students. The paper discusses technical issues, particularly those related to the loss of communications during the 3-hour duration of the exam. Although software processes were available to save and restore students' answers throughout the examination, problems still occurred and more robust software is required.},
	author = {Thomas, Pete and Price, Blaine and Paine, Carina and Richards, Michael},
	doi = {10.1111/1467-8535.00290},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas et al. - 2002 - Remote electronic examinations student experiences.pdf:pdf},
	isbn = {0007-1013},
	issn = {0007-1013},
	journal = {Journal of Machine Learning Research},
	number = {3},
	pages = {3371--3408},
	title = {{Remote electronic examinations: student experiences}},
	url = {http://oro.open.ac.uk/2572/},
	volume = {11},
	year = {2002}
}
@article{Blitzer2007,
	author = {Blitzer, J and Dredze, M and Pereira, F},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blitzer, Dredze, Pereira - 2007 - Biographies, Bollywood, boom-boxes and blenders Domain adaptation for sentiment classification.pdf:pdf},
	journal = {Proc. Assoc. Comput. Linguist. (ACL},
	title = {{Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification}},
	year = {2007}
}
@article{Mercado2016,
	abstract = {OBJECTIVE We investigated the effects of level of agent transparency on operator performance, trust, and workload in a context of human-agent teaming for multirobot management. BACKGROUND Participants played the role of a heterogeneous unmanned vehicle (UxV) operator and were instructed to complete various missions by giving orders to UxVs through a computer interface. An intelligent agent (IA) assisted the participant by recommending two plans-a top recommendation and a secondary recommendation-for every mission. METHOD A within-subjects design with three levels of agent transparency was employed in the present experiment. There were eight missions in each of three experimental blocks, grouped by level of transparency. During each experimental block, the IA was incorrect three out of eight times due to external information (e.g., commander's intent and intelligence). Operator performance, trust, workload, and usability data were collected. RESULTS Results indicate that operator performance, trust, and perceived usability increased as a function of transparency level. Subjective and objective workload data indicate that participants' workload did not increase as a function of transparency. Furthermore, response time did not increase as a function of transparency. CONCLUSION Unlike previous research, which showed that increased transparency resulted in increased performance and trust calibration at the cost of greater workload and longer response time, our results support the benefits of transparency for performance effectiveness without additional costs. APPLICATION The current results will facilitate the implementation of IAs in military settings and will provide useful data to the design of heterogeneous UxV teams.},
	author = {Mercado, Joseph E. and Rupp, Michael A. and Chen, Jessie Y. C. and Barnes, Michael J. and Barber, Daniel and Procci, Katelyn},
	doi = {10.1177/0018720815621206},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mercado et al. - 2016 - Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management.pdf:pdf},
	issn = {0018-7208},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	keywords = {address correspondence to joseph,agent teaming,army,e,human,human research and engineering,intelligent agent transparency,mercado,multi-uxv management,research laboratory,s,u},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {3},
	pages = {401--415},
	pmid = {26867556},
	title = {{Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management}},
	url = {http://journals.sagepub.com/doi/10.1177/0018720815621206},
	volume = {58},
	year = {2016}
}
@article{Liang2017,
	abstract = {This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1703.03055},
	author = {Liang, Xiaodan and Lin, Liang and Shen, Xiaohui and Feng, Jiashi and Yan, Shuicheng and Xing, Eric P.},
	doi = {10.1109/CVPR.2017.234},
	eprint = {1703.03055},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2017 - Interpretable Structure-Evolving LSTM(2).pdf:pdf},
	issn = {1703.03055},
	mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
	number = {61622214},
	pages = {1010--1019},
	title = {{Interpretable Structure-Evolving LSTM}},
	url = {http://arxiv.org/abs/1703.03055},
	year = {2017}
}
@article{Martins2013,
	abstract = {We present fast, accurate, direct non-projective dependency parsers with third-order features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-of- the-art accuracies for the largest datasets (English, Czech, and German).},
	author = {Martins, Andre and Almeida, Miguel and Smith, Noah A},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins, Almeida, Smith - 2013 - Turning on the Turbo Fast Third-Order Non-Projective Turbo Parsers.pdf:pdf},
	isbn = {9781937284510},
	journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	pages = {617--622},
	title = {{Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers}},
	url = {http://www.aclweb.org/anthology/P13-2109{\%}5Cnhttp://www.cs.cmu.edu/{~}nasmith/papers/martins+almeida+smith.acl13.pdf},
	year = {2013}
}
@article{Evans2017,
	abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework ({\$}\backslashpartial{\$}ILP), which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
	archivePrefix = {arXiv},
	arxivId = {1711.04574},
	author = {Evans, Richard and Grefenstette, Edward},
	eprint = {1711.04574},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans, Grefenstette - 2017 - Learning Explanatory Rules from Noisy Data.pdf:pdf},
	mendeley-groups = {!Paper 3},
	pages = {1--64},
	title = {{Learning Explanatory Rules from Noisy Data}},
	url = {http://arxiv.org/abs/1711.04574},
	volume = {61},
	year = {2017}
}
@article{Etchells2006a,
	abstract = {There is much interest in rule extraction from neural networks and a plethora of different methods have been proposed for this purpose. We discuss the merits of pedagogical and decompositional approaches to rule extraction from trained neural networks, and show that some currently used methods for binary data comply with a theoretical formalism for extraction of Boolean rules from continuously valued logic. This formalism is extended into a generic methodology for rule extraction from smooth decision surfaces fitted to discrete or quantized continuous variables independently of the analytical structure of the underlying model, and in a manner that is efficient even for high input dimensions. This methodology is then tested with Monks' data, for which exact rules are obtained and to Wisconsin's breast cancer data, where a small number of high-order rules are identified whose discriminatory performance can be directly visualized.},
	author = {Etchells, Terence A. and Lisboa, Paulo J G},
	doi = {10.1109/TNN.2005.863472},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Etchells, Lisboa - 2006 - Orthogonal Search-Based Rule Extraction (OSRE) for Trained Neural Networks A Practical and Efficient Approach.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Neura networks,Rule extraction},
	number = {2},
	pages = {374--384},
	pmid = {16566465},
	title = {{Orthogonal Search-Based Rule Extraction (OSRE) for Trained Neural Networks: A Practical and Efficient Approach}},
	volume = {17},
	year = {2006}
}
@article{Cooijmans2016,
	abstract = {We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.},
	archivePrefix = {arXiv},
	arxivId = {1603.09025},
	author = {Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'{e}}sar and G{\"{u}}l{\c{c}}ehre, {\c{C}}ağlar and Courville, Aaron},
	doi = {10.1227/01.NEU.0000210260.55124.A4},
	eprint = {1603.09025},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cooijmans et al. - 2016 - Recurrent Batch Normalization.pdf:pdf},
	isbn = {9783319464657},
	issn = {16113349},
	mendeley-groups = {!Paper 3/Training LSTMs},
	number = {Section 3},
	pages = {1--13},
	pmid = {26774160},
	title = {{Recurrent Batch Normalization}},
	url = {http://arxiv.org/abs/1603.09025},
	year = {2016}
}
@misc{,
	mendeley-groups = {Report},
	title = {{2001-Li-ICDM}}
}
@article{Erhan2014,
	abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
	archivePrefix = {arXiv},
	arxivId = {1312.2249},
	author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
	doi = {10.1109/CVPR.2014.276},
	eprint = {1312.2249},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan et al. - 2014 - Scalable Object Detection Using Deep Neural Networks.pdf:pdf},
	isbn = {978-1-4799-5118-5},
	issn = {10636919},
	journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
	mendeley-groups = {Progress Report},
	pages = {2155--2162},
	title = {{Scalable Object Detection Using Deep Neural Networks}},
	url = {http://arxiv.org/abs/1312.2249{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909673},
	year = {2014}
}
@article{Fern2014,
	author = {Fern, Manuel and Cernadas, Eva},
	file = {:C$\backslash$:/Users/Workk/Documents/delgado14a.pdf:pdf},
	pages = {3133--3181},
	title = {{Do we Need Hundreds of Classifiers to Solve Real World Classification Problems ?}},
	volume = {15},
	year = {2014}
}
@article{Bengio2009,
	abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
	archivePrefix = {arXiv},
	arxivId = {submit/0500581},
	author = {Bengio, Yoshua},
	doi = {10.1561/2200000006},
	eprint = {0500581},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
	isbn = {2200000006},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	mendeley-groups = {Papers/Paper 1,Report/Features,Progress Report,Report},
	number = {1},
	pages = {1--127},
	pmid = {17348934},
	primaryClass = {submit},
	title = {{Learning Deep Architectures for AI}},
	volume = {2},
	year = {2009}
}
@article{Turney2010,
	abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
	archivePrefix = {arXiv},
	arxivId = {1003.1141},
	author = {Turney, Peter D. and Pantel, Patrick},
	doi = {10.1613/jair.2934},
	eprint = {1003.1141},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turney, Pantel - 2010 - From frequency to meaning Vector space models of semantics.pdf:pdf},
	isbn = {1076-9757},
	issn = {10769757},
	journal = {Journal of Artificial Intelligence Research},
	mendeley-groups = {Report/Explaining predictions,Report/Clustering/properties},
	pages = {141--188},
	title = {{From frequency to meaning: Vector space models of semantics}},
	volume = {37},
	year = {2010}
}
@article{Kulkarni2015,
	abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
	archivePrefix = {arXiv},
	arxivId = {1503.03167},
	author = {Kulkarni, Td and Whitney, W},
	doi = {10.1063/1.4914407},
	eprint = {1503.03167},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network.pdf:pdf},
	issn = {10897550},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Progress Report},
	pages = {2539--2547},
	title = {{Deep Convolutional Inverse Graphics Network}},
	url = {http://arxiv.org/abs/1503.03167},
	year = {2015}
}
@article{Garc??a2009,
	abstract = {Classification in imbalanced domains is a recent challenge in data mining. We refer to imbalanced classification when data presents many examples from one class and few from the other class, and the less representative class is the one which has more interest from the point of view of the learning task. One of the most used techniques to tackle this problem consists in preprocessing the data previously to the learning process. This preprocessing could be done through under-sampling; removing examples, mainly belonging to the majority class; and over-sampling, by means of replicating or generating new minority examples. In this paper, we propose an under-sampling procedure guided by evolutionary algorithms to perform a training set selection for enhancing the decision trees obtained by the C4.5 algorithm and the rule sets obtained by PART rule induction algorithm. The proposal has been compared with other under-sampling and over-sampling techniques and the results indicate that the new approach is very competitive in terms of accuracy when comparing with over-sampling and it outperforms standard under-sampling. Moreover, the obtained models are smaller in terms of number of leaves or rules generated and they can considered more interpretable. The results have been contrasted through non-parametric statistical tests over multiple data sets. Crown Copyright ?? 2009.},
	author = {Garc??a, Salvador and Fern??ndez, Alberto and Herrera, Francisco},
	doi = {10.1016/j.asoc.2009.04.004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garca, Fernndez, Herrera - 2009 - Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with.pdf:pdf},
	isbn = {15684946},
	issn = {15684946},
	journal = {Applied Soft Computing Journal},
	keywords = {Data reduction,Decision trees,Evolutionary algorithms,Imbalanced classification,Rule induction,Training set selection},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {4},
	pages = {1304--1314},
	title = {{Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with evolutionary training set selection over imbalanced problems}},
	volume = {9},
	year = {2009}
}
@book{Edwards2017,
	abstract = {ABSTRACT This article reflects the kinds of situations and spaces where people and algorithms meet. In what situations do people become aware of algorithms? How do they experience and make sense of these algorithms, given their often hidden and invisible nature? To what extent does an awareness of algorithms affect people's use of these platforms, if at all? To help answer these questions, this article examines people's personal stories about the Facebook algorithm through tweets and interviews with 25 ordinary users. To understand the spaces where people and algorithms meet, this article develops the notion of the algorithmic imaginary. It is argued that the algorithmic imaginary - ways of thinking about what algorithms are, what they should be and how they function - is not just productive of different moods and sensations but plays a generative role in moulding the Facebook algorithm itself. Examining how algorithms make people feel, then, seems crucial if we want to understand their social power.},
	author = {Edwards, Lilian and Veale, Michael},
	booktitle = {SSRN Electronic Journal},
	doi = {10.2139/ssrn.2972855},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards, Veale - 2017 - Slave to the Algorithm Why a Right to Explanationn is Probably Not the Remedy You are Looking for.pdf:pdf},
	isbn = {3540445668},
	issn = {1556-5068},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability},
	pages = {1--65},
	title = {{Slave to the Algorithm? Why a Right to Explanationn is Probably Not the Remedy You are Looking for}},
	url = {https://www.ssrn.com/abstract=2972855},
	volume = {2017},
	year = {2017}
}
@article{Kusner2010,
	author = {Kusner, Matt J and Weinberger, Kilian Q and Louis, St and Edu, Kilian Wustl},
	file = {:E$\backslash$:/kusnerb15.pdf:pdf},
	title = {{From Word Embeddings To Document Distances}},
	year = {2010}
}
@article{Lavra??1999,
	abstract = {Widespread use of medical information systems and explosive growth of medical databases require traditional manual data analysis to be coupled with methods for efficient computer-assisted analysis. This paper presents selected data mining techniques that can be applied in medicine, and in particular some machine learning techniques including the mechanisms that make them better suited for the analysis of medical databases (derivation of symbolic rules, use of background knowledge, sensitivity and specificity of induced descriptions). The importance of the interpretability of results of data analysis is discussed and illustrated on selected medical applications.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Lavra??, Nada},
	doi = {10.1016/S0933-3657(98)00062-1},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavra - 1999 - Selected techniques for data mining in medicine.pdf:pdf},
	isbn = {0933-3657},
	issn = {09333657},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Data mining,Machine learning,Medical applications},
	mendeley-groups = {Report/Medical domain},
	number = {1},
	pages = {3--23},
	pmid = {10225344},
	title = {{Selected techniques for data mining in medicine}},
	volume = {16},
	year = {1999}
}
@article{Shrikumar2017,
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
	archivePrefix = {arXiv},
	arxivId = {1704.02685},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	eprint = {1704.02685},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf},
	mendeley-groups = {Report},
	title = {{Learning Important Features Through Propagating Activation Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	year = {2017}
}
@article{Pennington2014,
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
	archivePrefix = {arXiv},
	arxivId = {1504.06654},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
	doi = {10.3115/v1/D14-1162},
	eprint = {1504.06654},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
	isbn = {9781937284961},
	issn = {10495258},
	journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1532--1543},
	pmid = {1710995},
	title = {{GloVe: Global Vectors for Word Representation}},
	year = {2014}
}
@article{Towell1990a,
	author = {Towell, Geoffrey G and Shavlik, Jude W and Noordeweir, Michiel O},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik, Noordeweir - 1990 - Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks.pdf:pdf},
	journal = {Proceedings of the Eighth National Conference on Artificial Intelligence},
	pages = {861--866},
	title = {{Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks}},
	year = {1990}
}
@article{Vilnis2015,
	abstract = {Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.},
	archivePrefix = {arXiv},
	arxivId = {1412.6623},
	author = {Vilnis, Luke and McCallum, Andrew},
	eprint = {1412.6623},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vilnis, McCallum - 2015 - Word Representations via Gaussian Embedding.pdf:pdf},
	journal = {Iclr},
	mendeley-groups = {Progress Report},
	pages = {12},
	title = {{Word Representations via Gaussian Embedding}},
	url = {http://arxiv.org/abs/1412.6623},
	year = {2015}
}
@article{Kiros,
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros et al. - Unknown - Skip-Thought Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	number = {786},
	pages = {1--9},
	title = {{Skip-Thought Vectors}}
}
@article{Pfenning2001,
	abstract = {We reconsider the foundations of modal logic, following Martin-L{\"{o}}f's methodology of distinguishing judgments from propositions. We give constructive meaning explanations for necessity and possibility, which yields a simple and uniform system of natural deduction for intuitionistic modal logic that does not exhibit anomalies found in other proposals. We also give a new presentation of lax logic and find that the lax modality is already expressible using possibility and necessity. Through a computational interpretation of proofs in modal logic we further obtain a new formulation of Moggi's monadic metalanguage.$\backslash$n},
	author = {Pfenning, Frank and Davies, Rowan},
	doi = {10.1017/S0960129501003322},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfenning, Davies - 2001 - A judgmental reconstruction of modal logic.pdf:pdf},
	isbn = {0960129501003},
	issn = {0960-1295},
	journal = {Mathematical Structures in Computer Science},
	mendeley-groups = {Progress Report},
	number = {4},
	pages = {511--540},
	title = {{A judgmental reconstruction of modal logic}},
	url = {http://www.journals.cambridge.org/abstract{\_}S0960129501003322},
	volume = {11},
	year = {2001}
}
@article{Grodzicki2008a,
	abstract = {This paper considers the multilabel classification problem, which$\backslash$nis a generalization of traditional two-class or multi-class classification$\backslash$nproblem. In multilabel classification a set of labels (categories)$\backslash$nis given and each training instance is associated with a subset of$\backslash$nthis label-set. The task is to output the appropriate subset of labels$\backslash$n(generally of unknown size) for a given, unknown testing instance.$\backslash$nSome improvements to the existing neural network multilabel classification$\backslash$nalgorithm, named BP-MLL, are proposed here. The modifications concern$\backslash$nthe form of the global error function used in BP-MLL. The modified$\backslash$nclassification system is tested in the domain of functional genomics,$\backslash$non the yeast genome data set. Experimental results show that proposed$\backslash$nmodifications visibly improve the performance of the neural network$\backslash$nbased multilabel classifier. The results are statistically significant.$\backslash$n漏 2008 Springer-Verlag Berlin Heidelberg.},
	author = {Grodzicki, Rafa{\l} and Ma{\'{n}}dziuk, Jacek and Wang, Lipo},
	doi = {10.1007/978-3-540-87700-4_41},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grodzicki, Ma{\'{n}}dziuk, Wang - 2008 - Improved multilabel classification with neural networks.pdf:pdf},
	isbn = {3540876995},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Backpropagation,Bioinformatics,Functional genomics,Learning system,Multilabel,Neural network},
	number = {2},
	pages = {409--416},
	title = {{Improved multilabel classification with neural networks}},
	volume = {5199 LNCS},
	year = {2008}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - arffs.groovy:groovy},
	title = {arffs}
}
@article{David1992,
	author = {David, Douglass R Cuttingl and Kargerl, R and Tukey, John W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/David, Kargerl, Tukey - 1992 - Scatter Gather Browsing A Cluster-based Large Document Approach Collections to Scatter Gather.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	title = {{Scatter / Gather : Browsing A Cluster-based Large Document Approach Collections to Scatter / Gather}},
	year = {1992}
}
@article{Maas2011,
	abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
	author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	doi = {978-1-932432-87-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:pdf},
	isbn = {9781932432879},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Datasets,!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {142--150},
	title = {{Learning Word Vectors for Sentiment Analysis}},
	year = {2011}
}
@article{Read2015a,
	abstract = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
	archivePrefix = {arXiv},
	arxivId = {1503.09022},
	author = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
	eprint = {1503.09022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Hollm{\'{e}}n - 2015 - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
	keywords = {meta-labels,multi-label classification,neural net-,problem transformation},
	mendeley-groups = {Interim Review},
	pages = {1--23},
	title = {{Multi-label Classification using Labels as Hidden Nodes}},
	url = {http://arxiv.org/abs/1503.09022},
	year = {2015}
}
@article{Zhao2017,
	author = {Zhao, Rui and Mao, Kezhi},
	doi = {10.1109/TFUZZ.2017.2690222},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Mao - 2017 - Fuzzy Bag-of-Words Model for Document Representation.pdf:pdf},
	issn = {1063-6706},
	journal = {IEEE Transactions on Fuzzy Systems},
	mendeley-groups = {Annotated/Document representation,!Paper 3/task/newsgroups},
	number = {8},
	pages = {1--1},
	title = {{Fuzzy Bag-of-Words Model for Document Representation}},
	url = {http://ieeexplore.ieee.org/document/7891009/},
	volume = {14},
	year = {2017}
}
@article{Ager2012,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}LabelFix (2).pdf:pdf},
	mendeley-groups = {Temp},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Lai2016,
	abstract = {We analyze three critical components of word embedding training: the model, the corpus, and the training parameters. We systematize existing neural-network-based word embedding algorithms and compare them using the same corpus. We evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks and using it to initialize neural networks. We also provide several simple guidelines for training word embeddings. First, we discover that corpus domain is more important than corpus size. We recommend choosing a corpus in a suitable domain for the desired task, after that, using a larger corpus yields better results. Second, we find that faster models provide sufficient performance in most cases, and more complex models can be used if the training corpus is sufficiently large. Third, the early stopping metric for iterating should rely on the development set of the desired task rather than the validation loss of training embedding.},
	archivePrefix = {arXiv},
	arxivId = {1507.05523},
	author = {Lai, Siwei and Liu, Kang and He, Shizhu and Zhao, Jun},
	doi = {10.1109/MIS.2016.45},
	eprint = {1507.05523},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 2016 - How to generate a good word embedding.pdf:pdf},
	issn = {15411672},
	journal = {IEEE Intelligent Systems},
	keywords = {distributed representation,intelligent systems,neural network,word embedding},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Large Movie Review,!Paper 3/task/Sentiment treebank},
	number = {6},
	pages = {5--14},
	title = {{How to generate a good word embedding}},
	volume = {31},
	year = {2016}
}
@article{Miyato2016,
	abstract = {Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.},
	archivePrefix = {arXiv},
	arxivId = {1605.07725},
	author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
	doi = {10.2507/daaam.scibook.2010.27},
	eprint = {1605.07725},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyato, Dai, Goodfellow - 2016 - Adversarial Training Methods for Semi-Supervised Text Classification.pdf:pdf},
	isbn = {2840601737},
	issn = {18766102},
	mendeley-groups = {!Paper 3/task/Large Movie Review},
	pages = {1--11},
	pmid = {19963286},
	title = {{Adversarial Training Methods for Semi-Supervised Text Classification}},
	url = {http://arxiv.org/abs/1605.07725},
	year = {2016}
}
@article{Ross2017,
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	archivePrefix = {arXiv},
	arxivId = {1703.03717},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	eprint = {1703.03717},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
	mendeley-groups = {!Paper 3/task/newsgroups},
	title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
	url = {http://arxiv.org/abs/1703.03717},
	year = {2017}
}
@article{Ho2006,
	abstract = {An accurate classifier with linguistic interpretability using a small number of relevant genes is beneficial to microarray data analysis and development of inexpensive diagnostic tests. Several frequently used techniques for designing classifiers of microarray data, such as support vector machine, neural networks, k-nearest neighbor, and logistic regression model, suffer from low interpretabilities. This paper proposes an interpretable gene expression classifier (named iGEC) with an accurate and compact fuzzy rule base for microarray data analysis. The design of iGEC has three objectives to be simultaneously optimized: maximal classification accuracy, minimal number of rules, and minimal number of used genes. An "intelligent" genetic algorithm IGA is used to efficiently solve the design problem with a large number of tuning parameters. The performance of iGEC is evaluated using eight commonly-used data sets. It is shown that iGEC has an accurate, concise, and interpretable rule base (1.1 rules per class) on average in terms of test classification accuracy (87.9{\%}), rule number (3.9), and used gene number (5.0). Moreover, iGEC not only has better performance than the existing fuzzy rule-based classifier in terms of the above-mentioned objectives, but also is more accurate than some existing non-rule-based classifiers. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
	author = {Ho, Shinn Ying and Hsieh, Chih Hung and Chen, Hung Ming and Huang, Hui Ling},
	doi = {10.1016/j.biosystems.2006.01.002},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ho et al. - 2006 - Interpretable gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis.pdf:pdf},
	issn = {03032647},
	journal = {BioSystems},
	keywords = {Fuzzy classifier,Gene expression,Intelligent genetic algorithm,Microarray data analysis,Pattern recognition},
	mendeley-groups = {Report},
	number = {3},
	pages = {165--176},
	pmid = {16490299},
	title = {{Interpretable gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis}},
	volume = {85},
	year = {2006}
}
@article{Dai,
	abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
	archivePrefix = {arXiv},
	arxivId = {1507.07998},
	author = {Dai, Andrew M and Olah, Christopher and Le, Quoc V.},
	eprint = {1507.07998},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Olah - Unknown - Document Embedding with Paragraph Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	pages = {1--8},
	title = {{Document Embedding with Paragraph Vectors}},
	url = {http://arxiv.org/abs/1507.07998},
	year = {2015}
}
@article{Sourek2015,
	abstract = {We propose a method combining relational-logic representations with deep neural network learning. Domain-specific knowledge is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structure of given training or testing examples. Different networks corresponding to different examples share their scikit-learns, which co-evolve during training by stochastic gradient descend algorithm. Notable relational concepts can be discovered by interpreting shared hidden layer scikit-learns corresponding to the rules. Experiments on 78 relational learning benchmarks demonstrate the favorable performance of the method.},
	archivePrefix = {arXiv},
	arxivId = {1508.05128},
	author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
	eprint = {1508.05128},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sourek et al. - 2015 - Lifted Relational Neural Networks.pdf:pdf},
	journal = {CoRR},
	keywords = {lifted models,neural networks,relational learning},
	mendeley-groups = {Progress Report},
	pages = {1--21},
	title = {{Lifted Relational Neural Networks}},
	url = {http://arxiv.org/abs/1508.05128},
	volume = {abs/1508.0},
	year = {2015}
}
@article{Pascanu2012,
	abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	archivePrefix = {arXiv},
	arxivId = {1211.5063},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	doi = {10.1109/72.279181},
	eprint = {1211.5063},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - 2012 - On the difficulty of training Recurrent Neural Networks.pdf:pdf},
	isbn = {08997667 (ISSN)},
	issn = {1045-9227},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pmid = {18267787},
	title = {{On the difficulty of training Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1211.5063},
	year = {2012}
}
@article{Joachims1998,
	abstract = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.},
	author = {Joachims, Thorsten},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims - 1998 - 1 Introduction 2 Text Categorization 3 Support Vector Machines.pdf:pdf},
	isbn = {3540644172},
	journal = {Machine Learning},
	number = {LS-8 Report 23},
	pages = {137--142},
	title = {{1 Introduction 2 Text Categorization 3 Support Vector Machines}},
	url = {http://www.springerlink.com/index/drhq581108850171.pdf},
	volume = {1398},
	year = {1998}
}
@article{Ou2007,
	abstract = {Multi-class pattern classification has many applications including text document classification, speech recognition, object recognition, etc. Multi-class pattern classification using neural networks is not a trivial extension from two-class neural networks. This paper presents a comprehensive and competitive study in multi-class neural learning with focuses on issues including neural network architecture, encoding schemes, training methodology and training time complexity. Our study includes multi-class pattern classification using either a system of multiple neural networks or a single neural network, and modeling pattern classes using one-against-all, one-against-one, one-against-higher-order, and P-against-Q. We also discuss implementations of these approaches and analyze training time complexity associated with each approach. We evaluate six different neural network system architectures for multi-class pattern classification along the dimensions of imbalanced data, large number of pattern classes, large vs. small training data through experiments conducted on well-known benchmark data. ?? 2006 Pattern Recognition Society.},
	author = {Ou, Guobin and Murphey, Yi Lu},
	doi = {10.1016/j.patcog.2006.04.041},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ou, Murphey - 2007 - Multi-class pattern classification using neural networks.pdf:pdf},
	isbn = {0769521282},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Machine learning,Multi-class classification,Neural networks,Pattern recognition},
	number = {1},
	pages = {4--18},
	title = {{Multi-class pattern classification using neural networks}},
	volume = {40},
	year = {2007}
}
@article{Kim2017,
	abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
	archivePrefix = {arXiv},
	arxivId = {1702.00887},
	author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
	eprint = {1702.00887},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2017 - Structured Attention Networks.pdf:pdf},
	issn = {1702.00887},
	mendeley-groups = {!Paper 3/Structured LSTMs},
	pages = {1--21},
	title = {{Structured Attention Networks}},
	url = {http://arxiv.org/abs/1702.00887},
	year = {2017}
}
@article{Rolfe2013,
	abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
	archivePrefix = {arXiv},
	arxivId = {1301.3775},
	author = {Rolfe, Jason Tyler and LeCun, Yan},
	eprint = {1301.3775},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rolfe, LeCun - 2013 - Discriminative Recurrent Sparse Auto-Encoders.pdf:pdf},
	journal = {CoRR},
	pages = {15},
	title = {{Discriminative Recurrent Sparse Auto-Encoders}},
	url = {http://arxiv.org/abs/1301.3775},
	year = {2013}
}
@article{Wallach2008,
	abstract = {This thesis introduces new methods for statistically modelling text using topic models. Topic models have seen many successes in recent years, and are used in a variety of applications, including analysis of news articles, topic-based search interfaces and navigation tools for digital libraries. Despite these recent successes, the field of topic modelling is still relatively new and there remains much to be explored. One notice- able absence from most of the previous work on topic modelling is consideration of language and document structurefrom low-level structures, including word order and syntax, to higher-level structures, such as relationships between documents. The focus of this thesis is therefore structured topic modelsmodels that combine latent topics with information about document structure, ranging from local sen- tence structure to inter-document relationships. These models draw on techniques from Bayesian statistics, including hierarchical Dirichlet distributions and processes, Pitman-Yor processes, and Markov chain Monte Carlo methods. Several methods for estimating the parameters of Dirichlet-multinomial distributions are also compared. The main contribution of this thesis is the introduction of three structured topic mod- els. The first is a topic-based language model. This model captures both word order and latent topics by extending a Bayesian topic model to incorporate n-gram statistics. A bigram version of the new model does better at predicting future words than either a topic model or a trigram language model. It also provides interpretable topics. The second model arises from a Bayesian reinterpretation of a classic generative de- pendency parsing model. The new model demonstrates that parsing performance can be substantially improved by a careful choice of prior and by sampling hyperparame- ters. Additionally, the generative nature of the model facilitates the inclusion of latent state variables, which act as specialised part-of-speech tags or syntactic topics. The third is a model that captures high-level relationships between documents. This model uses nonparametric Bayesian priors and Markov chain Monte Carlo methods to infer topic-based document clusters. The model assigns a higher probability to un- seen test documents than either a clustering model without topics or a Bayesian topic model without document clusters. The model can be extended to incorporate author information, resulting in finer-grained clusters and better predictive performance.},
	author = {Wallach, Hanna M},
	file = {:E$\backslash$:/Downloads/Work/1eefe0e5c69d6e14840e2d2b30f62a09a28b.pdf:pdf},
	journal = {Doctor},
	mendeley-groups = {Annotated/Topic models/Unsupervised Topic Models},
	number = {2001},
	pages = {136},
	title = {{Structured Topic Models for Language}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.2537{\&}rep=rep1{\&}type=pdf},
	year = {2008}
}
@article{Martens,
	author = {Martens, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens - Unknown - Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary.pdf:pdf},
	journal = {Knowledge Creation Diffusion Utilization},
	mendeley-groups = {Annotated/Applications/Financial Engineering},
	pages = {1--2},
	title = {{Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary}}
}
@article{Management2011,
	author = {Management, Spatial and Naukowe, Bogucki Wydawnictwo},
	doi = {10.2478/v10117-011-0021-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Management, Naukowe - 2011 - COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ... COMPARISON OF VALUES OF P.pdf:pdf},
	isbn = {9788362662623},
	mendeley-groups = {Report/Features},
	number = {2},
	title = {{COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ... COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ON THE SAME SETS OF DATA}},
	volume = {30},
	year = {2011}
}
@article{Kiros2014,
	abstract = {In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2710v1},
	author = {Kiros, Ryan and Zemel, Rs and Salakhutdinov, Ruslan},
	eprint = {arXiv:1406.2710v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2014 - A Multiplicative Model for Learning Distributed Text-Based Attribute Representations.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {1--11},
	title = {{A Multiplicative Model for Learning Distributed Text-Based Attribute Representations}},
	url = {http://arxiv.org/abs/1406.2710},
	year = {2014}
}
@article{Twomey1998,
	author = {Twomey, Janet M. and Smith, Alice E.},
	doi = {10.1109/5326.704579},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Twomey, Smith - 1998 - Bias and variance of validation methods for function approximation neural networks under conditions of sparse dat.pdf:pdf},
	issn = {10946977},
	journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
	keywords = {Function approximation,Neural network,Resampling,Validation},
	number = {3},
	pages = {417--430},
	title = {{Bias and variance of validation methods for function approximation neural networks under conditions of sparse data}},
	volume = {28},
	year = {1998}
}
@article{Xu2003,
	abstract = {In this paper, we propose a novel document clustering method based on the non-negative factorization of the term- document matrix of the given document corpus. In the la- tent semantic space derived by the non-negative matrix fac- torization (NMF), each axis captures the base topic of a par- ticular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value. Our experimental evalua- tions show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clus- tering methods not only in the easy and reliable derivation of document clustering results, but also in document clus- tering accuracies.},
	archivePrefix = {arXiv},
	arxivId = {1410.0993},
	author = {Xu, Wei and Liu, Xin and Gong, Yihong},
	doi = {10.1145/860484.860485},
	eprint = {1410.0993},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Liu, Gong - 2003 - Document clustering based on non-negative matrix factorization.pdf:pdf},
	isbn = {1581136463},
	issn = {01635840},
	journal = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval - SIGIR '03},
	keywords = {document clustering,non-negative matrix factorization},
	mendeley-groups = {Annotated/NMF},
	pages = {267},
	title = {{Document clustering based on non-negative matrix factorization}},
	url = {http://portal.acm.org/citation.cfm?doid=860435.860485},
	year = {2003}
}
@article{Nogueira2011,
	author = {Nogueira, T.M. and Camargo, H.a. and Rezende, S.O.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Camargo, Rezende - 2011 - Fuzzy Rules for Document Classification to Improve Information Retrieval.pdf:pdf},
	journal = {Mirlabs.Org},
	keywords = {fuzzy clustering,imprecision,information retrieval,text categorization,text mining,uncertainty},
	mendeley-groups = {Annotated/Decision Trees},
	pages = {210--217},
	title = {{Fuzzy Rules for Document Classification to Improve Information Retrieval}},
	url = {http://www.mirlabs.org/ijcisim/regular{\_}papers{\_}2011/Paper25.pdf},
	volume = {3},
	year = {2011}
}
@article{Li2014,
	author = {Li, Li and Zhang, Longkai and Wang, Houfeng},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Wang - 2014 - Muli-label Text Categorization with Hidden Components.pdf:pdf},
	journal = {Emnlp},
	pages = {1816--1821},
	title = {{Muli-label Text Categorization with Hidden Components}},
	year = {2014}
}
@article{Hu2016,
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the scikit-learns of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	archivePrefix = {arXiv},
	arxivId = {1603.06318},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	doi = {10.18653/v1/P16-1228},
	eprint = {1603.06318},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules(2).pdf:pdf},
	journal = {Acl},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1--18},
	title = {{Harnessing Deep Neural Networks with Logic Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	year = {2016}
}
@article{Oquab2014,
	abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large-scale visual recognition challenge (ILSVRC2012). The suc-cess of CNNs is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification meth-ods. Learning CNNs, however, amounts to estimating mil-lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi-ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep-resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
	author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
	doi = {10.1109/CVPR.2014.222},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab et al. - 2014 - Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks.pdf:pdf},
	isbn = {9781479951178},
	issn = {10636919},
	journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1717--1724},
	title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
	year = {2014}
}
@article{Radford2017,
	abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
	archivePrefix = {arXiv},
	arxivId = {1704.01444},
	author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
	eprint = {1704.01444},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford, Jozefowicz, Sutskever - 2017 - Learning to Generate Reviews and Discovering Sentiment.pdf:pdf},
	mendeley-groups = {!Paper 3/task,!Paper 3/Interpretable LSTMs,!Paper 3/task/Large Movie Review,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank,!Paper 3/Understanding LSTMs},
	title = {{Learning to Generate Reviews and Discovering Sentiment}},
	url = {http://arxiv.org/abs/1704.01444},
	year = {2017}
}
@article{Kheder2014,
	author = {Kheder, Waad Ben and Matrouf, Driss and Bousquet, Pierre-Michel and Bonastre, Jean-Fran{\c{c}}ois and Ajili, Moez},
	doi = {10.1007/978-3-319-11397-5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kheder et al. - 2014 - Statistical Language and Speech Processing.pdf:pdf},
	isbn = {978-3-319-11396-8},
	journal = {Statistical Language and Speech Processing},
	keywords = {i-vectors},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {97--107},
	title = {{Statistical Language and Speech Processing}},
	url = {http://link.springer.com/10.1007/978-3-319-11397-5{\%}5Cnhttp://link.springer.com/content/pdf/10.1007/978-3-319-11397-5.pdf},
	volume = {8791},
	year = {2014}
}
@article{Zilke2016,
abstract = {Neural network classifiers are known to be able to learn very accurate models. In the recent past, researchers have even been able to train neural networks with multiple hidden layers (deep neural networks) more effectively and efficiently. However, the major downside of neural networks is that it is not trivial to understand the way how they derive their classification decisions. To solve this problem, there has been research on extracting better understandable rules from neural networks. However, most authors focus on nets with only one single hidden layer. The present paper introduces a new decompositional algorithm - DeepRED - that is able to extract rules from deep neural networks. The evaluation of the proposed algorithm shows its ability to outperform a pedagogical baseline on several tasks, including the successful extraction of rules from a neural network realizing the XOR function.},
author = {Zilke, Jan Ruben and Menc{\'{i}}a, Eneldo Loza and Janssen, Frederik},
doi = {10.1007/978-3-319-46307-0_29},
file = {:E$\backslash$:/PhD/DS16DeepRED.pdf:pdf},
isbn = {9783319463063},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {457--473},
title = {{DeepRED - Rule extraction from deep neural networks}},
volume = {9956 LNAI},
year = {2016}
}


@article{Cheng2016,
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	archivePrefix = {arXiv},
	arxivId = {1601.06733},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	eprint = {1601.06733},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Dong, Lapata - 2016 - Long Short-Term Memory-Networks for Machine Reading.pdf:pdf},
	title = {{Long Short-Term Memory-Networks for Machine Reading}},
	url = {http://arxiv.org/abs/1601.06733},
	year = {2016}
}
@article{Letham2015,
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if {\ldots} then. . . statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1511.01644v1},
	author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
	doi = {10.1214/15-AOAS848},
	eprint = {arXiv:1511.01644v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2015 - Interpretable classifiers using rules and bayesian analysis Building a better stroke prediction model.pdf:pdf},
	isbn = {9781577356288},
	issn = {19417330},
	journal = {Annals of Applied Statistics},
	keywords = {Bayesian analysis,Classification,Interpretability},
	mendeley-groups = {Annotated/Rule-based classiifers,Report},
	number = {3},
	pages = {1350--1371},
	title = {{Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model}},
	volume = {9},
	year = {2015}
}
@article{Hamilton2014,
	abstract = {The rise in prevalence of algorithmically curated feeds in online news and social media sites raises a new question for designers, critics, and scholars of media: how aware are users of the role of algorithms and filters in their news sources? This paper situates this problem within the history of design for interaction, with an emphasis on the contemporary challenges of studying, and designing for, the algorithmic "curation" of feeds. Such a problem presents particular challenges when, as is common, neither the user nor the researcher has access to the actual proprietary algorithms at work. Author},
	author = {Hamilton, Kevin and Karahalios, Karrie and Sandvig, Christian and Eslami, Motahhare},
	doi = {10.1145/2559206.2578883},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamilton et al. - 2014 - A path to understanding the effects of algorithm awareness.pdf:pdf},
	isbn = {9781450324748},
	journal = {Proceedings of the extended abstracts of the 32nd annual ACM conference on Human factors in computing systems - CHI EA '14},
	mendeley-groups = {Annotated/Psychology},
	pages = {631--642},
	title = {{A path to understanding the effects of algorithm awareness}},
	url = {http://dl.acm.org/citation.cfm?doid=2559206.2578883},
	year = {2014}
}
@article{Socher2013a,
	abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank.pdf:pdf},
	isbn = {9781937284978},
	journal = {Proceedings of the {\ldots}},
	pages = {1631--1642},
	title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
	url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
	year = {2013}
}
@article{Melis2017,
	abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
	archivePrefix = {arXiv},
	arxivId = {1707.05589},
	author = {Melis, G{\'{a}}bor and Dyer, Chris and Blunsom, Phil},
	eprint = {1707.05589},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melis, Dyer, Blunsom - 2017 - On the State of the Art of Evaluation in Neural Language Models.pdf:pdf},
	isbn = {9781604562170},
	mendeley-groups = {!Paper 3/Language models},
	pages = {1--10},
	title = {{On the State of the Art of Evaluation in Neural Language Models}},
	url = {http://arxiv.org/abs/1707.05589},
	year = {2017}
}
@article{Kindermans2017,
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
	archivePrefix = {arXiv},
	arxivId = {1711.00867},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"{u}}tt, Kristof T. and D{\"{a}}hne, Sven and Erhan, Dumitru and Kim, Been},
	doi = {10.1016/j.jns.2003.09.014},
	eprint = {1711.00867},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kindermans et al. - 2017 - The (Un)reliability of saliency methods.pdf:pdf},
	isbn = {0022-510X (Print)},
	issn = {0022510X},
	mendeley-groups = {!Paper 3/Interpretability General},
	pages = {1--12},
	pmid = {14706220},
	title = {{The (Un)reliability of saliency methods}},
	url = {http://arxiv.org/abs/1711.00867},
	year = {2017}
}
@article{Li2017,
	abstract = {An important goal in behaviour analytics is to connect disease state or genome variation with observable differences in behaviour. Despite advances in sensor technology and imaging, informative behaviour quantification remains challenging. The nematode worm C. elegans provides a unique opportunity to test analysis approaches because of its small size, compact nervous system, and the availability of large databases of videos of freely behaving animals with known genetic differences. Despite its relative simplicity, there are still no reports of generative models that can capture essential differences between even well-described mutant strains. Here we show that a multilayer recurrent neural network (RNN) can produce diverse behaviours that are difficult to distinguish from real worms' behaviour and that some of the artificial neurons in the RNN are interpretable and correlate with observable features such as body curvature, speed, and reversals. Although the RNN is not trained to perform classification, we find that artificial neuron responses provide features that perform well in worm strain classification.},
	author = {Li, Kezhi and Javer, Avelino and Keaveny, Eric E. and Brown, Andre E.X. and Buchanan, E Kelly and Linderman, Scott and Paninski, Liam},
	doi = {10.1101/222208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Recurrent Neural Networks with Interpretable Cells Predict and Classify Worm Behaviour.pdf:pdf},
	journal = {Doi.Org},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	number = {Nips},
	pages = {222208},
	title = {{Recurrent Neural Networks with Interpretable Cells Predict and Classify Worm Behaviour}},
	url = {https://www.biorxiv.org/content/early/2017/11/20/222208},
	year = {2017}
}
@inproceedings{Chen2014,
	abstract = {Distributional semantics and frame semantics are two representative views on language understanding in the statistical world and the linguistic world, respectively. In this paper, we combine the best of two worlds to automatically induce the semantic slots for spoken dialogue systems. Given a collection of unlabeled audio files, we exploit continuous-valued word embeddings to augment a probabilistic frame-semantic parser that identifies key semantic slots in an unsupervised fashion. In experiments, our results on a real-world spoken dialogue dataset show that the distributional word representations significantly improve the adaptation of FrameNet-style parses of ASR decodings to the target semantic space; that comparing to a state-of-the-art baseline, a 13{\%} relative average precision improvement is achieved by leveraging word vectors trained on two 100-billion words datasets; and that the proposed technology can be used to reduce the costs for designing task-oriented spoken dialogue systems.},
	author = {Chen, Yun Nung and Wang, William Yang and Rudnicky, Alexander I.},
	booktitle = {2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings},
	doi = {10.1109/SLT.2014.7078639},
	isbn = {9781479971299},
	keywords = {Distributional semantics,Frame semantics,Unsupervised slot induction},
	mendeley-groups = {Report/Features},
	pages = {584--589},
	title = {{Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems}},
	year = {2014}
}
@article{Fyshe,
	author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - Unknown - A Compositional and Interpretable Semantic Space.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations,Annotated/Word Vectors},
	title = {{A Compositional and Interpretable Semantic Space}}
}
@article{Rudinger2017,
	abstract = {We introduce the notion of a multi-vector sentence representation based on a " one vector per proposition " philosophy, which we term skip-prop vectors. By representing each predicate-argument structure in a complex sentence as an individual vector, skip-prop is (1) a response to empirical evidence that single-vector sentence representations degrade with sentence length, and (2) a repre-sentation that maintains a semantically useful level of granularity. We demonstrate the feasibility of training skip-prop vectors, introducing a method adapted from skip-thought vectors, and compare skip-prop with " one vector per sentence " and " one vector per token " approaches.},
	author = {Rudinger, Rachel and Duh, Kevin and {Van Durme}, Benjamin},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudinger, Duh, Van Durme - 2017 - Skip-Prop Representing Sentences with One Vector Per Proposition.pdf:pdf},
	journal = {Iwcs},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Skip-Prop: Representing Sentences with One Vector Per Proposition}},
	year = {2017}
}
@article{Samanta2003,
	abstract = {A study is presented to compare the performance of bearing fault detection using two different classifiers, namely, artificial neural networks (ANNs) and support vector machines (SMVs). The time-domain vibration signals of a rotating machine with normal and defective bearings are processed for feature extraction. The extracted features from original and preprocessed signals are used as inputs to the classifiers for two-class (normal or fault) recognition. The classifier parameters, e.g., the number of nodes in the hidden layer in case of ANNs and the radial basis function kernel parameter (width) in case of SVMs along with the selection of input features are optimized using genetic algorithms. The classifiers are trained with a subset of the experimental data for known machine conditions and are tested using the remaining set of data. The procedure is illustrated using the experimental vibration data of a rotating machine. The roles of different vibration signals and signal preprocessing techniques are investigated. The results show the effectiveness of the features and the classifiers in detection of machine condition. ?? 2003 Elsevier Ltd. All rights reserved.},
	author = {Samanta, B. and Al-Balushi, K. R. and Al-Araimi, S. A.},
	doi = {10.1016/j.engappai.2003.09.006},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samanta, Al-Balushi, Al-Araimi - 2003 - Artificial neural networks and support vector machines with genetic algorithm for bearing fault.pdf:pdf},
	isbn = {09521976 (ISSN)},
	issn = {09521976},
	journal = {Engineering Applications of Artificial Intelligence},
	keywords = {Bearing faults,Condition monitoring,Feature selection,Genetic algorithm,Neural network,Rotating machines,Signal processing,Support vector machines},
	number = {7-8},
	pages = {657--665},
	title = {{Artificial neural networks and support vector machines with genetic algorithm for bearing fault detection}},
	volume = {16},
	year = {2003}
}
@article{Merity2017a,
	abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the scikit-learn-dropped LSTM which uses DropConnect on hidden-to-hidden scikit-learns as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
	archivePrefix = {arXiv},
	arxivId = {1708.02182},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	eprint = {1708.02182},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merity, Keskar, Socher - 2017 - Regularizing and Optimizing LSTM Language Models.pdf:pdf},
	mendeley-groups = {!Paper 3/Language models},
	title = {{Regularizing and Optimizing LSTM Language Models}},
	url = {http://arxiv.org/abs/1708.02182},
	year = {2017}
}
@article{Setionoa,
	author = {Setiono, Rudy},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono - Unknown - by pruning and hidden-unit splitting 1 Introduction.pdf:pdf},
	number = {1},
	pages = {1--34},
	title = {{by pruning and hidden-unit splitting 1 Introduction}},
	volume = {9}
}
@article{Seera2014a,
	abstract = {In this paper, a hybrid intelligent system that consists of the Fuzzy Min-Max neural network, the Classification and Regression Tree, and the Random Forest model is proposed, and its efficacy as a decision support tool for medical data classification is examined. The hybrid intelligent system aims to exploit the advantages of the constituent models and, at the same time, alleviate their limitations. It is able to learn incrementally from data samples (owing to Fuzzy Min-Max neural network), explain its predicted outputs (owing to the Classification and Regression Tree), and achieve high classification performances (owing to Random Forest). To evaluate the effectiveness of the hybrid intelligent system, three benchmark medical data sets, viz., Breast Cancer Wisconsin, Pima Indians Diabetes, and Liver Disorders from the UCI Repository of Machine Learning, are used for evaluation. A number of useful performance metrics in medical applications which include accuracy, sensitivity, specificity, as well as the area under the Receiver Operating Characteristic curve are computed. The results are analyzed and compared with those from other methods published in the literature. The experimental outcomes positively demonstrate that the hybrid intelligent system is effective in undertaking medical data classification tasks. More importantly, the hybrid intelligent system not only is able to produce good results but also to elucidate its knowledge base with a decision tree. As a result, domain users (i.e., medical practitioners) are able to comprehend the prediction given by the hybrid intelligent system; hence accepting its role as a useful medical decision support tool. ?? 2013 Elsevier Ltd. All rights reserved.},
	author = {Seera, Manjeevan and Lim, Chee Peng},
	doi = {10.1016/j.eswa.2013.09.022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seera, Lim - 2014 - A hybrid intelligent system for medical data classification.pdf:pdf},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Classification and regression tree,Fuzzy Min-Max neural network,Hybrid intelligent systems,Medical decision support,Random forest},
	mendeley-groups = {Annotated/Decision Trees},
	number = {5},
	pages = {2239--2249},
	publisher = {Elsevier Ltd},
	title = {{A hybrid intelligent system for medical data classification}},
	url = {http://dx.doi.org/10.1016/j.eswa.2013.09.022},
	volume = {41},
	year = {2014}
}
@article{Karpathy2015,
	abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	archivePrefix = {arXiv},
	arxivId = {1506.02078},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	doi = {10.1007/978-3-319-10590-1_53},
	eprint = {1506.02078},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
	isbn = {978-3-319-10589-5},
	issn = {978-3-319-10589-5},
	mendeley-groups = {!Paper 3,!Paper 3/Understanding LSTMs},
	pages = {1--12},
	pmid = {26353135},
	title = {{Visualizing and Understanding Recurrent Networks}},
	url = {http://arxiv.org/abs/1506.02078},
	year = {2015}
}
@article{Ghemawat2003,
	abstract = {We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.},
	author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
	journal = {ACM SIGOPS Operating Systems Review},
	keywords = {clustered storage,data storage,fault tolerance,scalability},
	number = {5},
	pages = {29},
	title = {{The Google file system}},
	volume = {37},
	year = {2003}
}
@article{Hauser2010,
	abstract = {The authors test methods, based on cognitively simple decision rules, that predict which products consumers select for their consideration sets. Drawing on qualitative research, the authors propose disjunctions-of- conjunctions (DOC) decision rules that generalize well-studied decision models, such as disjunctive, conjunctive, lexicographic, and subset conjunctive rules. They propose two machine-learning methods to estimate cognitively simple DOC rules. They observe consumers' consideration sets for global positioning systems for both calibration and validation data.They compare the proposed methods with both machine- learning and hierarchical Bayes methods, each based on five extant compensatory and noncompensatory rules. For the validation data, the cognitively simple DOC-based methods predict better than the ten benchmark methods on an information theoretic measure and on hit rates. The results are robust with respect to format by which consideration is measured, sample, and presentation of profiles. The article closes with an illustration of how DOC-based rules can affect managerial decisions.},
	author = {Hauser, John R and Toubia, Olivier and Evgeniou, Theodoros and Befurt, Rene and Dzyabura, Daria},
	doi = {10.1509/jmkr.47.3.485},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hauser et al. - 2010 - Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets.pdf:pdf},
	isbn = {00222437},
	issn = {0022-2437},
	journal = {Journal of Marketing Research},
	keywords = {cognitive,conjoint analysis,consideration sets,consumer,decision theory,disjunctions of conjunctions,heuristics,lexicography,machine learning,noncompensatory decisions,simplicity},
	mendeley-groups = {Annotated/Applications/Marketing},
	number = {3},
	pages = {485--496},
	pmid = {50522113},
	title = {{Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets}},
	volume = {47},
	year = {2010}
}
@article{Faruqui2015,
	abstract = {Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninter-pretable components whose relationship to the categories of traditional lexical seman-tic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguis-tic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9{\%} sparse. We analyze their performance on state-of-the-art eval-uation methods for distributional models of word vectors and find they are competi-tive to standard distributional approaches.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1506.05230v1},
	author = {Faruqui, Manaal and Dyer, Chris},
	doi = {10.3115/v1/P15-2076},
	eprint = {arXiv:1506.05230v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui, Dyer - 2015 - Non-distributional Word Vector Representations.pdf:pdf},
	isbn = {9781941643730},
	journal = {Acl-2015},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {464--469},
	title = {{Non-distributional Word Vector Representations}},
	year = {2015}
}
@article{Fallis2013,
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Fallis, A.G},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fallis - 2013 - No Title No Title.pdf:pdf},
	isbn = {9788578110796},
	issn = {1098-6596},
	journal = {Journal of Chemical Information and Modeling},
	keywords = {icle},
	number = {9},
	pages = {1689--1699},
	pmid = {25246403},
	title = {{No Title No Title}},
	volume = {53},
	year = {2013}
}
@article{Hullermeier2008,
	abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison (RPC), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (scikit-learned) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare RPC to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that RPC is superior in terms of computational efficiency, and at least competitive in terms of accuracy. ?? 2008 Elsevier B.V. All rights reserved.},
	author = {H{\"{u}}llermeier, Eyke and F{\"{u}}rnkranz, Johannes and Cheng, Weiwei and Brinker, Klaus},
	doi = {10.1016/j.artint.2008.08.002},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{u}}llermeier et al. - 2008 - Label ranking by learning pairwise preferences.pdf:pdf},
	isbn = {0004-3702},
	issn = {00043702},
	journal = {Artificial Intelligence},
	keywords = {Constraint classification,Pairwise classification,Preference learning,Ranking},
	mendeley-groups = {Annotated/Ranking,Progress Report},
	number = {16-17},
	pages = {1897--1916},
	title = {{Label ranking by learning pairwise preferences}},
	volume = {172},
	year = {2008}
}
@article{GethsiyalAugasta2012a,
	abstract = {Artificial neural networks often achieve high classification accuracy$\backslash$nrates, but they are considered as black boxes due to their lack of$\backslash$nexplanation capability. This paper proposes the new rule extraction$\backslash$nalgorithm RxREN to overcome this drawback. In pedagogical approach the$\backslash$nproposed algorithm extracts the rules from trained neural networks for$\backslash$ndatasets with mixed mode attributes. The algorithm relies on reverse$\backslash$nengineering technique to prune the insignificant input neurons and to$\backslash$ndiscover the technological principles of each significant input neuron$\backslash$nof neural network in classification. The novelty of this algorithm lies$\backslash$nin the simplicity of the extracted rules and conditions in rule are$\backslash$ninvolving both discrete and continuous mode of attributes.$\backslash$nExperimentation using six different real datasets namely iris, wbc,$\backslash$nhepatitis, pid, ionosphere and creditg show that the proposed algorithm$\backslash$nis quite efficient in extracting smallest set of rules with high$\backslash$nclassification accuracy than those generated by other neural network$\backslash$nrule extraction methods.},
	author = {{Gethsiyal Augasta}, M. and Kathirvalavakumar, T.},
	doi = {10.1007/s11063-011-9207-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gethsiyal Augasta, Kathirvalavakumar - 2012 - Reverse engineering the neural networks for rule extraction in classification problems(2).pdf:pdf},
	issn = {13704621},
	journal = {Neural Processing Letters},
	keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	number = {2},
	pages = {131--150},
	title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
	volume = {35},
	year = {2012}
}
@article{Giraud-Carrier1998,
	author = {Giraud-Carrier, Christophe},
	doi = {10.1002/9781119990413.ch1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giraud-Carrier - 1998 - Beyond predictive accuracy what.pdf:pdf},
	isbn = {9780470749838},
	journal = {Proceedings of the ECML-98 Workshop on Upgrading Learning to Meta-Level: Model Selection and Data Transformation},
	pages = {78--85},
	title = {{Beyond predictive accuracy: what?}},
	year = {1998}
}
@article{Arras2017,
	abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
	archivePrefix = {arXiv},
	arxivId = {1612.07843},
	author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
	doi = {10.1371/journal.pone.0181142},
	eprint = {1612.07843},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2017 - What is relevant in a text document An interpretable machine learning approach.pdf:pdf},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Explanations},
	number = {8},
	pages = {1--19},
	title = {{"What is relevant in a text document?": An interpretable machine learning approach}},
	volume = {12},
	year = {2017}
}
@article{Martens2011,
	abstract = {This paper proposes a complete framework to assess the overall performance of classification models from a user perspective in terms of accuracy, comprehensibility, and justifiability. A review is provided of accuracy and comprehensibility measures, and a novel metric is introduced that allows one to measure the justifiability of classification models. Furthermore, taxonomy of domain constraints is introduced, and an overview of the existing approaches to impose constraints and include domain knowledge in data mining techniques is presented. Finally, justifiability metric is applied to a credit scoring and customer churn prediction case. ?? 2011 Elsevier B.V. All rights reserved.},
	author = {Martens, David and Vanthienen, Jan and Verbeke, Wouter and Baesens, Bart},
	doi = {10.1016/j.dss.2011.01.013},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2011 - Performance of classification models from a user perspective.pdf:pdf},
	isbn = {0167-9236},
	issn = {01679236},
	journal = {Decision Support Systems},
	keywords = {Classification,Comprehensibility,Data mining,Justifiability,Metrics},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {4},
	pages = {782--793},
	publisher = {Elsevier B.V.},
	title = {{Performance of classification models from a user perspective}},
	url = {http://dx.doi.org/10.1016/j.dss.2011.01.013},
	volume = {51},
	year = {2011}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Bull.jar:jar},
	title = {{Bull}}
}
@article{Han2010a,
	abstract = {Due to the visually polysemous barrier, videos and images may be annotated by multiple tags. Discovering the correlations among different tags can significantly help predicting precise labels for videos and images. Many of recent studies toward multi-label learning construct a linear subspace embed- ding with encoded multi-label information, such that data points sharing many common labels tend to be close to each other in the embedded subspace. Motivated by the advances of compressive sensing research, a sparse representation that selects a compact subset to describe the input data can be more discriminative. In this paper, we propose a sparse multi-label learning method to circumvent the visually polysemous barrier of multiple tags. Our approach learns a multi-label encoded sparse linear embedding space from a related dataset, and maps the target data into the learned new representation space to achieve better annotation performance. Instead of using l1-norm penalty (lasso) to induce sparse representation, we propose to formulate the multi-label learning as a penalized least squares optimization problem with elastic-net penalty. By casting the video concept detection and image annotation tasks into a sparse multi-label transfer learning framework in this paper, ridge regression, lasso, elastic net, and the multi-label extended sparse discriminant analysis methods are, respectively, well explored and compared},
	author = {Han, Yahong and Wu, Fei and Zhuang, Yueting and He, Xiaofei},
	doi = {10.1109/TCSVT.2010.2057015},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2010 - Multi-label transfer learning with sparse representation.pdf:pdf},
	issn = {10518215},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Image annotation,multi-label learning,sparse representation,transfer learning,video concept detection},
	number = {8},
	pages = {1110--1121},
	title = {{Multi-label transfer learning with sparse representation}},
	volume = {20},
	year = {2010}
}
@article{Escalante2018,
	author = {Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jacques, Julio and Bar{\'{o}}, Xavier and Ayache, Stephane and Viegas, Evelyne and G{\"{u}}{\c{c}}l{\"{u}}t{\"{u}}rk, Yağmur and G{\"{u}}{\c{c}}l{\"{u}}, Umut and Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jacques, Julio and Madadi, Meysam and Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jr, Julio Jacques},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Escalante et al. - 2018 - Design of an Explainable Machine Learning Challenge for Video Interviews To cite this version HAL Id hal-016.pdf:pdf},
	mendeley-groups = {!Paper 3/Interpreting Vision},
	title = {{Design of an Explainable Machine Learning Challenge for Video Interviews To cite this version : HAL Id : hal-01668386 Design of an Explainable Machine Learning Challenge for Video Interviews}},
	year = {2018}
}
@article{Bologna,
	author = {Bologna, Guido and Pellegrini, Christian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna, Pellegrini - Unknown - Accurate Decomposition of Standard MLP Classification Responses into Symbolic Rules.pdf:pdf},
	mendeley-groups = {Progress Report},
	title = {{Accurate Decomposition of Standard MLP Classification Responses into Symbolic Rules}}
}
@article{Santos2015,
	abstract = {Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1504.06580v2},
	author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
	eprint = {arXiv:1504.06580v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos, Xiang, Zhou - 2015 - Classifying Relations by Ranking with Convolutional Neural Networks.pdf:pdf},
	isbn = {9781941643723},
	journal = {Acl-2015},
	keywords = {Bing Xiang,Bowen Zhou,Cicero Nogueira dos Santos},
	mendeley-groups = {Progress Report},
	number = {3},
	pages = {626--634},
	title = {{Classifying Relations by Ranking with Convolutional Neural Networks}},
	url = {http://arxiv.org/pdf/1504.06580.pdf},
	year = {2015}
}
@book{Bishop2006a,
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {Bishop, Christopher M},
	booktitle = {Pattern Recognition},
	doi = {10.1117/1.2819119},
	eprint = {0-387-31073-8},
	isbn = {9780387310732},
	issn = {10179909},
	number = {4},
	pages = {738},
	pmid = {8943268},
	title = {{Pattern Recognition and Machine Learning}},
	url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
	volume = {4},
	year = {2006}
}
@article{Alon2009,
	abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure. ?? 2009 Elsevier Inc. All rights reserved.},
	author = {Alon, Uri},
	journal = {Molecular Cell},
	number = {6},
	pages = {726--728},
	title = {{How To Choose a Good Scientific Problem}},
	volume = {35},
	year = {2009}
}
@article{Franca2014a,
	abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph min-ing and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integratedBCP with a well-known neural-symbolic system, C-IL2 P, to perform learning from nu-merical vectors. C-IL2 P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, han-dles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary re-sults indicating that a reduction of more than 90{\{}{\%}{\}} of features can be achieved with a small loss of accuracy.},
	author = {Fran{\c{c}}a, Manoel V M and Zaverucha, Gerson and {D'Avila Garcez}, Artur S.},
	doi = {10.1007/s10994-013-5392-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}a, Zaverucha, D'Avila Garcez - 2014 - Fast relational learning using bottom clause propositionalization with artificial neural netw.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Artificial neural networks,Inductive logic programming,Neural-symbolic integration,Propositionalization,Relational learning},
	mendeley-groups = {Progress Report},
	number = {1},
	pages = {81--104},
	title = {{Fast relational learning using bottom clause propositionalization with artificial neural networks}},
	volume = {94},
	year = {2014}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - TalesOfTheDervishes (2).pdf.pdf:pdf},
	title = {{TalesOfTheDervishes (2).pdf}}
}
@article{Hailesilassie2016,
	author = {Hailesilassie, Tameru},
	file = {:E$\backslash$:/PhD/1610.05267.pdf:pdf},
	keywords = {- artificial neural network,decompositional,deep neural network,eclectic,pedagogical,rule extraction},
	number = {7},
	pages = {376--381},
	title = {{Rule Extraction Algorithm for Deep Neural Networks:}},
	volume = {14},
	year = {2016}
}
@article{Read2010,
	author = {Read, Jesse},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2010 - Scalable Multi-label Classification.pdf:pdf},
	mendeley-groups = {!Paper 3/task/newsgroups},
	title = {{Scalable Multi-label Classification}},
	volume = {1994},
	year = {2010}
}
@article{G.Towell1993a,
	author = {{G. Towell}, Geoffrey and {W. Shavlik}, Jude},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G. Towell, W. Shavlik - 1993 - Interpretation of artificial neural networks Mapping knowledge based neural networks into rules.pdf:pdf},
	journal = {Advances in Neural Information Processing Systems},
	pages = {977--984},
	title = {{Interpretation of artificial neural networks: Mapping knowledge based neural networks into rules}},
	volume = {5},
	year = {1993}
}
@article{Fader2011,
	abstract = {Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the ReVerb Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TextRunner and woepos. More than 30{\%} of ReVerb's extractions are at precision 0.8 or higher---compared to virtually none for earlier systems. The paper concludes with a detailed analysis of ReVerb's errors, suggesting directions for future work.},
	author = {Fader, Anthony and Soderland, Stephen and Etzioni, Oren},
	doi = {10.1234/12345678},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fader, Soderland, Etzioni - 2011 - Identifying relations for open information extraction.pdf:pdf},
	isbn = {978-1-937284-11-4},
	issn = {1937284115},
	journal = {Proceedings of the Conference on {\ldots}},
	pages = {1535--1545},
	title = {{Identifying relations for open information extraction}},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145596{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2145596},
	year = {2011}
}
@article{Bader2004,
	abstract = {Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.},
	archivePrefix = {arXiv},
	arxivId = {cs/0408069},
	author = {Bader, Sebastian and Hitzler, Pascal and Hoelldobler, Steffen},
	eprint = {0408069},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bader, Hitzler, Hoelldobler - 2004 - The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challe.pdf:pdf},
	journal = {Network},
	keywords = {logic programs,neural networks,neural symbolic integra},
	mendeley-groups = {Categories/Logic},
	pages = {12},
	primaryClass = {cs},
	title = {{The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challenge for Artificial Intelligence}},
	url = {http://arxiv.org/abs/cs/0408069},
	year = {2004}
}
@article{Liang2017a,
	abstract = {This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1703.03055},
	author = {Liang, Xiaodan and Lin, Liang and Shen, Xiaohui and Feng, Jiashi and Yan, Shuicheng and Xing, Eric P.},
	doi = {10.1109/CVPR.2017.234},
	eprint = {1703.03055},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2017 - Interpretable Structure-Evolving LSTM(2).pdf:pdf},
	issn = {1703.03055},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	number = {61622214},
	pages = {1010--1019},
	title = {{Interpretable Structure-Evolving LSTM}},
	url = {http://arxiv.org/abs/1703.03055},
	year = {2017}
}
@article{Craven1993,
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
	journal = {Machine Learning: Proceedings of the Tenth International Conference},
	pages = {73--80},
	title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
	year = {1993}
}
@article{Lisboa2002,
	abstract = {The purpose of this review is to assess the evidence of healthcare benefits involving the application of artificial neural networks to the clinical functions of diagnosis, prognosis and survival analysis, in the medical domains of oncology, critical care and cardiovascular medicine. The primary source of publications is PUBMED listings under Randomised Controlled Trials and Clinical Trials. The r{\^{o}}le of neural networks is introduced within the context of advances in medical decision support arising from parallel developments in statistics and artificial intelligence. This is followed by a survey of published Randomised Controlled Trials and Clinical Trials, leading to recommendations for good practice in the design and evaluation of neural networks for use in medical intervention.},
	author = {Lisboa, P.J.G.},
	doi = {10.1016/S0893-6080(01)00111-3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lisboa - 2002 - A review of evidence of health benefit from artificial neural networks in medical intervention.pdf:pdf},
	isbn = {0893-6080 (Print)$\backslash$r0893-6080 (Linking)},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {clinical trials,decision support systems,diagnosis,prognosis,prospective studies,randomised controlled trials,review,survival analysis},
	mendeley-groups = {Report/Features,Report},
	number = {1},
	pages = {11--39},
	pmid = {11958484},
	title = {{A review of evidence of health benefit from artificial neural networks in medical intervention}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608001001113},
	volume = {15},
	year = {2002}
}
@article{Karwath2002,
	abstract = {BACKGROUND: The inference of homology between proteins is a key problem in molecular biology The current best approaches only identify approximately 50{\%} of homologies (with a false positive rate set at 1/1000).$\backslash$n$\backslash$nRESULTS: We present Homology Induction (HI), a new approach to inferring homology. HI uses machine learning to bootstrap from standard sequence similarity search methods. First a standard method is run, then HI learns rules which are true for sequences of high similarity to the target (assumed homologues) and not true for general sequences, these rules are then used to discriminate sequences in the twilight zone. To learn the rules HI describes the sequences in a novel way based on a bioinformatic knowledge base, and the machine learning method of inductive logic programming. To evaluate HI we used the PDB40D benchmark which lists sequences of known homology but low sequence similarity. We compared the HI methodology with PSI-BLAST alone and found HI performed significantly better. In addition, Receiver Operating Characteristic (ROC) curve analysis showed that these improvements were robust for all reasonable error costs. The predictive homology rules learnt by HI by can be interpreted biologically to provide insight into conserved features of homologous protein families.$\backslash$n$\backslash$nCONCLUSIONS: HI is a new technique for the detection of remote protein homology--a central bioinformatic problem. HI with PSI-BLAST is shown to outperform PSI-BLAST for all error costs. It is expect that similar improvements would be obtained using HI with any sequence similarity method.},
	author = {Karwath, Andreas and King, Ross D},
	doi = {10.1186/1471-2105-3-11},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karwath, King - 2002 - Homology induction the use of machine learning to improve sequence similarity searches.pdf:pdf},
	isbn = {10.1186/1471-2105-3-11},
	issn = {1471-2105},
	journal = {BMC bioinformatics},
	keywords = {Algorithms,Animals,Artificial Intelligence,Computational Biology,Computational Biology: methods,Databases, Protein,Fungal Proteins,Fungal Proteins: genetics,Internet,Mice,Oryza sativa,Plant Proteins,Plant Proteins: genetics,Predictive Value of Tests,Retroviridae Proteins,Retroviridae Proteins: genetics,Sensitivity and Specificity,Sequence Alignment,Sequence Alignment: methods,Sequence Homology, Amino Acid},
	mendeley-groups = {Report/Biologicla domain},
	number = {1},
	pages = {11},
	pmid = {11972320},
	title = {{Homology induction: the use of machine learning to improve sequence similarity searches.}},
	url = {http://www.biomedcentral.com/1471-2105/3/11},
	volume = {3},
	year = {2002}
}
@article{Macqueen,
	author = {Macqueen, J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Macqueen - Unknown - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MULTIVARIATE OBSERVATIONS.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	number = {233},
	pages = {281--297},
	title = {{SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MULTIVARIATE OBSERVATIONS}},
	volume = {233}
}
@article{Thrun1995,
	abstract = {Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neural networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illustrate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.},
	author = {Thrun, S.},
	doi = {10.1007/BFb0100473},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun - 1995 - Extracting rules from artificial neural networks with distributed representations.pdf:pdf},
	isbn = {1049-5258},
	issn = {16113349},
	journal = {Advances in Neural Information Processing Systems},
	pages = {505--512},
	title = {{Extracting rules from artificial neural networks with distributed representations}},
	year = {1995}
}
@article{Duchi2011,
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1103.4296v1},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	doi = {10.1109/CDC.2012.6426698},
	eprint = {arXiv:1103.4296v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
	isbn = {9780982252925},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
	mendeley-groups = {Annotated/Software},
	pages = {2121--2159},
	pmid = {2868127},
	title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	volume = {12},
	year = {2011}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Zhou- Extracting symbolic rules from trained neural.url:url},
	title = {{Zhou- Extracting symbolic rules from trained neural}}
}
@article{Jiang2016,
	abstract = {This paper involves deriving high quality information from unstructured text data through the integration of rich document representations to improve machine learning text classification problems. Previous research has applied Neural Network Language Models (NNLMs) to document classification performance, and word vector representations have been used to measure semantics among text. Never have they been combined together and shown to have improved text classification performance. Our belief is that the inference and clustering abilities of word vectors coupled with the power of a neural network can create more accurate classification predictions. The first phase our work focused on word vector representations for classification purposes. This approach included analyzing two distinct text sources with pre-marked binary outcomes for classification, creating a benchmark metric, and comparing against word vector representations within the feature space as a classifier. The results showed promise, obtaining an area under the curve of 0.95 utilizing word vectors, relative to the benchmark case of 0.93. The second phase of the project focused on utilizing an extension of the neural network model used in phase one to represent a document in its entirety as opposed to being represented word by word. Preliminary results indicated a slight improvement over the baseline model of approximately 2-3 percent.},
	author = {Jiang, Suqi and Lewris, Jason and Voltmer, Michael and Wang, Hongning},
	doi = {10.1109/SIEDS.2016.7489319},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2016 - Integrating rich document representations for text classification.pdf:pdf},
	isbn = {9781509009701},
	journal = {2016 IEEE Systems and Information Engineering Design Symposium, SIEDS 2016},
	keywords = {Natural Language Processing,Text Classification,Text Mining,Word2vec},
	mendeley-groups = {Annotated/Document representation},
	pages = {303--308},
	title = {{Integrating rich document representations for text classification}},
	year = {2016}
}
@article{Kingma2015,
	abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
	archivePrefix = {arXiv},
	arxivId = {1506.02557},
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
	doi = {10.1016/S0733-8619(03)00096-3},
	eprint = {1506.02557},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Salimans, Welling - 2015 - Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
	isbn = {1506.02557},
	issn = {10495258},
	mendeley-groups = {!Paper 3/Bayesian Networks},
	number = {Mcmc},
	pages = {1--9},
	pmid = {15062530},
	title = {{Variational Dropout and the Local Reparameterization Trick}},
	url = {http://arxiv.org/abs/1506.02557},
	year = {2015}
}
@article{Letham2010,
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. We introduce a generative model called the Bayesian List Machine for fitting decision lists, a type of interpretable classifier, to data. We use the model to predict stroke in atrial fibrillation patients, and produce predictive models that are simple enough to be understood by patients yet significantly outperform the medical scoring systems currently in use.},
	author = {Letham, Benjamin and Rudin, Cynthia and Mccormick, Tyler H and Madigan, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2010 - An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis.pdf:pdf},
	isbn = {9781577356288},
	journal = {AAAI Technical Report WS-13-17},
	keywords = {AAAI Technical Report WS-13-17},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {609},
	pages = {65--67},
	title = {{An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis}},
	year = {2010}
}
@article{Erhan2010a,
	abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1206.5538v1},
	author = {Erhan, Dumitru and Courville, Aaron and Vincent, Pascal},
	doi = {10.1145/1756006.1756025},
	eprint = {arXiv:1206.5538v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan, Courville, Vincent - 2010 - Why Does Unsupervised Pre-training Help Deep Learning.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	mendeley-groups = {Papers/Paper 1,Report/Features,Progress Report,Report},
	number = {2007},
	pages = {625--660},
	title = {{Why Does Unsupervised Pre-training Help Deep Learning ?}},
	url = {http://portal.acm.org/citation.cfm?id=1756025},
	volume = {11},
	year = {2010}
}
@article{Wainer2017,
	author = {Wainer, Jacques and Cawley, Gavin},
	file = {:D$\backslash$:/PhD/PGR/16-174.pdf:pdf},
	keywords = {bootstrap,cross-validation,hyperparameters,k-fold,resampling,svm},
	pages = {1--35},
	title = {{Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters}},
	volume = {18},
	year = {2017}
}
@article{Than,
	author = {Than, Khoat and Ho, Tu Bao},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Than, Ho - Unknown - Fully Sparse Topic ModelsPKDD 2012.pdf.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations,Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {1--16},
	title = {{Fully Sparse Topic Models[PKDD 2012].pdf}}
}
@article{Schockaert2015,
	author = {Schockaert, S and Derrac, J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert, Derrac - 2015 - Commonsense Reasoning Based on Betweenness and Direction in Distributional Models.pdf:pdf},
	journal = {2015 AAAI Spring Symposium Series},
	mendeley-groups = {Categories/Commonsense Reasoning},
	number = {April},
	pages = {3--6},
	title = {{Commonsense Reasoning Based on Betweenness and Direction in Distributional Models}},
	url = {http://users.cs.cf.ac.uk/S.Schockaert/Publications{\_}files/AAAI-SS2015.pdf},
	year = {2015}
}
@article{Socher2013,
	abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
	archivePrefix = {arXiv},
	arxivId = {1512.03385},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
	doi = {10.1371/journal.pone.0073791},
	eprint = {1512.03385},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank(2).pdf:pdf},
	isbn = {9781937284978},
	issn = {1932-6203},
	journal = {Proceedings of the {\ldots}},
	mendeley-groups = {!Paper 3/task/Sentiment treebank},
	number = {October},
	pages = {1631--1642},
	pmid = {24086296},
	title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
	url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
	year = {2013}
}
@article{Fu1998a,
	author = {Fu, Limin},
	doi = {10.1109/72.712152},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu - 1998 - A neural-network model for learning domain rules based on its activation function characteristics.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Certainty factor,Generalization,Machine learning,Neural network,Rule learning,Sample complexity,VC-dimension},
	number = {5},
	pages = {787--795},
	title = {{A neural-network model for learning domain rules based on its activation function characteristics}},
	volume = {9},
	year = {1998}
}
@article{Zaremba2014,
	abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99{\%} accuracy.},
	archivePrefix = {arXiv},
	arxivId = {1410.4615},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	doi = {10.1016/S0893-6080(96)00073-1},
	eprint = {1410.4615},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever - 2014 - Learning to Execute.pdf:pdf},
	isbn = {1410.4615},
	issn = {08936080},
	mendeley-groups = {!Paper 3/task/Sentiment treebank},
	pages = {1--25},
	title = {{Learning to Execute}},
	url = {http://arxiv.org/abs/1410.4615},
	year = {2014}
}
@article{Lee1999,
	abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1408.1149},
	author = {Lee, D D and Seung, H S},
	doi = {10.1038/44565},
	eprint = {arXiv:1408.1149},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Seung - 1999 - Learning the parts of objects by non-negative matrix factorization.pdf:pdf},
	isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
	issn = {0028-0836},
	journal = {Nature},
	keywords = {Algorithms,Face,Humans,Learning,Models, Neurological,Perception,Perception: physiology,Semantics},
	mendeley-groups = {Annotated/NMF},
	number = {6755},
	pages = {788--91},
	pmid = {10548103},
	title = {{Learning the parts of objects by non-negative matrix factorization.}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10548103},
	volume = {401},
	year = {1999}
}
@article{Rankings,
	author = {Rankings, Phrase Direction and Score, Ndcg and Rankings, Cluster and Tree, Decision and Validation, Using Cross},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rankings et al. - Unknown - Cross - validation Process.pdf:pdf},
	pages = {7--8},
	title = {{Cross - validation Process}}
}
@article{Ross2017a,
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	archivePrefix = {arXiv},
	arxivId = {1703.03717},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	eprint = {1703.03717},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
	mendeley-groups = {!Paper 3/task/newsgroups},
	title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
	url = {http://arxiv.org/abs/1703.03717},
	year = {2017}
}
@article{Andrews2002,
	abstract = {This paper describes RULEX, a technique for providing an explanation component for local cluster (LC) neural networks. RULEX extracts symbolic rules from the scikit-learns of a trained LC net. LC nets are a special class of multilayer perceptrons that use sigmoid functions to generate localised functions. LC nets are well suited to both function approximation and discrete classification tasks. The restricted LC net is constrained in such a way that the local functions are 'axis parallel' thus facilitating rule extraction. This paper presents results for the LC net on a wide variety of benchmark problems and shows that RULEX produces comprehensible, accurate rules that exhibit a high degree of fidelity with the LC network from which they were extracted. ?? 2002 Elsevier Science B.V. All rights reserved.},
	author = {Andrews, Robert and Geva, Shloma},
	doi = {10.1016/S0925-2312(01)00577-X},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Geva - 2002 - Rule extraction from local cluster neural nets.pdf:pdf},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Knowledge extraction,Local response networks,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Progress Report,Report},
	number = {August 2002},
	pages = {1--20},
	title = {{Rule extraction from local cluster neural nets}},
	volume = {47},
	year = {2002}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 1606.03657.pdf.pdf:pdf},
	title = {1606.03657.pdf}
}
@article{Read2015,
	abstract = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
	archivePrefix = {arXiv},
	arxivId = {1503.09022},
	author = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
	eprint = {1503.09022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Hollm{\'{e}}n - 2015 - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
	keywords = {meta-labels,multi-label classification,neural net-,problem transformation},
	pages = {1--23},
	title = {{Multi-label Classification using Labels as Hidden Nodes}},
	url = {http://arxiv.org/abs/1503.09022},
	year = {2015}
}
@article{OldenD.A.2002a,
	author = {{Olden  D. A.}, J D Y Jackson},
	doi = {10.1016/S0304-3800(02)00064-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olden D. A. - 2002 - Illuminating the black box a ramdomization approach for understanding variable contributions in artificial neuronal.pdf:pdf},
	issn = {03043800},
	journal = {Ecological Modelling},
	keywords = {connection scikit-learns, sensitivity analysis, neural i},
	mendeley-groups = {Literature Review},
	pages = {135--150},
	title = {{Illuminating the "black box": a ramdomization approach for understanding variable contributions in artificial neuronal networks.}},
	volume = {154},
	year = {2002}
}
@article{Read2013,
	abstract = {abstract Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs compara-tive experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
	author = {Read, Jesse},
	doi = {10.4018/978-1-60566-058-5.ch021},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2013 - Multi-label Classification.pdf:pdf},
	isbn = {978-1-4244-1065-1},
	issn = {1548-3924},
	mendeley-groups = {Report/Multi-label},
	pages = {2002--2004},
	title = {{Multi-label Classification}},
	year = {2013}
}
@article{Wang,
	abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn sim-ilarity metric directly from images. It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sam-pling algorithm is proposed to learn the model with dis-tributed asynchronized stochastic gradient. Extensive ex-periments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
	archivePrefix = {arXiv},
	arxivId = {1404.4661},
	author = {Wang, Jiang and Song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jingbin and Philbin, James and Chen, Bo and Wu, Ying},
	doi = {10.1109/CVPR.2014.180},
	eprint = {1404.4661},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Learning Fine-grained Image Similarity with Deep Ranking.pdf:pdf},
	isbn = {9781479951178},
	issn = {10636919},
	mendeley-groups = {Report/Features,Progress Report},
	title = {{Learning Fine-grained Image Similarity with Deep Ranking}}
}
@article{Vellido2012,
	abstract = {This article investigates methods for the accurate and robust differentiation of metastases from glioblastomas on the basis of single-voxel (1)H MRS information. Single-voxel (1)H MR spectra from a total of 109 patients (78 glioblastomas and 31 metastases) from the multicenter, international INTERPRET database, plus a test set of 40 patients (30 glioblastomas and 10 metastases) from three different centers in the Barcelona (Spain) metropolitan area, were analyzed using a robust method for feature (spectral frequency) selection coupled with a linear-in-the-parameters single-layer perceptron classifier. For the test set, a parsimonious selection of five frequencies yielded an area under the receiver operating characteristic curve of 0.86, and an area under the convex hull of the receiver operating characteristic curve of 0.91. Moreover, these accurate results for the discrimination between glioblastomas and metastases were obtained using a small number of frequencies that are amenable to metabolic interpretation, which should ease their use as diagnostic markers. Importantly, the prediction can be expressed as a simple formula based on a linear combination of these frequencies. As a result, new cases could be straightforwardly predicted by integrating this formula into a computer-based medical decision support system. This work also shows that the combination of spectra acquired at different TEs (short TE, 20-32 ms; long TE, 135-144 ms) is key to the successful discrimination between glioblastomas and metastases from single-voxel (1)H MRS.},
	author = {Vellido, A. and Romero, E. and Juli??-Sap??, M. and Maj??s, C. and Moreno-Torres, ?? and Pujol, J. and Ar??s, C.},
	doi = {10.1002/nbm.1797},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vellido et al. - 2012 - Robust discrimination of glioblastomas from metastatic brain tumors on the basis of single-voxel 1H MRS.pdf:pdf},
	isbn = {1099-1492 (Electronic)$\backslash$n0952-3480 (Linking)},
	issn = {09523480},
	journal = {NMR in Biomedicine},
	keywords = {Feature selection,Glioblastomas,High-grade malignant tumors,Medical decision support system,Metastases,Pattern recognition,SV 1H MRS},
	mendeley-groups = {Report},
	number = {6},
	pages = {819--828},
	pmid = {22081447},
	title = {{Robust discrimination of glioblastomas from metastatic brain tumors on the basis of single-voxel 1H MRS}},
	volume = {25},
	year = {2012}
}
@article{Santos2015a,
	abstract = {Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1504.06580v2},
	author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
	eprint = {arXiv:1504.06580v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos, Xiang, Zhou - 2015 - Classifying Relations by Ranking with Convolutional Neural Networks.pdf:pdf},
	isbn = {9781941643723},
	journal = {Acl-2015},
	keywords = {Bing Xiang,Bowen Zhou,Cicero Nogueira dos Santos},
	mendeley-groups = {Progress Report},
	number = {3},
	pages = {626--634},
	title = {{Classifying Relations by Ranking with Convolutional Neural Networks}},
	url = {http://arxiv.org/pdf/1504.06580.pdf},
	year = {2015}
}
@article{Severyn2015,
	abstract = {This paper describes our deep learning system for sentiment anal-ysis of tweets. The main contribution of this work is a new model for initializing the parameter scikit-learns of the convolutional neural network, which is crucial to train an accurate model while avoid-ing the need to inject any additional features. Briefly, we use an unsupervised neural language model to train initial word embed-dings that are further tuned by our deep learning model on a distant supervised corpus. At a final stage, the pre-trained parameters of the network are used to initialize the model. We train the latter on the supervised training data recently made available by the official system evaluation campaign on Twitter Sentiment Analysis orga-nized by Semeval-2015. A comparison between the results of our approach and the systems participating in the challenge on the of-ficial test sets, suggests that our model could be ranked in the first two positions in both the phrase-level subtask A (among 11 teams) and on the message-level subtask B (among 40 teams). This is an important evidence on the practical value of our solution.},
	author = {Severyn, Aliaksei and Moschitti, Alessandro},
	doi = {10.1145/2766462.2767830},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Severyn, Moschitti - 2015 - Twitter Sentiment Analysis with Deep Convolutional Neural Networks.pdf:pdf},
	isbn = {9781450336215},
	journal = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '15},
	keywords = {convolutional neural networks,twitter sentiment analysis},
	mendeley-groups = {Progress Report},
	pages = {959--962},
	title = {{Twitter Sentiment Analysis with Deep Convolutional Neural Networks}},
	url = {http://dl.acm.org/citation.cfm?doid=2766462.2767830},
	year = {2015}
}
@article{Blei2003,
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	archivePrefix = {arXiv},
	arxivId = {1111.6189v1},
	author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
	doi = {10.1162/jmlr.2003.3.4-5.993},
	eprint = {1111.6189v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:pdf},
	isbn = {9781577352815},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {993--1022},
	pmid = {21362469},
	title = {{Latent Dirichlet Allocation}},
	volume = {3},
	year = {2003}
}
@article{Ganin2015,
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	archivePrefix = {arXiv},
	arxivId = {1505.07818},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
	doi = {10.1088/1475-7516/2015/08/013},
	eprint = {1505.07818},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
	issn = {1475-7516},
	mendeley-groups = {Progress Report},
	pages = {1--35},
	title = {{Domain-Adversarial Training of Neural Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	volume = {17},
	year = {2015}
}
@article{Schaul2010,
	author = {Schaul, Tom and Bayer, Justin and Wierstra, Daan and Sun, Yi},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaul et al. - 2010 - PyBrain.pdf:pdf},
	issn = {1532-4435},
	journal = {The Journal of Machine {\ldots}},
	keywords = {neural networks,optimization,python,reinforcement learning},
	pages = {743--746},
	title = {{PyBrain}},
	url = {http://dl.acm.org/citation.cfm?id=1756030},
	volume = {11},
	year = {2010}
}
@article{Chih-WeiHsuChih-ChungChang2008,
	abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
	doi = {10.1177/02632760022050997},
	eprint = {0-387-31073-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chih-Wei Hsu, Chih-Chung Chang - 2008 - A Practical Guide to Support Vector Classification.pdf:pdf},
	isbn = {013805326X},
	issn = {1464-410X},
	journal = {BJU international},
	number = {1},
	pages = {1396--400},
	pmid = {18190633},
	title = {{A Practical Guide to Support Vector Classification}},
	url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
	volume = {101},
	year = {2008}
}
@article{Marshal2016,
	author = {Marshal, David and Lai, Yukun and Marshal, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marshal, Lai, Marshal - 2016 - Part 1 PhD progress review.pdf:pdf},
	number = {October},
	title = {{Part 1 : PhD progress review}},
	year = {2016}
}
@article{Arras2016,
	abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
	archivePrefix = {arXiv},
	arxivId = {1612.07843},
	author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
	eprint = {1612.07843},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2016 - {\&}quotWhat is Relevant in a Text Document{\&}quot An Interpretable Machine Learning Approach.pdf:pdf},
	isbn = {1111111111},
	mendeley-groups = {Annotated/Explanations,!Paper 3/task/newsgroups},
	pages = {1--23},
	title = {{"What is Relevant in a Text Document?": An Interpretable Machine Learning Approach}},
	url = {http://arxiv.org/abs/1612.07843},
	year = {2016}
}
@article{Ganin2015a,
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	archivePrefix = {arXiv},
	arxivId = {1505.07818},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
	doi = {10.1088/1475-7516/2015/08/013},
	eprint = {1505.07818},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
	issn = {1475-7516},
	mendeley-groups = {Progress Report},
	pages = {1--35},
	title = {{Domain-Adversarial Training of Neural Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	volume = {17},
	year = {2015}
}
@article{Zaidan2007,
	abstract = {We propose a new framework for supervised machine learning. Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with “rationales.” When annotating an example, the human teacher will also highlight evidence supporting this annotation—thereby teaching the machine learner why the example belongs to the category. We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance signi?cantly on a sample task, namely sentiment classi?cation of movie reviews. We hypothesize that in some situations, providing rationales is a more fruitful use of an annotator's time than annotating more examples.},
	author = {Zaidan, O. and Zaidan, O. and Eisner, J. and Eisner, J. and Piatko, C. and Piatko, C.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaidan et al. - 2007 - Using “annotator rationales” to improve machine learning for text categorization.pdf:pdf},
	isbn = {9781932432657},
	journal = {Proceedings of NAACL-HLT},
	mendeley-groups = {Report/Explaining predictions,Annotated},
	number = {April},
	pages = {260--267},
	title = {{Using “annotator rationales” to improve machine learning for text categorization}},
	url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Using+Annotator+Rationales+to+Improve+Machine+Learning+for+Text+Categorization{\#}0},
	volume = {260},
	year = {2007}
}
@article{Number2016,
	author = {Number, N I and Code, Post and Code, Sort and Account, Bank and Name, Society and Branch, Society and Roll, Building Society and Contact, Emergency and Number, Student},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Number et al. - 2016 - ENGAGEMENT OF A POST GRADUATE STUDENT DEMONSTRATOR.pdf:pdf},
	number = {August},
	pages = {1--7},
	title = {{ENGAGEMENT OF A POST GRADUATE STUDENT DEMONSTRATOR}},
	year = {2016}
}
@article{Kriegel2011,
	author = {Kriegel, Hans-peter and Kr, Peer},
	doi = {10.1002/widm.30},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kriegel, Kr - 2011 - Density-based clustering.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	number = {June},
	pages = {231--240},
	title = {{Density-based clustering}},
	volume = {1},
	year = {2011}
}
@article{Collobert2000,
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling, achieving or exceeding state-of-the-art performance in each on four benchmark tasks. Our goal was to design a flexible architecture that can learn representations useful for the tasks, thus avoiding excessive task-specific feature engineering (and therefore disregarding a lot of prior knowledge). Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabelled training data. This work is then used as a basis for building a freely available tagging system with excellent performance while requiring minimal computational resources.},
	archivePrefix = {arXiv},
	arxivId = {1103.0398},
	author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	doi = {10.1145/2347736.2347755},
	eprint = {1103.0398},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2000 - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
	isbn = {1532-4435},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {Deep Learning,Natural Language Processing,Neural Networks},
	mendeley-groups = {Literature Review},
	pages = {1--48},
	title = {{Natural Language Processing (almost) from Scratch}},
	volume = {1},
	year = {2000}
}
@article{Li2014,
	abstract = {Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the '' forced topic'' problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.},
	author = {Li, Ximing and Ouyang, Jihong and Lu, You and Zhou, Xiaotang and Tian, Tian},
	doi = {10.1007/s10791-014-9244-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - Group topic model organizing topics into groups.pdf:pdf},
	issn = {15737659},
	journal = {Information Retrieval},
	keywords = {Document clustering,Group,Latent Dirichlet allocation,Online learning,Topic modeling,Variational inference},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
	number = {1},
	pages = {1--25},
	title = {{Group topic model: organizing topics into groups}},
	volume = {18},
	year = {2014}
}
@article{Burges2005,
	author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
	doi = {10.1145/1102351.1102363},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:pdf},
	isbn = {1595931805},
	issn = {00243205},
	journal = {Icml 2005},
	keywords = {Learning to Rank,RankNet},
	mendeley-groups = {Progress Report},
	pages = {89--96},
	pmid = {16483612},
	title = {{Learning to rank using gradient descent}},
	year = {2005}
}
@article{Hornik1989,
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	doi = {10.1016/0893-6080(89)90020-8},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
	isbn = {08936080 (ISSN)},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
	number = {5},
	pages = {359--366},
	pmid = {74},
	title = {{Multilayer feedforward networks are universal approximators}},
	volume = {2},
	year = {1989}
}
@article{Author2015a,
	archivePrefix = {arXiv},
	arxivId = {1506.03694},
	author = {Author, First and Author, Second and Author, Third},
	eprint = {1506.03694},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Author, Author - 2015 - Learning language through pictures.pdf:pdf},
	number = {2013},
	pages = {1--2},
	title = {{Learning language through pictures}},
	url = {http://arxiv.org/pdf/1506.03694v2.pdf},
	year = {2015}
}
@article{Ribeiro,
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, " sufficient " conditions for predic-tions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by ex-plaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - Unknown - Anchors High-Precision Model-Agnostic Explanations.pdf:pdf},
	mendeley-groups = {!Paper 3},
	title = {{Anchors: High-Precision Model-Agnostic Explanations}}
}
@article{Xie2013,
	abstract = {Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets demonstrate the effectiveness of our model.},
	archivePrefix = {arXiv},
	arxivId = {1309.6874},
	author = {Xie, Pengtao and Xing, Eric P},
	eprint = {1309.6874},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Xing - 2013 - Integrating Document Clustering and Topic Modeling.pdf:pdf},
	journal = {Proceedings of the 29th conference on uncertainty in artificial intelligence},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
	pages = {694--703},
	title = {{Integrating Document Clustering and Topic Modeling}},
	url = {http://www.cs.cmu.edu/{~}pengtaox/papers/uai2013paper.pdf},
	year = {2013}
}
@article{Smilkov2016,
	abstract = {Embeddings are ubiquitous in machine learning, appearing in recommender systems, NLP, and many other applications. Researchers and developers often need to explore the properties of a specific embedding, and one way to analyze embeddings is to visualize them. We present the Embedding Projector, a tool for interactive visualization and interpretation of embeddings.},
	archivePrefix = {arXiv},
	arxivId = {1611.05469},
	author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Vi{\'{e}}gas, Fernanda B. and Wattenberg, Martin},
	eprint = {1611.05469},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smilkov et al. - 2016 - Embedding Projector Interactive Visualization and Interpretation of Embeddings.pdf:pdf},
	mendeley-groups = {Report},
	number = {Nips},
	title = {{Embedding Projector: Interactive Visualization and Interpretation of Embeddings}},
	url = {http://arxiv.org/abs/1611.05469},
	year = {2016}
}
@article{Williamson2010,
	abstract = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric mixed membership model - each data point is modeled with a collection of components of different proportions. Though powerful, the HDP makes an assumption that the probability of a component being exhibited by a data point is positively correlated with its proportion within that data point. This might be an undesirable assumption. For example, in topic modeling, a topic (component) might be rare throughout the corpus but dominant within those documents (data points) where it occurs. We develop the IBP compound Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data prevalence and within-data proportion in a mixed membership model. The ICD combines properties from the HDP and the Indian buffet process (IBP), a Bayesian nonparametric prior on binary matrices. The ICD assigns a subset of the shared mixture components to each data point. This subset, the data point's ``focus'', is determined independently from the amount that each of its components contribute. We develop an ICD mixture model for text, the focused topic model (FTM), and show superior performance over the HDP-based topic model.},
	author = {Williamson, Sinead and Wang, Chong and Heller, Katherine A},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williamson, Wang, Heller - 2010 - The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling.pdf:pdf},
	isbn = {9781605589077},
	journal = {Icml},
	mendeley-groups = {Annotated/Interpretable representations,!Paper 3/task/newsgroups},
	pages = {1151--1158},
	title = {{The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling}},
	url = {http://www.icml2010.org/papers/397.pdf},
	year = {2010}
}
@article{Saad2007a,
	abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., {\&} Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373-389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e. calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity-complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
	author = {Saad, Emad W. and Wunsch, Donald C.},
	doi = {10.1016/j.neunet.2006.07.005},
	isbn = {0893-6080},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Evolutionary algorithm,Explanation capability of neural networks,Hyperplanes,Inversion,Neural network explanation,Pedagogical,Rule extraction},
	number = {1},
	pages = {78--93},
	pmid = {17029713},
	title = {{Neural network explanation using inversion}},
	volume = {20},
	year = {2007}
}
@article{Besana1991,
	abstract = {Forty-six heroin abusers were hospitalized and treated with meperidine either alone or in association with clonidine. Meperidine was given orally in rapidly decreasing doses according to three different schedules. The majority of patients (87{\%}) successfully completed the detoxification program. The best meperidine starting posology was 200 mg four times daily, which allowed stoppage of the opioid treatment after gradual reduction of the daily dose in a mean time of 9.5 days. Association with clonidine was not proven to be useful. This study shows that meperidine can be effectively used in rapidly decreasing doses in the pharmacological detoxification treatment of hospitalized heroin addicts.},
	archivePrefix = {arXiv},
	arxivId = {1301.3781},
	author = {Besana, Carlo and Memoli, Massimo and Salvioni, Piero M. and Finazzi, Renato A. and Inversi, Felice and Rugarli, Claudio},
	doi = {10.3109/10826089109058901},
	eprint = {1301.3781},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/N13-1090.pdf:pdf},
	isbn = {0020-773X (Print)$\backslash$r0020-773X (Linking)},
	issn = {10826084},
	journal = {Substance Use and Misuse},
	keywords = {Clonidine,Detoxification,Drug addiction,Heroin misuse,Meperidine},
	number = {5},
	pages = {505--513},
	pmid = {1938007},
	title = {{Meperidine in detoxification of hospitalized heroin addicts}},
	url = {http://research.microsoft.com/en-},
	volume = {26},
	year = {1991}
}
@article{Mitchell1997a,
	author = {Mitchell, Tom and Simon, Herb and Pomerleau, Dean},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell, Simon, Pomerleau - 1997 - Multitask Learning Rich Caruana 23 September1997.pdf:pdf},
	number = {September},
	title = {{Multitask Learning Rich Caruana 23 September1997}},
	year = {1997}
}
@article{Setiono2000,
	abstract = {An effective algorithm for extracting M-of-N rules from trained feedforward neural networks is proposed. Two components of the algorithm distinguish our method from previously proposed algorithms which extract symbolic rules from neural networks. First, we train a network where each input of the data can only have one of the two possible values, -1 or one. Second, we apply the hyperbolic tangent function to each connection from the input layer to the hidden layer of the network. By applying this squashing function, the activation values at the hidden units are effectively computed as the hyperbolic tangent (or the sigmoid) of the scikit-learned inputs, where the scikit-learns have magnitudes that are equal one. By restricting the inputs and the scikit-learns to binary values either -1 or one, the extraction of M-of-N rules from the networks becomes trivial. We demonstrate the effectiveness of the proposed algorithm on several widely tested datasets. For datasets consisting of thousands of patterns with many attributes, the rules extracted by the algorithm are surprisingly simple and accurate.},
	author = {Setiono, Rudy},
	doi = {10.1109/72.839020},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono - 2000 - Extracting M-of-N rules from trained neural networks.pdf:pdf},
	isbn = {1045-9227},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	number = {2},
	pages = {512--519},
	pmid = {18249780},
	title = {{Extracting M-of-N rules from trained neural networks}},
	volume = {11},
	year = {2000}
}
@article{Bastani2017,
	abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
	archivePrefix = {arXiv},
	arxivId = {1706.09773},
	author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
	eprint = {1706.09773},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim, Bastani - 2017 - Interpretability via Model Extraction.pdf:pdf},
	keywords = {()},
	mendeley-groups = {Annotated/Fairness},
	title = {{Interpretability via Model Extraction}},
	url = {http://arxiv.org/abs/1706.09773},
	year = {2017}
}
@article{Goodfellow2014,
	abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2661v1},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	doi = {10.1017/CBO9781139058452},
	eprint = {arXiv:1406.2661v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
	isbn = {1406.2661},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems 27},
	mendeley-groups = {Annotated/Representation Learning,Annotated/Generative Adversarial Nets},
	pages = {2672--2680},
	pmid = {1000183096},
	title = {{Generative Adversarial Nets}},
	url = {http://@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
doi = {1606.06565},
eprint = {1606.06565},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
isbn = {0387310738},
mendeley-groups = {!Paper 3,11Thesis/Interpretability/Safety},
pages = {1--29},
title = {{Concrete Problems in AI Safety}},
url = {http://arxiv.org/abs/1606.06565},
year = {2016}
}
papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	year = {2014}
}

@article{Aldarwish2017,
abstract = {The use of Social Network Sites (SNS) is increasing nowadays especially by the younger generations. The availability of SNS allows users to express their interests, feelings and share daily routine. Many researchers prove that using user-generated content (UGC) in a correct way may help determine people's mental health levels. Mining the UGC could help to predict the mental health levels and depression. Depression is a serious medical illness, which interferes most with the ability to work, study, eat, sleep and having fun. However, from the user profile in SNS, we can collect all the information that relates to person's mood, and negativism. In this research, our aim is to investigate how SNS user's posts can help classify users according to mental health levels. We propose a system that uses SNS as a source of data and screening tool to classify the user using artificial intelligence according to the UGC on SNS. We created a model that classify the UGC using two different classifiers: Support Vector Machine (SVM), and Na{\"{i}}ve Bayes.},
author = {Aldarwish, Maryam Mohammed and Ahmad, Hafiz Farooq},
doi = {10.1109/ISADS.2017.41},
file = {:E$\backslash$:/07940253.pdf:pdf},
isbn = {9781509040414},
journal = {Proceedings - 2017 IEEE 13th International Symposium on Autonomous Decentralized Systems, ISADS 2017},
keywords = {Social Network Sites (SNS),Support Vector Machine (SVM),User Generated Content (UGC)},
pages = {277--280},
title = {{Predicting Depression Levels Using Social Media Posts}},
year = {2017}
}

@article{Zhang,
	abstract = {Multilabel learning is an extension of standard binary classi cation where the goal is to predict a set of labels (we call an individual label a tag) for each input example. The recent probabilistic classi er chain (PCC) method learns a series of probabilistic models that capture tag correlations. In this paper, we show how the PCC model may be viewed as a neural network with connections between output nodes. We then show that using a hidden layer in the neural network, instead of connections between output nodes, brings advantages that include tractable test-time inference and removing the need to select a xed tag ordering. Moreover, the hidden units capture nonlinear latent structure, which improves classi cation accuracy, and allows correlations between tags to be visualized explicitly. Compared to previous neural network methods for multilabel learning, we explain several design decisions that lead to a notable decrease in training time and a notable increase in accuracy. Empirical results show that the new method outperforms existing MLL methods on benchmark datasets. A nal contribution of the paper is to introduce a new multilabel dataset of movies where the tags are genres. Experimentally, the new method performs best on this dataset also.},
	author = {Zhang, Min-ling and Zhou, Zhi-hua},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - Unknown - Neural Networks for Multi-Label Learning.pdf:pdf},
	journal = {Performance Evaluation},
	keywords = {backpropagation,machine learning,multi-label learning,neural net-,text categorization,works},
	pages = {1--22},
	title = {{Neural Networks for Multi-Label Learning}}
}
@article{Code,
	author = {Code, Legal},
	file = {:E$\backslash$:/000000135948.pdf:pdf},
	title = {{Bag-of-Concepts : Comprehending Document Representation through Clustering Words in Distributed Representation Bag-of-Concepts : 단어에 대한 분산표상의}}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Ganesha.docx:docx},
	title = {{Ganesha}}
}
@article{Thomas2018,
	author = {Thomas, Student},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas - 2018 - Interim Report Student Details October 2015 October 2018 Section A Student Self-Assessment A . 1 Thesis Title and Hypot.pdf:pdf},
	title = {{Interim Report Student Details October 2015 October 2018 Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis Fine-tuning vector spaces by improving directions A . 2 Overall Progress}},
	year = {2018}
}
@article{Rocktaschel2015,
	author = {Rockt{\"{a}}schel, Tim and Singh, Sameer and Riedel, Sebastian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rockt{\"{a}}schel, Singh, Riedel - 2015 - Injecting logical background knowledge into embeddings for relation extraction.pdf:pdf},
	journal = {Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	title = {{Injecting logical background knowledge into embeddings for relation extraction}},
	year = {2015}
}
@article{Ding2015,
	abstract = {We propose a deep learning method for event- driven stock market prediction. First, events are extracted from news text, and represented as dense vectors, trained using a novel neural tensor net- work. Second, a deep convolutional neural network is used to model both short-term and long-term in- fluences of events on stock price movements. Ex- perimental results show that our model can achieve nearly 6{\%} improvements on S{\&}P 500 index predic- tion and individual stock prediction, respectively, compared to state-of-the-art baseline methods. In addition, market simulation results show that our system is more capable of making profits than pre- viously reported systems trained on S{\&}P 500 stock historical data.},
	author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2015 - Deep learning for event-driven stock prediction.pdf:pdf},
	isbn = {9781577357384},
	issn = {10450823},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	keywords = {Technical Papers — Web Mining},
	number = {Ijcai},
	pages = {2327--2333},
	title = {{Deep learning for event-driven stock prediction}},
	volume = {2015-Janua},
	year = {2015}
}
@article{Samek,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.07979v1},
	author = {Samek, Wojciech},
	eprint = {arXiv:1706.07979v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek - Unknown - Methods for Interpreting and Understanding Deep Neural Networks.pdf:pdf},
	keywords = {activation maximization,deep neural networks,layer-wise,relevance propagation,sensitivity analysis,taylor decomposition},
	mendeley-groups = {Annotated/Overarching Interpretability},
	title = {{Methods for Interpreting and Understanding Deep Neural Networks}}
}
@article{Agera,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders.pdf:pdf},
	title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Guarini2013,
	author = {Guarini, Marcello},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guarini - 2013 - Scholarship at UWindsor Case Classification , Similarities , Spaces of Reasons , and Coherences C ASE C LASSIFICATION ,.pdf:pdf},
	pages = {187--201},
	title = {{Scholarship at UWindsor Case Classification , Similarities , Spaces of Reasons , and Coherences C ASE C LASSIFICATION , S IMILARITIES ,}},
	year = {2013}
}
@article{Karaletsos2015a,
	abstract = {Representation learning systems typically rely on massive amounts of labeled data in order to be trained effectively. Recently, high-dimensional parametric models like convolutional neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Human-in-the-loop systems like crowdsourcing are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. We propose to combine generative unsupervised feature learning with learning from similarity orderings in order to learn models which take advantage of privileged information coming from the crowd. We use a fast variational algorithm to learn the model on standard datasets and demonstrate applicability to two image datasets, where classification is drastically improved. We show how triplet-samples of the crowd can supplement labels as a source of information to shape latent spaces with rich semantic information.},
	archivePrefix = {arXiv},
	arxivId = {1506.05011},
	author = {Karaletsos, Theofanis and Belongie, Serge and R{\"{a}}tsch, Gunnar},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1506.05011},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karaletsos, Belongie, R{\"{a}}tsch - 2015 - Bayesian representation learning with oracle constraints.pdf:pdf},
	journal = {Iclr},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Bayesian representation learning with oracle constraints}},
	url = {http://arxiv.org/abs/1506.05011},
	year = {2015}
}
@article{Liu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1705.09207v2},
	author = {Liu, Yang and Lapata, Mirella},
	eprint = {arXiv:1705.09207v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lapata - 2017 - Learning Structured Text Representations.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations},
	title = {{Learning Structured Text Representations}},
	year = {2017}
}
@article{Greff2017,
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	archivePrefix = {arXiv},
	arxivId = {1503.04069},
	author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
	doi = {10.1109/TNNLS.2016.2582924},
	eprint = {1503.04069},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf:pdf},
	isbn = {9788578110796},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
	mendeley-groups = {!Paper 3/Training LSTMs},
	number = {10},
	pages = {2222--2232},
	pmid = {25246403},
	title = {{LSTM: A Search Space Odyssey}},
	volume = {28},
	year = {2017}
}
@misc{Bologna2004,
	abstract = {Although many authors generated comprehensible models from individual networks, much less work has been done in the explanation of ensembles. DIMLP is a special neural network model from which rules are generated at the level of a single network and also at the level of an ensemble of networks. We applied ensembles of 25 DIMLP networks to several datasets of the public domain and a classification problem related to post-translational modifications of proteins. For the classification problems of the public domain, the average predictive accuracy of rulesets extracted from ensembles of neural networks was significantly better than the average predictive accuracy of rulesets generated from ensembles of decision trees. By varying the architectures of DIMLP networks we found that the average predictive accuracy of rules, as well as their complexity were quite stable. The comparison to other rule extraction techniques applied to neural networks showed that rules generated from DIMLP ensembles gave very good results. In the last problem related to bioinformatics, the best result obtained by ensembles of DIMLP networks was also significantly better than the best result obtained by ensembles of decision trees. Thus, although neural networks take much longer to train than decision trees and also rules are generated at a greater computational cost (however, still polynomial), at least for several classification problems it was worth using neural network ensembles, as extracted rules were more accurate, on average. The DIMLP software is available for PC-Linux under http://us.expasy.org/people/Guido.Bologna.html ?? 2004 Elsevier B.V. All rights reserved.},
	author = {Bologna, Guido},
	booktitle = {Journal of Applied Logic},
	doi = {10.1016/j.jal.2004.03.004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna - 2004 - Is it worth generating rules from neural network ensembles.htm:htm},
	isbn = {1570-8683},
	issn = {15708683},
	keywords = {Decision trees,Ensembles,Neural networks,Proteins,Rule extraction},
	number = {3},
	pages = {325--348},
	title = {{Is it worth generating rules from neural network ensembles?}},
	volume = {2},
	year = {2004}
}
@article{Craven1993,
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
	journal = {Machine Learning: Proceedings of the Tenth International Conference},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	pages = {73--80},
	title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
	year = {1993}
}
@article{Murdoch2017,
	abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
	archivePrefix = {arXiv},
	arxivId = {1702.02540},
	author = {Murdoch, W. James and Szlam, Arthur},
	doi = {10.5121/ijci.2015.4221},
	eprint = {1702.02540},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Szlam - 2017 - Automatic Rule Extraction from Long Short Term Memory Networks.pdf:pdf},
	issn = {23208430},
	mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/Interpretable LSTMs,!Paper 3/task,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank},
	number = {2016},
	pages = {1--12},
	title = {{Automatic Rule Extraction from Long Short Term Memory Networks}},
	url = {http://arxiv.org/abs/1702.02540},
	year = {2017}
}
@article{Bordes2012,
	abstract = {Open-text (or open-domain) semantic parsers are designed to interpret any state- ment in natural language by inferring a corresponding meaning representation (MR). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose here a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words, which are mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (WordNet and ConceptNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and word-sense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.},
	author = {Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio, Yoshua},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes et al. - 2012 - Joint learning of words and meaning representations for open-text semantic parsing.pdf:pdf},
	issn = {15337928},
	journal = {International {\ldots}},
	mendeley-groups = {Progress Report},
	pages = {127--135},
	title = {{Joint learning of words and meaning representations for open-text semantic parsing}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}BordesGWB12.pdf},
	volume = {22},
	year = {2012}
}
@article{Wachter2017,
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	archivePrefix = {arXiv},
	arxivId = {1711.00399},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	doi = {10.2139/ssrn.3063289},
	eprint = {1711.00399},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wachter, Mittelstadt, Russell - 2017 - Counterfactual Explanations without Opening the Black Box Automated Decisions and the GDPR.pdf:pdf},
	issn = {1556-5068},
	mendeley-groups = {!Paper 3/Justifying Interpretability},
	number = {1},
	pages = {1--47},
	title = {{Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR}},
	url = {http://arxiv.org/abs/1711.00399},
	year = {2017}
}
@article{Derrac2014,
	abstract = {Place types taxonomies tend to have a shallow structure, which limits their predictive value. Although existing place type taxonomies could in principle be refined, the result would inevitably be highly subjective and application-specific. Instead, in this paper, we propose a methodology to enrich place types taxonomies with a ternary betweenness relation derived from Flickr. In particular, we first construct a semantic space of place types by applying dimensionality reduction methods to tag co-occurrence data obtained from Flickr. Our hypothesis is that natural properties of place types should correspond to convex regions in this space. Specifically, knowing that places P 1,...,Pn have a given property, we could then induce that all places which are located in the convex hull of {\{}P1,...,P n{\}} in the semantic space are also likely to have this property. To avoid relying on computationally expensive convex hull algorithms, we propose to derive a ternary betweenness relation from the semantic space, and to approximate the convex hull at the symbolic level based on this relation. We present experimental results which support the usefulness of our approach. {\textcopyright} 2014 Springer International Publishing.},
	author = {Derrac, Joaqu{\'{i}}n and Schockaert, Steven},
	doi = {10.1007/978-3-319-04939-7_8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derrac, Schockaert - 2014 - Enriching taxonomies of place types using Flickr.pdf:pdf},
	isbn = {9783319049380},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {174--192},
	title = {{Enriching taxonomies of place types using Flickr}},
	volume = {8367 LNCS},
	year = {2014}
}
@article{Joulin2016,
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
	archivePrefix = {arXiv},
	arxivId = {1607.01759},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	doi = {1511.09249v1},
	eprint = {1607.01759},
	file = {:E$\backslash$:/Downloads/Work/1607.01759.pdf:pdf},
	isbn = {9781577357384},
	issn = {10450823},
	mendeley-groups = {Annotated/Interpretable Classifiers,!Paper 3/Word vectors,!Paper 3/task/Yelp},
	pmid = {1000303116},
	title = {{Bag of Tricks for Efficient Text Classification}},
	url = {http://arxiv.org/abs/1607.01759},
	year = {2016}
}
@article{Sun2016,
	abstract = {Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-the-art document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.},
	archivePrefix = {arXiv},
	arxivId = {1603.07603},
	author = {Sun, Fei and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Cheng, Xueqi},
	eprint = {1603.07603},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2016 - Semantic Regularities in Document Representations.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations},
	title = {{Semantic Regularities in Document Representations}},
	url = {http://arxiv.org/abs/1603.07603},
	year = {2016}
}
@misc{Cao2015,
	abstract = {We develop a Ranking framework upon Recursive Neural Networks (R2N2) to rank sentences for multi-document sum- marization. It formulates the sentence ranking task as a hi- erarchical regression process, which simultaneously mea- sures the salience of a sentence and its constituents (e.g., phrases) in the parsing tree. This enables us to draw on word-level to sentence-level supervisions derived from refer- ence summaries. In addition, recursive neural networks are used to automatically learn ranking features over the tree, with hand-crafted feature vectors of words as inputs. Hier- archical regressions are then conducted with learned features concatenating raw features. Ranking scores of sentences and words are utilized to effectively select informative and non- redundant sentences to generate summaries. Experiments on the DUC 2001, 2002 and 2004 multi-document summariza- tion datasets show that R2N2 outperforms state-of-the-art ex- tractive summarization approaches. Introduction},
	archivePrefix = {arXiv},
	arxivId = {1509.00685},
	author = {Cao, Ziqiang and Wei, Furu and Dong, Li and Li, Sujian and Zhou, Ming},
	booktitle = {Aaai},
	doi = {10.1162/153244303322533223},
	eprint = {1509.00685},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2015 - Ranking with recursive neural networks and its application to multi-document summarization.pdf:pdf},
	isbn = {9781577357018},
	issn = {19909772},
	keywords = {NLP and Knowledge Representation Track},
	mendeley-groups = {Progress Report},
	pages = {2153--2159},
	pmid = {18244602},
	title = {{Ranking with recursive neural networks and its application to multi-document summarization}},
	year = {2015}
}
@article{Dou2013,
	abstract = {Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system--HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.},
	author = {Dou, Wenwen and Yu, Li and Wang, Xiaoyu and Ma, Zhiqiang and Ribarsky, William},
	doi = {10.1109/TVCG.2013.162},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dou et al. - 2013 - HierarchicalTopics Visually exploring large text collections using topic hierarchies.pdf:pdf},
	isbn = {1077-2626},
	issn = {10772626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Hierarchical topic representation,rose tree,topic modeling,visual analytics},
	mendeley-groups = {Annotated/Explanations},
	number = {12},
	pages = {2002--2011},
	pmid = {24051766},
	title = {{HierarchicalTopics: Visually exploring large text collections using topic hierarchies}},
	volume = {19},
	year = {2013}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 4ae075de7bf9ea2cac01fe02e2920ee5c789.pdf.pdf:pdf},
	mendeley-groups = {Report},
	title = {4ae075de7bf9ea2cac01fe02e2920ee5c789.pdf}
}
@article{Plikynas2004a,
	author = {Plikynas, Darius},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plikynas - 2004 - Decision rules extraction from neural network A modified pedagogical approach.pdf:pdf},
	journal = {Information Technology and Control},
	keywords = {decisions reasonong,information extraction,neural networks},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {31},
	pages = {53--59},
	title = {{Decision rules extraction from neural network: A modified pedagogical approach}},
	url = {http://itc.ktu.lt/itc31/Plikyn31.pdf},
	volume = {2},
	year = {2004}
}
@article{Richards2001,
	abstract = {This paper describes the analysis of a database of diabetic patients' clinical records and death certificates. The objective of the study was to find rules that describe associations between observations made of patients at their first visit to the hospital and early mortality. Pre-processing was carried out and a knowledge discovery in databases (KDD) package, developed by the Lanner Group and the University of East Anglia, was used for rule induction using simulated annealing. The most significant discovered rules describe an association that was not generally known or accepted by the medical community, however, recent independent studies confirm their validity. ?? 2001 Elsevier Science B.V.},
	author = {Richards, G. and Rayward-Smith, V. J. and S??nksen, P. H. and Carey, S. and Weng, C.},
	doi = {10.1016/S0933-3657(00)00110-X},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Richards et al. - 2001 - Data mining for indicators of early mortality in a database of clinical records.pdf:pdf},
	isbn = {0933-3657},
	issn = {09333657},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Data mining,Diabetes,Neuropathy,Rule induction},
	mendeley-groups = {Report/Medical domain},
	number = {3},
	pages = {215--231},
	pmid = {11377148},
	title = {{Data mining for indicators of early mortality in a database of clinical records}},
	volume = {22},
	year = {2001}
}
@article{Windows2014,
	abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Windows, Microsoft and Os, Mac and When, Considerable Points and Wei, Yanhao and Yildirim, Pinar and den Bulte, Christophe and Dellarocas, Chris and Weekly, The and Weekly, I C T Issues and {W. E. Henley} and Vyas, Shilpan Dineshkumar and Uk, The and Trend, Security and Trend, Finance Security and Technology, Banker and Insights, Financial and Longtop, About and Technologies, Financial and Tan, Arhnue and Darby, Sime and Corp, I O I and Loong, Kim and SW공학센터 and Skan, Julian and Lumb, Richard and Masood, Samad and Conway, Sean K. and Shue, Kelly and Service, Social Networking and Server, Web and Module, Server and Security, Financial and Scheme, Lending and Sample, Char and Schaffer, Kim and Report, Survey and Report, Industry Issue and Pos, Mobile and Permissions, App and Experience, Web and Links, App and Payments, Mobile and Support, Fingerprint and Store, Play and Paper, Working and Online, Fast Identity and Okten, Cagla and Osili, Una Okonkwo and November, Security Focus and Name, Last and Name, First and Training, Online and Training, Practical and Darin, C and Training, Rank Online and Kimberly, M and Deepa, G and Board, Ethics and Principal, Enter and Primary, Investigator and Systems, Food and Study, Emu Behaviour and Co-investigator, New and Mohamad, Rosli and Building, Accountancy and Ismail, Noor Azizi and March, Security Focus and Lin, Mingfeng and Prabhala, N.R. and Viswanathan, Siva and Lee, Kwanghoe and Park, Sean and Lee, Jihye Jenna and Park, Sean and Law, Fintech and Straight, R Jason and Vice, Senior and Privacy, Chief and Straight, Unitedlex and Douglass, Duncan B and Avery, B Y Christopher and Fanger, Gwen and Douglass, Duncan B and KPMG and Kempe, David and Kleinberg, Jon and Tardos, {\'{E}}va and Karma, Credit and Issues, Special and Issue, Market and Internet, Secure and Service, Payment and Insight, L G Business and Indicators, Hot and Huang, Cheng-Lung and Chen, Mu-Chen and Wang, Chieh-Jen and Group, Alibaba and Go, Rnaseq P F and Go, Rnaseq P F and Go, David Down and Go, David Down and Go, Rnaseq P F and From, Industry Overview and Freedman, Seth and Jin, Ginger and Forgot, Email Password and Foust, By Dean and February, Aaron Pressman and Corp, Fair Isaac and Fair, Bill and Isaac, Earl and Fellowes, Matt and Isaac, Fair and Fico, The and Isaac, Fair and Sanders, Anthony B and Bank, Deutsche and York, New and Street, Wall and Fico, The and For, Everything and Home, T H E and Finance, Segye and February, Security Focus and {Eroglu S., Toprak S., Urgan O, MD, Ozge E. Onur, MD, Arzu Denizbasi, MD, Haldun Akoglu, MD, Cigdem Ozpolat, MD, Ebru Akoglu}, Md and {Ernst {\&} Young} and Economics, Size South Mountain and Duarte, Jefferson and Siegel, Stephan and Young, Lance and Debnath, Souvik and Ganguly, Niloy and Mitra, Pabitra and Data, Big and Dapp, Thomas S. and Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R. A. and Holmberg, S. D. and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, SVD Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise and Contributor, Tom Groenfeldt and Conference, Mobile Security and Call, Secure Monitor and Compass, Gyro and Cho, Byungchul and Park, Jong-man and Chavan, Jayshree and Chat, We and Channel, Omni and Challenge, Business and Brief, Keri and Brief, Keri and Block, Bitcoin and Economics, Size South Mountain and Big, Using and Based, Data and Analysis, Network and Bankitx, A and Bank, Challenger and Bank, Challenger and Bank, Challenger and Bankers, British and Street, High and Bank, British Business and Authority, Financial Conduct and Accenture and Ioffe, Sergey and Szegedy, Christian},
	doi = {10.1007/s13398-014-0173-7.2},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Windows et al. - 2014 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
	isbn = {9780874216561},
	issn = {0717-6163},
	journal = {Uma {\'{e}}tica para quantos?},
	keywords = {12,2007,3,Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\&} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cornway,Corporate Finance,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,Industrial Organization,J.,JUVENTUD,Lumb,MODIFICACIONES CORPORALES,Male,Masood,Motivation,Movement,Public,R.,Risk-Taking,S.,S.K.,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Skan,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\&} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,advantages,aesthetics,and e-banking,and on cor-,anomaly detection,as none were found,authentication,autoinjury and health,body,business model,candidate,classification,collaboration,competition,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,credit access,credit financing,credit score,credit scoring,critical success factors,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,customer satisfaction,customer scoring,data mining,decision tree,department of economics at,e-,e- banking,e-banking,e-commerce,e-payment,e-trading,electronic communication and computation,emergency,endogenous tie,epidural,esth{\'{e}}tique,est{\'{e}}tica,feature sim-,finance includes e-payment,financial fervices technology,financial services innovation,find any reports of,fintech,fintech analysis,fintech start-ups,functions,genetic programming,global fintech comparison,high resolution images,if neuraxial anes-,in practice,indonesia,information technology,ing with neuraxial anesthesia,internet bank,internet primary bank,jarunee wonglimpiyarat,jeunesse,jibc december 2007,juvenile cultures,juventud,limitations,luation of non-urgent visits,m-commerce,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,multimodal biometric,needle through a,nes corporales,network security,networks,neural networks,no,patents analysis,perforaci{\'{o}}n corporal,piel,professor of marketing,professor of marketing at,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,recommender system,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,smart cards,social network analysis,social networks,social status,spinal,strategic,strategy,support vector machine,sustainable reconstruction,sydney fintech,sydney start-ups,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,the university of pennsylvania,the wharton school of,to a busy urban,traditional banking services,unimodal biometric,university of pennsylvania,vol,was reviewed to see,youth},
	mendeley-groups = {!Paper 3/Training LSTMs,!Paper 3/Training Feedforward/CNN},
	number = {2},
	pages = {81--87},
	pmid = {15003161},
	title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
	url = {http://arxiv.org/abs/1502.03167{\%}5Cnhttp://www.americanbanker.com/issues/179{\_}124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/15003161{\%}5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1},
	volume = {XXXIII},
	year = {2014}
}
@article{Martens2007,
	abstract = {In recent years, support vector machines (SVMs) were successfully applied to a wide range of applications. However, since the classifier is described as a complex mathematical function, it is rather incomprehensible for humans. This opacity property prevents them from being used in many real-life applications where both accuracy and comprehensibility are required, such as medical diagnosis and credit risk evaluation. To overcome this limitation, rules can be extracted from the trained SVM that are interpretable by humans and keep as much of the accuracy of the SVM as possible. In this paper, we will provide an overview of the recently proposed rule extraction techniques for SVMs and introduce two others taken from the artificial neural networks domain, being Trepan and G-REX. The described techniques are compared using publicly available datasets, such as Ripley's synthetic dataset and the multi-class iris dataset. We will also look at medical diagnosis and credit scoring where comprehensibility is a key requirement and even a regulatory recommendation. Our experiments show that the SVM rule extraction techniques lose only a small percentage in performance compared to SVMs and therefore rank at the top of comprehensible classification techniques. ?? 2006 Elsevier B.V. All rights reserved.},
	author = {Martens, David and Baesens, Bart and {Van Gestel}, Tony and Vanthienen, Jan},
	doi = {10.1016/j.ejor.2006.04.051},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2007 - Comprehensible credit scoring models using rule extraction from support vector machines.pdf:pdf},
	isbn = {0377-2217},
	issn = {03772217},
	journal = {European Journal of Operational Research},
	keywords = {Classification,Credit scoring,Rule extraction,Support vector machine},
	mendeley-groups = {Annotated/Applications/Credit scoring},
	number = {3},
	pages = {1466--1476},
	title = {{Comprehensible credit scoring models using rule extraction from support vector machines}},
	volume = {183},
	year = {2007}
}
@article{Zhang2010,
	abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
	author = {Zhang, Min-Ling and Zhang, Kun},
	doi = {10.1145/1835804.1835930},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
	isbn = {9781450300551},
	issn = {9781577355687},
	journal = {Kdd},
	mendeley-groups = {Progress Report},
	pages = {999--1007},
	title = {{Multi-label learning by exploiting label dependency}},
	url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
	year = {2010}
}
@article{Franca2014,
	abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph min-ing and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integratedBCP with a well-known neural-symbolic system, C-IL2 P, to perform learning from nu-merical vectors. C-IL2 P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, han-dles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary re-sults indicating that a reduction of more than 90{\{}{\%}{\}} of features can be achieved with a small loss of accuracy.},
	author = {Fran{\c{c}}a, Manoel V M and Zaverucha, Gerson and {D'Avila Garcez}, Artur S.},
	doi = {10.1007/s10994-013-5392-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}a, Zaverucha, D'Avila Garcez - 2014 - Fast relational learning using bottom clause propositionalization with artificial neural netw.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Artificial neural networks,Inductive logic programming,Neural-symbolic integration,Propositionalization,Relational learning},
	mendeley-groups = {Progress Report},
	number = {1},
	pages = {81--104},
	title = {{Fast relational learning using bottom clause propositionalization with artificial neural networks}},
	volume = {94},
	year = {2014}
}
@article{Miller2017,
	abstract = {In his seminal book The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity [2004, Sams Indianapolis, IN, USA], Alan Cooper ar-gues that a major reason why software is of-ten poorly designed (from a user perspective) is that programmers are in charge of design de-cisions, rather than interaction designers. As a result, programmers design software for them-selves, rather than for their target audience; a phenomenon he refers to as the 'inmates run-ning the asylum'. This paper argues that ex-plainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But ex-plainable AI is more likely to succeed if re-searchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science; and if evalu-ation of these models is focused more on people than on technology. From a light scan of litera-ture, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
	author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller, Howe, Sonenberg - 2017 - Explainable AI Beware of Inmates Running the Asylum.pdf:pdf},
	journal = {IJCAI - Workshop on Explainable AI},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability},
	title = {{Explainable AI: Beware of Inmates Running the Asylum}},
	year = {2017}
}
@article{Kim2000,
	author = {Kim, D and Lee, J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Lee - 2000 - Handling continuous-valued attributes in decision tree with neural network modeling.pdf:pdf},
	isbn = {3-540-67602-3},
	journal = {Machine Learning: Ecml 2000},
	mendeley-groups = {Report/Decision Trees?,Papers/Paper 1,Progress Report,Report},
	pages = {211--219},
	title = {{Handling continuous-valued attributes in decision tree with neural network modeling}},
	volume = {1810},
	year = {2000}
}
@article{Version2017,
	author = {Version, Document},
	file = {:E$\backslash$:/Member-SIGIR.pdf:pdf},
	isbn = {9781450350228},
	keywords = {entity embedding,entity rank-,list completion,maximum margin},
	title = {{Kent Academic Repository}},
	year = {2017}
}
@article{Exner2012,
	abstract = {In this paper, we describe an end-to-end system that automatically extracts RDF triples describing entity relations and properties from unstructured text. This system is based on a pipeline of text processing modules that includes a semantic parser and a coreference solver. By using coreference chains, we group entity actions and properties described in different sentences and convert them{\textless}br/{\textgreater}{\textless}br{\textgreater}$\backslash$r$\backslash$ninto entity triples. We applied our system to over 114,000 Wikipedia articles and we could extract more than 1,000,000 triples. Using an ontology-mapping system that we bootstrapped using existing DBpedia triples, we mapped 189,000 extracted triples onto the DBpedia namespace. These extracted entities are availableonline in the N-Triple format.},
	author = {Exner, Peter and Nugues, Pierre},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Exner, Nugues - 2012 - Entity extraction From unstructured text to dbpedia rdf triples.pdf:pdf},
	issn = {16130073},
	journal = {CEUR Workshop Proceedings},
	mendeley-groups = {Report},
	number = {Iswc},
	pages = {58--69},
	title = {{Entity extraction: From unstructured text to dbpedia rdf triples}},
	volume = {906},
	year = {2012}
}
@article{Duch2001a,
	abstract = {A new methodology of extraction, optimization, and application of sets of logical rules is described. Neural networks are used for initial rule extraction, local or global minimization procedures for optimization, and Gaussian uncertainties of measurements are assumed during application of logical rules. Algorithms for extraction of logical rules from data with real-valued features require determination of linguistic variables or membership functions. Contest-dependent membership functions for crisp and fuzzy linguistic variables are introduced and methods of their determination described. Several neural and machine learning methods of logical rule extraction generating initial rules are described, based on constrained multilayer perceptron, networks with localized transfer functions or on separability criteria for determination of linguistic variables. A tradeoff between accuracy/simplicity is explored at the rule extraction stage and between rejection/error level at the optimization stage. Gaussian uncertainties of measurements are assumed during application of crisp logical rules, leading to "soft trapezoidal" membership functions and allowing to optimize the linguistic variables using gradient procedures. Numerous applications of this methodology to benchmark and real-life problems are reported and very simple crisp logical rules for many datasets provided.},
	author = {Duch, W??odzis??aw and Adamczak, Rafat and Gra??bczewski, Krzysztof},
	doi = {10.1109/72.914524},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duch, Adamczak, Grabczewski - 2001 - A new methodology of extraction, optimization and application of crisp and fuzzy logical rules.pdf:pdf},
	isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Backpropagation,Data mining,Decision trees,Feature selection,Fuzzy systems,Logical rule-based systems,Neural networks},
	mendeley-groups = {Progress Report},
	number = {2},
	pages = {277--306},
	pmid = {18244384},
	title = {{A new methodology of extraction, optimization and application of crisp and fuzzy logical rules}},
	volume = {12},
	year = {2001}
}
@article{Bastani,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1705.08504v1},
	author = {Bastani, Osbert and Kim, Carolyn},
	eprint = {arXiv:1705.08504v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim - Unknown - Interpreting Blackbox Models via Model Extraction.pdf:pdf},
	mendeley-groups = {Annotated},
	title = {{Interpreting Blackbox Models via Model Extraction}}
}
@article{ActuarialSocietyofSouthAfrica2016,
	author = {{Actuarial Society of South Africa}},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Actuarial Society of South Africa - 2016 - Student handbook 2016.pdf:pdf},
	pages = {1--51},
	title = {{Student handbook 2016}},
	url = {http://www.actuarialsociety.org.za/Portals/2/Documents/Education Office Documents/2016 Policies/2016 Student Handbook{\_}19022016.pdf},
	year = {2016}
}
@article{Setiono2009,
	abstract = {We address an important issue in knowledge discovery using neural networks that has been left out in a recent article "Knowledge discovery using a neural network simultaneous optimization algorithm on a real world classification problem" by Sexton et al. [R.S. Sexton, S. McMurtrey, D.J. Cleavenger, Knowledge discovery using a neural network simultaneous optimization algorithm on a real world classification problem, European Journal of Operational Research 168 (2006) 1009-1018]. This important issue is the generation of comprehensible rule sets from trained neural networks. In this note, we present our neural network rule extraction algorithm that is very effective in discovering knowledge embedded in a neural network. This algorithm is particularly appropriate in applications where comprehensibility as well as accuracy are required. For the same data sets used by Sexton et al. our algorithm produces accurate rule sets that are concise and comprehensible, and hence helps validate the claim that neural networks could be viable alternatives to other data mining tools for knowledge discovery. ?? 2007 Elsevier B.V. All rights reserved.},
	author = {Setiono, Rudy and Baesens, Bart and Mues, Christophe},
	doi = {10.1016/j.ejor.2007.09.022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Baesens, Mues - 2009 - A note on knowledge discovery using neural networks and its application to credit card screening.pdf:pdf},
	isbn = {0377-2217},
	issn = {03772217},
	journal = {European Journal of Operational Research},
	keywords = {Credit screening,Knowledge discovery,Neural networks,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	number = {1},
	pages = {326--332},
	title = {{A note on knowledge discovery using neural networks and its application to credit card screening}},
	volume = {192},
	year = {2009}
}
@article{Le2014,
	author = {Le, Quoc and Mikolov, Tomas and Com, Tmikolov Google},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov, Com - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
	mendeley-groups = {Annotated/Document representation,!Paper 3/task/Sentiment treebank},
	title = {{Distributed Representations of Sentences and Documents}},
	volume = {32},
	year = {2014}
}
@article{Sarikaya2014a,
	abstract = {Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.},
	author = {Sarikaya, Ruhi and Hinton, Geoffrey E. and Deoras, Anoop},
	doi = {10.1109/TASLP.2014.2303296},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarikaya, Hinton, Deoras - 2014 - Application of deep belief networks for natural language understanding.pdf:pdf},
	isbn = {2329-9290 VO  - 22},
	issn = {15587916},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	keywords = {Call-routing,DBN,Deep learning,Deep neural nets,Natural language understanding,RBM},
	number = {4},
	pages = {778--784},
	title = {{Application of deep belief networks for natural language understanding}},
	volume = {22},
	year = {2014}
}
@article{Krueger2016,
	abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.},
	archivePrefix = {arXiv},
	arxivId = {1606.01305},
	author = {Krueger, David and Maharaj, Tegan and Kram{\'{a}}r, J{\'{a}}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
	eprint = {1606.01305},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krueger et al. - 2016 - Zoneout Regularizing RNNs by Randomly Preserving Hidden Activations.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pages = {1--11},
	title = {{Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations}},
	url = {http://arxiv.org/abs/1606.01305},
	year = {2016}
}
@article{Kamkarhaghighi2017,
	author = {Kamkarhaghighi, Mehran and Makrehchi, Masoud},
	doi = {10.1016/j.eswa.2017.08.021},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamkarhaghighi, Makrehchi - 2017 - Content Tree Word Embedding for document representation.pdf:pdf},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Word embedding,Content tree,Word2Vec,GloVe,Sentime},
	mendeley-groups = {Annotated/Document representation},
	pages = {241--249},
	publisher = {Elsevier Ltd},
	title = {{Content Tree Word Embedding for document representation}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417417305596},
	volume = {90},
	year = {2017}
}
@article{Yosinski2014,
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	archivePrefix = {arXiv},
	arxivId = {1411.1792},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	eprint = {1411.1792},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - How transferable are features in deep neural networks.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems 27 (Proceedings of NIPS)},
	mendeley-groups = {Progress Report,Interim Review,Report},
	pages = {1--9},
	title = {{How transferable are features in deep neural networks?}},
	url = {http://arxiv.org/abs/1411.1792},
	volume = {27},
	year = {2014}
}
@article{Tai2015a,
	abstract = {Because of their superior ability to pre-serve sequence information over time, Long Short-Term Memory (LSTM) net-works, a type of recurrent neural net-work with a more complex computational unit, have obtained strong results on a va-riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn-tactic properties that would naturally com-bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti-ment Treebank).},
	archivePrefix = {arXiv},
	arxivId = {1503.0075},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	doi = {10.1515/popets-2015-0023},
	eprint = {1503.0075},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tai, Socher, Manning - 2015 - Improved semantic representations from tree-structured long short-term memory networks.pdf:pdf},
	isbn = {9781941643723},
	issn = {9781941643723},
	journal = {Proceedings of ACL},
	mendeley-groups = {Progress Report,!Paper 3/Interpretable LSTMs,!Paper 3/task/Sentiment treebank},
	pages = {1556--1566},
	pmid = {18267787},
	title = {{Improved semantic representations from tree-structured long short-term memory networks}},
	year = {2015}
}
@article{Selvaraju2016,
	abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-scikit-learned Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
	archivePrefix = {arXiv},
	arxivId = {1611.07450},
	author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
	eprint = {1611.07450},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Annotated/Explanations},
	pages = {1--4},
	title = {{Grad-CAM: Why did you say that?}},
	url = {http://arxiv.org/abs/1611.07450},
	year = {2016}
}
@article{Peng2015a,
	abstract = {Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information.},
	archivePrefix = {arXiv},
	arxivId = {1506.07220},
	author = {Peng, Yangtuo and Jiang, Hui},
	eprint = {1506.07220},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Jiang - 2015 - Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks.pdf:pdf},
	mendeley-groups = {Literature Review,Interim Review},
	pages = {5},
	title = {{Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks}},
	url = {http://arxiv.org/abs/1506.07220},
	year = {2015}
}
@article{Li2016a,
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
	archivePrefix = {arXiv},
	arxivId = {1506.01066},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	eprint = {1506.01066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Visualizing and Understanding Neural Models in NLP.pdf:pdf},
	journal = {Naacl},
	keywords = {Neural Network,Visualization},
	pages = {1--10},
	title = {{Visualizing and Understanding Neural Models in NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	year = {2016}
}
@article{Martensa,
	author = {Martens, David and Provost, Foster},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens, Provost - Unknown - Explaining Data-Driven Document Classifications.pdf:pdf},
	keywords = {comprehensibility,document classification,instance level explanation,text mining},
	mendeley-groups = {Annotated/Explanations,!Paper 3/task/newsgroups},
	title = {{Explaining Data-Driven Document Classifications *}}
}
@article{Bologna2000,
	author = {Bologna, Guido},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna - 2000 - Rule Extraction from a Multi layer Perceptron with Staircase Activation Functions The DIMLP model.pdf:pdf},
	mendeley-groups = {Progress Report},
	pages = {0--5},
	title = {{Rule Extraction from a Multi layer Perceptron with Staircase Activation Functions The DIMLP model}},
	year = {2000}
}
@article{Arjovsky2017,
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	archivePrefix = {arXiv},
	arxivId = {1701.04862},
	author = {Arjovsky, Martin and Bottou, L{\'{e}}on},
	doi = {10.2507/daaam.scibook.2010.27},
	eprint = {1701.04862},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arjovsky, Bottou - 2017 - Towards Principled Methods for Training Generative Adversarial Networks.pdf:pdf},
	isbn = {1584880309},
	issn = {17269687},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	title = {{Towards Principled Methods for Training Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1701.04862},
	year = {2017}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - arffs.groovy:groovy},
	title = {arffs}
}
@article{Zhang2016,
	abstract = {We present a novel subset scan method to detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup, and identify the characteristics of this subgroup. This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias or regions of poor classifier fit. This allows consideration of not just subgroups of a priori interest or small dimensions, but the space of all possible subgroups of features. To address the difficulty of considering these exponentially many possible subgroups, we use subset scan and parametric bootstrap-based methods. Extending this method, we can penalize the complexity of the detected subgroup and also identify subgroups with high classification errors. We demonstrate these methods and find interesting results on the COMPAS crime recidivism and credit delinquency data.},
	archivePrefix = {arXiv},
	arxivId = {1611.08292},
	author = {Zhang, Zhe and Neill, Daniel B.},
	eprint = {1611.08292},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Neill - 2016 - Identifying Significant Predictive Bias in Classifiers.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	number = {June},
	pages = {1--5},
	title = {{Identifying Significant Predictive Bias in Classifiers}},
	url = {http://arxiv.org/abs/1611.08292},
	year = {2016}
}
@article{Chaney2012,
	abstract = {Managing large collections of documents is an important$\backslash$nproblem for many areas of science, industry, and$\backslash$nculture. Probabilistic topic modeling offers a promising$\backslash$nsolution. Topic modeling is an unsupervised machine$\backslash$nlearning method that learns the underlying themes in$\backslash$na large collection of otherwise unorganized documents.$\backslash$nThis discovered structure summarizes and organizes the$\backslash$ndocuments. However, topic models are high-level statistical$\backslash$ntools—a user must scrutinize numerical distributions$\backslash$nto understand and explore their results. In this$\backslash$npaper, we present a method for visualizing topic models.$\backslash$nOur method creates a navigator of the documents,$\backslash$nallowing users to explore the hidden structure that a$\backslash$ntopic model discovers. These browsing interfaces reveal$\backslash$nmeaningful patterns in a collection, helping end-users$\backslash$nexplore and understand its contents in new ways. We$\backslash$nprovide open source software of our method.},
	author = {Chaney, Ajb and Blei, Dm},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaney, Blei - 2012 - Visualizing Topic Models.pdf:pdf},
	isbn = {9781577355564},
	journal = {Icwsm},
	pages = {419--422},
	title = {{Visualizing Topic Models.}},
	year = {2012}
}
@article{Blei2006,
	abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:0712.1486v1},
	author = {Blei, David M. and Lafferty, John D.},
	doi = {10.1145/1143844.1143859},
	eprint = {arXiv:0712.1486v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, Lafferty - 2006 - Correlated Topic Models.pdf:pdf},
	isbn = {1595933832},
	issn = {19326157},
	journal = {Advances in Neural Information Processing Systems 18},
	mendeley-groups = {Annotated/Topic models},
	pages = {147--154},
	pmid = {9013932},
	title = {{Correlated Topic Models}},
	url = {papers2://publication/uuid/1191CDB8-6BB3-4201-8EFB-6F7B8CBA0E8F},
	year = {2006}
}
@article{Simonyan2013,
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	archivePrefix = {arXiv},
	arxivId = {1312.6034},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	eprint = {1312.6034},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	pages = {1--8},
	title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
	url = {http://arxiv.org/abs/1312.6034},
	year = {2013}
}
@article{Burges1998,
	abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
	archivePrefix = {arXiv},
	arxivId = {1111.6189v1},
	author = {Burges, CJC Christopher J C},
	doi = {10.1023/A:1009715923555},
	eprint = {1111.6189v1},
	isbn = {0818672404},
	issn = {13845810},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
	number = {2},
	pages = {121--167},
	pmid = {5207842081938259593},
	title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
	url = {http://www.springerlink.com/index/Q87856173126771Q.pdf{\%}5Cnhttp://link.springer.com/article/10.1023/A:1009715923555},
	volume = {2},
	year = {1998}
}
@article{Zhang2017a,
	abstract = {The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet to establish a direct multimodal mapping between medical images and diagnostic reports that can read images, generate diagnostic reports, retrieve images by symptom descriptions, and visualize attention, to provide justifications of the network diagnosis process. MDNet includes an image model and a language model. The image model is proposed to enhance multi-scale feature ensembles and utilization efficiency. The language model, integrated with our improved attention mechanism, aims to read and explore discriminative image feature descriptions from reports to learn a direct mapping from sentence words to image pixels. The overall network is trained end-to-end by using our developed optimization strategy. Based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we conduct sufficient experiments to demonstrate that MDNet outperforms comparative baselines. The proposed image model obtains state-of-the-art performance on two CIFAR datasets as well.},
	archivePrefix = {arXiv},
	arxivId = {1707.02485},
	author = {Zhang, Zizhao and Xie, Yuanpu and Xing, Fuyong and McGough, Mason and Yang, Lin},
	doi = {10.1109/CVPR.2017.378},
	eprint = {1707.02485},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - MDNet A Semantically and Visually Interpretable Medical Image Diagnosis Network.pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {1063-6919},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	title = {{MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network}},
	url = {http://arxiv.org/abs/1707.02485},
	year = {2017}
}
@article{Fischer2012a,
	abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1311.0966v3},
	author = {Fischer, Asja and Igel, Christian},
	doi = {10.1007/978-3-642-33275-3_2},
	eprint = {arXiv:1311.0966v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer, Igel - 2012 - An Introduction to Restricted Boltzmann Machines.pdf:pdf},
	isbn = {978-3-642-33274-6},
	issn = {1875-7855},
	journal = {Lecture Notes in Computer Science: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
	pages = {14--36},
	pmid = {24309266},
	title = {{An Introduction to Restricted Boltzmann Machines}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33275-3{\_}2},
	volume = {7441},
	year = {2012}
}
@article{Mathieu2016,
	abstract = {We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentanglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.},
	archivePrefix = {arXiv},
	arxivId = {1611.03383},
	author = {Mathieu, Michael and Zhao, Junbo and Sprechmann, Pablo and Ramesh, Aditya and LeCun, Yann},
	eprint = {1611.03383},
	file = {:D$\backslash$:/PhD/Code/ThesisPipeline/ThesisPipeline/papers/1611.03383.pdf:pdf},
	issn = {10495258},
	number = {Nips},
	title = {{Disentangling factors of variation in deep representations using adversarial training}},
	url = {http://arxiv.org/abs/1611.03383},
	year = {2016}
}
@article{Zhang2016,
	abstract = {We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their component sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions.},
	archivePrefix = {arXiv},
	arxivId = {1605.04469},
	author = {Zhang, Ye and Marshall, Iain and Wallace, Byron C.},
	eprint = {1605.04469},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Marshall, Wallace - 2016 - Rationale-Augmented Convolutional Neural Networks for Text Classification.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Rationale-Augmented Convolutional Neural Networks for Text Classification}},
	url = {http://arxiv.org/abs/1605.04469},
	year = {2016}
}
@article{Blundell2015,
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the scikit-learns of a neural network, called Bayes by Backprop. It regularises the scikit-learns by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the scikit-learns can be used to improve generalisation in non-linear regression problems, and how this scikit-learn uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	archivePrefix = {arXiv},
	arxivId = {1505.05424},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	eprint = {1505.05424},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blundell et al. - 2015 - scikit-learn Uncertainty in Neural Networks.pdf:pdf},
	isbn = {9781510810587},
	mendeley-groups = {!Paper 3/Bayesian Networks},
	title = {{scikit-learn Uncertainty in Neural Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	volume = {37},
	year = {2015}
}
@article{Fu1994,
	abstract = {The neural network approach has proven useful for the development of artificial intelligence systems.  However, a disadvantage with this approach is that the knowledge embedded in the neural network is opaque.  In this paper, we show how to interpret neural network knowledge in sympobic form.  We lay down required definitions for this treatment, formulate the interpretation algorithm, and formally verigy its soundness.  The main result is a formalized relationship between a neural network and a rule-based system.  In addition, it has been demonstrated that the neural network generates rules of better performance than the decision tree approach in noisy conditions.},
	author = {Fu, LiMin},
	doi = {10.1109/21.299696},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu - 1994 - Rule generation from neural networks.pdf:pdf},
	issn = {00189472},
	journal = {IEEE Transactions on Systems, Man and Cybernetics},
	number = {8},
	pages = {1114--1124},
	title = {{Rule generation from neural networks}},
	volume = {24},
	year = {1994}
}
@article{Fallis2013a,
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Fallis, A.G},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fallis - 2013 - No Title No Title(2).pdf:pdf},
	isbn = {9788578110796},
	issn = {1098-6596},
	journal = {Journal of Chemical Information and Modeling},
	keywords = {icle},
	number = {9},
	pages = {1689--1699},
	pmid = {25246403},
	title = {{No Title No Title}},
	volume = {53},
	year = {2013}
}
@article{Kim2015,
	abstract = {We present the Mind the Gap Model (MGM), an approach for interpretable fea- ture extraction and selection. By placing interpretability criteria directly into the model, we allowfor the model to both optimize parameters related to interpretabil- ity and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and dis- ease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.},
	author = {Kim, Been and Shah, Julie and Doshi-Velez, Finale},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Shah, Doshi-Velez - 2015 - Mind the Gap A Generative Approach to Interpretable Feature Selection and Extraction.pdf:pdf},
	issn = {10495258},
	journal = {Nips},
	mendeley-groups = {Annotated/Interpretable representations},
	pages = {1--9},
	title = {{Mind the Gap : A Generative Approach to Interpretable Feature Selection and Extraction}},
	year = {2015}
}
@article{Merity2017,
	abstract = {Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing. Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance. Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations. We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling. Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures. These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.},
	archivePrefix = {arXiv},
	arxivId = {1708.01009},
	author = {Merity, Stephen and McCann, Bryan and Socher, Richard},
	eprint = {1708.01009},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merity, McCann, Socher - 2017 - Revisiting Activation Regularization for Language RNNs.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Revisiting Activation Regularization for Language RNNs}},
	url = {http://arxiv.org/abs/1708.01009},
	year = {2017}
}
@article{Yosinski2015,
	abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
	archivePrefix = {arXiv},
	arxivId = {1506.06579},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	eprint = {1506.06579},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visualization(2).pdf:pdf},
	journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
	pages = {12},
	title = {{Understanding Neural Networks Through Deep Visualization}},
	url = {http://arxiv.org/abs/1506.06579},
	year = {2015}
}
@article{Chen2012a,
	author = {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian and Sha, Fei},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2012 - Marginalized denoising autoencoders for domain adaptation.pdf:pdf},
	journal = {arXiv preprint arXiv:1206.4683},
	keywords = {boring formatting information, machine learning, I},
	title = {{Marginalized denoising autoencoders for domain adaptation}},
	year = {2012}
}
@article{Rocktaschel2015,
	abstract = {While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention scikit-learns produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.},
	archivePrefix = {arXiv},
	arxivId = {1509.06664},
	author = {Rockt{\"{a}}schel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Blunsom, Phil},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {1509.06664},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rockt{\"{a}}schel et al. - 2015 - Reasoning about Entailment with Neural Attention.pdf:pdf},
	isbn = {9781941643723},
	issn = {10450823},
	mendeley-groups = {!Paper 3/LSTM Types},
	number = {2015},
	pages = {1--9},
	pmid = {9377276},
	title = {{Reasoning about Entailment with Neural Attention}},
	url = {http://arxiv.org/abs/1509.06664},
	year = {2015}
}
@article{Sun2006,
	author = {Sun, Hongmao},
	doi = {10.1002/cmdc.200500047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun - 2006 - An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability.pdf:pdf},
	mendeley-groups = {Annotated/Applications/Scientific Discovery},
	pages = {315--322},
	title = {{An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability}},
	volume = {07110},
	year = {2006}
}
@article{Vembu2011,
	abstract = {Label ranking is a complex prediction task where the goal is to map instances to a total order over a finite set of predefined labels. An interesting aspect of this problem is that it subsumes several supervised learning problems, such as multiclass prediction, multilabel classification, and hierarchical classification. Unsurprisingly, there exists a plethora of label ranking algorithms in the literature due, in part, to this versatile nature of the problem. In this paper, we survey these algorithms.},
	author = {Vembu, Shankar and G{\"{a}}rtner, Thomas},
	doi = {10.1007/978-3-642-14125-6_3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vembu, G{\"{a}}rtner - 2011 - Label ranking algorithms A survey.pdf:pdf},
	isbn = {9783642141249},
	journal = {Preference Learning},
	pages = {45--64},
	title = {{Label ranking algorithms: A survey}},
	year = {2011}
}
@article{Bai2004,
	author = {Bai, Xue and Padman, Rema and Airoldi, Edoardo},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Padman, Airoldi - 2004 - Sentiment Extraction from Unstructured Text using Tabu Search-Enhanced Markov Blanket.pdf:pdf},
	journal = {Proceedings of the International Workshop on Mining for and from the Semantic Web},
	keywords = {bayesian models,opinion,semantic learning,sematic orientation,sentiments},
	mendeley-groups = {Report},
	number = {July},
	pages = {24--35},
	title = {{Sentiment Extraction from Unstructured Text using Tabu Search-Enhanced Markov Blanket}},
	url = {http://ra.adm.cs.cmu.edu/anon/home/ftp/usr/ftp/isri2004/CMU-ISRI-04-127.pdf},
	year = {2004}
}
@article{Garcez2003a,
	author = {d'Avila Garcez, Artur S and Lamb, Luis C},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez, Lamb - 2003 - Reasoning about Time and Knowledge in neural-symbolic learning systems.pdf:pdf},
	isbn = {0262201526},
	issn = {10495258},
	journal = {Advances in neural information processing systems},
	title = {{Reasoning about Time and Knowledge in neural-symbolic learning systems}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}CS03.pdf},
	year = {2003}
}
@article{Mao2014,
	abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1410.1090v1},
	author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
	eprint = {arXiv:1410.1090v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
	journal = {arXiv:1410.1090 [cs]},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Explain Images with Multimodal Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1410.1090{\%}5Cnhttp://www.arxiv.org/pdf/1410.1090.pdf},
	year = {2014}
}
@article{Schuhmacher2015,
	author = {Schuhmacher, Michael and Dietz, Laura and Ponzetto, Simone Paolo},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schuhmacher, Dietz, Ponzetto - 2015 - Ranking Entities for Web Queries Through Text and Knowledge.pdf:pdf},
	isbn = {9781450337946},
	keywords = {entities,information retrieval,knowledge bases},
	mendeley-groups = {Report/Rankings},
	pages = {1461--1470},
	title = {{Ranking Entities for Web Queries Through Text and Knowledge}},
	year = {2015}
}
@article{Ji2017,
	abstract = {We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.},
	archivePrefix = {arXiv},
	arxivId = {1702.01829},
	author = {Ji, Yangfeng and Smith, Noah},
	doi = {10.18653/v1/P17-1092},
	eprint = {1702.01829},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji, Smith - 2017 - Neural Discourse Structure for Text Categorization.pdf:pdf},
	isbn = {9781945626753},
	mendeley-groups = {!Paper 3/Structured LSTMs,!Paper 3/task/Yelp},
	title = {{Neural Discourse Structure for Text Categorization}},
	url = {http://arxiv.org/abs/1702.01829},
	year = {2017}
}
@article{Faruqui2015a,
	abstract = {Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substan- tial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms.},
	archivePrefix = {arXiv},
	arxivId = {1411.4166},
	author = {Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K and Dyer, Chris and Hovy, Eduard and Smith, Noah A},
	doi = {10.3115/v1/N15-1184},
	eprint = {1411.4166},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui et al. - 2015 - Retrofitting Word Vectors to Semantic Lexicons.pdf:pdf},
	isbn = {9781941643495},
	journal = {Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL},
	mendeley-groups = {!Paper 3/task/Sentiment treebank},
	number = {i},
	pages = {1606--1615},
	title = {{Retrofitting Word Vectors to Semantic Lexicons}},
	year = {2015}
}
@article{Kayande2009,
	abstract = {Model-based decision support systems (DSS) improve performance in many contexts that are data-rich, uncertain, and require repetitive decisions. But such DSS are often not designed to help users understand and internalize the underlying factors driving DSS recommendations. Users then feel uncertain about DSS recommendations, leading them to possibly avoid using the system. We argue that a DSS must be designed to induce an alignment of a decision maker's mental model with the decision model embedded in the DSS. Such an alignment requires effort from the decision maker and guidance from the DSS. We experimentally evaluate two DSS design characteristics that facilitate such alignment: (i) feedback on the upside potential for performance improvement and (ii) feedback on corrective actions to improve decisions. We show that, in tandem, these two types of DSS feedback induce decision makers to align their mental models with the decision model, a process we call deep learning, whereas individually these two types of feedback have little effect on deep learning. We also show that deep learning, in turn, improves user evaluations of the DSS. We discuss how our findings could lead to DSS design improvements and better returns on DSS investments. [ABSTRACT FROM AUTHOR] Copyright of Information Systems Research is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	author = {Kayande, Ujwal and {De Bruyn}, Arnaud and Lilien, Gary L. and Rangaswamy, Arvind and van Bruggen, Gerrit H.},
	doi = {10.1287/isre.1080.0198},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kayande et al. - 2009 - How incorporating feedback mechanisms in a DSS affects DSS evaluations.pdf:pdf},
	isbn = {10477047},
	issn = {10477047},
	journal = {Information Systems Research},
	keywords = {DSS design,Decision support systems,Evaluations,Feedback,Learning,Mental models},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {4},
	pages = {527--546},
	title = {{How incorporating feedback mechanisms in a DSS affects DSS evaluations}},
	volume = {20},
	year = {2009}
}
@article{Li2014a,
	author = {Li, Li and Zhang, Longkai and Wang, Houfeng},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Wang - 2014 - Muli-label Text Categorization with Hidden Components.pdf:pdf},
	journal = {Emnlp},
	mendeley-groups = {Interim Review},
	pages = {1816--1821},
	title = {{Muli-label Text Categorization with Hidden Components}},
	year = {2014}
}
@article{Greff2016,
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	archivePrefix = {arXiv},
	arxivId = {1503.04069},
	author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
	doi = {10.1109/TNNLS.2016.2582924},
	eprint = {1503.04069},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2016 - LSTM A Search Space Odyssey.pdf:pdf},
	isbn = {9788578110796},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	mendeley-groups = {Annotated/Representation Learning},
	pmid = {25246403},
	title = {{LSTM: A Search Space Odyssey}},
	year = {2016}
}
@article{Jurman2016,
	author = {Jurman, Nicholas},
	file = {:C$\backslash$:/Users/Workk/Documents/Research{\_}Student{\_}Travel{\_}Application{\_}completed.pdf:pdf},
	title = {{N / a}},
	year = {2016}
}
@article{Craven1996,
	author = {Craven, Mark W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven - 1996 - EXTRACTING COMPREHENSIBLE MODELS By.pdf:pdf},
	isbn = {0591144956},
	journal = {Evaluation},
	title = {{EXTRACTING COMPREHENSIBLE MODELS By}},
	year = {1996}
}
@article{Schockaert,
	author = {Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert - Unknown - Lexical inference as a spatial reasoning problem.pdf:pdf},
	title = {{Lexical inference as a spatial reasoning problem}}
}
@article{Johansson2009,
	abstract = {Some data mining problems require predictive models to be not only accurate but also comprehensible. Comprehensibility enables human inspection and understanding of the model, making it possible to trace why individual predictions are made. Since most high-accuracy techniques produce opaque models, accuracy is, in practice, regularly sacrificed for comprehensibility. One frequently studied technique, often able to reduce this accuracy vs. comprehensibility tradeoff, is rule extraction, i.e., the activity where another, transparent, model is generated from the opaque. In this paper, it is argued that techniques producing transparent models, either directly from the dataset, or from an opaque model, could benefit from using an oracle guide. In the experiments, genetic programming is used to evolve decision trees, and a neural network ensemble is used as the oracle guide. More specifically, the datasets used by the genetic programming when evolving the decision trees, consist of several different combinations of the original training data and ldquooracle datardquo, i.e., training or test data instances, together with corresponding predictions from the oracle. In total, seven different ways of combining regular training data with oracle data were evaluated, and the results, obtained on 26 UCI datasets, clearly show that the use of an oracle guide improved the performance. As a matter of fact, trees evolved using training data only had the worst test set accuracy of all setups evaluated. Furthermore, statistical tests show that two setups, both using the oracle guide, produced significantly more accurate trees, compared to the setup using training data only.},
	author = {Johansson, Ulf and Niklasson, Lars},
	doi = {10.1109/CIDM.2009.4938655},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johansson, Niklasson - 2009 - Evolving decision trees using oracle guides.pdf:pdf},
	isbn = {9781424427659},
	journal = {2009 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2009 - Proceedings},
	mendeley-groups = {Report/Medical domain},
	pages = {238--244},
	title = {{Evolving decision trees using oracle guides}},
	year = {2009}
}
@article{Johnson2016,
	abstract = {One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson {\&} Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.},
	archivePrefix = {arXiv},
	arxivId = {1602.02373},
	author = {Johnson, Rie and Zhang, Tong},
	eprint = {1602.02373},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson, Zhang - 2016 - Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings.pdf:pdf},
	isbn = {9781510829008},
	issn = {1938-7228},
	mendeley-groups = {!Paper 3/task/Large Movie Review},
	title = {{Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings}},
	url = {http://arxiv.org/abs/1602.02373},
	volume = {48},
	year = {2016}
}
@article{Andrews1995,
	abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy. {\textcopyright} 1995.},
	author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
	doi = {10.1016/0950-7051(96)81920-4},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Diederich, Tickle - 1995 - Survey and critique of techniques for extracting rules from trained artificial neural networks.pdf:pdf},
	isbn = {09507051 (ISSN)},
	issn = {09507051},
	journal = {Knowledge-Based Systems},
	keywords = {fuzzy neural networks,inferencing,knowledge insertion,rule extraction,rule refinement},
	number = {6},
	pages = {373--389},
	title = {{Survey and critique of techniques for extracting rules from trained artificial neural networks}},
	volume = {8},
	year = {1995}
}
@article{Dasigi2014a,
	abstract = {Automatically identifying anomalous newswire events is a hard problem. We discuss the com-plexity of the problem and introduce a novel technique to model events based on recursive neural networks to represent events as composition of their semantic arguments. Our model learns to differentiate between normal and anomalous events. We model anomaly detection as a binary classification problem and show that the model learns useful features to classify anomaly. We use headlines from the weird news category publicly available on newswire websites to extract anomalous training examples and those from Gigaword as normal examples. We evaluate the classifier on human annotated data and obtain an accuracy of 65.44{\%}. We also show that our model is at least as competent as the least competent human annotator in anomaly detection.},
	author = {Dasigi, Pradeep and Hovy, Eduard},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasigi, Hovy - 2014 - Modeling Newswire Events using Neural Networks for Anomaly Detection.pdf:pdf},
	isbn = {9781941643266},
	journal = {Coling-2014},
	pages = {1414--1422},
	title = {{Modeling Newswire Events using Neural Networks for Anomaly Detection}},
	year = {2014}
}
@article{Zeiler2014,
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	archivePrefix = {arXiv},
	arxivId = {1311.2901},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	doi = {10.1007/978-3-319-10590-1_53},
	eprint = {1311.2901},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks arXiv1311.2901v3 cs.CV 28 Nov 2013.pdf:pdf},
	isbn = {978-3-319-10589-5},
	issn = {978-3-319-10589-5},
	journal = {Computer Vision-ECCV 2014},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {818--833},
	pmid = {26353135},
	title = {{Visualizing and Understanding Convolutional Networks arXiv:1311.2901v3 [cs.CV] 28 Nov 2013}},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53{\%}5Cnhttp://arxiv.org/abs/1311.2901{\%}5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
	volume = {8689},
	year = {2014}
}
@article{Inan2016,
	abstract = {Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.},
	archivePrefix = {arXiv},
	arxivId = {1611.01462},
	author = {Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
	eprint = {1611.01462},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inan, Khosravi, Socher - 2016 - Tying Word Vectors and Word Classifiers A Loss Framework for Language Modeling.pdf:pdf},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
	pages = {1--13},
	title = {{Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling}},
	url = {http://arxiv.org/abs/1611.01462},
	year = {2016}
}
@article{Rubin2012,
	abstract = {Machine learning approaches to multi-label document classification have to date largely relied on discriminative modeling techniques such as support vector machines. A drawback of these approaches is that performance rapidly drops off as the total number of labels and the number of labels per document increase. This problem is amplified when the label frequencies exhibit the type of highly skewed distributions that are often observed in real-world datasets. In this paper we investigate a class of generative statistical topic models for multi-label documents that associate individual word tokens with different labels. We investigate the advantages of this approach relative to discriminative models, particularly with respect to classification problems involving large numbers of relatively rare labels. We compare the performance of generative and discriminative approaches on document labeling tasks ranging from datasets with several thousand labels to datasets with tens of labels. The experimental results indicate that probabilistic generative models can achieve competitive multi-label classification performance compared to discriminative methods, and have advantages for datasets with many labels and skewed label frequencies.},
	archivePrefix = {arXiv},
	arxivId = {1107.2462},
	author = {Rubin, Timothy N. and Chambers, America and Smyth, Padhraic and Steyvers, Mark},
	doi = {10.1007/s10994-011-5272-5},
	eprint = {1107.2462},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubin et al. - 2012 - Statistical topic models for multi-label document classification.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Dependency-LDA,Document modeling,Graphical models,LDA,Multi-label classification,Probabilistic generative models,Text classification,Topic models},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	number = {1-2},
	pages = {157--208},
	title = {{Statistical topic models for multi-label document classification}},
	volume = {88},
	year = {2012}
}
@article{Sellam2018,
	author = {Sellam, Thibault and Lin, Kevin and Huang, Ian Yiran and Vondrick, Carl and Research, Google and Wu, Eugene},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sellam et al. - 2018 - {\&}quot I Like the Way You Think! {\&}quot Inspecting the Internal Logic of Recurrent Neural Networks.pdf:pdf},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	pages = {1--3},
	title = {{" I Like the Way You Think! " Inspecting the Internal Logic of Recurrent Neural Networks}},
	url = {http://sellam.me/assets/papers/sellam-sysML.pdf},
	year = {2018}
}
@article{Setiono2008a,
	abstract = {In this paper, we present a recursive algorithm for extracting classification rules from feedforward neural networks (NNs) that have been trained on data sets having both discrete and continuous attributes. The novelty of this algorithm lies in the conditions of the extracted rules: the rule conditions involving discrete attributes are disjoint from those involving continuous attributes. The algorithm starts by first generating rules with discrete attributes only to explain the classification process of the NN. If the accuracy of a rule with only discrete attributes is not satisfactory, the algorithm refines this rule by recursively generating more rules with discrete attributes not already present in the rule condition, or by generating a hyperplane involving only the continuous attributes. We show that for three real-life credit scoring data sets, the algorithm generates rules that are not only more accurate but also more comprehensible than those generated by other NN rule extraction methods.},
	author = {Setiono, Rudy and Baesens, Bart and Mues, Christophe},
	doi = {10.1109/TNN.2007.908641},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Baesens, Mues - 2008 - Recursive neural network rule extraction for data with mixed attributes.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Continuous attributes,Credit scoring,Discrete attributes,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {2},
	pages = {299--307},
	pmid = {18269960},
	title = {{Recursive neural network rule extraction for data with mixed attributes}},
	volume = {19},
	year = {2008}
}
@article{Krakovna2016,
	abstract = {As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks, state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining a long short-term memory (LSTM) model with a hidden Markov model (HMM), a simpler and more transparent model. We add the HMM state probabilities to the output layer of the LSTM, and then train the HMM and LSTM either sequentially or jointly. The LSTM can make use of the information from the HMM, and fill in the gaps when the HMM is not performing well. A small hybrid model usually performs better than a standalone LSTM of the same size, especially on smaller data sets. We test the algorithms on text data and medical time series data, and find that the LSTM and HMM learn complementary information about the features in the text.},
	archivePrefix = {arXiv},
	arxivId = {1611.05934},
	author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
	eprint = {1611.05934},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krakovna, Doshi-Velez - 2016 - Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models(2).pdf:pdf},
	mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
	number = {Whi},
	title = {{Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}},
	url = {http://arxiv.org/abs/1611.05934},
	year = {2016}
}
@article{Google,
	abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep re-current architecture that combines recent advances in com-puter vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target de-scription sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descrip-tions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1411.4555v2},
	author = {Google, Oriol Vinyals and Google, Alexander Toshev and Google, Samy Bengio and Google, Dumitru Erhan},
	doi = {10.1109/CVPR.2015.7298935},
	eprint = {arXiv:1411.4555v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Google et al. - Unknown - Show and Tell A Neural Image Caption Generator.pdf:pdf},
	isbn = {9781467369640},
	issn = {9781467369640},
	title = {{Show and Tell: A Neural Image Caption Generator}}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - self.txt:txt},
	title = {self}
}
@article{Siddharth2016,
	abstract = {We develop a framework for incorporating structured graphical models in the $\backslash$emph{\{}encoders{\}} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes.},
	archivePrefix = {arXiv},
	arxivId = {1611.07492},
	author = {Siddharth, N. and Paige, Brooks and Desmaison, Alban and {Van de Meent}, Jan-Willem and Wood, Frank and Goodman, Noah D. and Kohli, Pushmeet and Torr, Philip H. S.},
	eprint = {1611.07492},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siddharth et al. - 2016 - Inducing Interpretable Representations with Variational Autoencoders.pdf:pdf},
	journal = {arXiv preprint},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	number = {Nips 2016},
	title = {{Inducing Interpretable Representations with Variational Autoencoders}},
	url = {http://arxiv.org/abs/1611.07492},
	year = {2016}
}
@article{Cao2015,
	author = {Cao, Tru and Lim, Ee Peng and Zhou, Zhi Hua and Ho, Tu Bao and Cheung, David and Motoda, Hiroshi},
	doi = {10.1007/978-3-319-18038-0},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2015 - Advances in knowledge discovery and data mining 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam,.pdf:pdf},
	isbn = {9783319180373},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Annotated/Document representation},
	pages = {212--225},
	title = {{Advances in knowledge discovery and data mining: 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam, May 19-22, 2015 proceedings, part I}},
	volume = {9077},
	year = {2015}
}
@article{Hu2016,
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the scikit-learns of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	archivePrefix = {arXiv},
	arxivId = {1603.06318},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	eprint = {1603.06318},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules.pdf:pdf},
	journal = {arXiv preprint},
	mendeley-groups = {Papers/Paper 1,Literature Review,Annotated/Rule-based classiifers,Report},
	pages = {1--18},
	title = {{Harnessing Deep Neural Networks with Logic Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	year = {2016}
}
@article{Tai2015,
	abstract = {Because of their superior ability to pre-serve sequence information over time, Long Short-Term Memory (LSTM) net-works, a type of recurrent neural net-work with a more complex computational unit, have obtained strong results on a va-riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn-tactic properties that would naturally com-bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti-ment Treebank).},
	archivePrefix = {arXiv},
	arxivId = {1503.0075},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	doi = {10.1515/popets-2015-0023},
	eprint = {1503.0075},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tai, Socher, Manning - 2015 - Improved semantic representations from tree-structured long short-term memory networks.pdf:pdf},
	isbn = {9781941643723},
	issn = {9781941643723},
	journal = {Proceedings of ACL},
	pages = {1556--1566},
	pmid = {18267787},
	title = {{Improved semantic representations from tree-structured long short-term memory networks}},
	year = {2015}
}
@article{Pedregosa2012,
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	archivePrefix = {arXiv},
	arxivId = {1201.0490},
	author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
	doi = {10.1007/s13398-014-0173-7.2},
	eprint = {1201.0490},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedregosa et al. - 2012 - Scikit-learn Machine Learning in Python.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	mendeley-groups = {Annotated/Software},
	pages = {2825--2830},
	pmid = {1000044560},
	title = {{Scikit-learn: Machine Learning in Python}},
	url = {http://arxiv.org/abs/1201.0490},
	volume = {12},
	year = {2012}
}
@article{Bloehdorn2006,
	abstract = {Recent work has shown improvements in text clustering and classification tasks by integrating conceptual features extracted from ontologies. In this paper we present text mining experiments in the medical domain in which the ontological structures used are acquired automatically in an unsupervised learning process from the text corpus in question. We compare results obtained using the automatically learned ontologies with those obtained using manually engineered ones. Our results show that both types of ontologies improve results on text clustering and classification tasks, whereby the automatically acquired ontologies yield a improvement competitive with the manually engineered ones.$\backslash$nER -},
	author = {Bloehdorn, S and Cimiano, P and Hotho, A},
	doi = {10.1007/3-540-31314-1_40},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bloehdorn, Cimiano, Hotho - 2006 - Learning Ontologies to Improve Text Clustering and Classification.pdf:pdf},
	isbn = {9783540313137},
	journal = {From Data and Information Analysis to Knowledge Engineering},
	number = {2005},
	pages = {334--341},
	title = {{Learning Ontologies to Improve Text Clustering and Classification}},
	year = {2006}
}
@article{Ager,
	author = {Ager, Thomas},
	mendeley-groups = {Temp},
	title = {{Annual Review Year 2 : Obtaining interpretable classifiers from text-based neural networks}}
}
@article{Zhai2016,
	abstract = {In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the scikit-learns learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the scikit-learns of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.},
	archivePrefix = {arXiv},
	arxivId = {1512.04466},
	author = {Zhai, Shuangfei and Zhang, Zhongfei},
	eprint = {1512.04466},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhai, Zhang - 2016 - Semisupervised Autoencoder for Sentiment Analysis.pdf:pdf},
	isbn = {9781577357605},
	journal = {Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016)},
	keywords = {Technical Papers: Machine Learning Applications},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1394--1400},
	title = {{Semisupervised Autoencoder for Sentiment Analysis}},
	url = {http://arxiv.org/abs/1512.04466},
	volume = {13902},
	year = {2016}
}
@article{Craven,
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - Unknown - Extracting Thee-Structured Representations of Thained Networks.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees},
	title = {{Extracting Thee-Structured Representations of Thained Networks}}
}
@article{Bergera,
	author = {Berger, Mark J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - Unknown - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
	mendeley-groups = {Interim Review},
	pages = {1--8},
	title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}}
}
@article{Boureau2010,
	abstract = {Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter responses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be broken down into two steps: (1) a coding step, which performs a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pooling step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pooling schemes have been proposed in the literature. The goal of this paper is threefold. We seek to establish the relative importance of each step of mid-level feature extraction through a comprehensive cross evaluation of several types of coding modules (hard and soft vector quantization, sparse coding) and pooling schemes (by taking the average, or the maximum), which obtains state-of-the-art performance or better on several recognition benchmarks. We show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding. We provide theoretical and empirical insight into the remarkable performance of max pooling. By teasing apart components shared by modern mid-level feature extractors, our approach aims to facilitate the design of better recognition architectures.},
	author = {Boureau, Y. Lan and Bach, Francis and LeCun, Yann and Ponce, Jean},
	doi = {10.1109/CVPR.2010.5539963},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boureau et al. - 2010 - Learning mid-level features for recognition.pdf:pdf},
	isbn = {9781424469840},
	issn = {10636919},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	mendeley-groups = {Report/Features,Progress Report},
	pages = {2559--2566},
	title = {{Learning mid-level features for recognition}},
	year = {2010}
}
@article{Marshal2016,
	author = {Marshal, David and Lai, Yukun and Marshal, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marshal, Lai, Marshal - 2016 - Part 1 PhD progress review(2).pdf:pdf},
	title = {{Part 1 : PhD progress review}},
	year = {2016}
}
@article{Cambria2013,
	abstract = {The way people express their opinions has radically changed in the past few years thanks to the advent of online collaborative media. The distillation of knowledge from this huge amount of unstructured information can be a key factor for marketers who want to create an identity for their product or brand in the minds of their customers. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. Existing approaches to opinion mining, in fact, are still far from being able to infer the cognitive and affective information associated with natural language as they mainly rely on knowledge bases that are too limited to efficiently process text at concept-level. In this context, standard clustering techniques have been previously employed on an affective common-sense knowledge base in attempt to discover how different natural language concepts are semantically and affectively related to each other and, hence, to accordingly mine on-line opinions. In this work, a novel cognitive model based on the combined use of multi-dimensional scaling and artificial neural networks is exploited for better modelling the way multi-word expressions are organised in a brain-like universe of natural language concepts. The integration of a biologically inspired paradigm with standard principal component analysis helps to better grasp the non-linearities of the resulting vector space and, hence, improve the affective common-sense reasoning capabilities of the system. ?? 2012 Elsevier B.V. All rights reserved.},
	author = {Cambria, Erik and Mazzocco, Thomas and Hussain, Amir},
	doi = {10.1016/j.bica.2013.02.003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cambria, Mazzocco, Hussain - 2013 - Application of multi-dimensional scaling and artificial neural networks for biologically inspired op.pdf:pdf},
	isbn = {1467351644},
	issn = {2212683X},
	journal = {Biologically Inspired Cognitive Architectures},
	keywords = {AI,ANN,Cognitive modelling,NLP,Sentic computing},
	pages = {41--53},
	publisher = {Elsevier B.V.},
	title = {{Application of multi-dimensional scaling and artificial neural networks for biologically inspired opinion mining}},
	url = {http://dx.doi.org/10.1016/j.bica.2013.02.003},
	volume = {4},
	year = {2013}
}
@article{Kumar2015,
	abstract = {{\textless}p{\textgreater} A framework for automated detection and classification of cancer from microscopic biopsy images using clinically significant and biologically interpretable features is proposed and examined. The various stages involved in the proposed methodology include enhancement of microscopic images, segmentation of background cells, features extraction, and finally the classification. An appropriate and efficient method is employed in each of the design steps of the proposed framework after making a comparative analysis of commonly used method in each category. For highlighting the details of the tissue and structures, the contrast limited adaptive histogram equalization approach is used. For the segmentation of background cells, {\textless}math id="M1"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}k{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -means segmentation algorithm is used because it performs better in comparison to other commonly used segmentation methods. In feature extraction phase, it is proposed to extract various biologically interpretable and clinically significant shapes as well as morphology based features from the segmented images. These include gray level texture features, color based features, color gray level texture features, Law's Texture Energy based features, Tamura's features, and wavelet features. Finally, the {\textless}math id="M2"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}K{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -nearest neighborhood method is used for classification of images into normal and cancerous categories because it is performing better in comparison to other commonly used methods for this application. The performance of the proposed framework is evaluated using well-known parameters for four fundamental tissues (connective, epithelial, muscular, and nervous) of randomly selected 1000 microscopic biopsy images. {\textless}/p{\textgreater}},
	author = {Kumar, Rajesh and Srivastava, Rajeev and Srivastava, Subodh},
	doi = {10.1155/2015/457906},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Srivastava, Srivastava - 2015 - Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significan.pdf:pdf},
	issn = {2314-5129, 2314-5137},
	journal = {Journal of Medical Engineering},
	mendeley-groups = {Annotated/Applications/Medical},
	number = {2015},
	pages = {1--14},
	pmid = {21329180},
	title = {{Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significant and Biologically Interpretable Features}},
	url = {http://www.hindawi.com/journals/jme/2015/457906/},
	volume = {2015},
	year = {2015}
}
@article{Malioutov2013,
	abstract = {We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean 'sensing' matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting. We show competitive classification accuracy using the proposed approach. Copyright 2013 by the author(s).},
	author = {Malioutov, D M and Varshney, K R},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malioutov, Varshney - 2013 - Exact rule learning via Boolean compressed sensing.pdf:pdf},
	journal = {30th International Conference on Machine Learning, ICML 2013},
	keywords = {Classification accuracy; Exact recoveries; Group,Learning systems; Signal reconstruction,Recovery},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	number = {PART 3},
	pages = {1802--1810},
	title = {{Exact rule learning via Boolean compressed sensing}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84897531301{\&}partnerID=40{\&}md5=896c81fac3b62d2c5b6e9f3cf3a5a96d},
	year = {2013}
}
@article{Garrette2014,
	abstract = {First-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, scikit-learned knowledge, for example regarding word meaning. This paper describes a mapping between predicates of logical form and points in a vector space. This mapping is then used to project distributional inferences to inference rules in logical form. We then describe first steps of an approach that uses this mapping to recast first-order semantics into the probabilistic models that are part of Statistical Relational AI. Specifically, we show how Discourse Representation Structures can be combined with distributional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as negation and factivity as well as scikit-learned information on word meaning in context.},
	author = {Garrette, Dan and Erk, Katrin and Mooney, Raymond},
	doi = {10.1007/978-94-007-7284-7_3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garrette, Erk, Mooney - 2014 - A Formal Approach to Linking Logical Form and Vector-Space Lexical Semantics.pdf:pdf},
	isbn = {978-94-007-7283-0},
	journal = {Computing Meaning SE - 3},
	mendeley-groups = {Progress Report},
	pages = {27--48},
	title = {{A Formal Approach to Linking Logical Form and Vector-Space Lexical Semantics}},
	url = {http://link.springer.com/chapter/10.1007/978-94-007-7284-7{\_}3{\%}5Cnhttp://dx.doi.org/10.1007/978-94-007-7284-7{\_}3},
	volume = {47},
	year = {2014}
}
@article{Giatsoglou2017,
	abstract = {Sentiment analysis and opinion mining are valuable for extraction of useful subjective information out of text documents. These tasks have become of great importance, especially for business and marketing professionals, since online posted products and services reviews impact markets and consumers shifts. This work is motivated by the fact that automating retrieval and detection of sentiments expressed for certain products and services embeds complex processes and pose research challenges, due to the textual phenomena and the language specific expression variations. This paper proposes a fast, flexible, generic methodology for sentiment detection out of textual snippets which express people's opinions in different languages. The proposed methodology adopts a machine learning approach with which textual documents are represented by vectors and are used for training a polarity classification model. Several documents' vector representation approaches have been studied, including lexicon-based, word embedding-based and hybrid vectorizations. The competence of these feature representations for the sentiment classification task is assessed through experiments on four datasets containing online user reviews in both Greek and English languages, in order to represent high and weak inflection language groups. The proposed methodology requires minimal computational resources, thus, it might have impact in real world scenarios where limited resources is the case.},
	author = {Giatsoglou, Maria and Vozalis, Manolis G. and Diamantaras, Konstantinos and Vakali, Athena and Sarigiannidis, George and Chatzisavvas, Konstantinos Ch},
	doi = {10.1016/j.eswa.2016.10.043},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giatsoglou et al. - 2017 - Sentiment analysis leveraging emotions and word embeddings.pdf:pdf},
	isbn = {0957-4174},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Hybrid vectorization,Machine learning,Multilingual sentiment analysis,Online user reviews,Text analysis,Vector representation},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Sentiment treebank},
	pages = {214--224},
	publisher = {Elsevier Ltd},
	title = {{Sentiment analysis leveraging emotions and word embeddings}},
	url = {http://dx.doi.org/10.1016/j.eswa.2016.10.043},
	volume = {69},
	year = {2017}
}
@article{Haury2011,
	abstract = {Biomarker discovery from high-dimensional data is a crucial problem with enormous applications in biology and medicine. It is also extremely challenging from a statistical viewpoint, but surprisingly few studies have investigated the relative strengths and weaknesses of the plethora of existing feature selection methods. In this study we compare feature selection methods on public gene expression datasets for breast cancer prognosis, in terms of predictive performance, stability and functional interpretability of the signatures they produce. We observe that the feature selection method has a significant influence on the accuracy, stability and interpretability of signatures. Surprisingly, complex wrapper and embedded methods generally do not outperform simple univariate feature selection methods, and ensemble feature selection has generally no positive effect. Overall a simple Student's t-test seems to provide the best results.},
	archivePrefix = {arXiv},
	arxivId = {1101.5008},
	author = {Haury, Anne Claire and Gestraud, Pierre and Vert, Jean Philippe},
	doi = {10.1371/journal.pone.0028210},
	eprint = {1101.5008},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haury, Gestraud, Vert - 2011 - The influence of feature selection methods on accuracy, stability and interpretability of molecular signa.pdf:pdf},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Applications/Scientific Discovery},
	number = {12},
	pages = {1--12},
	pmid = {22205940},
	title = {{The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures}},
	volume = {6},
	year = {2011}
}
@article{GethsiyalAugasta2012,
	abstract = {Artificial neural networks often achieve high classification accuracy$\backslash$nrates, but they are considered as black boxes due to their lack of$\backslash$nexplanation capability. This paper proposes the new rule extraction$\backslash$nalgorithm RxREN to overcome this drawback. In pedagogical approach the$\backslash$nproposed algorithm extracts the rules from trained neural networks for$\backslash$ndatasets with mixed mode attributes. The algorithm relies on reverse$\backslash$nengineering technique to prune the insignificant input neurons and to$\backslash$ndiscover the technological principles of each significant input neuron$\backslash$nof neural network in classification. The novelty of this algorithm lies$\backslash$nin the simplicity of the extracted rules and conditions in rule are$\backslash$ninvolving both discrete and continuous mode of attributes.$\backslash$nExperimentation using six different real datasets namely iris, wbc,$\backslash$nhepatitis, pid, ionosphere and creditg show that the proposed algorithm$\backslash$nis quite efficient in extracting smallest set of rules with high$\backslash$nclassification accuracy than those generated by other neural network$\backslash$nrule extraction methods.},
	author = {{Gethsiyal Augasta}, M. and Kathirvalavakumar, T.},
	doi = {10.1007/s11063-011-9207-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gethsiyal Augasta, Kathirvalavakumar - 2012 - Reverse engineering the neural networks for rule extraction in classification problems.pdf:pdf},
	issn = {13704621},
	journal = {Neural Processing Letters},
	keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {2},
	pages = {131--150},
	title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
	volume = {35},
	year = {2012}
}
@article{Hoyer2004,
	abstract = {Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of `sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.},
	archivePrefix = {arXiv},
	arxivId = {cs/0408058},
	author = {Hoyer, Patrik O.},
	doi = {10.1109/ICMLC.2011.6016966},
	eprint = {0408058},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoyer - 2004 - Non-negative matrix factorization with sparseness constraints.pdf:pdf},
	isbn = {0780395174},
	issn = {1532-4435},
	keywords = {data-adaptive representations,non-negative matrix factorization,sparseness},
	mendeley-groups = {Annotated/NMF},
	pages = {1457--1469},
	pmid = {1000253614},
	primaryClass = {cs},
	title = {{Non-negative matrix factorization with sparseness constraints}},
	url = {http://arxiv.org/abs/cs/0408058},
	volume = {5},
	year = {2004}
}
@article{Mimno2011,
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Un-fortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional sub- spaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mimno et al. - 2011 - Optimizing semantic coherence in topic models.pdf:pdf},
	isbn = {9781937284114},
	issn = {1937284115},
	journal = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
	keywords = {topic coherence,topic models,topics evaluation},
	mendeley-groups = {Report/Clustering},
	number = {2},
	pages = {262--272},
	title = {{Optimizing semantic coherence in topic models}},
	year = {2011}
}
@article{Isinkaye2015,
	author = {Isinkaye, F O},
	doi = {10.1016/j.eij.2015.06.005},
	file = {:D$\backslash$:/PhD/PGR/1-s2.0-S1110866515000341-main.pdf:pdf},
	keywords = {collaborative filtering,content-based filtering,hybrid filtering technique},
	pages = {261--273},
	publisher = {Ministry of Higher Education and Scientific Research},
	title = {{Recommendation systems : Principles , methods and evaluation}},
	year = {2015}
}
@article{Murdoch2018,
	abstract = {The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.},
	archivePrefix = {arXiv},
	arxivId = {1801.05453},
	author = {Murdoch, W. James and Liu, Peter J. and Yu, Bin},
	eprint = {1801.05453},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Liu, Yu - 2018 - Beyond Word Importance Contextual Decomposition to Extract Interactions from LSTMs.pdf:pdf},
	mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank},
	pages = {1--14},
	title = {{Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs}},
	url = {http://arxiv.org/abs/1801.05453},
	year = {2018}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - S000437020800101X.htm:htm},
	title = {{S000437020800101X}}
}
@article{Stubbs2007,
	abstract = {The use of robots, especially autonomous mobile robots, to support work is expected to increase over the next few decades. However, little empirical research examines how users form mental models of robots, how they collaborate with them, and what factors contribute to the success or failure of human-robot collaboration. A two-year observational study of a collaborative human-robot system suggests that the factors disrupting the creation of common ground for interactive communication change at different levels of robot autonomy. Our observations of users collaborating with the remote robot showed differences in how the users reached common ground with the robot in terms of an accurate, shared understanding of the robot's context, planning, and actions - a process called grounding. We focus on how the types and levels of robot autonomy affect grounding. We also examine the challenges a highly autonomous system presents to people's ability to maintain a shared mental model of the robot},
	author = {Stubbs, Kristen and Wettergreen, David and Hinds, Pamela J.},
	doi = {10.1109/MIS.2007.21},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stubbs, Wettergreen, Hinds - 2007 - Autonomy and common ground in human-robot interaction A field study.pdf:pdf},
	isbn = {1541-1672 VO  - 22},
	issn = {15411672},
	journal = {IEEE Intelligent Systems},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {2},
	pages = {42--50},
	title = {{Autonomy and common ground in human-robot interaction: A field study}},
	volume = {22},
	year = {2007}
}
@article{Graves2013,
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	archivePrefix = {arXiv},
	arxivId = {1308.0850},
	author = {Graves, Alex},
	doi = {10.1145/2661829.2661935},
	eprint = {1308.0850},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
	isbn = {2000201075},
	issn = {18792782},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pages = {1--43},
	pmid = {23459267},
	title = {{Generating Sequences With Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	year = {2013}
}
@article{Miller1956,
	abstract = {First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Miller, George A.},
	doi = {10.1037/h0043158},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1956 - The magical number seven, plus or minus two some limits on our capacity for processing information.pdf:pdf},
	isbn = {0198568770;},
	issn = {1939-1471},
	journal = {Psychological Review},
	mendeley-groups = {Annotated/Psychology},
	number = {2},
	pages = {81--97},
	pmid = {8022966},
	title = {{The magical number seven, plus or minus two: some limits on our capacity for processing information.}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0043158},
	volume = {63},
	year = {1956}
}
@article{Leek2014,
	author = {Leek, Jeffrey T},
	doi = {10.1038/nrg2825.Tackling},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leek - 2014 - NIH Public Access.pdf:pdf},
	mendeley-groups = {Annotated/Artifacts in the data},
	number = {10},
	pages = {1--15},
	title = {{NIH Public Access}},
	volume = {11},
	year = {2014}
}
@article{Valizadegan2009,
	abstract = {Learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels. The ranking algorithms are often evaluated using information retrieval measures, such as Normalized Discounted Cumulative Gain (NDCG) [1] and Mean Average Precision (MAP) [2]. Until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures. The main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function. We propose a probabilistic framework that addresses this challenge by optimizing the expectation of NDCG over all the possible permutations of documents. A relaxation strategy is used to approximate the average of NDCG over the space of permutation, and a bound optimization approach is proposed to make the computation efficient. Extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets. 1},
	author = {Valizadegan, Hamed and Jin, R},
	doi = {10.1561/1500000016},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Valizadegan, Jin - 2009 - Learning to rank by optimizing ndcg measure.pdf:pdf},
	isbn = {9781615679119},
	issn = {1554-0669},
	journal = {Advances in neural {\ldots}},
	mendeley-groups = {Annotated/Ranking},
	pages = {1--9},
	title = {{Learning to rank by optimizing ndcg measure}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2009{\_}0344.pdf},
	year = {2009}
}
@article{Zenker2015,
	author = {Zenker, Frank},
	doi = {10.1007/978-3-319-15021-5},
	file = {:E$\backslash$:/GaerdenforsandZenker2014ACSEditorsintroduction{\_}final{\_}20141130.pdf:pdf},
	isbn = {9783319150215},
	number = {February 2017},
	title = {{Editors ' Introduction : Conceptual Spaces at Work Applications of Conceptual Spaces : The Case for Geometric Knowledge Representation Frank Zenker and Peter G{\"{a}}rdenfors ( Eds .)}},
	year = {2015}
}
@article{Chen2014a,
	abstract = {Abstract Denoising auto - encoders (DAEs) have been successfully used to learn new representations for a wide range of machine learning tasks. During training, DAEs make many passes over the training dataset and reconstruct it from partial corruption generated ... $\backslash$n},
	author = {Chen, M and Weinberger, K and Sha, F and Bengio, Y},
	doi = {10.1007/s11222-007-9033-z},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Marginalized Denoising Auto-encoders for Nonlinear Representations.pdf:pdf},
	isbn = {9781634393973},
	issn = {0960-3174},
	journal = {Proceedings of The 31st {\ldots}},
	keywords = {ICML2014},
	title = {{Marginalized Denoising Auto-encoders for Nonlinear Representations}},
	url = {http://jmlr.org/proceedings/papers/v32/cheng14.pdf{\%}5Cnpapers3://publication/uuid/B8D413D0-9096-41DC-A234-EBFC9C94CBAB},
	volume = {32},
	year = {2014}
}
@article{Bottou2012a,
	abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
	author = {Bottou, Leon},
	doi = {10.1007/978-3-642-35289-8_25},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
	isbn = {978-3-642-35288-1},
	issn = {2045-2322},
	journal = {Neural Networks: Tricks of the Trade},
	number = {1},
	pages = {421--436},
	pmid = {25382349},
	title = {{Stochastic Gradient Descent Tricks}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
	volume = {1},
	year = {2012}
}
@article{Palangi2017,
abstract = {We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In our application of TPRN, internal representations-learned by end-to-end optimization in a deep neural network performing a textual question-answering (QA) task-can be interpreted using basic concepts from linguistic theory. No performance penalty need be paid for this increased interpretability: the proposed model performs comparably to a state-of-the-art system on the SQuAD QA task. The internal representation which is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Fine-grained analysis reveals specific correspondences between the learned roles and parts of speech as assigned by a standard tagger (Toutanova et al. 2003), and finds several discrepancies in the model's favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means to build representations using symbols and roles, with an inductive bias favoring use of these in an approximately discrete manner.},
archivePrefix = {arXiv},
arxivId = {1705.08432},
author = {Palangi, Hamid and Smolensky, Paul and He, Xiaodong and Deng, Li},
eprint = {1705.08432},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palangi et al. - 2017 - Question-Answering with Grammatically-Interpretable Representations.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
pages = {5350--5357},
title = {{Question-answering with grammatically-interpretable representations}},
url = {http://arxiv.org/abs/1705.08432},
year = {2018}
}

@article{Garcez2014,
	abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
	author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
	doi = {10.13140/2.1.1779.4243},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez et al. - 2014 - Neural-Symbolic Learning and Reasoning Contributions and Challenges.pdf:pdf},
	journal = {Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, Stanford},
	number = {November},
	pages = {18--21},
	title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
	year = {2014}
}
@article{Guillame-Bert2010,
	abstract = {Artificial Neural Networks have previously been applied in neuro-symbolic learning to learn ground logic program rules. However, there are few results of learning relations using neuro-symbolic learning. This paper presents the system PAN, which can learn relations. The inputs to PAN are one or more atoms, representing the conditions of a logic rule, and the output is the conclusion of the rule. The symbolic inputs may include functional terms of arbitrary depth and arity, and the output may include terms constructed from the input functors. Symbolic inputs are encoded as an integer using an invertible encoding function, which is used in reverse to extract the output terms. The main advance of this system is a convention to allow construction of Artificial Neural Networks able to learn rules with the same power of expression as first order definite clauses. The system is tested on three examples and the results are discussed.},
	author = {Guillame-Bert, M. and Broda, K. and d'Avila Garcez, a.},
	doi = {10.1109/IJCNN.2010.5596491},
	isbn = {978-1-4244-6916-1},
	issn = {1098-7576},
	journal = {Neural Networks (IJCNN), The 2010 International Joint Conference on},
	pages = {18--23},
	title = {{First-order logic learning in Artificial Neural Networks}},
	year = {2010}
}
@article{Karpathy2015a,
	abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\{}{\%}{\}} vs. 60.9{\{}{\%}{\}}) and the UCF-101 datasets with (88.6{\{}{\%}{\}} vs. 88.0{\{}{\%}{\}}) and without additional optical flow information (82.6{\{}{\%}{\}} vs. 72.8{\{}{\%}{\}}).},
	archivePrefix = {arXiv},
	arxivId = {1503.08909v2},
	author = {Karpathy, a. and Fei-Fei, Li},
	doi = {10.1109/CVPR.2015.7298932},
	eprint = {1503.08909v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Image Des.pdf:pdf},
	isbn = {9781467369640},
	issn = {10636919},
	journal = {Cvpr2015},
	mendeley-groups = {Progress Report},
	title = {{Deep Visual-Semantic Alignments for Generating Image Des}},
	year = {2015}
}
@article{Lake2015,
	abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	archivePrefix = {arXiv},
	arxivId = {10.1126/science.aab3050},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tnenbaum, Joshua B.},
	doi = {10.1126/science.aab3050},
	eprint = {science.aab3050},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tnenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
	isbn = {0036-8075},
	issn = {0036-8075},
	journal = {Science},
	mendeley-groups = {Progress Report,Interim Review},
	number = {6266},
	pages = {1332--1338},
	pmid = {26659050},
	primaryClass = {10.1126},
	title = {{Human-level concept learning through probabilistic program induction}},
	url = {https://www.sciencemag.org/content/350/6266/1332.full.pdf},
	volume = {350},
	year = {2015}
}
@inproceedings{Chen2014b,
	abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
	archivePrefix = {arXiv},
	arxivId = {1512.00567},
	author = {Chen, Danqi and Manning, Christopher},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	doi = {10.3115/v1/D14-1082},
	eprint = {1512.00567},
	isbn = {9781937284961},
	issn = {9781937284961},
	mendeley-groups = {Report/Features},
	pages = {740--750},
	pmid = {8190083},
	title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
	url = {http://aclweb.org/anthology/D14-1082},
	year = {2014}
}
@article{Grosenick2008,
	author = {Grosenick, Logan and Greer, Stephanie and Knutson, Brian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grosenick, Greer, Knutson - 2008 - Interpretable classi ers for FMRI improve prediction of purchases.pdf:pdf},
	journal = {Analysis},
	mendeley-groups = {Report},
	number = {Xx},
	pages = {1--10},
	title = {{Interpretable classi ers for FMRI improve prediction of purchases}},
	volume = {X},
	year = {2008}
}
@article{Y.2015a,
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1312.6184v5},
	author = {Y., Lecun and Y., Bengio and G., Hinton},
	doi = {10.1038/nature14539},
	eprint = {arXiv:1312.6184v5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Y., Y., G. - 2015 - Deep learning.pdf:pdf},
	isbn = {3135786504},
	issn = {0028-0836},
	journal = {Nature},
	number = {7553},
	pages = {436--444},
	pmid = {26017442},
	title = {{Deep learning}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930630277{\&}partnerID=40{\&}md5=befeefa64ddca265c713cf81f4e2fc54},
	volume = {521},
	year = {2015}
}
@article{Reading2008,
	author = {Reading, Additional},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reading - 2008 - Reproduced with permission of the copyright owner . Further reproduction prohibited without permission .pdf:pdf},
	journal = {Pediatric Infectious Disease},
	number = {3},
	title = {{Reproduced with permission of the copyright owner . Further reproduction prohibited without permission .}},
	volume = {34},
	year = {2008}
}
@article{Dong2017,
	abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
	archivePrefix = {arXiv},
	arxivId = {1703.04096},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
	doi = {10.1109/CVPR.2017.110},
	eprint = {1703.04096},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2017 - Improving Interpretability of Deep Neural Networks with Semantic Information(2).pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {1063-6919},
	mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
	pages = {4306--4314},
	title = {{Improving Interpretability of Deep Neural Networks with Semantic Information}},
	url = {http://arxiv.org/abs/1703.04096},
	year = {2017}
}
@article{Freitas2010,
	author = {Freitas, AA and Wieser, DC and Apweiler, R},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas, Wieser, Apweiler - 2010 - On the importance of comprehensible classification models for protein function prediction.pdf:pdf},
	journal = {IEEE/ACM Transactions on},
	mendeley-groups = {Annotated/Applications/Scientific Discovery,Report/Biologicla domain},
	number = {1},
	pages = {172--182},
	title = {{On the importance of comprehensible classification models for protein function prediction}},
	url = {http://dl.acm.org/citation.cfm?id=1719290},
	volume = {7},
	year = {2010}
}
@article{Zhu2015,
	abstract = {The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we pro-pose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hier-archies, e.g., language or image parse structures. We leverage the models for semantic composi-tion to understand the meaning of text, a funda-mental problem in natural language understand-ing, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that with-out considering the structures.},
	author = {Zhu, Xiaodan and Sobhani, Parinaz and Guo, Hongyu},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Sobhani, Guo - 2015 - Long Short-Term Memory Over Recursive Structures.pdf:pdf},
	isbn = {9781510810587},
	journal = {International Conference on Machine Learning (ICML)},
	mendeley-groups = {!Paper 3/Interpretable LSTMs,!Paper 3/task/Sentiment treebank},
	pages = {1604--1612},
	title = {{Long Short-Term Memory Over Recursive Structures}},
	volume = {37},
	year = {2015}
}
@article{Maire1999a,
	abstract = {The core problem of rule-extraction from feed-forward networks is an inversion problem. In this article, we solve this inversion problem by backpropagating unions of polyhedra. We obtain as a by-product a new rule-extraction technique for which the fidelity of the extracted rules can be made arbitrarily high.},
	author = {Maire, F.},
	doi = {10.1016/S0893-6080(99)00013-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maire - 1999 - Rule-extraction by backpropagation of polyhedra.pdf:pdf},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Artificial neural network,Inversion,Polyhedron,Rule-extraction},
	number = {4-5},
	pages = {717--725},
	pmid = {12662679},
	title = {{Rule-extraction by backpropagation of polyhedra}},
	volume = {12},
	year = {1999}
}
@article{Chuang2012,
	abstract = {Topic models aid analysis of text corpora by identifying la- tent topics based on co-occurring words. Real-world de- ployments of topic models, however, often require intensive expert verification and model refinement. In this paper we present Termite, a visual analysis tool for assessing topic model quality. Termite uses a tabular layout to promote comparison of terms both within and across latent topics. We contribute a novel saliency measure for selecting relevant terms and a seriation algorithm that both reveals clustering structure and promotes the legibility of related terms. In a series of examples, we demonstrate how Termite allows analysts to identify coherent and significant themes.},
	author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
	doi = {10.1145/2254556.2254572},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chuang, Manning, Heer - 2012 - Termite Visualization Techniques for Assessing Textual Topic Models.pdf:pdf},
	isbn = {9781450312875},
	journal = {Proceedings of the International Working Conference on Advanced Visual Interfaces - AVI '12},
	keywords = {seriation,text visualization,topic models},
	pages = {74},
	title = {{Termite : Visualization Techniques for Assessing Textual Topic Models}},
	url = {http://dl.acm.org/citation.cfm?doid=2254556.2254572},
	year = {2012}
}
@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
	title = {{No Title}}
}
@article{Srivastava2014a,
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller scikit-learns. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
	archivePrefix = {arXiv},
	arxivId = {1102.4807},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	doi = {10.1214/12-AOS1000},
	eprint = {1102.4807},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
	isbn = {1532-4435},
	issn = {15337928},
	journal = {Journal of Machine Learning Research (JMLR)},
	keywords = {deep learning,model combination,neural networks,regularization},
	pages = {1929--1958},
	title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
	volume = {15},
	year = {2014}
}
@article{Li2015,
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
	archivePrefix = {arXiv},
	arxivId = {1506.01066},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	doi = {10.18653/v1/N16-1082},
	eprint = {1506.01066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Visualizing and Understanding Neural Models in NLP.pdf:pdf},
	isbn = {9781941643914},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Visualizing and Understanding Neural Models in NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	year = {2015}
}
@article{Liu2017,
	abstract = {In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.},
	archivePrefix = {arXiv},
	arxivId = {1705.09207},
	author = {Liu, Yang and Lapata, Mirella},
	eprint = {1705.09207},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lapata - 2017 - Learning Structured Text Representations(2).pdf:pdf},
	mendeley-groups = {!Paper 3/Structured LSTMs},
	title = {{Learning Structured Text Representations}},
	url = {http://arxiv.org/abs/1705.09207},
	year = {2017}
}
@misc{Hruschka2006a,
	abstract = {Multilayer perceptrons adjust their internal parameters performing vector mappings from the input to the output space. Although they may achieve high classification accuracy, the knowledge acquired by such neural networks is usually incomprehensible for humans. This fact is a major obstacle in data mining applications, in which ultimately understandable patterns (like classification rules) are very important. Therefore, many algorithms for rule extraction from neural networks have been developed. This work presents a method to extract rules from multilayer perceptrons trained in classification problems. The rule extraction algorithm basically consists of two steps. First, a clustering genetic algorithm is applied to find clusters of hidden unit activation values. Then, classification rules describing these clusters, in relation to the inputs, are generated. The proposed approach is experimentally evaluated in four datasets that are benchmarks for data mining applications and in a real-world meteorological dataset, leading to interesting results. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	author = {Hruschka, Eduardo R. and Ebecken, Nelson F F},
	booktitle = {Neurocomputing},
	doi = {10.1016/j.neucom.2005.12.127},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hruschka, Ebecken - 2006 - Extracting rules from multilayer perceptrons in classification problems A clustering-based approach.htm:htm},
	issn = {09252312},
	keywords = {Clustering,Genetic algorithms,Rule extraction from neural networks},
	number = {1-3},
	pages = {384--397},
	title = {{Extracting rules from multilayer perceptrons in classification problems: A clustering-based approach}},
	volume = {70},
	year = {2006}
}
@article{Srivastava2013a,
	abstract = {We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.},
	archivePrefix = {arXiv},
	arxivId = {abs/1309.6865},
	author = {Srivastava, Nitish and Salakhutdinov, Rr and Hinton, Ge},
	eprint = {1309.6865},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava, Salakhutdinov, Hinton - 2013 - Modeling Documents with Deep Boltzmann Machines.pdf:pdf},
	journal = {arXiv preprint arXiv: {\ldots}},
	mendeley-groups = {!Paper 3/task/newsgroups},
	primaryClass = {abs},
	title = {{Modeling Documents with Deep Boltzmann Machines}},
	url = {http://arxiv.org/abs/1309.6865{\%}5Cnhttp://www.arxiv.org/pdf/1309.6865.pdf},
	year = {2013}
}
@article{Semeniuta2016,
	abstract = {This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to $\backslash$textit{\{}forward{\}} connections of feed-forward architectures or RNNs, we propose to drop neurons directly in $\backslash$textit{\{}recurrent{\}} connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for Long Short-Term Memory network, the most popular type of RNN cells. Our experiments on NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.},
	archivePrefix = {arXiv},
	arxivId = {1603.05118},
	author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
	eprint = {1603.05118},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Semeniuta, Severyn, Barth - 2016 - Recurrent Dropout without Memory Loss.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Recurrent Dropout without Memory Loss}},
	url = {http://arxiv.org/abs/1603.05118},
	year = {2016}
}
@article{Mcauley2013,
	author = {Mcauley, Julian},
	isbn = {9781450324090},
	journal = {RecSys '13 Proceedings of the 7th ACM conference on Recommender systems},
	keywords = {recommender systems,topic models},
	mendeley-groups = {Annotated/Datasets},
	pages = {165--172},
	title = {{Hidden Factors and Hidden Topics : Understanding Rating Dimensions with Review Text}},
	year = {2013}
}
@article{Hechtlinger2016,
	abstract = {State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a "black box". In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing.},
	archivePrefix = {arXiv},
	arxivId = {1611.07634},
	author = {Hechtlinger, Yotam},
	eprint = {1611.07634},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hechtlinger - 2016 - Interpretation of Prediction Models Using the Input Gradient.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	number = {Nips},
	title = {{Interpretation of Prediction Models Using the Input Gradient}},
	url = {http://arxiv.org/abs/1611.07634},
	year = {2016}
}
@article{Chatfield2014,
	abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. A particularly significant one is data augmentation, which achieves a boost in performance in shallow methods analogous to that observed with CNN-based methods. Finally, we are planning to provide the configurations and code that achieve the state-of-the-art performance on the PASCAL VOC Classification challenge, along with alternative configurations trading-off performance, computation speed and compactness.},
	archivePrefix = {arXiv},
	arxivId = {1405.3531},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	doi = {10.5244/C.28.6},
	eprint = {1405.3531},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatfield et al. - 2014 - Return of the Devil in the Details Delving Deep into Convolutional Nets.pdf:pdf},
	isbn = {1-901725-52-9},
	issn = {1-901725-52-9},
	journal = {arXiv preprint arXiv: {\ldots}},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1--11},
	title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
	url = {http://arxiv.org/abs/1405.3531},
	year = {2014}
}

@book{Bird,
	title={Natural language processing with Python: analyzing text with the natural language toolkit},
	author={Bird, Steven and Klein, Ewan and Loper, Edward},
	year={2009},
	publisher={" O'Reilly Media, Inc."}
}
@article{Ticklea,
	author = {Tickle, Alan B and Andrews, Robert and Golea, Mostefa and Diederich, Joachim},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tickle et al. - Unknown - The truth is in there directions and challenges in extracting rules from trained artificial neural networks.pdf:pdf},
	journal = {Neural Networks},
	mendeley-groups = {Progress Report},
	title = {{The truth is in there : directions and challenges in extracting rules from trained artificial neural networks}}
}
@book{Bishop2006,
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {Bishop, Christopher M CM Christopher M.},
	booktitle = {Pattern Recognition},
	chapter = {Graphical},
	doi = {10.1117/1.2819119},
	editor = {Jordan, M and Kleinberg, J and Sch{\"{o}}lkopf, B},
	eprint = {0-387-31073-8},
	isbn = {978-0387310732},
	issn = {10179909},
	number = {4},
	pages = {738},
	pmid = {8943268},
	publisher = {Springer},
	series = {Information science and statistics},
	title = {{Pattern Recognition and Machine Learning}},
	url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf{\%}5Cnhttp://soic.iupui.edu/syllabi/semesters/4142/INFO{\_}B529{\_}Liu{\_}s.pdf{\%}5Cnhttp://www.library.wisc.edu/selectedtocs/bg0137.pdf},
	volume = {4},
	year = {2006}
}
@article{Ba2016,
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	archivePrefix = {arXiv},
	arxivId = {1607.06450},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	eprint = {1607.06450},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf:pdf},
	isbn = {978-3-642-04273-7},
	issn = {1607.06450},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Layer Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	year = {2016}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Ganesha.docx:docx},
	title = {{Ganesha}}
}
@article{Guidotti2018,
	archivePrefix = {arXiv},
	arxivId = {1802.01933},
	author = {Guidotti, Riccardo and Monreale, Anna and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	eprint = {1802.01933},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guidotti et al. - 2018 - A Survey Of Methods For Explaining Black Box Models.pdf:pdf},
	keywords = {KDD Lab,survey},
	mendeley-groups = {!Paper 3},
	pages = {1--45},
	title = {{A Survey Of Methods For Explaining Black Box Models}},
	year = {2018}
}
@article{Tian2011,
	abstract = {We use the term "index predictor" to denote a score that consists of K binary rules such as "age {\textgreater} 60" or "blood pressure {\textgreater} 120 mm Hg." The index predictor is the sum of these binary scores, yielding a value from 0 to K. Such indices as often used in clinical studies to stratify population risk: They are usually derived from subject area considerations. In this paper, we propose a fast data-driven procedure for automatically constructing such indices for linear, logistic, and Cox regression models. We also extend the procedure to create indices for detecting treatment-marker interactions. The methods are illustrated on a study with protein biomarkers as well as a large microarray gene expression study.},
	author = {Tian, Lu and Tibshirani, Robert},
	doi = {10.1093/biostatistics/kxq047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Tibshirani - 2011 - Adaptive index models for marker-based risk stratification.pdf:pdf},
	isbn = {1468-4357 (Electronic)$\backslash$r1465-4644 (Linking)},
	issn = {14654644},
	journal = {Biostatistics},
	keywords = {Degree of freedom,Index predictor,International prognostic index},
	mendeley-groups = {Annotated/Applications/Medical},
	number = {1},
	pages = {68--86},
	pmid = {20663850},
	title = {{Adaptive index models for marker-based risk stratification}},
	volume = {12},
	year = {2011}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - vpetite.txt:txt},
	title = {vpetite}
}
@article{Tickle,
	author = {Tickle, Alan B and Diederich, Joachim},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tickle, Diederich - Unknown - From Trained Artificial Neural Networks.pdf:pdf},
	mendeley-groups = {Progress Report},
	title = {{From Trained Artificial Neural Networks}}
}
@article{Kaikhah,
	author = {Kaikhah, Khosrow and Doddameti, Sandesh},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaikhah, Doddameti - Unknown - Discovering Trends in Large Datasets Using Neural Networks.pdf:pdf},
	mendeley-groups = {Papers/Paper 1,Report},
	pages = {1--23},
	title = {{Discovering Trends in Large Datasets Using Neural Networks}}
}
@article{Gladkova2016,
	abstract = {This paper presents an analysis of exist- ing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evalua- tions is “interpretability” of word embed- dings: a “good” embedding produces re- sults that make sense in terms of tradi- tional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of dis- tributional meaning representations. We argue for a shift from abstract ratings of word embedding “quality” to exploration of their strengths and weaknesses},
	author = {Gladkova, Anna and Drozd, Aleksandr},
	doi = {10.18653/v1/W16-2507},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gladkova, Drozd - 2016 - Intrinsic Evaluations of Word Embeddings What Can We Do Better.pdf:pdf},
	isbn = {9781945626142},
	journal = {Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP},
	mendeley-groups = {Report/Features},
	number = {August},
	pages = {36--42},
	title = {{Intrinsic Evaluations of Word Embeddings: What Can We Do Better?}},
	year = {2016}
}
@article{Zhu2012,
	abstract = {A supervised topic model can use side information such as ratings or labels associated with doc- uments or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective func- tions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet alloca- tion (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) un- der a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or re- gression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, espe- cially for classification.},
	archivePrefix = {arXiv},
	arxivId = {0912.5507},
	author = {Zhu, Jun and Ahmed, a and Xing, Ep},
	doi = {10.1145/1553374.1553535},
	eprint = {0912.5507},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Ahmed, Xing - 2012 - MedLDA Maximum Margin Supervised Topic Models.pdf:pdf},
	isbn = {978-1-60558-516-1},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {latent dirichlet allocation,max-margin learning,maximum entropy discrimination,supervised topic models,support vector machines},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,!Paper 3/task/newsgroups},
	pages = {2237--2278},
	title = {{MedLDA: Maximum Margin Supervised Topic Models}},
	url = {http://jmlr.csail.mit.edu/papers/volume13/zhu12a/zhu12a.pdf},
	volume = {13},
	year = {2012}
}
@article{Dai2015,
	abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
	author = {Dai, Andrew M and Le, Quoc V},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
	mendeley-groups = {!Paper 3/task/Large Movie Review,!Paper 3/task/Sentiment treebank,!Paper 3/task/newsgroups},
	pages = {1--10},
	pmid = {414454},
	title = {{Semi-supervised Sequence Learning}},
	url = {http://arxiv.org/abs/1511.01432},
	year = {2015}
}
@article{Mikolov2013,
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	archivePrefix = {arXiv},
	arxivId = {1310.4546},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	doi = {10.1162/jmlr.2003.3.4-5.951},
	eprint = {1310.4546},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
	isbn = {2150-8097},
	issn = {10495258},
	journal = {Nips},
	mendeley-groups = {Annotated/Word Vectors,Interim Review},
	pages = {1--9},
	pmid = {903},
	title = {{Distributed Representations of Words and Phrases and their Compositionality}},
	year = {2013}
}
@article{Marchant2009,
	abstract = {The biomisation method is used to reconstruct Latin American vegetation at 6000±500 and 18 000±1000 radiocarbon years before present ({\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP) from pollen data. Tests using modern pollen data from 381 samples derived from 287 locations broadly reproduce potential natural vegetation. The strong temperature gradient associated with the Andes is recorded by a transition from high altitude cool grass/shrubland and cool mixed forest to mid-altitude cool temperate rain forest, to tropical dry, seasonal and rain forest at low altitudes. Reconstructed biomes from a number of sites do not match the potential vegetation due to local factors such as human impact, methodological artefacts and mechanisms of pollen representivity of the parent vegetation. At 6000±500 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP 255 samples are analysed from 127 sites. Differences between the modern and the 6000±500 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP reconstruction are comparatively small; change relative to the modern reconstruction are mainly to biomes characteristic of drier climate in the north of the region with a slight more mesic shift in the south. Cool temperate rain forest remains dominant in western South America. In northwestern South America a number of sites record transitions from tropical seasonal forest to tropical dry forest and tropical rain forest to tropical seasonal forest. Sites in Central America show a change in biome assignment, but to more mesic vegetation, indicative of greater plant available moisture, e.g. on the Yucat{\`{a}}n peninsula sites record warm evergreen forest, replacing tropical dry forest and warm mixed forest presently recorded. At 18 000±1000 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP 61 samples from 34 sites record vegetation reflecting a generally cool and dry environment. Cool grass/shrubland is prevalent in southeast Brazil whereas Amazonian sites record tropical dry forest, warm temperate rain forest and tropical seasonal forest. Southernmost South America is dominated by cool grass/shrubland, a single site retains cool temperate rain forest indicating that forest was present at some locations at the LGM. Some sites in Central Mexico and lowland Colombia remain unchanged in the biome assignments of warm mixed forest and tropical dry forest respectively, although the affinities that these sites have to different biomes do change between 18 000±1000 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP and present. The "unresponsive" nature of these sites results from their location and the impact of local edaphic influence. [ABSTRACT FROM AUTHOR] Copyright of Climate of the Past is the property of Copernicus Gesellschaft mbH and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	author = {Marchant, R and Cleef, A and Harrison, S P and Hooghiemstra, H and Markgraf, V and van Boxel, J and Ager, T and Almeida, L and Anderson, R and Baied, C and Behling, H and Berrio, J C and Burbridge, R and Bjorck, S and Byrne, R and Bush, M and Duivenvoorden, J and Flenley, J and {De Oliveira}, P and van Geel, B},
	journal = {Climate of the Past},
	keywords = {BIOTIC communities -- Research,CARBON,CLIMATIC changes -- Research,CLIMATOLOGY -- Research,ISOTOPES,LATIN America,RADIOCARBON dating,VEGETATION {\&} climate},
	number = {4},
	pages = {725--767},
	publisher = {Copernicus Gesellschaft mbH},
	title = {{Pollen-based biome reconstructions for Latin America at 0, 6000 and 18000 radiocarbon years ago}},
	url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=eih{\&}AN=47908619{\&}site=ehost-live},
	volume = {5},
	year = {2009}
}
@article{Diao2014,
	abstract = {Recommendation and review sites offer a wealth of infor-mation beyond ratings. For instance, on IMDb users leave reviews, commenting on different aspects of a movie (e.g. actors, plot, visual effects), and expressing their sentiments (positive or negative) on these aspects in their reviews. This suggests that uncovering aspects and sentiments will allow us to gain a better understanding of users, movies, and the process involved in generating ratings. The ability to answer questions such as " Does this user care more about the plot or about the special effects? " or " What is the quality of the movie in terms of acting? " helps us to understand why certain ratings are generated. This can be used to provide more meaningful recommendations. In this work we propose a probabilistic model based on collaborative filtering and topic modeling. It allows us to capture the interest distribution of users and the content distribution for movies; it provides a link between inter-est and relevance on a per-aspect basis and it allows us to differentiate between positive and negative sentiments on a per-aspect basis. Unlike prior work our approach is entirely unsupervised and does not require knowledge of the aspect specific ratings or genres for inference. We evaluate our model on a live copy crawled from IMDb. Our model offers superior performance by joint modeling. Moreover, we are able to address the cold start problem — by utilizing the information inherent in reviews our model demonstrates improvement for new users and movies.},
	author = {Diao, Qiming and Qiu, Minghui and Wu, Chao-Yuan and Smola, Alexander J. and Jiang, Jing and Wang, Chong},
	doi = {10.1145/2623330.2623758},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Diao et al. - 2014 - Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS).pdf:pdf},
	isbn = {9781450329569},
	journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Yelp},
	pages = {193--202},
	title = {{Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)}},
	url = {http://dl.acm.org/citation.cfm?doid=2623330.2623758},
	year = {2014}
}
@article{Iyyer2015a,
	author = {Iyyer, M and Manjunatha, V},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iyyer, Manjunatha - 2015 - Deep unordered composition rivals syntactic methods for text classification.pdf:pdf},
	journal = {Proceedings of the 53rd  {\ldots}},
	mendeley-groups = {Literature Review},
	title = {{Deep unordered composition rivals syntactic methods for text classification}},
	url = {https://www.cs.colorado.edu/{~}jbg/docs/2015{\_}acl{\_}dan.pdf},
	year = {2015}
}
@article{Schockaert2015a,
	author = {Schockaert, Steven and Lee, Jae Hee},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert, Lee - 2015 - Qualitative reasoning about directions in semantic spaces.pdf:pdf},
	isbn = {9781577357384},
	issn = {10450823},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	pages = {3207--3213},
	title = {{Qualitative reasoning about directions in semantic spaces}},
	volume = {2015-Janua},
	year = {2015}
}
@article{Boz,
	author = {Boz, Olcay and Ave, Laurel},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boz, Ave - Unknown - Decision Tree DecText - Decision Tree Extractor.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees},
	pages = {1--7},
	title = {{Decision Tree DecText - Decision Tree Extractor}}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Bull.jar:jar},
	title = {{Bull}}
}
@article{Barnes2017,
	abstract = {There has been a good amount of progress in sentiment analysis over the past 10 years, including the proposal of new methods and the creation of benchmark datasets. In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task. Accordingly, it is hard to understand how well a certain model generalizes across different tasks and datasets. In this paper, we contribute to this situation by comparing several models on six different benchmarks, which belong to different domains and additionally have different levels of granularity (binary, 3-class, 4-class and 5-class). We show that Bi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are particularly good at fine-grained sentiment tasks (i. e., with more than two classes). Incorporating sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the SenTube datasets.},
	archivePrefix = {arXiv},
	arxivId = {1709.04219},
	author = {Barnes, Jeremy and Klinger, Roman and im Walde, Sabine Schulte},
	eprint = {1709.04219},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnes, Klinger, Walde - 2017 - Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets.pdf:pdf},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {2--12},
	title = {{Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets}},
	url = {http://arxiv.org/abs/1709.04219},
	year = {2017}
}

@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - CM1102 Web Applications Exercise 3 CSS for multi-column layout and drop down menus Objectives of the exercise.pdf:pdf},
	pages = {1--2},
	title = {{CM1102 Web Applications Exercise 3 : CSS for multi-column layout and drop down menus Objectives of the exercise :}}
}
@article{Baehrens2010,
	abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
	archivePrefix = {arXiv},
	arxivId = {0912.1128},
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Muller, Klaus-Robert},
	eprint = {0912.1128},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baehrens et al. - 2010 - How to Explain Individual Classification Decisions.pdf:pdf},
	isbn = {1532-4435},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {Ames mutagenicity,black box model,explaining,kernel methods,nonlinear},
	mendeley-groups = {Report/Explaining predictions},
	pages = {1803--1831},
	title = {{How to Explain Individual Classification Decisions}},
	volume = {11},
	year = {2010}
}
@article{Benitez1997,
	abstract = {Artificial neural networks are efficient computing models which have shown their strengths in solving hard problems in artificial intelligence. They have also been shown to be universal approximators. Notwithstanding, one of the major criticisms is their being black boxes, since no satisfactory explanation of their behavior has been offered. In this paper, we provide such an interpretation of neural networks so that they will no longer be seen as black boxes. This is stated after establishing the equality between a certain class of neural nets and fuzzy rule-based systems. This interpretation is built with fuzzy rules using a new fuzzy logic operator which is defined after introducing the concept of f-duality. In addition, this interpretation offers an automated knowledge acquisition procedure.},
	author = {Ben{\'{i}}tez, J. M. and Castro, J. L. and Requena, I.},
	doi = {10.1109/72.623216},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben{\'{i}}tez, Castro, Requena - 1997 - Are artificial neural networks black boxes.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Equality between neural nets and fuzzy rule-based,Fuzzy additive systems,Interpretation of neural nets,f-duality,i-or operator},
	number = {5},
	pages = {1156--1164},
	pmid = {18255717},
	title = {{Are artificial neural networks black boxes?}},
	volume = {8},
	year = {1997}
}
@article{Towell1993,
	author = {Towell, G and Shavlik, J W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik - 1993 - The Extraction of Refined Rules From Knowledge Based Neural Networks.pdf:pdf},
	journal = {Machine Learning},
	keywords = {integrated learning},
	number = {1},
	pages = {71--101},
	title = {{The Extraction of Refined Rules From Knowledge Based Neural Networks}},
	volume = {13},
	year = {1993}
}

@article{Leike2017,
	abstract = {We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.},
	archivePrefix = {arXiv},
	arxivId = {1711.09883},
	author = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A. and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
	eprint = {1711.09883},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leike et al. - 2017 - AI Safety Gridworlds.pdf:pdf},
	mendeley-groups = {!Paper 3},
	title = {{AI Safety Gridworlds}},
	url = {http://arxiv.org/abs/1711.09883},
	year = {2017}
}
@article{Hornik1993a,
	abstract = {We show that standard feedforward networks with as few as a single hidden layer can uniformly approximate continuous functions on compacta provided that the activation function $\psi$ is locally Riemann integrable and nonpolynomial, and have universal Lp ($\mu$) approximation capabilities for finite and compactly supported input environment measures $\mu$ provided that $\psi$ is locally bounded and nonpolynomial. In both cases, the input-to-hidden scikit-learns and hidden layer biases can be constrained to arbitrarily small sets; if in addition $\psi$ is locally analytic a single universal bias will do.},
	author = {Hornik, Kurt},
	doi = {10.1016/S0893-6080(09)80018-X},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik - 1993 - Some new results on neural network approximation.pdf:pdf},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {-universal approximation capabilit ies,feedfor wa rd networks,small scikit-learn sets,universal bias},
	number = {8},
	pages = {1069--1072},
	publisher = {Pergamon Press Ltd.},
	title = {{Some new results on neural network approximation}},
	url = {http://www.sciencedirect.com/science/article/pii/S089360800980018X},
	volume = {6},
	year = {1993}
}
@article{Craven1994,
	abstract = {Concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to...},
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1994 - Using sampling and queries to extract rules from trained neural networks.pdf:pdf},
	journal = {Proc.$\backslash$ Intl.$\backslash$ Conf.$\backslash$ Machine Learning},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {37--45},
	title = {{Using sampling and queries to extract rules from trained neural networks}},
	year = {1994}
}
@article{Zhu2016,
	abstract = {Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference.},
	archivePrefix = {arXiv},
	arxivId = {1602.07428},
	author = {Zhu, Jun and Song, Jiaming and Chen, Bei},
	doi = {10.1.1.160.2072},
	eprint = {1602.07428},
	file = {:E$\backslash$:/Downloads/Work/1602.07428.pdf:pdf},
	isbn = {978-1-4503-1285-1},
	issn = {13669516},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	number = {1},
	title = {{Max-Margin Nonparametric Latent Feature Models for Link Prediction}},
	url = {http://arxiv.org/abs/1602.07428},
	volume = {6},
	year = {2016}
}
@article{Sourek2015a,
	abstract = {We propose a method combining relational-logic representations with deep neural network learning. Domain-specific knowledge is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structure of given training or testing examples. Different networks corresponding to different examples share their scikit-learns, which co-evolve during training by stochastic gradient descend algorithm. Notable relational concepts can be discovered by interpreting shared hidden layer scikit-learns corresponding to the rules. Experiments on 78 relational learning benchmarks demonstrate the favorable performance of the method.},
	archivePrefix = {arXiv},
	arxivId = {1508.05128},
	author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
	eprint = {1508.05128},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sourek et al. - 2015 - Lifted Relational Neural Networks.pdf:pdf},
	journal = {CoRR},
	keywords = {lifted models,neural networks,relational learning},
	pages = {1--21},
	title = {{Lifted Relational Neural Networks}},
	url = {http://arxiv.org/abs/1508.05128},
	volume = {abs/1508.0},
	year = {2015}
}
@article{Glorot2011,
	author = {Glorot, X and Bordes, a and Bengio, Y},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain adaptation for large-scale sentiment classification A deep learning approach(2).pdf:pdf},
	keywords = {auto-encoders,deep-learning},
	mendeley-groups = {Literature Review,Progress Report},
	number = {1},
	title = {{Domain adaptation for large-scale sentiment classification: A deep learning approach}},
	url = {http://eprints.pascal-network.org/archive/00008597/},
	year = {2011}
}
@article{Gal2015,
	abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
	archivePrefix = {arXiv},
	arxivId = {1512.05287},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	doi = {10.1201/9781420049176},
	eprint = {1512.05287},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal, Ghahramani - 2015 - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:pdf},
	isbn = {9789537619084},
	issn = {0302-9743},
	mendeley-groups = {!Paper 3/Training LSTMs,!Paper 3/task/Sentiment treebank},
	number = {Nips},
	pmid = {21803542},
	title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1512.05287},
	year = {2015}
}
@article{WenminLi,
	abstract = {Previous studies propose that associative classification has high$\backslash$nclassification accuracy and strong flexibility at handling unstructured$\backslash$ndata. However, it still suffers from the huge set of mined rules and$\backslash$nsometimes biased classification or overfitting since the classification$\backslash$nis based on only a single high-confidence rule. The authors propose a$\backslash$nnew associative classification method, CMAR, i.e., Classification based$\backslash$non Multiple Association Rules. The method extends an efficient frequent$\backslash$npattern mining method, FP-growth, constructs a class$\backslash$ndistribution-associated FP-tree, and mines large databases efficiently.$\backslash$nMoreover, it applies a CR-tree structure to store and retrieve mined$\backslash$nassociation rules efficiently, and prunes rules effectively based on$\backslash$nconfidence, correlation and database coverage. The classification is$\backslash$nperformed based on a scikit-learned {\&}chi;2 analysis using multiple$\backslash$nstrong association rules. Our extensive experiments on 26 databases from$\backslash$nthe UCI machine learning database repository show that CMAR is$\backslash$nconsistent, highly effective at classification of various kinds of$\backslash$ndatabases and has better average classification accuracy in comparison$\backslash$nwith CBA and C4.5. Moreover, our performance study shows that the method$\backslash$nis highly efficient and scalable in comparison with other reported$\backslash$nassociative classification methods},
	author = {{Wenmin Li} and {Jiawei Han} and {Jian Pei}},
	doi = {10.1109/ICDM.2001.989541},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wenmin Li, Jiawei Han, Jian Pei - Unknown - CMAR accurate and efficient classification based on multiple class-association rules.pdf:pdf},
	isbn = {0-7695-1119-8},
	issn = {15504786},
	journal = {Proceedings 2001 IEEE International Conference on Data Mining},
	mendeley-groups = {Report},
	pages = {369--376},
	title = {{CMAR: accurate and efficient classification based on multiple class-association rules}},
	url = {http://ieeexplore.ieee.org/document/989541/}
}
@article{Craven1993,
	author = {{M.W. Craven}, J.W. Shavlik},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/M.W. Craven - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
	mendeley-groups = {Papers/Paper 1,Report},
	number = {4},
	pages = {434--441},
	title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
	volume = {41},
	year = {1993}
}
@article{Pazzani2000,
	author = {Pazzani, Michael J},
	doi = {10.1109/5254.850821},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pazzani - 2000 - Knowledge discovery from data.pdf:pdf},
	issn = {1094-7167},
	journal = {Intelligent systems and their applications, IEEE},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {2},
	pages = {10--12},
	title = {{Knowledge discovery from data?}},
	volume = {15},
	year = {2000}
}
@article{Lakkaraju2016,
	author = {Lakkaraju, Himabindu and Bach, Stephen and Leskovec, Jure},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju, Bach, Leskovec - 2016 - Interpretable Decision Sets A Joint Framework for Description and Prediction.pdf:pdf},
	journal = {The 22th {\{}ACM{\}} {\{}SIGKDD{\}} International Conference on Knowledge Discovery and Data Mining, {\{}KDD{\}} '16, San Fransisco, CA, USA, August, 2016 International Conference on Knowledge Discovery and Data Mining, {\{}{\{}{\}}KDD{\{}{\}}{\}} '16, San Fransisco, CA, USA, August, 2016},
	title = {{Interpretable Decision Sets: A Joint Framework for Description and Prediction}},
	volume = {1},
	year = {2016}
}
@article{Schnabel2015,
	abstract = {We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations ofwords from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.},
	author = {Schnabel, Tobias and Labutov, Igor and Mimno, David and Joachims, Thorsten},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schnabel et al. - 2015 - Evaluation methods for unsupervised word embeddings.pdf:pdf},
	isbn = {9781941643327},
	journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	keywords = {Distributional semantics,Evaluation techniques},
	mendeley-groups = {Report/Features},
	number = {September},
	pages = {298--307},
	pmid = {1847047},
	title = {{Evaluation methods for unsupervised word embeddings}},
	year = {2015}
}
@article{Laine2016,
	abstract = {In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63{\%} to 12.89{\%} in CIFAR-10 with 4000 labels and from 18.44{\%} to 6.83{\%} in SVHN with 500 labels.},
	archivePrefix = {arXiv},
	arxivId = {1610.02242},
	author = {Laine, Samuli and Aila, Timo},
	eprint = {1610.02242},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laine, Aila - 2016 - Temporal Ensembling for Semi-Supervised Learning.pdf:pdf},
	mendeley-groups = {Progress Report},
	number = {November 2016},
	pages = {1--9},
	title = {{Temporal Ensembling for Semi-Supervised Learning}},
	url = {http://arxiv.org/abs/1610.02242},
	year = {2016}
}
@article{Szafron2004,
	abstract = {Proteome Analyst (PA) (http://www.cs.ualberta.ca/{\~{}}bioinfo/PA/) is a publicly available, high-throughput, web-based system for predicting various properties of each protein in an entire proteome. Using machine-learned classifiers, PA can predict, for example, the GeneQuiz general function and Gene Ontology (GO) molecular function of a protein. In addition, PA is currently the most accurate and most comprehensive system for predicting subcellular localization, the location within a cell where a protein performs its main function. Two other capabilities of PA are notable. First, PA can create a custom classifier to predict a new property, without requiring any programming, based on labeled training data (i.e. a set of examples, each with the correct classification label) provided by a user. PA has been used to create custom classifiers for potassium-ion channel proteins and other general function ontologies. Second, PA provides a sophisticated explanation feature that shows why one prediction is chosen over another. The PA system produces a Na{\"{i}}ve Bayes classifier, which is amenable to a graphical and interactive approach to explanations for its predictions; transparent predictions increase the user's confidence in, and understanding of, PA.},
	author = {Szafron, Duane and Lu, Paul and Greiner, Russell and Wishart, David S. and Poulin, Brett and Eisner, Roman and Lu, Zhiyong and Anvik, John and Macdonell, Cam and Fyshe, Alona and Meeuwis, David},
	doi = {10.1093/nar/gkh485},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szafron et al. - 2004 - Proteome Analyst Custom predictions with explanations in a web-based tool for high-throughput proteome annotatio.pdf:pdf},
	isbn = {1362-4962 (Electronic)},
	issn = {03051048},
	journal = {Nucleic Acids Research},
	mendeley-groups = {Report/Biologicla domain},
	number = {WEB SERVER ISS.},
	pages = {365--371},
	pmid = {15215412},
	title = {{Proteome Analyst: Custom predictions with explanations in a web-based tool for high-throughput proteome annotations}},
	volume = {32},
	year = {2004}
}
@article{Bhatia2015,
	abstract = {Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that rescikit-learning discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classification-based methods.},
	archivePrefix = {arXiv},
	arxivId = {1509.01599},
	author = {Bhatia, Parminder and Ji, Yangfeng and Eisenstein, Jacob},
	eprint = {1509.01599},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatia, Ji, Eisenstein - 2015 - Better Document-level Sentiment Analysis from RST Discourse Parsing.pdf:pdf},
	isbn = {9781941643327},
	mendeley-groups = {!Paper 3/Structured LSTMs,!Paper 3/task/Sentiment treebank},
	title = {{Better Document-level Sentiment Analysis from RST Discourse Parsing}},
	url = {http://arxiv.org/abs/1509.01599},
	year = {2015}
}
@article{Yin2017,
	abstract = {Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
	archivePrefix = {arXiv},
	arxivId = {1702.01923},
	author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"{u}}tze, Hinrich},
	eprint = {1702.01923},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin et al. - 2017 - Comparative Study of CNN and RNN for Natural Language Processing.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Comparative Study of CNN and RNN for Natural Language Processing}},
	url = {http://arxiv.org/abs/1702.01923},
	year = {2017}
}
@article{Collobert2000,
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling, achieving or exceeding state-of-the-art performance in each on four benchmark tasks. Our goal was to design a flexible architecture that can learn representations useful for the tasks, thus avoiding excessive task-specific feature engineering (and therefore disregarding a lot of prior knowledge). Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabelled training data. This work is then used as a basis for building a freely available tagging system with excellent performance while requiring minimal computational resources.},
	archivePrefix = {arXiv},
	arxivId = {1103.0398},
	author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	doi = {10.1145/2347736.2347755},
	eprint = {1103.0398},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2000 - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
	isbn = {1532-4435},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {Deep Learning,Natural Language Processing,Neural Networks},
	pages = {1--48},
	title = {{Natural Language Processing (almost) from Scratch}},
	volume = {1},
	year = {2000}
}
@article{Fong2017,
	abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, e.g. problems in health, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we introduce a paradigm that learns the minimally salient part of an image by directly editing it and learning from the corresponding changes to its output. Unlike previous works, our method is model-agnostic and testable because it is grounded in replicable image perturbations.},
	archivePrefix = {arXiv},
	arxivId = {1704.03296},
	author = {Fong, Ruth and Vedaldi, Andrea},
	eprint = {1704.03296},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong, Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meaningful Perturbation.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Report},
	title = {{Interpretable Explanations of Black Boxes by Meaningful Perturbation}},
	url = {http://arxiv.org/abs/1704.03296},
	year = {2017}
}
@article{Guo2014,
	abstract = {Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presented approaches can be integrated into most of the classical linear models in NLP. Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features, among which the distributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score},
	author = {Guo, Jiang and Che, Wanxiang and Wang, Haifeng and Liu, Ting},
	isbn = {9781937284961},
	journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	mendeley-groups = {Report/Features},
	number = {2005},
	pages = {110--120},
	pmid = {1685741},
	title = {{Revisiting Embedding Features for Simple Semi-supervised Learning}},
	url = {http://www.aclweb.org/anthology/D14-1012.pdf},
	year = {2014}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The-Ten-Principal-Upanishads.pdf.pdf:pdf},
	title = {{The-Ten-Principal-Upanishads.pdf}}
}
@article{Wu2013,
	abstract = {Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker's training data for synthesizing the converted speech. Both objective and subjective evaluations indicate that our proposed method outperforms JD-GMM and conventional unit selection methods. Copyright {\textcopyright} 2013 ISCA.},
	archivePrefix = {arXiv},
	arxivId = {1502.01710},
	author = {Wu, Zhizheng and Virtanen, Tuomas and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou},
	doi = {10.1063/1.4906785},
	eprint = {1502.01710},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2013 - character-level-convolutional-networks-for-text-classification.pdf:pdf},
	isbn = {0123456789},
	issn = {19909772},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	keywords = {Multi-frame exemplar,Temporal information,Unit selection,Voice conversion},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Yelp},
	pages = {3057--3061},
	pmid = {25246403},
	title = {character-level-convolutional-networks-for-text-classification},
	year = {2013}
}
@article{Jiang2003,
	abstract = {A new approach of constructing andtraining neural networks for pattern classi{\$}cation is proposed. Data clusters are generated andtrainedsequentially basedon distinct local subsets of the training data. Obtainedclusters are then usedto construct a feed-forwardnetwork, which is further trainedusing standardalgorithms operating on the global training set. The network obtained using this approach e6ectively inherits the knowledge from the local training procedure before improving on its generalization ability through the subsequent global training. Various experiments demonstrate the superiority of this approach over competing methods.},
	author = {Jiang, Xudong and {Kam Sie Wah}, Alvin Harvey},
	doi = {10.1016/S0031-3203(02)00087-0},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Kam Sie Wah - 2003 - Constructing and training feed-forward neural networks for pattern classification.pdf:pdf},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Classification,Clustering,Generalization,Local andglobal training,Neural networks},
	pages = {853--867},
	title = {{Constructing and training feed-forward neural networks for pattern classification}},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320302000870},
	volume = {36},
	year = {2003}
}
@article{Bharti2015,
	author = {Bharti, Kusum Kumari and Singh, Pramod Kumar},
	doi = {10.1016/j.eswa.2014.11.038},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bharti, Singh - 2015 - Expert Systems with Applications Hybrid dimension reduction by integrating feature selection with feature extract.pdf:pdf},
	issn = {0957-4174},
	journal = {Expert Systems With Applications},
	mendeley-groups = {Report/Features},
	number = {6},
	pages = {3105--3114},
	publisher = {Elsevier Ltd},
	title = {{Expert Systems with Applications Hybrid dimension reduction by integrating feature selection with feature extraction method for text clustering}},
	url = {http://dx.doi.org/10.1016/j.eswa.2014.11.038},
	volume = {42},
	year = {2015}
}
@misc{Cambria2013a,
	abstract = {The way people express their opinions has radically changed in the past few years thanks to the advent of online collaborative media. The distillation of knowledge from this huge amount of unstructured information can be a key factor for marketers who want to create an identity for their product or brand in the minds of their customers. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. Existing approaches to opinion mining, in fact, are still far from being able to infer the cognitive and affective information associated with natural language as they mainly rely on knowledge bases that are too limited to efficiently process text at concept-level. In this context, standard clustering techniques have been previously employed on an affective common-sense knowledge base in attempt to discover how different natural language concepts are semantically and affectively related to each other and, hence, to accordingly mine on-line opinions. In this work, a novel cognitive model based on the combined use of multi-dimensional scaling and artificial neural networks is exploited for better modelling the way multi-word expressions are organised in a brain-like universe of natural language concepts. The integration of a biologically inspired paradigm with standard principal component analysis helps to better grasp the non-linearities of the resulting vector space and, hence, improve the affective common-sense reasoning capabilities of the system. ?? 2012 Elsevier B.V. All rights reserved.},
	author = {Cambria, Erik and Mazzocco, Thomas and Hussain, Amir},
	booktitle = {Biologically Inspired Cognitive Architectures},
	doi = {10.1016/j.bica.2013.02.003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cambria, Mazzocco, Hussain - 2013 - Application of multi-dimensional scaling and artificial neural networks for biologically inspired op.htm:htm},
	isbn = {1467351644},
	issn = {2212683X},
	keywords = {AI,ANN,Cognitive modelling,NLP,Sentic computing},
	pages = {41--53},
	title = {{Application of multi-dimensional scaling and artificial neural networks for biologically inspired opinion mining}},
	volume = {4},
	year = {2013}
}
@article{Li2016,
	abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
	archivePrefix = {arXiv},
	arxivId = {1612.08220},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	eprint = {1612.08220},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Monroe, Jurafsky - 2016 - Understanding Neural Networks through Representation Erasure.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/task/Sentiment treebank},
	title = {{Understanding Neural Networks through Representation Erasure}},
	url = {http://arxiv.org/abs/1612.08220},
	year = {2016}
}
@article{Saad2007b,
	abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., {\&} Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373-389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e. calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity-complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
	author = {Saad, Emad W. and Wunsch, Donald C.},
	doi = {10.1016/j.neunet.2006.07.005},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saad, Wunsch - 2007 - Neural network explanation using inversion.pdf:pdf},
	isbn = {0893-6080},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Evolutionary algorithm,Explanation capability of neural networks,Hyperplanes,Inversion,Neural network explanation,Pedagogical,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {1},
	pages = {78--93},
	pmid = {17029713},
	title = {{Neural network explanation using inversion}},
	volume = {20},
	year = {2007}
}
@article{Derrac2015,
	abstract = {Commonsense reasoning patterns such as interpolation and a fortiori inference have proven useful for dealing with gaps in structured knowledge bases. An important difficulty in applying these reasoning patterns in practice is that they rely on fine-grained knowledge of how different concepts and entities are semantically related. In this paper, we show how the required semantic relations can be learned from a large collection of text documents. To this end, we first induce a conceptual space from the text documents, using multi-dimensional scaling. We then rely on the key insight that the required semantic relations correspond to qualitative spatial relations in this conceptual space. Among others, in an entirely unsupervised way, we identify salient directions in the conceptual space which correspond to interpretable relative properties such as 'more fruity than' (in a space of wines), resulting in a symbolic and interpretable representation of the conceptual space. To evaluate the quality of our semantic relations, we show how they can be exploited by a number of commonsense reasoning based classifiers. We experimentally show that these classifiers can outperform standard approaches, while being able to provide intuitive explanations of classification decisions. A number of crowdsourcing experiments provide further insights into the nature of the extracted semantic relations.},
	author = {Derrac, Joaqu??n and Schockaert, Steven},
	doi = {10.1016/j.artint.2015.07.002},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derrac, Schockaert - 2015 - Inducing semantic relations from conceptual spaces A data-driven approach to plausible reasoning.pdf:pdf},
	isbn = {0004-3702},
	issn = {00043702},
	journal = {Artificial Intelligence},
	keywords = {Commonsense reasoning,Conceptual spaces,Dimensionality reduction,Qualitative spatial relations},
	mendeley-groups = {Annotated/Past work,Papers/Paper 1,Categories/Commonsense Reasoning,Report/Features,Progress Report,Report},
	pages = {66--94},
	publisher = {Elsevier B.V.},
	title = {{Inducing semantic relations from conceptual spaces: A data-driven approach to plausible reasoning}},
	url = {http://dx.doi.org/10.1016/j.artint.2015.07.002},
	volume = {228},
	year = {2015}
}
@article{Yeh2016,
	author = {Yeh, Chih-kuan and Wu, Wei-chieh and Ko, Wei-jen and Wang, Yu-chiang Frank},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeh et al. - 2016 - Learning Deep Latent Spaces for Multi-Label Classification.pdf:pdf},
	mendeley-groups = {Progress Report,Interim Review},
	title = {{Learning Deep Latent Spaces for Multi-Label Classification}},
	year = {2016}
}
@article{Liang2016,
	abstract = {By taking the semantic object parsing task as an exemplar application scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network, which is the generalization of LSTM from sequential data or multi-dimensional data to general graph-structured data. Particularly, instead of evenly and fixedly dividing an image to pixels or patches in existing multi-dimensional LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each arbitrary-shaped superpixel as a semantically consistent node, and adaptively construct an undirected graph for each image, where the spatial relations of the superpixels are naturally used as edges. Constructed on such an adaptive graph topology, the Graph LSTM is more naturally aligned with the visual patterns in the image (e.g., object boundaries or appearance similarities) and provides a more economical information propagation route. Furthermore, for each optimization step over Graph LSTM, we propose to use a confidence-driven scheme to update the hidden and memory states of nodes progressively till all nodes are updated. In addition, for each node, the forgets gates are adaptively learned to capture different degrees of semantic correlation with neighboring nodes. Comprehensive evaluations on four diverse semantic object parsing datasets well demonstrate the significant superiority of our Graph LSTM over other state-of-the-art solutions.},
	archivePrefix = {arXiv},
	arxivId = {1603.07063},
	author = {Liang, Xiaodan and Shen, Xiaohui and Feng, Jiashi and Lin, Liang and Yan, Shuicheng},
	doi = {10.1007/978-3-319-46448-0_8},
	eprint = {1603.07063},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2016 - Semantic object parsing with graph LSTM.pdf:pdf},
	isbn = {9783319464473},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Graph LSTM,Object parsing,Recurrent neural networks},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	pages = {125--143},
	pmid = {4520227},
	title = {{Semantic object parsing with graph LSTM}},
	volume = {9905 LNCS},
	year = {2016}
}
@article{NorouziM2009,
	abstract = {In this thesis, we present a method for learning problem-specific hierarchical features specialized for vision applications. Recently, a greedy layerwise learning mechanism has been proposed for tuning parameters of fully connected hierarchical networks. This approach views layers of a network as Restricted Boltzmann Machines (RBM), and trains them separately from the bottom layer upwards. We develop Convolutional RBM (CRBM), an extension of the RBM model in which connections are local and scikit-learns are shared to respect the spatial structure of images. We switch between the CRBM and down-sampling layers and stack them on top of each other to build a multilayer hierarchy of alternating filtering and pooling. This framework learns generic features such as oriented edges at the bottom levels and features specific to an object class such as object parts in the top layers. Afterward, we feed the extracted features into a discriminative classifier for recognition. It is experimentally demonstrated that the features automatically learned by our algorithm are effective for object detection, by using them to obtain performance comparable to the state-of-the-art on handwritten digit classification and pedestrian detection.},
	author = {{Norouzi M}},
	doi = {10.1109/CVPR.2009.5206577},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi M - 2009 - Convolutional restricted Boltzmann machines for feature learning.pdf:pdf},
	isbn = {9781424439911},
	journal = {School of Computing Science-Simon Fraser University},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {2735--2742},
	title = {{Convolutional restricted Boltzmann machines for feature learning}},
	year = {2009}
}
@article{Bach2015a,
	abstract = {Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally considered `black box' predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher Vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification, (3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data. All experiments are performed on the PASCAL VOC 2007 data set.},
	archivePrefix = {arXiv},
	arxivId = {1512.00172},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
	doi = {10.1109/CVPR.2016.318},
	eprint = {1512.00172},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - Analyzing Classifiers Fisher Vectors and Deep Neural Networks.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {10636919},
	mendeley-groups = {Annotated/Artifacts in the data},
	title = {{Analyzing Classifiers: Fisher Vectors and Deep Neural Networks}},
	url = {http://arxiv.org/abs/1512.00172},
	year = {2015}
}
@article{Lai1990,
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 1990 - Recurrent Convolutional Neural Networks for Text Classification.pdf:pdf},
	keywords = {NLP and Machine Learning Track},
	mendeley-groups = {Annotated/Representation Learning,!Paper 3/task/newsgroups},
	pages = {2267--2273},
	title = {{Recurrent Convolutional Neural Networks for Text Classification}},
	year = {1990}
}
@article{Tang2014,
	abstract = {We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
	doi = {10.3115/1220575.1220648},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2014 - Learning Sentiment-Specific Word Embedding.pdf:pdf},
	isbn = {9781937284725},
	issn = {03029743},
	journal = {Acl},
	mendeley-groups = {Progress Report},
	pages = {1555--1565},
	pmid = {18487783},
	title = {{Learning Sentiment-Specific Word Embedding}},
	year = {2014}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - ScienceDirect - Artificial Intelligence Symbolic knowledge extraction from trained neural networks A sound approach.pdf:pdf},
	title = {{ScienceDirect - Artificial Intelligence : Symbolic knowledge extraction from trained neural networks: A sound approach}},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370200000771}
}
@article{Zhou2004,
	abstract = {In the research of rule extraction from neural networks, fidelity describes how well the rules mimic the behavior of a neural network while accuracy describes how well the rules can be generalized. This paper identifies the fidelity-accuracy dilemma. It argues to distinguish rule extraction using neural networks and rule extraction for neural networks according to their differing goals, where fidelity and accuracy should be excluded from the rule quality evaluation framework, respectively.},
	author = {Zhou, Zhi-Hua},
	doi = {10.1007/BF02944803},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou - 2004 - Rule Extraction Using Neural Networks or for Neural Networks ×.pdf:pdf},
	issn = {1000-9000},
	journal = {J. Comput. Sci. {\&} Technol.},
	keywords = {accuracy,fidelity,machine learning,neural networks,rule extraction},
	pages = {249--253},
	title = {{Rule Extraction: Using Neural Networks or for Neural Networks? ×}},
	url = {http://ac.els-cdn.com/S1877750313000185/1-s2.0-S1877750313000185-main.pdf?{\_}tid=f08c3f6c-e35d-11e4-9abc-00000aacb35f{\&}acdnat=1429095513{\_}7903f6ce12ea46c9f359f585fc91609a},
	volume = {19},
	year = {2004}
}
@article{Chrupaa2015,
	abstract = {We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.},
	archivePrefix = {arXiv},
	arxivId = {1506.03694},
	author = {Chrupa{\l}a, Grzegorz and K{\'{a}}d{\'{a}}r, {\'{A}}kos and Alishahi, Afra},
	eprint = {1506.03694},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chrupa{\l}a, K{\'{a}}d{\'{a}}r, Alishahi - 2015 - Learning language through pictures.pdf:pdf},
	mendeley-groups = {Progress Report},
	number = {September},
	pages = {8--9},
	title = {{Learning language through pictures}},
	url = {http://arxiv.org/abs/1506.03694},
	year = {2015}
}
@article{Donoho2004,
	abstract = {We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that un- der certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sam- pling. For such databases there is a generative model in terms of “parts” and NMF correctly identifies the ”parts”. We show that our theoretical results are predictive of the performance of publishedNMFcode, by run- ning the published algorithms on one of our synthetic image articulation databases.},
	archivePrefix = {arXiv},
	arxivId = {1512.00567},
	author = {Donoho, Dl and Stodden, Vc},
	doi = {10.1.1.85.8157},
	eprint = {1512.00567},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho, Stodden - 2004 - When does non-negative matrix factorization give a correct decomposition into parts.pdf:pdf},
	isbn = {9780262201520},
	issn = {09501991},
	journal = {Proc. Advances in Neural Information Processing Systems 16},
	mendeley-groups = {Annotated/NMF},
	pages = {1141--1148},
	pmid = {11585793},
	title = {{When does non-negative matrix factorization give a correct decomposition into parts?}},
	url = {http://academiccommons.columbia.edu/catalog/ac:140175},
	year = {2004}
}
@article{Hatzilygeroudis2004,
	abstract = {In this paper, we first present and compare existing categorization schemes for neuro-symbolic$\backslash$napproaches. We then stress the point that not all hybrid neuro-symbolic approaches can be$\backslash$naccommodated by existing categories. Such a case is rule-based neuro-symbolic approaches that propose$\backslash$na unified knowledge representation scheme suitable for use in expert systems. That kind of integrated$\backslash$nschemes have the two component approaches tightly and indistinguishably integrated, offer an$\backslash$ninteractive inference engine and can provide explanations. Therefore, we introduce a new category of$\backslash$nneuro-symbolic integrations, namely {\{}{\^{a}}{\}}€˜representational integrations{\{}{\^{a}}{\}}€™. Furthermore, two sub-categories of$\backslash$nrepresentational integrations are distinguished, based on which of the two component approaches of the$\backslash$nintegrations is given pre-eminence. Representative approaches as well as advantages and disadvantages$\backslash$nof both sub-categories are discussed.},
	author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatzilygeroudis, Prentzas - 2004 - Neuro-symbolic approaches for knowledge representation in expert systems.pdf:pdf},
	journal = {International Journal of Hybrid Intelligent Systems},
	keywords = {connectionist expert systems,neuro-symbolic integrations,rule-based expert systems},
	mendeley-groups = {Progress Report},
	number = {3, 4},
	pages = {111--126},
	title = {{Neuro-symbolic approaches for knowledge representation in expert systems}},
	url = {http://dl.acm.org/citation.cfm?id=1232821},
	volume = {1},
	year = {2004}
}
@article{Korde2012,
	abstract = {As most information (over 80{\%}) is stored as text, text mining is believed to have a high commercial potential value. knowledge may be discovered from many sources of information; yet, unstructured texts remain the largest readily available source of knowledge .Text classification which classifies the documents according to predefined categories .In this paper we are tried to give the introduction of text classification, process of text classification as well as the overview of the classifiers and tried to compare the some existing classifier on basis of few criteria like time complexity, principal and performance.},
	author = {Korde, Vandana and Mahender, C Namrata},
	doi = {10.5121/ijaia.2012.3208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korde, Mahender - 2012 - Text Classification and Classifiers A Survey.pdf:pdf},
	issn = {09762191},
	journal = {International Journal of Artificial Intelligence {\&} Applications},
	keywords = {classifiers,text classification,text representation},
	mendeley-groups = {Annotated/Decision Trees},
	number = {2},
	pages = {85--99},
	title = {{Text Classification and Classifiers: A Survey}},
	url = {http://www.airccse.org/journal/ijaia/papers/3212ijaia08.pdf},
	volume = {3},
	year = {2012}
}
@article{Chih-WeiHsuChih-ChungChang2008,
	abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
	author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
	journal = {BJU international},
	number = {1},
	pages = {1396--400},
	title = {{A Practical Guide to Support Vector Classification}},
	url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
	volume = {101},
	year = {2008}
}
@misc{Hatzilygeroudis2010,
	abstract = {Neurules are a kind of integrated rules integrating neurocomputing and production rules. Each neurule is represented as an adaline unit. Thus, the corresponding neurule base consists of a number of autonomous adaline units (neurules). In this paper, we present the construction process and the inference mechanism of neurules and explore their generalization capabilities. The construction process, which also implements corresponding learning algorithm, creates neurules from a given empirical data set. The inference mechanism of neurules is integrated in its nature; it combines neurocomputing with symbolic processes. It is also interactive, i.e., it interacts with the user asking him/her to provide values for some variables necessary to carry on inference. As shown via experiments, the neurules' integrated inference mechanism is more efficient than the inference mechanism used in connectionist expert systems. Furthermore, neurules generalize much better than their constituent neural component (adaline unit) and are comparable to the backpropagation neural net (BPNN).},
	author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
	booktitle = {IEEE Transactions on Knowledge and Data Engineering},
	doi = {10.1109/TKDE.2010.79},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatzilygeroudis, Prentzas - 2010 - Integrated rule-based learning and inference.htm:htm},
	isbn = {1041-4347},
	issn = {10414347},
	keywords = {Neurosymbolic integration,integrated inference,neurocomputing,rule-based reasoning},
	number = {11},
	pages = {1549--1562},
	title = {{Integrated rule-based learning and inference}},
	volume = {22},
	year = {2010}
}
@article{Abadi2016,
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	archivePrefix = {arXiv},
	arxivId = {1603.04467},
	author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	doi = {10.1038/nn.3331},
	eprint = {1603.04467},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
	isbn = {0010-0277},
	issn = {0270-6474},
	mendeley-groups = {!Paper 3},
	pmid = {16411492},
	title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
	url = {http://arxiv.org/abs/1603.04467},
	year = {2016}
}
@article{Predic2010a,
	author = {Predic, N Y P and June, Meetup},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Predic, June - 2010 - Introduction to.pdf:pdf},
	isbn = {1441923349},
	pages = {1--8},
	title = {{Introduction to}},
	year = {2010}
}
@article{Chemiavsky1991a,
	abstract = {properties for software complexity measures are explored. It is shown that a collection of properties suggested by Weyuker are inadequate for determining the quality of a software complexity measure},
	author = {Chemiavsky, John C and Smith, Carl H},
	doi = {10.1109/32.106988},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chemiavsky, Smith - 1991 - Concise Papers.pdf:pdf},
	isbn = {0098-5589},
	issn = {00985589},
	journal = {Foundations},
	number = {9144263},
	pages = {636--638},
	pmid = {158},
	title = {{Concise Papers}},
	volume = {17},
	year = {1991}
}
@article{Yang2014,
	abstract = {Abstract Genetic algorithms are among the most popular evolutionary algorithms in terms of the diversity of their applications. A vast majority of well-known optimization problems have been solved using genetic algorithms. In addition, genetic algorithms are population-based, and many modern evolutionary algorithms are directly based on genetic algorithms or have some strong similarities to them.},
	author = {Yang, Xin-She},
	doi = {10.1016/B978-0-12-416743-8.00005-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Man, Tang, Kwong - 1999 - Genetic Algorithms.pdf:pdf},
	isbn = {978-0-12-416743-8},
	issn = {0036-8733},
	journal = {Nature-Inspired Optimization Algorithms},
	keywords = {Evolutionary algorithm,Genetic algorithms,Genetic operators,Optimization},
	pages = {77--87},
	title = {{Genetic Algorithms}},
	url = {http://www.sciencedirect.com/science/article/pii/B9780124167438000051{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/B9780124167438000051},
	year = {2014}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 1704.03296.pdf.pdf:pdf},
	title = {1704.03296.pdf}
}
@article{Conll,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/243{\_}file{\_}Submission (1).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Po,
	author = {Po, Huang and Teaching, Zen and Po, Huang},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Po, Teaching, Po - Unknown - “ Up to now , you have refuted everything which has been said . You have done nothing to point out the true.pdf:pdf},
	title = {{“ Up to now , you have refuted everything which has been said . You have done nothing to point out the true Dharma to us .”}}
}
@article{Panchenko2016,
	abstract = {Word sense embeddings represent a word sense as a low-dimensional numeric vector. While this representation is potentially useful for NLP applications, its interpretability is inherently limited. We propose a simple technique that improves interpretability of sense vectors by mapping them to synsets of a lexical resource. Our experiments with AdaGram sense embeddings and BabelNet synsets showthat it is possible to retrieve synsets that correspond to automatically learned sense vectors with Precision of 0.87, Recall of 0.42 andAUC of 0.78.},
	author = {Panchenko, Alexander},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Panchenko - 2016 - Best of Both Worlds Making Word Sense Embeddings Interpretable.pdf:pdf},
	journal = {the 10th edition of the Language Resources and Evaluation Conference (LREC 2016)},
	keywords = {adagram,babelnet,lexical semantics,sense matching,word sense embeddings,wordnet},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {2649--2655},
	title = {{Best of Both Worlds: Making Word Sense Embeddings Interpretable}},
	url = {http://www.lrec-conf.org/proceedings/lrec2016/pdf/625{\_}Paper.pdf},
	year = {2016}
}
@article{Kuchaiev2017,
	abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.},
	archivePrefix = {arXiv},
	arxivId = {1703.10722},
	author = {Kuchaiev, Oleksii and Ginsburg, Boris},
	doi = {10.1109/CVPR.2016.90},
	eprint = {1703.10722},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuchaiev, Ginsburg - 2017 - Factorization tricks for LSTM networks.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {1664-1078},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
	pages = {1--6},
	pmid = {23554596},
	title = {{Factorization tricks for LSTM networks}},
	url = {http://arxiv.org/abs/1703.10722},
	year = {2017}
}
@article{Bibal2018,
	author = {Bibal, Adrien and Marion, Rebecca and Fr{\'{e}}nay, Beno{\^{i}}t},
	file = {:C$\backslash$:/Users/Workk/Documents/es2018-89.pdf:pdf},
	isbn = {9782875870476},
	number = {April},
	pages = {25--27},
	title = {{Finding the Most Interpretable MDS Rotation for Sparse Linear Models based on External Features}},
	year = {2018}
}
@article{Saaty2003,
	abstract = {In 1956, Miller [1] conjectured that there is an upper limit on our capacity to process information on simultaneously interacting elements with reliable accuracy and with validity. This limit is seven plus or minus two elements. He noted that the number 7 occurs in many aspects of life, from the seven wonders of the world to the seven seas and seven deadly sins. We demonstrate in this paper that in making preference judgments on pairs of elements in a group, as we do in the analytic hierarchy process (AHP), the number of elements in the group should be no more than seven. The reason is founded in the consistency of information derived from relations among the elements. When the number of elements increases past seven, the resulting increase in inconsistency is too small for the mind to single out the element that causes the greatest inconsistency to scrutinize and correct its relation to the other elements, and the result is confusion to the mind from the existing information. The AHP as a theory of measurement has a basic way to obtain a measure of inconsistency for any such set of pairwise judgments. When the number of elements is seven or less the inconsistency measurement is relatively large with respect to the number of elements involved; when the number is more it is relatively small. The most inconsistent judgment is easily determined in the first case and the individual providing the judgments can change it in an effort to improve the overall inconsistency. In the second case, as the inconsistency measurement is relatively small, improving inconsistency requires only small perturbations and the judge would be hard put to determine what that change should be, and how such a small change could be justified for improving the validity of the outcome. The mind is sufficiently sensitive to improve large inconsistencies but not small ones. And the implication of this is that the number of elements in a set should be limited to seven plus or minus two.},
	author = {Saaty, T.L. and Ozdemir, M.S.},
	doi = {10.1016/S0895-7177(03)90083-5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saaty, Ozdemir - 2003 - Why the magic number seven plus or minus two.pdf:pdf},
	isbn = {0895-7177},
	issn = {08957177},
	journal = {Mathematical and Computer Modelling},
	mendeley-groups = {Annotated/Psychology},
	number = {3},
	pages = {233--244},
	pmid = {8022966},
	title = {{Why the magic number seven plus or minus two}},
	volume = {38},
	year = {2003}
}
@article{Sutskever2014,
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	archivePrefix = {arXiv},
	arxivId = {1409.3215},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc},
	eprint = {1409.3215},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
	isbn = {1409.3215},
	journal = {Advances in Neural Information Processing Systems},
	pages = {1--9},
	title = {{Sequence to Sequence Learning with Neural Networks}},
	year = {2014}
}
@article{Read2014a,
	abstract = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
	archivePrefix = {arXiv},
	arxivId = {1502.05988},
	author = {Read, Jesse and Perez-Cruz, Fernando},
	eprint = {1502.05988},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Perez-Cruz - 2014 - Deep Learning for Multi-label Classification.pdf:pdf},
	pages = {1--8},
	title = {{Deep Learning for Multi-label Classification}},
	url = {http://arxiv.org/abs/1502.05988},
	year = {2014}
}
@article{Shrikumar2017,
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
	archivePrefix = {arXiv},
	arxivId = {1704.02685},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	eprint = {1704.02685},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences(2).pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Learning Important Features Through Propagating Activation Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	volume = {1},
	year = {2017}
}
@article{Lei2016,
	abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
	archivePrefix = {arXiv},
	arxivId = {1606.04155},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	eprint = {1606.04155},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lei, Barzilay, Jaakkola - 2016 - Rationalizing Neural Predictions.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Rationalizing Neural Predictions}},
	url = {http://arxiv.org/abs/1606.04155},
	year = {2016}
}
@article{Chorowski2015a,
	abstract = {People can understand complex structures if they relate to more isolated yet understandable concepts. Despite this fact, popular pattern recognition tools, such as decision tree or production rule learners, produce only flat models which do not build intermediate data representations. On the other hand, neural networks typically learn hierarchical but opaque models. We show how constraining neurons' scikit-learns to be nonnegative improves the interpretability of a network's operation. We analyze the proposed method on large data sets: the MNIST digit recognition data and the Reuters text categorization data. The patterns learned by traditional and constrained network are contrasted to those learned with principal component analysis and nonnegative matrix factorization.},
	author = {Chorowski, Jan and Zurada, Jacek M.},
	doi = {10.1109/TNNLS.2014.2310059},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chorowski, Zurada - 2015 - Learning understandable neural networks with nonnegative scikit-learn constraints.pdf:pdf},
	isbn = {2162-237X VO - 26},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Multilayer perceptron,pattern analysis,supervised learning,white-box models.},
	mendeley-groups = {Annotated/Word Vectors},
	number = {1},
	pages = {62--69},
	title = {{Learning understandable neural networks with nonnegative scikit-learn constraints}},
	volume = {26},
	year = {2015}
}
@article{Chen2014c,
	abstract = {We have provided a model and framework as a foundation for transparent interfaces via our Situation Awareness-based Agent Transparency (SAT) model. In this report we discuss the implications of agent transparency for operator trust and workload; we also review potential user interface designs (information visualization and displaying uncertainty information) to support agent transparency. Finally, we provide examples of transparent interface design efforts currently ongoing at the U.S. Army Research Laboratory's Human Research and Engineering Directorate under the Autonomy Research Pilot Initiative.},
	author = {Chen, Jessie Y. C. and Procci, Katelyn and Boyce, Michael and Wright, Julia and Garcia, Andre and Barnes, Michael J.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Situation Awareness-Based Agent Transparency.pdf:pdf},
	isbn = {ARL-TR-6905},
	journal = {US Army Research Laboratory},
	keywords = {autonomous systems,human-robot interaction,situation awareness (SA),transparency,trust},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {April},
	pages = {1--29},
	title = {{Situation Awareness-Based Agent Transparency}},
	year = {2014}
}
@article{XinYao1999a,
	abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection scikit-learns, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
	archivePrefix = {arXiv},
	arxivId = {1108.1530},
	author = {{Xin Yao}},
	doi = {10.1109/5.784219},
	eprint = {1108.1530},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xin Yao - 1999 - Evolving artificial neural networks.pdf:pdf},
	isbn = {9780470287194},
	issn = {00189219},
	journal = {Proceedings of the IEEE},
	keywords = {evolutionary computation,intelligent systems,neu-},
	number = {9},
	pages = {1423--1447},
	pmid = {9821520},
	title = {{Evolving artificial neural networks}},
	volume = {87},
	year = {1999}
}
@article{Zhu2014,
	abstract = {Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max- margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the “augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time efficiency. The classification performance is also improved over competitors on binary, multi- class and multi-label classification tasks.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1310.2816v1},
	author = {Zhu, Jun and Chen, Ning and Perkins, Hugh and Zhang, Bo},
	eprint = {arXiv:1310.2816v1},
	file = {:E$\backslash$:/Downloads/Work/zhu14a.pdf:pdf},
	isbn = {1532-4435},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {Gibbs classifiers,max-margin learning,regularized Bayesian inference,supervised topic models,support vector machines},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {1073--1110},
	title = {{Gibbs max-margin topic models with data augmentation}},
	volume = {15},
	year = {2014}
}
@article{Miyato2017,
	abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the output distribution. Virtual adversarial loss is defined as the robustness of the model's posterior distribution against local perturbation around each input data point. Our method is similar to adversarial training, but differs from adversarial training in that it determines the adversarial direction based only on the output distribution and that it is applicable to a semi-supervised setting. Because the directions in which we smooth the model are virtually adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward and backpropagations. In our experiments, we applied VAT to supervised and semi-supervised learning on multiple benchmark datasets. With additional improvement based on entropy minimization principle, our VAT achieves the state-of-the-art performance on SVHN and CIFAR-10 for semi-supervised learning tasks.},
	archivePrefix = {arXiv},
	arxivId = {1704.03976},
	author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	eprint = {1704.03976},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyato et al. - 2017 - Virtual Adversarial Training a Regularization Method for Supervised and Semi-supervised Learning.pdf:pdf},
	pages = {1--14},
	title = {{Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning}},
	url = {http://arxiv.org/abs/1704.03976},
	year = {2017}
}
@article{Funahashi1989,
	author = {Funahashi, K.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Funahashi - 1989 - On the Approximate Realisation of Continuous Mappings by Neural Networks.pdf:pdf},
	journal = {Neural Networks},
	keywords = {--neural network,alization,back propagation,continuous mapping,hidden layer,output function,re-,sigmoid function,unit},
	number = {3},
	pages = {183--192},
	title = {{On the Approximate Realisation of Continuous Mappings by Neural Networks}},
	volume = {2},
	year = {1989}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/396ec311349a6b596a6467e141ee6f578bf21d48.html:html},
	title = {language{\_}modeling @ github.com},
	url = {https://github.com/sebastianruder/NLP-progress/blob/master/english/language{\_}modeling.md}
}
@article{Donahue2011,
	abstract = {Traditional supervised visual learning simply asks annotators {\&}{\#}x201C;what{\&}{\#}x201D; label an image should have. We propose an approach for image classification problems requiring subjective judgment that also asks {\&}{\#}x201C;why{\&}{\#}x201D;, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the first, the annotator highlights the spatial region of interest he found most influential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to refine the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices.},
	author = {Donahue, Jeff and Grauman, Kristen},
	doi = {10.1109/ICCV.2011.6126394},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue, Grauman - 2011 - Annotator rationales for visual recognition.pdf:pdf},
	isbn = {9781457711015},
	issn = {1550-5499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	mendeley-groups = {Report/Explaining predictions,Annotated},
	number = {Iccv},
	pages = {1395--1402},
	title = {{Annotator rationales for visual recognition}},
	year = {2011}
}
@article{Read2010,
	author = {Read, Jesse},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2010 - Scalable Multi-label Classification(2).pdf:pdf},
	title = {{Scalable Multi-label Classification}},
	year = {2010}
}
@article{Huynh2011a,
	abstract = {The production of relatively large and opaque scikit-learn matrices by error backpropagation learning has inspired substantial research on how to extract symbolic human-readable rules from trained networks. While considerable progress has been made, the results at present are still relatively limited, in part due to the large numbers of symbolic rules that can be generated. Most past work to address this issue has focused on progressively more powerful methods for rule extraction (RE) that try to minimize the number of scikit-learns and/or improve rule expressiveness. In contrast, here we take a different approach in which we modify the error backpropagation training process so that it learns a different hidden layer representation of input patterns than would normally occur. Using five publicly available datasets, we show via computational experiments that the modified learning method helps to extract fewer rules without increasing individual rule complexity and without decreasing classification accuracy. We conclude that modifying error backpropagation so that it more effectively separates learned pattern encodings in the hidden layer is an effective way to improve contemporary RE methods.},
	author = {Huynh, Thuan Q. and Reggia, James A.},
	doi = {10.1109/TNN.2010.2094205},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huynh, Reggia - 2011 - Guiding hidden layer representations for improved rule extraction from neural networks.pdf:pdf},
	isbn = {1941-0093 (Electronic)$\backslash$n1045-9227 (Linking)},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Hidden layer representation,neural networks,penalty function,rule extraction},
	number = {2},
	pages = {264--275},
	pmid = {21138801},
	title = {{Guiding hidden layer representations for improved rule extraction from neural networks}},
	volume = {22},
	year = {2011}
}
@article{Bandara2017,
	abstract = {With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks, and in particular Long Short-Term Memory (LSTM) networks have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context, when trained across all available time series. However, if the time series database is heterogeneous accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model using LSTMs on subgroups of similar time series, which are identified by time series clustering techniques. The proposed methodology is able to consistently outperform the baseline LSTM model, and it achieves competitive results on benchmarking datasets, in particular outperforming all other methods on the CIF2016 dataset.},
	archivePrefix = {arXiv},
	arxivId = {1710.03222},
	author = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
	eprint = {1710.03222},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bandara, Bergmeir, Smyl - 2017 - Forecasting Across Time Series Databases using Long Short-Term Memory Networks on Groups of Similar Ser.pdf:pdf},
	keywords = {big data forecasting,lstm,neural networks,rnn,time series clustering},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	number = {Ml},
	title = {{Forecasting Across Time Series Databases using Long Short-Term Memory Networks on Groups of Similar Series}},
	url = {http://arxiv.org/abs/1710.03222},
	volume = {1999},
	year = {2017}
}
@article{Keil2006,
	abstract = {The study of explanation, while related to intuitive theories, concepts, and mental models, offers important new perspectives on high-level thought. Explanations sort themselves into several distinct types corresponding to patterns of causation, content domains, and explanatory stances, all of which have cognitive consequences. Although explanations are necessarily incomplete—often dramatically so in laypeople—those gaps are difficult to discern. Despite such gaps and the failure to recognize them fully, people do have skeletal explanatory senses, often implicit, of the causal structure of the world. They further leverage those skeletal understandings by knowing how to access additional explanatory knowledge in other minds and by being particularly adept at using situational support to build explanations on the fly in real time. Across development and cultures, there are differences in preferred explanatory schemes, but rarely are any kinds of schemes completely unavailable to a group},
	archivePrefix = {arXiv},
	arxivId = {NIHMS150003},
	author = {Keil, Frank C.},
	doi = {10.1146/annurev.psych.57.102904.190100},
	eprint = {NIHMS150003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keil - 2006 - Explanation and Understanding.pdf:pdf},
	isbn = {0066-4308 1545-2085},
	issn = {0066-4308},
	journal = {Annual Review of Psychology},
	keywords = {abstract the study of,and mental models,causality,cognition,cognitive development,concepts,domain specificity,expla-,explanation,illusions of,knowing,offers important new perspectives,on high-level thought,stances,theories,while related to intuitive},
	mendeley-groups = {Report/Explaining predictions},
	number = {1},
	pages = {227--254},
	pmid = {16318595},
	title = {{Explanation and Understanding}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.57.102904.190100},
	volume = {57},
	year = {2006}
}
@article{Strobelt2018,
	abstract = {Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.},
	archivePrefix = {arXiv},
	arxivId = {1606.07461},
	author = {Strobelt, Hendrik and Gehrmann, Sebastian and Pfister, Hanspeter and Rush, Alexander M.},
	doi = {10.1109/TVCG.2017.2744158},
	eprint = {1606.07461},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobelt et al. - 2018 - LSTMVis A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks.pdf:pdf},
	isbn = {1077-2626 VO - PP},
	issn = {10772626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {LSTM,Machine Learning,Recurrent Neural Networks,Visualization},
	mendeley-groups = {!Paper 3,!Paper 3/Understanding LSTMs},
	number = {1},
	pages = {667--676},
	pmid = {28866526},
	title = {{LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks}},
	volume = {24},
	year = {2018}
}
@article{Blei2010,
	abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
	archivePrefix = {arXiv},
	arxivId = {1003.0783},
	author = {Blei, David M. and McAuliffe, Jon D.},
	doi = {10.1002/asmb.540},
	eprint = {1003.0783},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, McAuliffe - 2010 - Supervised Topic Models.pdf:pdf},
	isbn = {160560352X},
	issn = {15241904},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {1--8},
	title = {{Supervised Topic Models}},
	url = {http://arxiv.org/abs/1003.0783},
	year = {2010}
}
@article{Garcez2014,
	abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
	author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
	doi = {10.13140/2.1.1779.4243},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez et al. - 2014 - Neural-Symbolic Learning and Reasoning Contributions and Challenges(2).pdf:pdf},
	journal = {Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, Stanford},
	pages = {18--21},
	title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
	year = {2014}
}
@article{Chandrashekar2014,
	author = {Chandrashekar, Girish and Sahin, Ferat},
	doi = {10.1016/j.compeleceng.2013.11.024},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrashekar, Sahin - 2014 - A survey on feature selection methods q.pdf:pdf},
	issn = {0045-7906},
	journal = {Computers and Electrical Engineering},
	mendeley-groups = {Report/Features,Report},
	number = {1},
	pages = {16--28},
	publisher = {Elsevier Ltd},
	title = {{A survey on feature selection methods q}},
	url = {http://dx.doi.org/10.1016/j.compeleceng.2013.11.024},
	volume = {40},
	year = {2014}
}
@article{Augasta2012a,
	abstract = {Though neural networks have achieved highest classification accuracy for many classification problems, the obtained results may not be interpretable as they are often considered as black box. To overcome this drawback researchers have developed many rule extraction algorithms. This paper has discussed on various rule extraction algorithms based on three different rule extraction approaches namely decompositional, pedagogical and eclectic. Also it evaluates the performance of those approaches by comparing different algorithms with these three approaches on three real datasets namely Wisconsin breast cancer, Pima Indian diabetes and Iris plants. 2012 IEEE.},
	author = {Augasta, M Gethsiyal and Kathirvalavakumar, T},
	doi = {10.1109/icprime.2012.6208380},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Augasta, Kathirvalavakumar - 2012 - Rule extraction from neural networks - A comparative study.pdf:pdf},
	isbn = {9781467310390},
	journal = {2012 International Conference on Pattern Recognition, Informatics and Medical Engineering, PRIME 2012, March 21, 2012 - March 23, 2012},
	keywords = {Biomedical engineering,Classification (of information),Data mining,Information science,Neural networks,Pattern recognition},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	pages = {404--408},
	title = {{Rule extraction from neural networks - A comparative study}},
	url = {http://dx.doi.org/10.1109/ICPRIME.2012.6208380},
	year = {2012}
}
@article{Garc??a2009,
	abstract = {The experimental analysis on the performance of a proposed method is a crucial and necessary task to carry out in a research. This paper is focused on the sta- tistical analysis of the results in the field of genetics-based machine Learning. It presents a study involving a set of techniques which can be used for doing a rigorous com- parison among algorithms, in terms of obtaining successful classification models. Two accuracy measures for multi- class problems have been employed: classification rate and Cohen's kappa. Furthermore, two interpretability measures have been employed: size of the rule set and number of antecedents. We have studied whether the samples of results obtained by genetics-based classifiers, using the performance measures cited above, check the necessary conditions for being analysed by means of parametrical tests. The results obtained state that the fulfillment of these conditions are problem-dependent and indefinite, which supports the use of non-parametric statistics in the experi- mental analysis. In addition, non-parametric tests can be satisfactorily employed for comparing generic classifiers over various data-sets considering any performance measure. According to these facts, we propose the use of the most powerful non-parametric statistical tests to carry out multiple comparisons. However, the statistical analysis conducted on interpretability must be carefully considered.},
	author = {Garc??a, S. and Fern??ndez, Alberto and Luengo, Julian and Herrera, F.},
	doi = {10.1007/s00500-008-0392-y},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garca et al. - 2009 - A study of statistical techniques and performance measures for genetics-based machine learning Accuracy and interp.pdf:pdf},
	issn = {14327643},
	journal = {Soft Computing},
	keywords = {Classification,Cohen's kappa,Genetic algorithms,Genetics-based machine learning,Interpretability,Non-parametric tests,Statistical tests},
	number = {10},
	pages = {959--977},
	title = {{A study of statistical techniques and performance measures for genetics-based machine learning: Accuracy and interpretability}},
	volume = {13},
	year = {2009}
}
@article{Speer,
	abstract = {Machine learning about language can be improved by sup-plying it with specific knowledge and sources of external in-formation. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embed-dings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowl-edge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a pur-pose. It is designed to represent the general knowledge in-volved in understanding language, improving natural lan-guage applications by allowing the application to better un-derstand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improve-ments on applications of word vectors, including solving SAT-style analogies.},
	author = {Speer, Robert and Chin, Joshua and Havasi, Catherine},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Speer, Chin, Havasi - Unknown - ConceptNet 5.5 An Open Multilingual Graph of General Knowledge.pdf:pdf},
	title = {{ConceptNet 5.5: An Open Multilingual Graph of General Knowledge}}
}
@article{Zhang2007a,
	abstract = {Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms. ?? 2007 Pattern Recognition Society.},
	author = {Zhang, Min Ling and Zhou, Zhi Hua},
	doi = {10.1016/j.patcog.2006.12.019},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - 2007 - ML-KNN A lazy learning approach to multi-label learning.pdf:pdf},
	isbn = {0031-3203},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Functional genomics,K-nearest neighbor,Lazy learning,Machine learning,Multi-label learning,Natural scene classification,Text categorization},
	number = {7},
	pages = {2038--2048},
	title = {{ML-KNN: A lazy learning approach to multi-label learning}},
	volume = {40},
	year = {2007}
}
@article{Bai2018,
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.},
	archivePrefix = {arXiv},
	arxivId = {1803.01271},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	eprint = {1803.01271},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Kolter, Koltun - 2018 - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	year = {2018}
}
@article{Zhang2014,
	abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
	author = {Zhang, Min Ling and Zhou, Zhi Hua},
	doi = {10.1109/TKDE.2013.39},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - 2014 - A review on multi-label learning algorithms.pdf:pdf},
	isbn = {1041-4347},
	issn = {10414347},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Multi-label learning,algorithm adaptation,label correlations-problem transformation},
	mendeley-groups = {Progress Report},
	number = {8},
	pages = {1819--1837},
	title = {{A review on multi-label learning algorithms}},
	volume = {26},
	year = {2014}
}
@article{Linegang2006,
	abstract = {The US Navy is funding the development of advanced automation systems to plan and execute unmanned vehicles missions, pushing towards a higher level of autonomy for automated planning systems. With effective systems, the human could play a role of mission manager and automation systems could perform mission planning and execution tasks with limited human involvement. Evaluations of the automation systems currently under development are identifying critical conflicts between human operator expectations and automated planning results. This paper presents a model of this human-automation interaction system and summarizes the resulting system design effort. This model provides a theory explaining the source of conflict between human and automation, and predicts that an ecological approach to display design would reduce that conflict. Based on that prediction, the paper describes initial results of an ecological approach to system analysis and design, intended to improve human-automation interaction for these types of advanced automation systems.},
	author = {Linegang, M. P. and Stoner, H. a. and Patterson, M. J. and Seppelt, B. D. and Hoffman, J. D. and Crittendon, Z. B. and Lee, John D.},
	doi = {10.1177/154193120605002304},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Linegang et al. - 2006 - Human-Automation Collaboration in Dynamic Mission Planning A Challenge Requiring an Ecological Approach.pdf:pdf},
	isbn = {10711813 (ISSN); 9780945289296 (ISBN)},
	issn = {1071-1813},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {23},
	pages = {2482--2486},
	title = {{Human-Automation Collaboration in Dynamic Mission Planning: A Challenge Requiring an Ecological Approach}},
	volume = {50},
	year = {2006}
}
@article{Jacobsson2005,
	abstract = {Rule extraction (RE) from recurrent neural networks (RNNs) refers to finding models of the underlying RNN, typically in the form of finite state machines, that mimic the network to a satisfactory degree while having the advantage of being more transparent. RE from RNNs can be argued to allow a deeper and more profound form of analysis of RNNs than other, more or less ad hoc methods. RE may give us understanding of RNNs in the intermediate levels between quite abstract theoretical knowledge of RNNs as a class of computing devices and quantitative performance evaluations of RNN instantiations. The development of techniques for extraction of rules from RNNs has been an active field since the early 1990s. This article reviews the progress of this development and analyzes it in detail. In order to structure the survey and evaluate the techniques, a taxonomy specifically designed for this purpose has been developed. Moreover, important open research issues are identified that, if addressed properly, possibly ca...},
	author = {Jacobsson, Henrik},
	doi = {10.1162/0899766053630350},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobsson - 2005 - Rule Extraction from Recurrent Neural Networks ATaxonomy and Review.pdf:pdf},
	isbn = {0899766053630350},
	issn = {0899-7667},
	journal = {Neural Computation},
	number = {6},
	pages = {1223--1263},
	title = {{Rule Extraction from Recurrent Neural Networks: ATaxonomy and Review}},
	volume = {17},
	year = {2005}
}
@article{Setiono1997c,
	abstract = {We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.},
	author = {Setiono, Rudy and Liu, Huan},
	doi = {10.1016/S0925-2312(97)00038-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Liu - 1997 - Neurolinear From neural networks to oblique decision rules.pdf:pdf},
	isbn = {3-540-62858-4},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Discretization,Oblique-rule,Pruning,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {1},
	pages = {1--24},
	title = {{Neurolinear: From neural networks to oblique decision rules}},
	volume = {17},
	year = {1997}
}
@article{Vellido2012a,
	abstract = {Peer Reviewed},
	author = {Vellido, Alfredo and Martin-Guerroro, J D and Lisboa, P},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vellido, Martin-Guerroro, Lisboa - 2012 - Making machine learning models interpretable.pdf:pdf},
	isbn = {9782874190490},
	journal = {20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
	mendeley-groups = {Report},
	number = {April},
	pages = {163--172},
	title = {{Making machine learning models interpretable}},
	year = {2012}
}

@article{Dong2017a,
	abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
	archivePrefix = {arXiv},
	arxivId = {1703.04096},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
	doi = {10.1109/CVPR.2017.110},
	eprint = {1703.04096},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2017 - Improving Interpretability of Deep Neural Networks with Semantic Information(2).pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {1063-6919},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	pages = {4306--4314},
	title = {{Improving Interpretability of Deep Neural Networks with Semantic Information}},
	url = {http://arxiv.org/abs/1703.04096},
	year = {2017}
}
@article{Li2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1503.00185v5},
	author = {Li, Jiwei and Luong, Minh-thang and Jurafsky, Dan and Hovy, Eduard},
	eprint = {arXiv:1503.00185v5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - When Are Tree Structures Necessary for Deep Learning of.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	title = {{When Are Tree Structures Necessary for Deep Learning of}},
	year = {2014}
}
@article{VanAssche2008,
	abstract = {The purpose of this column is to stimulate discussion among nurses regarding the importance of nursing theory-guided practice. The use of metaphor may shed light on defining nursing by its own terms. The time has come for nursing to recognize its worth as an autonomous discipline and own its contributions.},
	author = {{Van Assche}, Anneleen and Blockeel, Hendrik},
	doi = {10.1007/978-3-540-78469-2_26},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Assche, Blockeel - 2008 - Seeing the forest through the trees learning a comprehensible model from a first order ensemble.pdf:pdf},
	isbn = {3540784683},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Comprehensibility,Ensembles,First order decision trees},
	mendeley-groups = {Report/Biologicla domain,Report/Medical domain},
	pages = {269--279},
	pmid = {22228521},
	title = {{Seeing the forest through the trees learning a comprehensible model from a first order ensemble}},
	volume = {4894 LNAI},
	year = {2008}
}
@article{Bowman2015,
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	archivePrefix = {arXiv},
	arxivId = {1511.06349},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	eprint = {1511.06349},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:pdf},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	title = {{Generating Sentences from a Continuous Space}},
	url = {http://arxiv.org/abs/1511.06349},
	year = {2015}
}
@article{Burges2005a,
	author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
	doi = {10.1145/1102351.1102363},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:pdf},
	isbn = {1595931805},
	issn = {00243205},
	journal = {Icml 2005},
	keywords = {Learning to Rank,RankNet},
	mendeley-groups = {Progress Report},
	pages = {89--96},
	pmid = {16483612},
	title = {{Learning to rank using gradient descent}},
	year = {2005}
}
@article{Mikolov2013,
	abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lowerMikolov, T., Chen, K., Corrado, G., {\&} Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Arxiv, 1-12. http://doi.org/10.1162/153244303322533223 computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities. 1},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1301.3781v3},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	doi = {10.1162/153244303322533223},
	eprint = {arXiv:1301.3781v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {Arxiv},
	mendeley-groups = {Annotated/Word Vectors,Report/Features,Interim Review,Report},
	pages = {1--12},
	pmid = {18244602},
	title = {{Efficient Estimation of Word Representations in Vector Space}},
	url = {http://arxiv.org/abs/1301.3781},
	year = {2013}
}
@article{Sundararajan2017,
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	archivePrefix = {arXiv},
	arxivId = {1703.01365},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	eprint = {1703.01365},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundararajan, Taly, Yan - 2017 - Axiomatic Attribution for Deep Networks.pdf:pdf},
	issn = {1938-7228},
	mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3},
	title = {{Axiomatic Attribution for Deep Networks}},
	url = {http://arxiv.org/abs/1703.01365},
	year = {2017}
}
@article{Sun1998,
	abstract = {This paper presents a novel learning model CLARION, which is a hybrid model based on the two-level approach proposed by Sun. The model integrates neural, reinforcement, and symbolic learning methods to perform on-line, bottom-up learning (i.e., learning that goes from neural to symbolic representations). The model utilizes both procedural and declarative knowledge (in neural and symbolic representations, respectively), tapping into the synergy of the two types of processes. It was applied to deal with sequential decision tasks. Experiments and analyzes in various ways are reported that shed light on the advantages of the model.},
	author = {Sun, Ron and Peterson, Todd},
	doi = {10.1109/72.728364},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Peterson - 1998 - Autonomous learning of sequential tasks Experiments and analyzes.pdf:pdf},
	isbn = {0364-0213},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Hybrid models,Markov decision process,Multistrategy learning,Navigation,Reinforcement learning,Rule extraction,Sequential decision making},
	number = {6},
	pages = {1217--1234},
	pmid = {18255804},
	title = {{Autonomous learning of sequential tasks: Experiments and analyzes}},
	volume = {9},
	year = {1998}
}
@article{Kalchbrenner2014,
	abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25{\%} error reduction in the last task with respect to the strongest baseline.},
	archivePrefix = {arXiv},
	arxivId = {1404.2188v1},
	author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
	doi = {10.3115/v1/P14-1062},
	eprint = {1404.2188v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalchbrenner, Grefenstette, Blunsom - 2014 - A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
	isbn = {9781937284725},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	mendeley-groups = {!Paper 3/task/Sentiment treebank,Categories},
	pages = {655--665},
	title = {{A Convolutional Neural Network for Modelling Sentences}},
	url = {http://aclweb.org/anthology/P14-1062},
	year = {2014}
}
@article{Grosenick2008,
	abstract = {{\textless}para{\textgreater} Despite growing interest in applying machine learning to neuroimaging analyses, few studies have gone beyond classifying sensory input to directly predicting behavioral output. With spatial resolution on the order of millimeters and temporal r...},
	author = {Grosenick, Logan and Greer, Stephanie and Knutson, Brian},
	doi = {10.1109/TNSRE.2008.926701},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grosenick, Greer, Knutson - 2008 - Interpretable Classifiers for fMRI Improve Prediction of Purchases.pdf:pdf},
	isbn = {1558-0210 (Electronic)},
	issn = {15580210},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	keywords = {Accumbens,classification,discriminant,elastic net,frontal,functional magnetic resonance imaging (fMRI),human,insula,lasso,penalized discriminant analysis (PDA),prediction,purchasing,single-trial,sparse,spatiotemporal,support vector machine (SVM)},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {6},
	pages = {539--548},
	pmid = {19144586},
	title = {{Interpretable Classifiers for fMRI Improve Prediction of Purchases}},
	volume = {16},
	year = {2008}
}
@article{Thiagarajan2016,
	abstract = {With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior [1]. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted.},
	archivePrefix = {arXiv},
	arxivId = {1611.07429},
	author = {Thiagarajan, Jayaraman J. and Kailkhura, Bhavya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan},
	eprint = {1611.07429},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiagarajan et al. - 2016 - TreeView Peeking into Deep Neural Networks Via Feature-Space Partitioning.pdf:pdf},
	journal = {30th Conference on Neural Information Processing Systems (NIPS 2016)},
	mendeley-groups = {Annotated/Explanations,Annotated/Decision Trees},
	number = {Nips},
	title = {{TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning}},
	url = {http://arxiv.org/abs/1611.07429},
	year = {2016}
}
@article{Tallec2017,
	abstract = {Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Rescikit-learned Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT.},
	archivePrefix = {arXiv},
	arxivId = {1705.08209},
	author = {Tallec, Corentin and Ollivier, Yann},
	eprint = {1705.08209},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tallec, Ollivier - 2017 - Unbiasing Truncated Backpropagation Through Time.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pages = {1--13},
	title = {{Unbiasing Truncated Backpropagation Through Time}},
	url = {http://arxiv.org/abs/1705.08209},
	year = {2017}
}
@article{Wood,
	author = {Wood, Matthew and Rana, Omer and C, Eran Peer and C, Nathan Melly and C, Ryan Codrai and C, Matt Wood and C, Dom Routley and C, Mahima Dalal and C, Jamie Harkins and C, Jeremy Yee and C, Zara Siddique},
	pages = {1--4},
	title = {{Role in team}}
}
@article{Apte1994,
	abstract = {We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67{\%} recall/precision breakeven point to 80.5{\%}. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.},
	author = {Apt{\'{e}}, Chidanand and Damerau, Fred and Weiss, Sholom M.},
	doi = {10.1145/183422.183423},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Apt{\'{e}}, Damerau, Weiss - 1994 - Automated learning of decision rules for text categorization.pdf:pdf},
	isbn = {1046-8188},
	issn = {1046-8188},
	journal = {ACM Trans. Inf. Syst.},
	mendeley-groups = {Annotated/Decision Trees},
	number = {3},
	pages = {233--251},
	title = {{Automated learning of decision rules for text categorization}},
	url = {http://portal.acm.org/citation.cfm?id=183423{\&}dl=},
	volume = {12},
	year = {1994}
}
@article{Fyshe2014,
	abstract = {Vector space models (VSMs) represent word meanings as points in a high dimensional space. VSMs are typically created using a large text corpora, and so represent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics. Evaluations show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies. {\textcopyright} 2014 Association for Computational Linguistics.},
	archivePrefix = {arXiv},
	arxivId = {15334406},
	author = {Fyshe, Alona and Talukdar, Pp and Murphy, Brian and Mitchell, Tm},
	doi = {10.14440/jbm.2015.54.A},
	eprint = {15334406},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - 2014 - Interpretable Semantic Vectors from a Joint Model of Brain-and Text-Based Meaning.pdf:pdf},
	isbn = {9781937284725},
	issn = {0036-8075},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
	pages = {489--499},
	pmid = {25792328},
	title = {{Interpretable Semantic Vectors from a Joint Model of Brain-and Text-Based Meaning}},
	url = {http://www.cs.cmu.edu/{~}afyshe/papers/acl2014/jnnse{\_}acl2014.pdf},
	volume = {1},
	year = {2014}
}
@article{Words2002,
	author = {Words, Additional Key},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Words - 2002 - Cumulated Gain-based Evaluation of IR Techniques.pdf:pdf},
	mendeley-groups = {Report/Clustering/properties},
	number = {4},
	pages = {422--446},
	title = {{Cumulated Gain-based Evaluation of IR Techniques}},
	volume = {20},
	year = {2002}
}
@article{Yang2017,
	abstract = {We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.},
	archivePrefix = {arXiv},
	arxivId = {1711.03953},
	author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
	eprint = {1711.03953},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2017 - Breaking the Softmax Bottleneck A High-Rank RNN Language Model.pdf:pdf},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
	pages = {1--18},
	title = {{Breaking the Softmax Bottleneck: A High-Rank RNN Language Model}},
	url = {http://arxiv.org/abs/1711.03953},
	year = {2017}
}
@article{Lacoste-Julien2008,
	abstract = {Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in finding  a reduced dimensionality representation.  Specifically, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood.  By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification task and show how our model can identify shared topics across classes as well as class-dependent topics.},
	author = {Lacoste-Julien, Simon and Sha, Fei and Jordan, Michael I.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lacoste-Julien, Sha, Jordan - 2008 - DiscLDA Discriminative Learning for Dimensionality Reduction and Classification.pdf:pdf},
	isbn = {9781605609492},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {Computational, Information-Theoretic Learning with,Information Retrieval {\&} Textual Information Access,Learning/Statistics {\&} Optimisation},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,!Paper 3/task/newsgroups},
	pages = {1--8},
	title = {{DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification}},
	url = {http://eprints.pascal-network.org/archive/00004292/},
	volume = {21},
	year = {2008}
}
@article{Senin2013,
	author = {Senin, Pavel and Malinchik, Sergey},
	doi = {10.1109/ICDM.2013.52},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Senin, Malinchik - 2013 - SAX-VSM Interpretable time series classification using sax and vector space model.pdf:pdf},
	isbn = {978-0-7695-5108-1},
	issn = {15504786},
	journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
	keywords = {classification algorithms,time series analysis},
	mendeley-groups = {Report},
	number = {0704},
	pages = {1175--1180},
	title = {{SAX-VSM: Interpretable time series classification using sax and vector space model}},
	volume = {298},
	year = {2013}
}
@misc{Srivastava2014,
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller scikit-learns. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
	archivePrefix = {arXiv},
	arxivId = {1102.4807},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	booktitle = {Journal of Machine Learning Research (JMLR)},
	doi = {10.1214/12-AOS1000},
	eprint = {1102.4807},
	isbn = {1532-4435},
	issn = {15337928},
	keywords = {deep learning,model combination,neural networks,regularization},
	pages = {1929--1958},
	title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
	volume = {15},
	year = {2014}
}
@article{Hinton2015a,
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	archivePrefix = {arXiv},
	arxivId = {1503.02531},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	eprint = {1503.02531},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network.pdf:pdf},
	journal = {NIPS 2014 Deep Learning Workshop},
	mendeley-groups = {Literature Review,Interim Review},
	pages = {1--9},
	title = {{Distilling the Knowledge in a Neural Network}},
	url = {http://arxiv.org/abs/1503.02531},
	year = {2015}
}
@article{Narayanan2018,
	abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
	archivePrefix = {arXiv},
	arxivId = {1802.00682},
	author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
	eprint = {1802.00682},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine Learning Systems An Evaluation of the Human-Interpretabilit.pdf:pdf},
	pages = {1--21},
	title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
	url = {http://arxiv.org/abs/1802.00682},
	year = {2018}
}
@article{Cui,
	author = {Cui, Hang and Za{\"{i}}ane, Osmar R and Canada, T H},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui, Za{\"{i}}ane, Canada - Unknown - Hierarchical Structural Approach to Improving the Browsability of Web Search Engine Results.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	pages = {1--5},
	title = {{Hierarchical Structural Approach to Improving the Browsability of Web Search Engine Results}}
}
@article{Kulkarni2015b,
	abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
	archivePrefix = {arXiv},
	arxivId = {1503.03167},
	author = {Kulkarni, Td and Whitney, W},
	doi = {10.1063/1.4914407},
	eprint = {1503.03167},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network(2).pdf:pdf},
	issn = {10897550},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {2539--2547},
	title = {{Deep Convolutional Inverse Graphics Network}},
	url = {http://arxiv.org/abs/1503.03167},
	year = {2015}
}
@article{Chen,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1301.3618v2},
	author = {Chen, Danqi and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
	eprint = {arXiv:1301.3618v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - Neural Tensor Networks and Semantic Word Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	pages = {1--4},
	title = {{Neural Tensor Networks and Semantic Word Vectors}}
}
@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Fine-Tuning Vector Space Representations for Interpretable Text Classification.pdf:pdf},
	title = {{Fine-Tuning Vector Space Representations for Interpretable Text Classification}}
}
@article{Ai2016,
	author = {Ai, Qingyao and Yang, Liu and Guo, Jiafeng and Croft, W Bruce},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ai et al. - 2016 - Analysis of the Paragraph Vector Model for Information Retrieval.pdf:pdf},
	isbn = {9781450344975},
	keywords = {language model,paragraph vector},
	mendeley-groups = {Annotated/Document representation},
	title = {{Analysis of the Paragraph Vector Model for Information Retrieval}},
	year = {2016}
}
@article{Press2016,
	abstract = {We study the topmost scikit-learn matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that scikit-learn tying can reduce the size of neural translation models to less than half of their original size without harming their performance.},
	archivePrefix = {arXiv},
	arxivId = {1608.05859},
	author = {Press, Ofir and Wolf, Lior},
	eprint = {1608.05859},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Press, Wolf - 2016 - Using the Output Embedding to Improve Language Models.pdf:pdf},
	isbn = {9781510838604},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs,!Paper 3/task/Sentiment treebank},
	title = {{Using the Output Embedding to Improve Language Models}},
	url = {http://arxiv.org/abs/1608.05859},
	year = {2016}
}
@article{Salton1975,
	author = {Salton, G and Wong, A and Yang, C S},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Wong, Yang - 1975 - AVector Space Model for Automatic Indexing.pdf:pdf},
	keywords = {and phrases,automatic indexing,automatic information,content analysis,document,retrieval},
	mendeley-groups = {Report/Features},
	number = {11},
	title = {{AVector Space Model for Automatic Indexing}},
	volume = {18},
	year = {1975}
}
@article{Elazmeh2007,
	abstract = {The paper presents ongoing issues, challenges, and difficulties we face in applying machine learning methods to retrospectively collected clinical data. The objective of our research is to build a reliable prediction model for early assessment of emergency pediatric asthma exacerbations. This predictive model should be able to distinguish between patients with mild or moderate/severe asthma attacks at a medically acceptable level of performance. Our real-life data set presents us with some difficult challenges which we communicate in this paper. Our approach to overcoming some of these difficulties is to use external expert knowledge to aid with classification by decomposing the classification problem into a two-tier concept, where concepts can be explicitly described in terms of the external knowledge source. Such an approach also has the advantage of significantly reducing the size of the training set required. Copyright {\textcopyright} 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	author = {Elazmeh, W.a and Matwin, S.a and O'Sullivan, D.b and Michalowski, W.b and Farion, K.c},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elazmeh et al. - 2007 - Insights from predicting pediatric asthma exacerbations from retrospective clinical data.pdf:pdf},
	isbn = {9781577353324},
	journal = {AAAI Workshop - Technical Report},
	keywords = {Artificial intelligence,Classification (of infor,Clinical data,Evaluation methods,Expert knowledg,Learning systems},
	mendeley-groups = {Report/Medical domain},
	pages = {10--15},
	title = {{Insights from predicting pediatric asthma exacerbations from retrospective clinical data}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-51849160749{\&}partnerID=40{\&}md5=345e9c0a3f9c793b5c69b25dfab7d721},
	volume = {WS-07-05},
	year = {2007}
}
@article{Andrews1995a,
	abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy. {\textcopyright} 1995.},
	author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
	doi = {10.1016/0950-7051(96)81920-4},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Diederich, Tickle - 1995 - Survey and critique of techniques for extracting rules from trained artificial neural networks.pdf:pdf},
	isbn = {09507051 (ISSN)},
	issn = {09507051},
	journal = {Knowledge-Based Systems},
	keywords = {fuzzy neural networks,inferencing,knowledge insertion,rule extraction,rule refinement},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {6},
	pages = {373--389},
	title = {{Survey and critique of techniques for extracting rules from trained artificial neural networks}},
	volume = {8},
	year = {1995}
}
@article{Kulkarni2015a,
	abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
	archivePrefix = {arXiv},
	arxivId = {1503.03167},
	author = {Kulkarni, Td and Whitney, W},
	doi = {10.1063/1.4914407},
	eprint = {1503.03167},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network.pdf:pdf},
	issn = {10897550},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Progress Report},
	pages = {2539--2547},
	title = {{Deep Convolutional Inverse Graphics Network}},
	url = {http://arxiv.org/abs/1503.03167},
	year = {2015}
}
@article{Naacl2019,
	author = {Naacl, Anonymous},
	file = {:D$\backslash$:/Downloads/Work/naaclhlt2019{\_}latex{\_}{\_}Copy{\_}.pdf:pdf},
	pages = {1--9},
	title = {{No Title}},
	year = {2019}
}
@article{Thrun1995,
	abstract = {Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neu-ral networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illus-trate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.},
	author = {Thrun, Sebastian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun - 1995 - Extracting Rules from Artificial Neural Networks with Distributed Representations(2).pdf:pdf},
	isbn = {1049-5258},
	journal = {Advances in Neural Information Processing Systems 7},
	mendeley-groups = {Progress Report},
	title = {{Extracting Rules from Artificial Neural Networks with Distributed Representations}},
	year = {1995}
}
@article{Glorot2011a,
	author = {Glorot, X and Bordes, a and Bengio, Y},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain adaptation for large-scale sentiment classification A deep learning approach.pdf:pdf},
	keywords = {auto-encoders,deep-learning},
	mendeley-groups = {Papers/Paper 1,Interim Review,Report},
	number = {1},
	title = {{Domain adaptation for large-scale sentiment classification: A deep learning approach}},
	url = {http://eprints.pascal-network.org/archive/00008597/},
	year = {2011}
}
@misc{,
	title = {{{\~{}}1245963944HI}}
}
@article{Rezende2014,
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	archivePrefix = {arXiv},
	arxivId = {1401.4082},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1401.4082},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende, Mohamed, Wierstra - 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:pdf},
	isbn = {9781634393973},
	issn = {10495258},
	mendeley-groups = {!Paper 3/Bayesian Networks},
	pmid = {23459267},
	title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
	url = {http://arxiv.org/abs/1401.4082},
	year = {2014}
}
@article{Mao2014a,
	abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1410.1090v1},
	author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
	eprint = {arXiv:1410.1090v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
	journal = {arXiv:1410.1090 [cs]},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Explain Images with Multimodal Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1410.1090{\%}5Cnhttp://www.arxiv.org/pdf/1410.1090.pdf},
	year = {2014}
}
@article{Koppula,
	archivePrefix = {arXiv},
	arxivId = {1802.03816},
	author = {Koppula, Skanda and Sim, Khe Chai and Chin, Kean},
	eprint = {1802.03816},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koppula, Sim, Chin - Unknown - Understanding Recurrent Neural State Using Memory Signatures.pdf:pdf},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	title = {{Understanding Recurrent Neural State Using Memory Signatures}}
}
@article{Setiono1997b,
	abstract = {We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.},
	author = {Setiono, Rudy and Liu, Huan},
	doi = {10.1016/S0925-2312(97)00038-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Liu - 1997 - Neurolinear From neural networks to oblique decision rules(2).pdf:pdf},
	isbn = {3-540-62858-4},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Discretization,Oblique-rule,Pruning,Rule extraction},
	number = {1},
	pages = {1--24},
	title = {{Neurolinear: From neural networks to oblique decision rules}},
	volume = {17},
	year = {1997}
}
@article{Vincent2008a,
	abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	doi = {10.1145/1390156.1390294},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent et al. - 2008 - Extracting and composing robust features with denoising autoencoders.pdf:pdf},
	isbn = {9781605582054},
	issn = {1605582050},
	journal = {Proceedings of the 25th international conference on Machine learning},
	mendeley-groups = {Papers/Paper 1,Report/Features,Report},
	pages = {1096--1103},
	title = {{Extracting and composing robust features with denoising autoencoders}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.149.8111{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1390156.1390294},
	year = {2008}
}
@article{Vandewiele2016,
	abstract = {Models obtained by decision tree induction techniques excel in being interpretable.However, they can be prone to overfitting, which results in a low predictive performance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial. To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques.},
	archivePrefix = {arXiv},
	arxivId = {1611.05722},
	author = {Vandewiele, Gilles and Janssens, Olivier and Ongenae, Femke and {De Turck}, Filip and {Van Hoecke}, Sofie},
	eprint = {1611.05722},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vandewiele et al. - 2016 - GENESIM genetic extraction of a single, interpretable model.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees,Report},
	number = {Nips},
	title = {{GENESIM: genetic extraction of a single, interpretable model}},
	url = {http://arxiv.org/abs/1611.05722},
	year = {2016}
}
@article{Taddy2015,
	abstract = {There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.},
	archivePrefix = {arXiv},
	arxivId = {1504.07295},
	author = {Taddy, Matt},
	eprint = {1504.07295},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taddy - 2015 - Document Classification by Inversion of Distributed Language Representations.pdf:pdf},
	isbn = {9781941643730},
	journal = {Proceedings of the 53rd meeting of the Association for Computational Linquistics (ACL'15)},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {45--49},
	title = {{Document Classification by Inversion of Distributed Language Representations}},
	url = {http://arxiv.org/abs/1504.07295},
	year = {2015}
}
@article{Glorot2011,
	abstract = {The exponential increase in the availability of online reviews and recommendations makes sentiment classi cation an interesting topic in academic and industrial research. Reviews can span so many di erent domains that it is dicult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classi ers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classi ers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain Adaptation for Large-Scale Sentiment Classification A Deep Learning Approach(3).pdf:pdf},
	isbn = {978-1-4503-0619-5},
	journal = {Proceedings of the 28th International Conference on Machine Learning},
	number = {1},
	pages = {513--520},
	title = {{Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach}},
	url = {http://www.icml-2011.org/papers/342{\_}icmlpaper.pdf},
	year = {2011}
}
@article{Oquab2015,
	author = {Oquab, Maxime},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab - 2015 - Is object localization for free - Weakly-supervised learning with convolutional neural networks To cite this version Is.pdf:pdf},
	isbn = {9781467369640},
	mendeley-groups = {Progress Report},
	number = {iii},
	title = {{Is object localization for free ? - Weakly-supervised learning with convolutional neural networks To cite this version : Is object localization for free ? - Weakly-supervised learning with convolutional neural networks}},
	year = {2015}
}
@article{Talley2011,
	author = {Talley, Edmund M and Newman, David and Mimno, David and Herr, Bruce W and Wallach, Hanna M and Burns, Gully A P C and Leenders, A G Miriam and Mccallum, Andrew},
	doi = {10.1038/nmeth.1619},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Talley et al. - 2011 - correspondEnce Database of NIH grants using machine-learned categories and graphical clustering Predicting protei.pdf:pdf},
	isbn = {2712009002},
	issn = {1548-7091},
	journal = {Nature Publishing Group},
	mendeley-groups = {Report/Clustering},
	number = {6},
	pages = {443--444},
	publisher = {Nature Publishing Group},
	title = {{correspondEnce Database of NIH grants using machine-learned categories and graphical clustering Predicting protein associations with long noncoding RNAs}},
	url = {http://dx.doi.org/10.1038/nmeth.1619},
	volume = {8},
	year = {2011}
}
@article{Arora2012,
	abstract = {Topic Modeling is an approach used for automatic comprehension and classification of data in a variety of settings, and perhaps the canonical application is in uncovering thematic structure in a corpus of documents. A number of foundational works both in machine learning and in theory have suggested a probabilistic model for documents, whereby documents arise as a convex combination of (i.e. distribution on) a small number of topic vectors, each topic vector being a distribution on words (i.e. a vector of word-frequencies). Similar models have since been used in a variety of application areas; the Latent Dirichlet Allocation or LDA model of Blei et al. is especially popular. Theoretical studies of topic modeling focus on learning the model's parameters assuming the data is actually generated from it. Existing approaches for the most part rely on Singular Value Decomposition(SVD), and consequently have one of two limitations: these works need to either assume that each document contains only one topic, or else can only recover the span of the topic vectors instead of the topic vectors themselves. This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main tool in this context, which is an analog of SVD where all vectors are nonnegative. Using this tool we give the first polynomial-time algorithm for learning topic models without the above two limitations. The algorithm uses a fairly mild assumption about the underlying topic matrix called separability, which is usually found to hold in real-life data. A compelling feature of our algorithm is that it generalizes to models that incorporate topic-topic correlations, such as the Correlated Topic Model and the Pachinko Allocation Model. We hope that this paper will motivate further theoretical results that use NMF as a replacement for SVD - just as NMF has come to replace SVD in many applications.},
	archivePrefix = {arXiv},
	arxivId = {1204.1956},
	author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur},
	doi = {10.1109/FOCS.2012.49},
	eprint = {1204.1956},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Ge, Moitra - 2012 - Learning topic models - Going beyond SVD.pdf:pdf},
	isbn = {978-0-7695-4874-6},
	issn = {02725428},
	journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {1--10},
	title = {{Learning topic models - Going beyond SVD}},
	year = {2012}
}
@article{Krakovna2016a,
	abstract = {As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks, state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining a long short-term memory (LSTM) model with a hidden Markov model (HMM), a simpler and more transparent model. We add the HMM state probabilities to the output layer of the LSTM, and then train the HMM and LSTM either sequentially or jointly. The LSTM can make use of the information from the HMM, and fill in the gaps when the HMM is not performing well. A small hybrid model usually performs better than a standalone LSTM of the same size, especially on smaller data sets. We test the algorithms on text data and medical time series data, and find that the LSTM and HMM learn complementary information about the features in the text.},
	archivePrefix = {arXiv},
	arxivId = {1611.05934},
	author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
	eprint = {1611.05934},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krakovna, Doshi-Velez - 2016 - Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models(2).pdf:pdf},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	number = {Whi},
	title = {{Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}},
	url = {http://arxiv.org/abs/1611.05934},
	year = {2016}
}
@article{Rivest1987,
	abstract = {This paper introduces a new representation for Boolean functions, called decision lists,  and shows that they are eciently learnable from examples. More precisely, this result  is established for $\backslash$k-DL" {\{} the set of decision lists with conjunctive clauses of size k at  each decision. Since k-DL properly includes other well-known techniques for representing  Boolean functions such as k-CNF (formulae in conjunctive normal form with at most k  literals per clause), k-DNF (formulae in disjunctive normal form with at most k literals  per term), and decision trees of depth k, our result strictly increases the set of functions  which are known to be polynomially learnable, in the sense of Valiant (1984). Our proof is  constructive: we present an algorithm which can eciently construct an element of k-DL  consistent with a given set of examples, if one exists.}},
	author = {Rivest, Ronald L.},
	doi = {10.1023/A:1022607331053},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivest - 1987 - Learning Decision Lists.pdf:pdf},
	issn = {15730565},
	journal = {Machine Learning},
	keywords = {Boolean formulae,Learning from examples,decision lists,polynomial-time identification},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	number = {3},
	pages = {229--246},
	title = {{Learning Decision Lists}},
	volume = {2},
	year = {1987}
}
@article{Berger,
	author = {Berger, Mark J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - Unknown - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
	pages = {1--8},
	title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}}
}
@article{Wisdom2016,
	abstract = {Recurrent neural networks (RNNs) are powerful and effective for processing sequential data. However, RNNs are usually considered "black box" models whose internal structure and learned parameters are not interpretable. In this paper, we propose an interpretable RNN based on the sequential iterative soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery problem, which models a sequence of correlated observations with a sequence of sparse latent vectors. The architecture of the resulting SISTA-RNN is implicitly defined by the computational structure of SISTA, which results in a novel stacked RNN architecture. Furthermore, the scikit-learns of the SISTA-RNN are perfectly interpretable as the parameters of a principled statistical model, which in this case include a sparsifying dictionary, iterative step size, and regularization parameters. In addition, on a particular sequential compressive sensing task, the SISTA-RNN trains faster and achieves better performance than conventional state-of-the-art black box RNNs, including long-short term memory (LSTM) RNNs.},
	archivePrefix = {arXiv},
	arxivId = {1611.07252},
	author = {Wisdom, Scott and Powers, Thomas and Pitton, James and Atlas, Les},
	eprint = {1611.07252},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wisdom et al. - 2016 - Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery.pdf:pdf},
	mendeley-groups = {!Paper 3/Interpretable LSTMs},
	number = {Nips},
	pages = {1--8},
	title = {{Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery}},
	url = {http://arxiv.org/abs/1611.07252},
	year = {2016}
}
@article{Bach2015,
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
	doi = {10.1371/journal.pone.0130140},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.pdf:pdf},
	isbn = {10.1371/journal.pone.0130140},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Report},
	number = {7},
	pages = {1--46},
	pmid = {26161953},
	title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
	volume = {10},
	year = {2015}
}
@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Interim Report Student Details Section A Student Self-Assessment A . 1 Thesis Title and Hypothesis A . 2 Overall Pr.pdf:pdf},
	title = {{Interim Report Student Details Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis A . 2 Overall Progress}}
}
@article{Huang,
	author = {Huang, Sheng and Peng, Xueping and Niu, Zhendong and Wang, Kunshan},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - News Topic Detection based on Hierarchical Clustering and Named Entity.pdf:pdf},
	isbn = {9781612847290},
	keywords = {-news topic detection,agglomerative hierarchical,clustering,named entity,vector space model},
	mendeley-groups = {Report/Clustering},
	pages = {280--284},
	title = {{News Topic Detection based on Hierarchical Clustering and Named Entity}}
}
@article{Mnih2008,
	abstract = {Neural probabilistic language models (NPLMs) have been shown to be competi- tive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non- hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1},
	author = {Mnih, Andriy and Hinton, Geoffrey E.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih, Hinton - 2008 - A Scalable Hierarchical Distributed Language Model.pdf:pdf},
	isbn = {9781605609492},
	journal = {Advances in Neural Information Processing Systems},
	pages = {1--8},
	title = {{A Scalable Hierarchical Distributed Language Model.}},
	url = {http://discovery.ucl.ac.uk/63249/},
	year = {2008}
}
@article{Ross2017,
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	archivePrefix = {arXiv},
	arxivId = {1703.03717},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	eprint = {1703.03717},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Annotated/Explanations,!Paper 3/task/newsgroups},
	title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
	url = {http://arxiv.org/abs/1703.03717},
	year = {2017}
}
@article{BurelGregoire2018,
	abstract = {Social media posts tend to provide valuable reports during crises. However, this information can be hidden in large amounts of unrelated documents. Providing tools that automatically identify relevant posts, event types (e.g., hurricane, floods, etc.) and information categories (e.g., reports on affected individuals, donations and volunteering, etc.) in social media posts is vital for their efficient handling and consumption. We introduce the Crisis Event Extraction Service (CREES), an open-source web API that automatically classifies posts during crisis situations. The API provides annotations for crisis-related documents, event types and information categories through an easily deployable and accessible web API that can be integrated into multiple platform and tools. The annotation service is backed by Convolutional Neural Networks (CNNs) and validated against traditional machine learning models. Results show that the CNN-based API results can be relied upon when dealing with specific crises with the benefits associated with the usage word embeddings.},
	author = {{Burel, Gr{\'{e}}goire} and Alani, Harith},
	file = {:E$\backslash$:/gburel{\_}iscram18.pdf:pdf},
	title = {{Crisis Event Extraction Service ( CREES ) - Automatic Detection and Classification of Crisis-related Content on Social Media on Social Media}},
	year = {2018}
}
@article{F??rnkranz2008a,
	abstract = {Label ranking studies the problem of learning a mapping from instances to rankings over a predefined set of labels. Hitherto existing approaches to label ranking implicitly operate on an underlying (utility) scale which is not calibrated in the sense that it lacks a natural zero point. We propose a suitable extension of label ranking that incorporates the calibrated scenario and substantially extends the expressive power of these approaches. In particular, our extension suggests a conceptually novel technique for extending the common learning by pairwise comparison approach to the multilabel scenario, a setting previously not being amenable to the pairwise decomposition technique. The key idea of the approach is to introduce an artificial calibration label that, in each example, separates the relevant from the irrelevant labels. We show that this technique can be viewed as a combination of pairwise preference learning and the conventional relevance classification technique, where a separate classifier is trained to predict whether a label is relevant or not. Empirical results in the area of text categorization, image classification and gene analysis underscore the merits of the calibrated model in comparison to state-of-the-art multilabel learning methods.},
	author = {F??rnkranz, Johannes and H??llermeier, Eyke and {Loza Menc??a}, Eneldo and Brinker, Klaus},
	doi = {10.1007/s10994-008-5064-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frnkranz et al. - 2008 - Multilabel classification via calibrated label ranking.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Multi-label classification,Preference learning,Ranking},
	number = {2},
	pages = {133--153},
	title = {{Multilabel classification via calibrated label ranking}},
	volume = {73},
	year = {2008}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.dropbox:dropbox},
	title = {{No Title}}
}
@article{Melamud2016,
	abstract = {We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from scikit-learned contexts of substitute words.},
	archivePrefix = {arXiv},
	arxivId = {1601.00893},
	author = {Melamud, Oren and McClosky, David and Patwardhan, Siddharth and Bansal, Mohit},
	eprint = {1601.00893},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melamud et al. - 2016 - The Role of Context Types and Dimensionality in Learning Word Embeddings.pdf:pdf},
	isbn = {9781941643914},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Sentiment treebank},
	title = {{The Role of Context Types and Dimensionality in Learning Word Embeddings}},
	url = {http://arxiv.org/abs/1601.00893},
	year = {2016}
}
@article{Goyal2017,
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\~{}}90{\%} scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.},
	archivePrefix = {arXiv},
	arxivId = {1706.02677},
	author = {Goyal, Priya and Doll{\'{a}}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	doi = {10.1561/2400000003},
	eprint = {1706.02677},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet in 1 Hour.pdf:pdf},
	isbn = {9781601987167},
	issn = {2167-3888},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
	url = {http://arxiv.org/abs/1706.02677},
	year = {2017}
}
@article{Manly2017,
	author = {Manly, Bryan F J and Alberto, Jorge A Navarro},
	doi = {10.18637/jss.v078.b03},
	file = {:E$\backslash$:/Downloads/Work/v78b03 (1).pdf:pdf},
	isbn = {9781498728966},
	number = {June},
	title = {{Journal of Statistical Software}},
	volume = {78},
	year = {2017}
}
@article{Declerck2000,
	author = {Declerck, Carolyn H and Boone, Christophe and Emonds, Griet},
	file = {:E$\backslash$:/Downloads/Work/0c0bc5c5.pdf:pdf},
	number = {0},
	title = {{When do people cooperate ? The neuroeconomics of prosocial decision making Faculty of Applied Economics}},
	volume = {32},
	year = {2000}
}
@article{Conll,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/243{\_}file{\_}Submission.pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Ager2012,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/243{\_}Paper.pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Levy2015,
	author = {Levy, Omer},
	file = {:E$\backslash$:/Downloads/Work/570-1656-1-PB.pdf:pdf},
	pages = {211--225},
	title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
	volume = {3},
	year = {2015}
}
@article{Nguyen2015,
	author = {Nguyen, Dat Quoc and Billingsley, Richard and Du, Lan and Johnson, Mark},
	file = {:E$\backslash$:/Downloads/Work/582-1696-1-PB.pdf:pdf},
	pages = {299--313},
	title = {{Improving Topic Models with Latent Feature Word Representations}},
	volume = {3},
	year = {2015}
}
@article{Chung,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1412.3555v1},
	author = {Chung, Junyoung},
	eprint = {arXiv:1412.3555v1},
	file = {:E$\backslash$:/Downloads/Work/1412.3555.pdf:pdf},
	pages = {1--9},
	title = {{Gated Recurrent Neural Networks on Sequence Modeling arXiv : 1412 . 3555v1 [ cs . NE ] 11 Dec 2014}}
}
@article{Zhou2015,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1511.08630v2},
	author = {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C M},
	eprint = {arXiv:1511.08630v2},
	file = {:E$\backslash$:/Downloads/Work/1511.08630.pdf:pdf},
	title = {{A C-LSTM Neural Network for Text Classification}},
	year = {2015}
}
@article{Paul2008,
	author = {Paul, Michael and Girju, Roxana},
	file = {:E$\backslash$:/Downloads/Work/1730-8153-1-PB.pdf:pdf},
	keywords = {Technical Papers -- Machine Learning},
	pages = {545--550},
	title = {{A Two-Dimensional Topic-Aspect Model for Discovering Multi-Faceted Topics}},
	year = {2008}
}
@article{Foster2011,
	author = {Foster, Jennifer and Wagner, Joachim and Roux, Joseph Le and Hogan, Stephen and Nivre, Joakim and Hogan, Deirdre and Genabith, Josef Van},
	file = {:E$\backslash$:/Downloads/Work/3912-16625-1-PB.pdf:pdf},
	pages = {20--25},
	title = {{{\#} hardtoparse : POS Tagging and Parsing the Twitterverse Evaluation of WSJ-Trained Resources}},
	year = {2011}
}
@book{,
	file = {:E$\backslash$:/Downloads/Work/10290.pdf:pdf},
	isbn = {9780262039406},
	title = {{No Title}}
}
@article{Wu2018,
	author = {Wu, Mike and Hughes, Michael C and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-velez, Finale},
	file = {:E$\backslash$:/Downloads/Work/16285-76627-1-PB.pdf:pdf},
	keywords = {Humans and Artificial Intelligence Track},
	pages = {1670--1678},
	title = {{Beyond Sparsity : Tree Regularization of Deep Models for Interpretability}},
	year = {2018}
}
@article{Baumel2007,
	author = {Baumel, Tal and Nassour-kassis, Jumana and Cohen, Raphael and Elhadad, Michael},
	file = {:E$\backslash$:/Downloads/Work/16881-75991-1-PB.pdf:pdf},
	pages = {409--416},
	title = {{Multi-Label Classification of Patient Notes : Case Study on ICD Code Assignment}},
	year = {2007}
}
@article{Ros2017,
	author = {Ros, Andrew Slavin and Doshi-velez, Finale},
	file = {:E$\backslash$:/Downloads/Work/17337-76626-1-PB.pdf:pdf},
	keywords = {Humans and Artificial Intelligence Track},
	pages = {1660--1669},
	title = {{Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients}},
	year = {2017}
}
@article{Yan2015,
	author = {Yan, Yan and Yin, Xu-cheng and Li, Sujian and Yang, Mingyuan and Hao, Hong-wei},
	file = {:E$\backslash$:/Downloads/Work/650527.pdf:pdf},
	title = {{Learning Document Semantic Representation with Hybrid Deep Belief Network}},
	volume = {2015},
	year = {2015}
}
@article{Abe2013,
	author = {Abe, Shigeo},
	doi = {10.1109/21.362960},
	file = {:E$\backslash$:/Downloads/Work/90000209.pdf:pdf},
	number = {February 1995},
	title = {{Fuzzy rules extraction directly from numerical data for function Title University Repository : Kernel Fuzzy rules extraction directly from numerical data for function approximation}},
	year = {2013}
}
@article{Anderson,
	author = {Anderson, T W and Wiley, John},
	file = {:E$\backslash$:/Downloads/Work/194866033.pdf:pdf},
	title = {{An Introduction to Multivariate Statistical Analysis}}
}
@article{Westerveld,
	author = {Westerveld, Thijs and Vries, Arjen De and Jong, Franciska De},
	file = {:E$\backslash$:/Downloads/Work/9783540728948-c1.pdf:pdf},
	title = {{6 Generative Probabilistic Models}}
}
@article{Ager,
	author = {Ager, Thomas},
	file = {:E$\backslash$:/Downloads/Work/Ager.pdf:pdf},
	title = {{Neural Networks}}
}
@article{Ager2012a,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/Ager-CoNLL18 (1).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Larochelle2007,
	author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James},
	doi = {10.1145/1273496.1273556},
	file = {:E$\backslash$:/Downloads/Work/An{\_}empirical{\_}evaluation{\_}of{\_}deep{\_}architectures{\_}on{\_}p.pdf:pdf},
	number = {June 2014},
	title = {{An empirical evaluation of deep architectures on problems with many factors of variation An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation}},
	year = {2007}
}
@article{Corcoran2018,
	author = {Corcoran, Podraig},
	file = {:E$\backslash$:/Downloads/Work/AnnualProgressReview (2) (1).pdf:pdf},
	number = {October},
	title = {{Part 1 : PhD progress review}},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/AnnualProgressReview (2)-3.pdf:pdf},
	title = {{Part {\%} 1 {\%}:{\%} PhD {\%} progress {\%} review {\%}}}
}
@book{Washington,
	author = {Washington, Allyn J},
	file = {:E$\backslash$:/Downloads/Work/Basic Technical Mathematics with Calculus 11th ed - Allyn J. Washington, Richard Evans (Pearson, 2018).pdf:pdf},
	isbn = {9780134437736},
	title = {{Mathematics with Calculus}}
}
@article{Abdollahi2018,
	author = {Abdollahi, Behnoush and Nasraoui, Olfa},
	doi = {10.1007/978-3-319-90403-0},
	file = {:E$\backslash$:/Downloads/Work/behnoush-book-chapter5.pdf:pdf},
	isbn = {9783319904030},
	number = {June},
	pages = {0--15},
	title = {{Transparency in Fair Machine Learning : the Case of Explainable Recommender Chapter 1 Transparency in Fair Machine Learning : the Case of Explainable Recommender Systems}},
	year = {2018}
}
@article{Binns2018,
	author = {Binns, Reuben and Kleek, Max Van and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
	file = {:E$\backslash$:/Downloads/Work/Binns2018Reducing{\_}PrePrint.pdf:pdf},
	isbn = {9781450356206},
	title = {{‘ It ' s Reducing a Human Being to a Percentage '; Perceptions of Justice in Algorithmic Decisions}},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/boardingPass.pdf:pdf},
	pages = {7},
	title = {{Ager / Thomas Mr}},
	year = {2018}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCE04062018 (1).pdf:pdf},
	title = {{CCE04062018 (1).pdf}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCE04062018.pdf:pdf},
	title = {{CCE04062018.pdf}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCF06092019 (1).pdf:pdf},
	title = {{CCF06092019 (1).pdf}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCF06092019.pdf:pdf},
	title = {{CCF06092019.pdf}}
}
@article{Johnson,
	author = {Johnson, Rie},
	file = {:E$\backslash$:/Downloads/Work/cnn-semi-nips15.pdf:pdf},
	pages = {1--9},
	title = {{Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Completed Annual Progress Review Form (1).pdf:pdf},
	title = {{Part {\%} 1 {\%}:{\%} PhD {\%} progress {\%} review {\%}}}
}
@article{Corcoran2018a,
	author = {Corcoran, Podraig},
	file = {:E$\backslash$:/Downloads/Work/Completed Annual Progress Review Form.pdf:pdf},
	number = {October},
	title = {{Part 1 : PhD progress review}},
	year = {2018}
}
@article{Gardenfors2014,
	author = {G{\"{a}}rdenfors, Peter},
	doi = {10.1007/978-1-4020-9877-2},
	file = {:E$\backslash$:/Downloads/Work/Conceptual{\_}Spaces.pdf:pdf},
	isbn = {9781402098772},
	number = {October 2009},
	title = {{Conceptual spaces}},
	year = {2014}
}
@article{Ager2012b,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}CaptionFix (1).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Ager2012c,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}CaptionFix (2).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Ager2012d,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}CaptionFix.pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Conlla,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (2).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Conllb,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (3).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Conllc,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (4).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Conlld,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (5).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Ager2012e,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (7).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Ager2012f,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (8).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Ager2012g,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (11).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Ager2012h,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (12).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/CTAX1{\_}1214824-3{\_}09.pdf:pdf},
	pages = {1214824},
	title = {{COUNCIL TAX - Student Certificate TRETH CYNGOR - Tystysgrif Myfyriwr This certificate relates to the person , not the address /}},
	year = {2019}
}
@article{Rosenfield2016,
	author = {Rosenfield, Mark},
	file = {:E$\backslash$:/Downloads/Work/CVSOIPpaper.pdf:pdf},
	number = {January},
	title = {{Computer vision syndrome (a.k.a. digital eye strain)}},
	year = {2016}
}
@article{Lee2017,
	author = {Lee, Jaesong and Shin, Joong-hwi and Kim, Jun-seok and Corp, Naver},
	file = {:E$\backslash$:/Downloads/Work/D17-2021.pdf:pdf},
	pages = {121--126},
	title = {{Interactive Visualization and Manipulation of Attention-based Neural Machine Translation}},
	year = {2017}
}
@article{Lenci2008,
	author = {Lenci, Alessandro},
	file = {:E$\backslash$:/Downloads/Work/Distributional{\_}semantics{\_}in{\_}linguistic{\_}and{\_}cogniti.pdf:pdf},
	number = {January},
	title = {{Distributional semantics in linguistic and cognitive research Distributional semantics in linguistic and cognitive research}},
	year = {2008}
}
@article{Carvalho2019,
	author = {Carvalho, Diogo V and Pereira, Eduardo M and Cardoso, Jaime S},
	doi = {10.3390/electronics8080832},
	file = {:E$\backslash$:/Downloads/Work/electronics-08-00832.pdf:pdf},
	keywords = {explainability,interpretability,machine learning,xai},
	pages = {1--34},
	title = {{Machine Learning Interpretability : A Survey on Methods and Metrics}},
	year = {2019}
}
@article{Emnlp2018,
	author = {Emnlp, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/EMNLP2018{\_}{\_}{\_}Tom.pdf:pdf},
	pages = {1--8},
	title = {{Fine-Tuning Vector Space Representations for Interpretable Text Classification}},
	year = {2018}
}
@article{Lastname2011,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/feedback ch3.pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Lastname2011a,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/feedback ch25.pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Cernovsky2015,
	author = {Cernovsky, Zack},
	doi = {10.1080/0092623X.2015.1070779},
	file = {:E$\backslash$:/Downloads/Work/FetishPreferences{\_}aspublished{\_}Sept2015.pdf:pdf},
	number = {July},
	title = {{Fetishistic Preferences of Clients as Ranked by a Sex Worker}},
	year = {2015}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Frameworks{\_}for{\_}Properties{\_}Possible{\_}Worlds{\_}vs{\_}Conce.pdf:pdf},
	number = {January 1995},
	title = {{Frameworks for Properties : Possible Worlds vs . Conceptual Spaces Frameworks for Properties : Possible Worlds vs . Conceptual Spaces}},
	year = {2015}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/max-margin-tutorial.pdf:pdf},
	title = {{No Title}}
}
@article{Science2018,
	author = {Science, Computer and Monitoring, Progress and Report, Interim Review},
	file = {:E$\backslash$:/Downloads/Work/InterimReviewForm (2).pdf:pdf},
	title = {{Interim Report Student Details Schockaert Podraig Corcoran October 2015 October 2018 December 2016 Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis Obtaining and improving properties , rankings and rules from neural networks that process text data A . 2 Overall Progress}},
	year = {2018}
}
@article{Hill2014,
	author = {Hill, Harold},
	doi = {10.1068/p220887},
	file = {:E$\backslash$:/Downloads/Work/JohnstonHillCarmen92.pdf:pdf},
	number = {February 1993},
	title = {{Independent effects of lighting , orientation , and stereopsis on the hollow- face illusion}},
	year = {2014}
}
@article{Rosales-p2018,
	author = {Rosales-p, Alejandro},
	doi = {10.1109/MCI.2018.2806997},
	file = {:E$\backslash$:/Downloads/Work/MC2ESVM.pdf:pdf},
	number = {April},
	title = {{MC2ESVM : Multiclass Classification Based on Cooperative Evolution of Support MC 2 ESVM : Multiclass Classification based on Cooperative Evolution of Support Vector Machines}},
	year = {2018}
}
@book{,
	file = {:E$\backslash$:/Downloads/Work/Methods of Multivariate Analysis 3rd Ed.pdf:pdf},
	isbn = {9780470178966},
	title = {{METHODS OF MULTIVARIATE ANALYSIS}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Multivariate Analysis of Ecological Data using CANOCO.pdf:pdf},
	title = {{Multivariate Analysis of Ecological Data using CANOCO.pdf}}
}
@book{Joreskog,
	author = {J{\"{o}}reskog, Karl G and Wallentin, Fan Y},
	file = {:E$\backslash$:/Downloads/Work/Multivariate Analysis with LISREL.pdf:pdf},
	isbn = {9783319331522},
	title = {{Springer Series in Statistics Multivariate Analysis with LISREL}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Multivariate Data Analysis 7th Edition.pdf:pdf},
	title = {{Multivariate Data Analysis 7th Edition.pdf}}
}
@book{Primer,
	author = {Primer, A},
	file = {:E$\backslash$:/Downloads/Work/Multivariate Statistical Methods{\_} A Primer, Fourth Edition (2017) Bryan Manly.pdf:pdf},
	isbn = {9781498728966},
	title = {{No Title}}
}
@article{Xie2011,
	author = {Xie, Pengtao and Xing, Eric P},
	file = {:E$\backslash$:/Downloads/Work/naacl15.pdf:pdf},
	title = {{Incorporating Word Correlation Knowledge into Topic Modeling}},
	year = {2011}
}
@article{Volume,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 1 - The Undead King .pdf:pdf},
	title = {{No Title}}
}
@article{Volumea,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 2 - The Dark Knight.pdf:pdf},
	title = {{No Title}}
}
@article{Volumeb,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 3 - The Bloody Valkyrie.pdf:pdf},
	title = {{No Title}}
}
@article{Volumec,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 4 - The Lizardmen Heroes.pdf:pdf},
	title = {{No Title}}
}
@article{Volumed,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 5 - Men in The Kingdom [Part 01].pdf:pdf},
	title = {{No Title}}
}
@article{Volumee,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 6 - Men in The Kingdom [Part 02].pdf:pdf},
	title = {{No Title}}
}
@article{Invaders,
	author = {Invaders, The and Tomb, Large},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 7 - The Invaders of the Large Tomb.pdf:pdf},
	title = {{No Title}}
}
@article{Volumef,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 8 - The Two Leaders.pdf:pdf},
	title = {{No Title}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 9 - The Magic Caster of Destroy (v1.2).pdf:pdf},
	title = {{Overlord Volume 9 - The Magic Caster of Destroy (v1.2).pdf}}
}
@article{Caster,
	author = {Caster, The Magic},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 9 - The Magic Caster of Destroy.pdf:pdf},
	title = {{No Title}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 10 - The Ruler of Conspiracy (v2.3).pdf:pdf},
	title = {{Overlord Volume 10 - The Ruler of Conspiracy (v2.3).pdf}}
}
@article{Nguyenn,
	author = {Nguyenn, Viet},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 10 - The Ruler of Conspiracy.pdf:pdf},
	title = {{The Ruler of Conspiracy Translation : Nigel}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 11 - The Craftsman of Dwarf(v1.2).pdf:pdf},
	title = {{Overlord Volume 11 - The Craftsman of Dwarf(v1.2).pdf}}
}
@article{Machina,
	author = {Machina, Deus Ex and Doe, John},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 11 - The Craftsman of Dwarf.pdf:pdf},
	title = {{The Craftsmen of Dwarf Translation : Nigel}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 12 - The Paladin of the Holy Kingdom [Part 01] (v1.4) (1).pdf:pdf},
	title = {{Overlord Volume 12 - The Paladin of the Holy Kingdom [Part 01] (v1.4) (1).pdf}}
}
@article{Doe,
	author = {Doe, John and Storm, Rain},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 12 - The Paladin of the Holy Kingdom [Part 01].pdf:pdf},
	title = {{Translation : Nigel}}
}
@article{Character,
	author = {Character, Translated and Psychic, Sheets},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 13 - The Paladin of the Holy Kingdom [Part 02] (Outdated) .pdf:pdf},
	title = {{Translation : Nigel}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 13 - The Paladin of the Holy Kingdom [Part 02] (v2.1).pdf:pdf},
	title = {{Overlord Volume 13 - The Paladin of the Holy Kingdom [Part 02] (v2.1).pdf}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Overview of literature.pdf:pdf},
	pages = {10--11},
	title = {{No Title}}
}
@article{Science,
	author = {Science, Computer and Progress, Student and Review, Interim Progress and Main, Supervisors},
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}Interim{\_}Progress{\_}Review{\_}Form.pdf:pdf},
	title = {{Interim Progress Report Student Details PhD progress review A . 1 Thesis Title and Hypothesis A . 2 Overall Progress}}
}
@article{Ager2018,
	author = {Ager, Thomas and Schockaert, Steven and Ager, Thomas and Schockaert, Steven and Ager, Thomas and Schockaert, Steven and Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}List{\_}of{\_}Supervisor{\_}Meetings{\_}Form.pdf:pdf},
	pages = {2018},
	title = {{PGR Review : Supervisor Meetings}},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}Research{\_}Plan{\_}Form.pdf:pdf},
	title = {{Make codebase workable on ARCCA Complete Chapter 3 Complete Chapter 4 Complete Chapter 5 Code for CONLL paper Additional experimental results Journal paper}},
	year = {2018}
}
@article{Ager2018a,
	author = {Ager, Thomas},
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}Training{\_}Needs{\_}Assessment{\_}Form.pdf:pdf},
	pages = {3},
	title = {{PGR Review : Training Needs Assessment Training {\&} Support}},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/POLICY AND PROCEDURE FOR THE APPOINTMENT OF RESEARCH DEGREE EXAMINING BOARDS (VIVA EXAMINATION).pdf:pdf},
	pages = {1--3},
	title = {{POLICY AND PROCEDURE FOR THE APPOINTMENT OF RESEARCH DEGREE}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/POLICY AND PROCEDURE FOR THE CONDUCT OF RESEARCH DEGREE EXAMINATIONS.pdf:pdf},
	title = {{POLICY AND PROCEDURE FOR THE CONDUCT OF RESEARCH DEGREE}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/POLICY ON THE SUBMISSION AND PRESENTATION OF RESEARCH DEGREE THESES.PDF:PDF},
	title = {{POLICY ON THE SUBMISSION AND PRESENTATION OF RESEARCH DEGREE}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/RambachP1979{\_}AjiKan.pdf:pdf},
	title = {{RambachP1979{\_}AjiKan.pdf}}
}
@article{Deep2016,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1206.4683},
	author = {Deep, E Harnessing and Networks, Neural},
	eprint = {arXiv:1206.4683},
	file = {:E$\backslash$:/Downloads/Work/Reference List.pdf:pdf},
	isbn = {2200000006},
	pages = {787--795},
	title = {{Reference List}},
	volume = {3800},
	year = {2016}
}
@article{Schockaert2016,
	author = {Schockaert, Supervisor Steven and Ager, Thomas},
	file = {:E$\backslash$:/Downloads/Work/report1 (1).pdf:pdf},
	pages = {1--3},
	title = {{Thomas Ager : 9 Month Report}},
	year = {2016}
}
@article{Schockaert,
	author = {Schockaert, Supervisor Steven},
	file = {:E$\backslash$:/Downloads/Work/report1.pdf:pdf},
	pages = {1--3},
	title = {{Thomas Ager : 9 Month Report}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/SCIP App Paper.pdf:pdf},
	pages = {1--27},
	title = {{No Title}}
}
@article{Jeawak2019,
	author = {Jeawak, Shelan S},
	file = {:E$\backslash$:/Downloads/Work/Shelan{\_}Thesis.pdf:pdf},
	title = {{Exploiting Flickr Meta-Data for Predicting Environmental Features School of Computer Science {\&} Informatics}},
	year = {2019}
}

@article{Lastname2011b,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (1).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Lastname2011c,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (2).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Lastname2011d,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (3) workthru.pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Lastname2011e,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (4).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Lastname2011f,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (6).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Lastname2011g,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (7).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Thesis plan.pdf:pdf},
	pages = {13--15},
	title = {{Solution to the TF-IDF problem of no-context, utilizes nnets 1.}}
}
@article{Methods2018,
	author = {Methods, Clustering and Methods, Scoring and Models, Classification and Representations, Vector Space},
	file = {:E$\backslash$:/Downloads/Work/thesis.pdf:pdf},
	number = {June},
	pages = {1--7},
	title = {{3 3.1 Directions in Unsupervised Vector Spaces}},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Thomas Ager Thesis Structure (1).pdf:pdf},
	pages = {4--6},
	title = {{Thomas Ager Thesis Structure}}
}
@article{Pca,
	author = {Pca, Obtain and Product, Amazon and Mds, Obtain and Mds, Obtain and Product, Amazon and Doc, Obtain},
	file = {:E$\backslash$:/Downloads/Work/Time plan (1) (1).pdf:pdf},
	pages = {10--11},
	title = {{Thomas Ager Time Plan for Thesis}}
}
@article{Pcaa,
	author = {Pca, Obtain and Product, Amazon and Mds, Obtain and Mds, Obtain and Product, Amazon and Doc, Obtain and Pca, Obtain},
	file = {:E$\backslash$:/Downloads/Work/Time plan (2).pdf:pdf},
	pages = {12--14},
	title = {{Thomas Ager Time Plan for Thesis}}
}
@article{Zhang2010,
	author = {Zhang, Yin and Jin, Rong},
	doi = {10.1007/s13042-010-0001-0},
	file = {:E$\backslash$:/Downloads/Work/Understanding{\_}bag-of-words{\_}model{\_}A{\_}statistical{\_}fra.pdf:pdf},
	number = {December},
	pages = {0--16},
	title = {{Understanding bag-of-words model : A statistical framework Understanding Bag-of-Words Model : A Statistical Framework}},
	year = {2010}
}
@article{Matias,
	author = {Matias, J and Kivikangas, J Matias and Ekman, Inger and Chanel, Guillaume and J{\"{a}}rvel{\"{a}}, Simo and Cowley, Ben and Henttonen, Pentti and Ravaja, Niklas},
	file = {:E$\backslash$:/Downloads/Work/unige{\_}80604{\_}attachment01.pdf:pdf},
	keywords = {after that,and recent work,conveniently,details the most pertinent,method and the,practical use of it,the previous research section,theory behind the psychophysiological},
	number = {3},
	title = {{Article A review of the use of psychophysiological methods in game research activity Review on psychophysiological methods in game research}},
	volume = {3}
}
@article{Alvarez-melis2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.07538v2},
	author = {Alvarez-melis, David and Jaakkola, Tommi S},
	eprint = {arXiv:1806.07538v2},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/1806.07538.pdf:pdf},
	number = {NeurIPS},
	title = {{Towards Robust Interpretability with Self-Explaining Neural Networks}},
	year = {2018}
}
@article{Concept2018,
	author = {Concept, Elative and Testing, Importance},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/tcav{\_}relative{\_}concept{\_}importance{\_}testing{\_}with{\_}linear{\_}concept{\_}activation{\_}vectors (1).pdf:pdf},
	number = {26},
	title = {{TCAV : R ELATIVE CONCEPT IMPORTANCE TESTING}},
	year = {2018}
}
@article{Zhang,
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-chun},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/Zhang{\_}Interpretable{\_}Convolutional{\_}Neural{\_}CVPR{\_}2018{\_}paper.pdf:pdf},
	pages = {8827--8836},
	title = {{Interpretable Convolutional Neural Networks}}
}
@article{Kim2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1711.11279v5},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Jun, M L},
	eprint = {arXiv:1711.11279v5},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/1711.11279.pdf:pdf},
	title = {{Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCAV )}},
	year = {2017}
}
@article{Bengio2012,
	archivePrefix = {arXiv},
	arxivId = {1206.5538},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	doi = {10.1109/TPAMI.2013.50},
	eprint = {1206.5538},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2012 - Representation Learning A Review and New Perspectives.pdf:pdf},
	isbn = {0162-8828 VO - 35},
	issn = {1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	mendeley-groups = {Annotated/Representation Learning,Report/Features,11Thesis},
	number = {8},
	pages = {1798--1828},
	pmid = {23787338},
	title = {{Representation Learning: A Review and New Perspectives}},
	volume = {35},
	year = {2012}
}
@article{Dalessandro2014,
	author = {Dalessandro, C Perlich B and Stitelman, T Raeder O and Provost, F},
	doi = {10.1007/s10994-013-5375-2},
	file = {:E$\backslash$:/Perlich2014{\_}Article{\_}MachineLearningForTargetedDisp.pdf:pdf},
	keywords = {18th st,37 e,b,c,dalessandro,display advertising,editors,f,kiri wagstaff and cynthia,m6d research,new york,ny,o,perlich,predictive modeling,provost,raeder,rudin,stitelman,t,transfer learning,usa},
	mendeley-groups = {11Thesis/Applications},
	pages = {103--127},
	title = {{Machine learning for targeted display advertising : transfer learning in action}},
	year = {2014}
}

@article{Gilpina,
abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {1806.00069},
author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
doi = {10.1109/DSAA.2018.00018},
eprint = {1806.00069},
file = {:E$\backslash$:/PhD/Papedrs/1806.00069.pdf:pdf},
isbn = {9781538650905},
journal = {Proceedings - 2018 IEEE 5th International Conference on Data Science and Advanced Analytics, DSAA 2018},
keywords = {Deep learning and deep analytics,Fairness and transparency in data science,Machine learning theories,Models and systems},
mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
pages = {80--89},
title = {{Explaining explanations: An overview of interpretability of machine learning}},
year = {2019}
}

@article{Nam2014a,
	abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
	archivePrefix = {arXiv},
	arxivId = {1312.5419},
	author = {Nam, Jinseok and Kim, Jungi and {Loza Menc??a}, Eneldo and Gurevych, Iryna and F??rnkranz, Johannes},
	doi = {10.1007/978-3-662-44851-9_28},
	eprint = {1312.5419},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam et al. - 2014 - Large-scale multi-label text classification - Revisiting neural networks.pdf:pdf},
	isbn = {9783662448502},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Literature Review,Progress Report},
	number = {PART 2},
	pages = {437--452},
	title = {{Large-scale multi-label text classification - Revisiting neural networks}},
	volume = {8725 LNAI},
	year = {2014}
}
@article{Nam2014,
	abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
	archivePrefix = {arXiv},
	arxivId = {1312.5419},
	author = {Nam, Jinseok and Kim, Jungi and {Loza Menc{\'{i}}a}, Eneldo and Gurevych, Iryna and F{\"{u}}rnkranz, Johannes},
	doi = {10.1007/978-3-662-44851-9_28},
	eprint = {1312.5419},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam et al. - 2014 - Large-scale multi-label text classification - Revisiting neural networks(2).pdf:pdf},
	isbn = {9783662448502},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Report/Multi-label,11Thesis/Neural network multi-l;abe},
	number = {PART 2},
	pages = {437--452},
	title = {{Large-scale multi-label text classification - Revisiting neural networks}},
	volume = {8725 LNAI},
	year = {2014}
}
@article{Ager,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders.pdf:pdf},
	title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Agerb,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders(2).pdf:pdf},
	mendeley-groups = {Annotated/Past work,Progress Report},
	title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Higgins2017,
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	file = {:E$\backslash$:/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf:pdf},
	mendeley-groups = {11Thesis/Disentanglement},
	pages = {1--22},
	title = {{$\beta$ -VAE : L EARNING B ASIC V ISUAL C ONCEPTS WITH A C ONSTRAINED V ARIATIONAL F RAMEWORK}},
	year = {2017}
}
@article{Tang2015,
	abstract = {Document level sentiment classification remains a challenge: encoding the intrin- sic relations between sentences in the se- mantic meaning of a document. To ad- dress this, we introduce a neural network model to learn vector-based document rep- resentation in a unified, bottom-up fash- ion. The model first learns sentence rep- resentation with convolutional neural net- work or long short-term memory. After- wards, semantics of sentences and their relations are adaptively encoded in docu- ment representation with gated recurren- t neural network. We conduct documen- t level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimen- tal results show that: (1) our neural mod- el shows superior performances over sev- eral state-of-the-art algorithms; (2) gat- ed recurrent neural network dramatically outperforms standard recurrent neural net- work in document modeling for sentiment classification},
	archivePrefix = {arXiv},
	arxivId = {1508.04025},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	doi = {10.18653/v1/D15-1167},
	eprint = {1508.04025},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:pdf},
	isbn = {9781941643327},
	issn = {10495258},
	journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	mendeley-groups = {Annotated/Document representation,!Paper 3/task,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank,11Thesis/Document Representations},
	number = {September},
	pages = {1422--1432},
	title = {{Document Modeling with Gated Recurrent Neural Network for Sentiment Classification}},
	url = {http://aclweb.org/anthology/D15-1167},
	year = {2015}
}
@article{Yang2016,
	abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	archivePrefix = {arXiv},
	arxivId = {1606.02393},
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	doi = {10.18653/v1/N16-1174},
	eprint = {1606.02393},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2016 - Hierarchical Attention Networks for Document Classification.pdf:pdf},
	isbn = {9781941643914},
	journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Document representation,!Paper 3/Structured LSTMs,!Paper 3/task/Yelp,11Thesis/Document Representations},
	pages = {1480--1489},
	title = {{Hierarchical Attention Networks for Document Classification}},
	url = {http://aclweb.org/anthology/N16-1174},
	year = {2016}
}
@article{Koc,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1807.07279v2},
	author = {Koc, Aykut and Senel, Lutfi Kerem and Utlu, Ihsan and Ozaktas, Haldun M},
	eprint = {arXiv:1807.07279v2},
	file = {:E$\backslash$:/PhD/Papedrs/1807.07279.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {1--11},
	title = {{Imparting Interpretability to Word Embeddings while Preserving Semantic Structure}}
}
@article{Madotto2018,
	author = {Madotto, Andrea},
	file = {:E$\backslash$:/PhD/Papedrs/Interpreting Word Embeddings with Eigenvector Analysis.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Word Vectors,11Thesis/Interpretability/Word Vectors/Constraints,11Thesis/Interpretability/GAN's and VAE},
	number = {Nips},
	title = {{Interpreting Word Embeddings with Eigenvector Analysis}},
	year = {2018}
}
@article{Faruqui2015,
	abstract = {Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.},
	archivePrefix = {arXiv},
	arxivId = {1506.02004},
	author = {Faruqui, Manaal and Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah A.},
	eprint = {1506.02004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui et al. - 2015 - Sparse Overcomplete Word Vector Representations.pdf:pdf},
	isbn = {9781941643723},
	journal = {Acl-2015},
	mendeley-groups = {Annotated/Word Vectors,Progress Report,Interim Review,Annotated/NMF,11Thesis/Interpretability/Word Vectors/Post-processing,11Thesis/Interpretability/GAN's and VAE},
	pages = {1491--1500},
	title = {{Sparse Overcomplete Word Vector Representations}},
	url = {http://homes.cs.washington.edu/{~}nasmith/papers/faruqui+tsvetkov+yogatama+dyer+smith.acl15.pdf},
	year = {2015}
}
@article{Subramanian,
	author = {Subramanian, Anant and Pruthi, Danish and Jhamtani, Harsh and Berg-kirkpatrick, Taylor and Hovy, Eduard},
	file = {:E$\backslash$:/PhD/Papedrs/17433-76784-1-PB.pdf:pdf},
	keywords = {Natural Language Processing and Knowledge Representation Track},
	mendeley-groups = {11Thesis/Interpretability/Word Vectors/Post-processing,11Thesis/Interpretability/GAN's and VAE},
	pages = {4921--4928},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={32},
	number={1},
	title = {{SPINE: SParse Interpretable Neural Embeddings}},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/PhD/Papedrs/nguyen{\_}umd{\_}0117E{\_}20313.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Title of dissertation :}}
}
@article{From2019,
	author = {From, E X T},
	file = {:E$\backslash$:/PhD/Papedrs/TopicGAN Unsupervised Text Generation from Explainable Latent Topics.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {1--10},
	title = {{T OPIC GAN : U NSUPERVISED T EXT}},
	year = {2019}
}
@article{Bengio2013,
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	archivePrefix = {arXiv},
	arxivId = {1206.5538},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	doi = {10.1109/TPAMI.2013.50},
	eprint = {1206.5538},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2013 - Representation learning A review and new perspectives.pdf:pdf},
	isbn = {0162-8828 VO - 35},
	issn = {01628828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
	mendeley-groups = {Papers/Paper 1,Report},
	number = {8},
	pages = {1798--1828},
	pmid = {23787338},
	title = {{Representation learning: A review and new perspectives}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23459267{\%}5Cnhttp://arxiv.org/abs/1206.5538{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6472238},
	volume = {35},
	year = {2013}
}
@article{Shi2017,
	abstract = {Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such "two-step" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.},
	archivePrefix = {arXiv},
	arxivId = {1706.07276},
	author = {Shi, Bei and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Lai, Kwun Ping},
	doi = {10.1145/3077136.3080806},
	eprint = {1706.07276},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2017 - Jointly Learning Word Embeddings and Latent Topics.pdf:pdf},
	isbn = {9781450350228},
	keywords = {china,document modeling,e work described in,grant council of the,hong kong special administrative,project code,region,supported by grants from,the research,this paper is substantially,topic model,word embedding},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
	title = {{Jointly Learning Word Embeddings and Latent Topics}},
	url = {http://arxiv.org/abs/1706.07276{\%}0Ahttp://dx.doi.org/10.1145/3077136.3080806},
	year = {2017}
}
@article{Fares2017,
	author = {Fares, Murhaf and Kutuzov, Andrey and Oepen, Stephan and Velldal, Erik},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fares et al. - 2017 - Word vectors , reuse , and replicability Towards a community repository of large-text resources.pdf:pdf},
	mendeley-groups = {Report/Features},
	number = {May},
	pages = {271--276},
	title = {{Word vectors , reuse , and replicability : Towards a community repository of large-text resources}},
	year = {2017}
}
@article{Soneson2014,
	abstract = {BACKGROUND: With the large amount of biological data that is currently publicly available, many investigators combine multiple data sets to increase the sample size and potentially also the power of their analyses. However, technical differences ("batch effects") as well as differences in sample composition between the data sets may significantly affect the ability to draw generalizable conclusions from such studies.$\backslash$n$\backslash$nFOCUS: The current study focuses on the construction of classifiers, and the use of cross-validation to estimate their performance. In particular, we investigate the impact of batch effects and differences in sample composition between batches on the accuracy of the classification performance estimate obtained via cross-validation. The focus on estimation bias is a main difference compared to previous studies, which have mostly focused on the predictive performance and how it relates to the presence of batch effects.$\backslash$n$\backslash$nDATA: We work on simulated data sets. To have realistic intensity distributions, we use real gene expression data as the basis for our simulation. Random samples from this expression matrix are selected and assigned to group 1 (e.g., 'control') or group 2 (e.g., 'treated'). We introduce batch effects and select some features to be differentially expressed between the two groups. We consider several scenarios for our study, most importantly different levels of confounding between groups and batch effects.$\backslash$n$\backslash$nMETHODS: We focus on well-known classifiers: logistic regression, Support Vector Machines (SVM), k-nearest neighbors (kNN) and Random Forests (RF). Feature selection is performed with the Wilcoxon test or the lasso. Parameter tuning and feature selection, as well as the estimation of the prediction performance of each classifier, is performed within a nested cross-validation scheme. The estimated classification performance is then compared to what is obtained when applying the classifier to independent data.},
	author = {Soneson, Charlotte and Gerster, Sarah and Delorenzi, Mauro},
	doi = {10.1371/journal.pone.0100335},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Soneson, Gerster, Delorenzi - 2014 - Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation.pdf:pdf},
	isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Artifacts in the data},
	number = {6},
	pmid = {24967636},
	title = {{Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation}},
	volume = {9},
	year = {2014}
}

@article{Fyshe2015,
	abstract = {Vector Space Models (VSMs) of Semantics are useful tools for exploring the semantics of single words, and the composition of words to make phrasal meaning. While many meth- ods can estimate the meaning (i.e. vector) of a phrase, few do so in an interpretable way. We introduce a new method (CNNSE) that al- lows word and phrase vectors to adapt to the notion of composition. Our method learns a VSM that is both tailored to support a chosen semantic composition operation, and whose resulting features have an intuitive interpreta- tion. Interpretability allows for the exploration of phrasal semantics, which we leverage to an- alyze performance on a behavioral task. 1},
	author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - 2015 - A compositional and interpretable semantic space.pdf:pdf},
	isbn = {9781941643495},
	journal = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015)},
	mendeley-groups = {Progress Report,Interim Review,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
	pages = {32--41},
	title = {{A compositional and interpretable semantic space}},
	year = {2015}
}

@article{Arras2017,
	abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
	archivePrefix = {arXiv},
	arxivId = {1612.07843},
	author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
	doi = {10.1371/journal.pone.0181142},
	eprint = {1612.07843},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2017 - What is relevant in a text document An interpretable machine learning approach.pdf:pdf},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Explanations},
	number = {8},
	pages = {1--19},
	title = {{"What is relevant in a text document?": An interpretable machine learning approach}},
	volume = {12},
	year = {2017}
}
@article{Press2016,
	abstract = {We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.},
	archivePrefix = {arXiv},
	arxivId = {1608.05859},
	author = {Press, Ofir and Wolf, Lior},
	eprint = {1608.05859},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Press, Wolf - 2016 - Using the Output Embedding to Improve Language Models.pdf:pdf},
	isbn = {9781510838604},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs,!Paper 3/task/Sentiment treebank},
	title = {{Using the Output Embedding to Improve Language Models}},
	url = {http://arxiv.org/abs/1608.05859},
	year = {2016}
}
@article{Vincent2008a,
	abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	doi = {10.1145/1390156.1390294},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent et al. - 2008 - Extracting and composing robust features with denoising autoencoders.pdf:pdf},
	isbn = {9781605582054},
	issn = {1605582050},
	journal = {Proceedings of the 25th international conference on Machine learning},
	mendeley-groups = {Papers/Paper 1,Report/Features,Report},
	pages = {1096--1103},
	title = {{Extracting and composing robust features with denoising autoencoders}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.149.8111{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1390156.1390294},
	year = {2008}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 11 - The Craftsman of Dwarf(v1.2).pdf:pdf},
	title = {{Overlord Volume 11 - The Craftsman of Dwarf(v1.2).pdf}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/AnnualProgressReview (2)-3.pdf:pdf},
	title = {{Part {\%} 1 {\%}:{\%} PhD {\%} progress {\%} review {\%}}}
}
@article{Miyato2017,
	abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the output distribution. Virtual adversarial loss is defined as the robustness of the model's posterior distribution against local perturbation around each input data point. Our method is similar to adversarial training, but differs from adversarial training in that it determines the adversarial direction based only on the output distribution and that it is applicable to a semi-supervised setting. Because the directions in which we smooth the model are virtually adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward and backpropagations. In our experiments, we applied VAT to supervised and semi-supervised learning on multiple benchmark datasets. With additional improvement based on entropy minimization principle, our VAT achieves the state-of-the-art performance on SVHN and CIFAR-10 for semi-supervised learning tasks.},
	archivePrefix = {arXiv},
	arxivId = {1704.03976},
	author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	eprint = {1704.03976},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyato et al. - 2017 - Virtual Adversarial Training a Regularization Method for Supervised and Semi-supervised Learning.pdf:pdf},
	pages = {1--14},
	title = {{Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning}},
	url = {http://arxiv.org/abs/1704.03976},
	year = {2017}
}
@article{Pennington2014,
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
	archivePrefix = {arXiv},
	arxivId = {1504.06654},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
	doi = {10.3115/v1/D14-1162},
	eprint = {1504.06654},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
	isbn = {9781937284961},
	issn = {10495258},
	journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
	mendeley-groups = {Progress Report,Interim Review,Thesis/Word Vectors},
	pages = {1532--1543},
	pmid = {1710995},
	title = {{GloVe: Global Vectors for Word Representation}},
	year = {2014}
}
@article{Rifai2011,
	abstract = {Although the structure and composition of plant communities is known to influence the functioning of ecosystems, there is as yet no agreement as to how these should be described from a functional perspective. We tested the biomass ratio hypothesis, which postulates that ecosystem properties should depend on species traits and on species contribution to the total biomass of the community, in a successional sere following vineyard abandonment in the Mediterranean region of France. Ecosystem-specific net primary productivity, litter decomposition rate, and total soil carbon and nitrogen varied significantly with field age, and correlated with community-aggregated (i.e., weighed according to the relative abundance of species) functional leaf traits. The three easily measurable traits tested, specific leaf area, leaf dry matter content, and nitrogen concentration, provide a simple means to scale up from organ to ecosystem functioning in complex plant communities. We propose that they be called {\&}8220;functional markers,{\&}8221; and be used to assess the impacts of community changes on ecosystem properties induced, in particular, by global change drivers.},
	author = {Rifai, Salah and Muller, Xavier},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rifai, Muller - 2011 - Contractive Auto-Encoders Explicit Invariance During Feature Extraction.pdf:pdf},
	isbn = {978-1-4503-0619-5},
	journal = {Icml},
	number = {1},
	pages = {833--840},
	title = {{Contractive Auto-Encoders : Explicit Invariance During Feature Extraction}},
	url = {http://www.icml-2011.org/papers/455{\_}icmlpaper.pdf},
	volume = {85},
	year = {2011}
}
@article{Kim2013,
	author = {Kim, Joo-kyung},
	file = {:C$\backslash$:/Users/Workk/Documents/D13-1169.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	number = {October},
	pages = {1625--1630},
	title = {{Deriving adjectival scales from continuous space word representations}},
	year = {2013}
}
@article{Zhang,
	abstract = {Multilabel learning is an extension of standard binary classi cation where the goal is to predict a set of labels (we call an individual label a tag) for each input example. The recent probabilistic classi er chain (PCC) method learns a series of probabilistic models that capture tag correlations. In this paper, we show how the PCC model may be viewed as a neural network with connections between output nodes. We then show that using a hidden layer in the neural network, instead of connections between output nodes, brings advantages that include tractable test-time inference and removing the need to select a xed tag ordering. Moreover, the hidden units capture nonlinear latent structure, which improves classi cation accuracy, and allows correlations between tags to be visualized explicitly. Compared to previous neural network methods for multilabel learning, we explain several design decisions that lead to a notable decrease in training time and a notable increase in accuracy. Empirical results show that the new method outperforms existing MLL methods on benchmark datasets. A nal contribution of the paper is to introduce a new multilabel dataset of movies where the tags are genres. Experimentally, the new method performs best on this dataset also.},
	author = {Zhang, Min-ling and Zhou, Zhi-hua},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - Unknown - Neural Networks for Multi-Label Learning.pdf:pdf},
	journal = {Performance Evaluation},
	keywords = {backpropagation,machine learning,multi-label learning,neural net-,text categorization,works},
	pages = {1--22},
	title = {{Neural Networks for Multi-Label Learning}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/POLICY ON THE SUBMISSION AND PRESENTATION OF RESEARCH DEGREE THESES.PDF:PDF},
	title = {{POLICY ON THE SUBMISSION AND PRESENTATION OF RESEARCH DEGREE}}
}
@article{Fyshe2014,
	abstract = {Vector space models (VSMs) represent word meanings as points in a high dimensional space. VSMs are typically created using a large text corpora, and so represent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics. Evaluations show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies. {\textcopyright} 2014 Association for Computational Linguistics.},
	archivePrefix = {arXiv},
	arxivId = {15334406},
	author = {Fyshe, Alona and Talukdar, Pp and Murphy, Brian and Mitchell, Tm},
	doi = {10.14440/jbm.2015.54.A},
	eprint = {15334406},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - 2014 - Interpretable Semantic Vectors from a Joint Model of Brain-and Text-Based Meaning.pdf:pdf},
	isbn = {9781937284725},
	issn = {0036-8075},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
	pages = {489--499},
	pmid = {25792328},
	title = {{Interpretable Semantic Vectors from a Joint Model of Brain-and Text-Based Meaning}},
	url = {http://www.cs.cmu.edu/{~}afyshe/papers/acl2014/jnnse{\_}acl2014.pdf},
	volume = {1},
	year = {2014}
}
@article{Lei2017,
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	file = {:E$\backslash$:/PhD/Papedrs/7259-style-transfer-from-non-parallel-text-by-cross-alignment.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	number = {Nips},
	pages = {1--12},
	title = {{Style Transfer from Non-Parallel Text by}},
	year = {2017}
}
@article{Greff2016,
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	archivePrefix = {arXiv},
	arxivId = {1503.04069},
	author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
	doi = {10.1109/TNNLS.2016.2582924},
	eprint = {1503.04069},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2016 - LSTM A Search Space Odyssey.pdf:pdf},
	isbn = {9788578110796},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	mendeley-groups = {Annotated/Representation Learning},
	pmid = {25246403},
	title = {{LSTM: A Search Space Odyssey}},
	year = {2016}
}
@article{Arora2012,
	abstract = {Topic Modeling is an approach used for automatic comprehension and classification of data in a variety of settings, and perhaps the canonical application is in uncovering thematic structure in a corpus of documents. A number of foundational works both in machine learning and in theory have suggested a probabilistic model for documents, whereby documents arise as a convex combination of (i.e. distribution on) a small number of topic vectors, each topic vector being a distribution on words (i.e. a vector of word-frequencies). Similar models have since been used in a variety of application areas; the Latent Dirichlet Allocation or LDA model of Blei et al. is especially popular. Theoretical studies of topic modeling focus on learning the model's parameters assuming the data is actually generated from it. Existing approaches for the most part rely on Singular Value Decomposition(SVD), and consequently have one of two limitations: these works need to either assume that each document contains only one topic, or else can only recover the span of the topic vectors instead of the topic vectors themselves. This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main tool in this context, which is an analog of SVD where all vectors are nonnegative. Using this tool we give the first polynomial-time algorithm for learning topic models without the above two limitations. The algorithm uses a fairly mild assumption about the underlying topic matrix called separability, which is usually found to hold in real-life data. A compelling feature of our algorithm is that it generalizes to models that incorporate topic-topic correlations, such as the Correlated Topic Model and the Pachinko Allocation Model. We hope that this paper will motivate further theoretical results that use NMF as a replacement for SVD - just as NMF has come to replace SVD in many applications.},
	archivePrefix = {arXiv},
	arxivId = {1204.1956},
	author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur},
	doi = {10.1109/FOCS.2012.49},
	eprint = {1204.1956},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Ge, Moitra - 2012 - Learning topic models - Going beyond SVD.pdf:pdf},
	isbn = {978-0-7695-4874-6},
	issn = {02725428},
	journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {1--10},
	title = {{Learning topic models - Going beyond SVD}},
	year = {2012}
}
@article{Benitez1997,
	abstract = {Artificial neural networks are efficient computing models which have shown their strengths in solving hard problems in artificial intelligence. They have also been shown to be universal approximators. Notwithstanding, one of the major criticisms is their being black boxes, since no satisfactory explanation of their behavior has been offered. In this paper, we provide such an interpretation of neural networks so that they will no longer be seen as black boxes. This is stated after establishing the equality between a certain class of neural nets and fuzzy rule-based systems. This interpretation is built with fuzzy rules using a new fuzzy logic operator which is defined after introducing the concept of f-duality. In addition, this interpretation offers an automated knowledge acquisition procedure.},
	author = {Ben{\'{i}}tez, J. M. and Castro, J. L. and Requena, I.},
	doi = {10.1109/72.623216},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben{\'{i}}tez, Castro, Requena - 1997 - Are artificial neural networks black boxes.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Equality between neural nets and fuzzy rule-based,Fuzzy additive systems,Interpretation of neural nets,f-duality,i-or operator},
	number = {5},
	pages = {1156--1164},
	pmid = {18255717},
	title = {{Are artificial neural networks black boxes?}},
	volume = {8},
	year = {1997}
}
@article{Samek,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.07979v1},
	author = {Samek, Wojciech},
	eprint = {arXiv:1706.07979v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek - Unknown - Methods for Interpreting and Understanding Deep Neural Networks.pdf:pdf},
	keywords = {activation maximization,deep neural networks,layer-wise,relevance propagation,sensitivity analysis,taylor decomposition},
	mendeley-groups = {Annotated/Overarching Interpretability},
	title = {{Methods for Interpreting and Understanding Deep Neural Networks}}
}
@article{Karpathy2015a,
	abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\{}{\%}{\}} vs. 60.9{\{}{\%}{\}}) and the UCF-101 datasets with (88.6{\{}{\%}{\}} vs. 88.0{\{}{\%}{\}}) and without additional optical flow information (82.6{\{}{\%}{\}} vs. 72.8{\{}{\%}{\}}).},
	archivePrefix = {arXiv},
	arxivId = {1503.08909v2},
	author = {Karpathy, a. and Fei-Fei, Li},
	doi = {10.1109/CVPR.2015.7298932},
	eprint = {1503.08909v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Image Des.pdf:pdf},
	isbn = {9781467369640},
	issn = {10636919},
	journal = {Cvpr2015},
	mendeley-groups = {Progress Report},
	title = {{Deep Visual-Semantic Alignments for Generating Image Des}},
	year = {2015}
}
@article{Hornik1989,
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	doi = {10.1016/0893-6080(89)90020-8},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
	isbn = {08936080 (ISSN)},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
	number = {5},
	pages = {359--366},
	pmid = {74},
	title = {{Multilayer feedforward networks are universal approximators}},
	volume = {2},
	year = {1989}
}
@article{Kaikhah,
	author = {Kaikhah, Khosrow and Doddameti, Sandesh},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaikhah, Doddameti - Unknown - Discovering Trends in Large Datasets Using Neural Networks.pdf:pdf},
	mendeley-groups = {Papers/Paper 1,Report},
	pages = {1--23},
	title = {{Discovering Trends in Large Datasets Using Neural Networks}}
}
@article{Pascanu2012,
	abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	archivePrefix = {arXiv},
	arxivId = {1211.5063},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	doi = {10.1109/72.279181},
	eprint = {1211.5063},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - 2012 - On the difficulty of training Recurrent Neural Networks.pdf:pdf},
	isbn = {08997667 (ISSN)},
	issn = {1045-9227},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pmid = {18267787},
	title = {{On the difficulty of training Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1211.5063},
	year = {2012}
}
@article{Fong2017,
	abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, e.g. problems in health, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we introduce a paradigm that learns the minimally salient part of an image by directly editing it and learning from the corresponding changes to its output. Unlike previous works, our method is model-agnostic and testable because it is grounded in replicable image perturbations.},
	archivePrefix = {arXiv},
	arxivId = {1704.03296},
	author = {Fong, Ruth and Vedaldi, Andrea},
	eprint = {1704.03296},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong, Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meaningful Perturbation.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Report},
	title = {{Interpretable Explanations of Black Boxes by Meaningful Perturbation}},
	url = {http://arxiv.org/abs/1704.03296},
	year = {2017}
}
@article{Lisboa2002,
	abstract = {The purpose of this review is to assess the evidence of healthcare benefits involving the application of artificial neural networks to the clinical functions of diagnosis, prognosis and survival analysis, in the medical domains of oncology, critical care and cardiovascular medicine. The primary source of publications is PUBMED listings under Randomised Controlled Trials and Clinical Trials. The r{\^{o}}le of neural networks is introduced within the context of advances in medical decision support arising from parallel developments in statistics and artificial intelligence. This is followed by a survey of published Randomised Controlled Trials and Clinical Trials, leading to recommendations for good practice in the design and evaluation of neural networks for use in medical intervention.},
	author = {Lisboa, P.J.G.},
	doi = {10.1016/S0893-6080(01)00111-3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lisboa - 2002 - A review of evidence of health benefit from artificial neural networks in medical intervention.pdf:pdf},
	isbn = {0893-6080 (Print)$\backslash$r0893-6080 (Linking)},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {clinical trials,decision support systems,diagnosis,prognosis,prospective studies,randomised controlled trials,review,survival analysis},
	mendeley-groups = {Report/Features,Report},
	number = {1},
	pages = {11--39},
	pmid = {11958484},
	title = {{A review of evidence of health benefit from artificial neural networks in medical intervention}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608001001113},
	volume = {15},
	year = {2002}
}
@article{Yosinski2015,
	abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
	archivePrefix = {arXiv},
	arxivId = {1506.06579},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	eprint = {1506.06579},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visualization(2).pdf:pdf},
	journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
	pages = {12},
	title = {{Understanding Neural Networks Through Deep Visualization}},
	url = {http://arxiv.org/abs/1506.06579},
	year = {2015}
}
@article{Haury2011,
	abstract = {Biomarker discovery from high-dimensional data is a crucial problem with enormous applications in biology and medicine. It is also extremely challenging from a statistical viewpoint, but surprisingly few studies have investigated the relative strengths and weaknesses of the plethora of existing feature selection methods. In this study we compare feature selection methods on public gene expression datasets for breast cancer prognosis, in terms of predictive performance, stability and functional interpretability of the signatures they produce. We observe that the feature selection method has a significant influence on the accuracy, stability and interpretability of signatures. Surprisingly, complex wrapper and embedded methods generally do not outperform simple univariate feature selection methods, and ensemble feature selection has generally no positive effect. Overall a simple Student's t-test seems to provide the best results.},
	archivePrefix = {arXiv},
	arxivId = {1101.5008},
	author = {Haury, Anne Claire and Gestraud, Pierre and Vert, Jean Philippe},
	doi = {10.1371/journal.pone.0028210},
	eprint = {1101.5008},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haury, Gestraud, Vert - 2011 - The influence of feature selection methods on accuracy, stability and interpretability of molecular signa.pdf:pdf},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Applications/Scientific Discovery},
	number = {12},
	pages = {1--12},
	pmid = {22205940},
	title = {{The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures}},
	volume = {6},
	year = {2011}
}
@article{Samanta2003,
	abstract = {A study is presented to compare the performance of bearing fault detection using two different classifiers, namely, artificial neural networks (ANNs) and support vector machines (SMVs). The time-domain vibration signals of a rotating machine with normal and defective bearings are processed for feature extraction. The extracted features from original and preprocessed signals are used as inputs to the classifiers for two-class (normal or fault) recognition. The classifier parameters, e.g., the number of nodes in the hidden layer in case of ANNs and the radial basis function kernel parameter (width) in case of SVMs along with the selection of input features are optimized using genetic algorithms. The classifiers are trained with a subset of the experimental data for known machine conditions and are tested using the remaining set of data. The procedure is illustrated using the experimental vibration data of a rotating machine. The roles of different vibration signals and signal preprocessing techniques are investigated. The results show the effectiveness of the features and the classifiers in detection of machine condition. ?? 2003 Elsevier Ltd. All rights reserved.},
	author = {Samanta, B. and Al-Balushi, K. R. and Al-Araimi, S. A.},
	doi = {10.1016/j.engappai.2003.09.006},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samanta, Al-Balushi, Al-Araimi - 2003 - Artificial neural networks and support vector machines with genetic algorithm for bearing fault.pdf:pdf},
	isbn = {09521976 (ISSN)},
	issn = {09521976},
	journal = {Engineering Applications of Artificial Intelligence},
	keywords = {Bearing faults,Condition monitoring,Feature selection,Genetic algorithm,Neural network,Rotating machines,Signal processing,Support vector machines},
	number = {7-8},
	pages = {657--665},
	title = {{Artificial neural networks and support vector machines with genetic algorithm for bearing fault detection}},
	volume = {16},
	year = {2003}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 1704.03296.pdf.pdf:pdf},
	title = {1704.03296.pdf}
}
@article{Kiros2014,
	abstract = {In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2710v1},
	author = {Kiros, Ryan and Zemel, Rs and Salakhutdinov, Ruslan},
	eprint = {arXiv:1406.2710v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2014 - A Multiplicative Model for Learning Distributed Text-Based Attribute Representations.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {1--11},
	title = {{A Multiplicative Model for Learning Distributed Text-Based Attribute Representations}},
	url = {http://arxiv.org/abs/1406.2710},
	year = {2014}
}
@article{Poursabzi-sangdeh,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.07810v2},
	author = {Poursabzi-sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Vaughan, Jennifer Wortman and Wallach, Hanna},
	eprint = {arXiv:1802.07810v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.07810.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/General},
	pages = {1--20},
	title = {{Manipulating and Measuring Model Interpretability}}
}
@article{Etchells2006a,
	abstract = {There is much interest in rule extraction from neural networks and a plethora of different methods have been proposed for this purpose. We discuss the merits of pedagogical and decompositional approaches to rule extraction from trained neural networks, and show that some currently used methods for binary data comply with a theoretical formalism for extraction of Boolean rules from continuously valued logic. This formalism is extended into a generic methodology for rule extraction from smooth decision surfaces fitted to discrete or quantized continuous variables independently of the analytical structure of the underlying model, and in a manner that is efficient even for high input dimensions. This methodology is then tested with Monks' data, for which exact rules are obtained and to Wisconsin's breast cancer data, where a small number of high-order rules are identified whose discriminatory performance can be directly visualized.},
	author = {Etchells, Terence A. and Lisboa, Paulo J G},
	doi = {10.1109/TNN.2005.863472},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Etchells, Lisboa - 2006 - Orthogonal Search-Based Rule Extraction (OSRE) for Trained Neural Networks A Practical and Efficient Approach.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Neura networks,Rule extraction},
	number = {2},
	pages = {374--384},
	pmid = {16566465},
	title = {{Orthogonal Search-Based Rule Extraction (OSRE) for Trained Neural Networks: A Practical and Efficient Approach}},
	volume = {17},
	year = {2006}
}
@article{Setionoa,
	author = {Setiono, Rudy},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono - Unknown - by pruning and hidden-unit splitting 1 Introduction.pdf:pdf},
	number = {1},
	pages = {1--34},
	title = {{by pruning and hidden-unit splitting 1 Introduction}},
	volume = {9}
}
@article{Liu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1705.09207v2},
	author = {Liu, Yang and Lapata, Mirella},
	eprint = {arXiv:1705.09207v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lapata - 2017 - Learning Structured Text Representations.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations},
	title = {{Learning Structured Text Representations}},
	year = {2017}
}
@article{Bologna2000,
	author = {Bologna, Guido},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna - 2000 - Rule Extraction from a Multi layer Perceptron with Staircase Activation Functions The DIMLP model.pdf:pdf},
	mendeley-groups = {Progress Report},
	pages = {0--5},
	title = {{Rule Extraction from a Multi layer Perceptron with Staircase Activation Functions The DIMLP model}},
	year = {2000}
}
@article{Wu2018,
	author = {Wu, Mike and Hughes, Michael C and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-velez, Finale},
	file = {:E$\backslash$:/Downloads/Work/16285-76627-1-PB.pdf:pdf},
	keywords = {Humans and Artificial Intelligence Track},
	pages = {1670--1678},
	title = {{Beyond Sparsity : Tree Regularization of Deep Models for Interpretability}},
	year = {2018}
}
@article{Salton1975,
	author = {Salton, G and Wong, A and Yang, C S},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Wong, Yang - 1975 - AVector Space Model for Automatic Indexing.pdf:pdf},
	keywords = {and phrases,automatic indexing,automatic information,content analysis,document,retrieval},
	mendeley-groups = {Report/Features},
	number = {11},
	title = {{AVector Space Model for Automatic Indexing}},
	volume = {18},
	year = {1975}
}
@article{Zhu2012,
	abstract = {A supervised topic model can use side information such as ratings or labels associated with doc- uments or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective func- tions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet alloca- tion (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) un- der a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or re- gression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, espe- cially for classification.},
	archivePrefix = {arXiv},
	arxivId = {0912.5507},
	author = {Zhu, Jun and Ahmed, a and Xing, Ep},
	doi = {10.1145/1553374.1553535},
	eprint = {0912.5507},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Ahmed, Xing - 2012 - MedLDA Maximum Margin Supervised Topic Models.pdf:pdf},
	isbn = {978-1-60558-516-1},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {latent dirichlet allocation,max-margin learning,maximum entropy discrimination,supervised topic models,support vector machines},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,!Paper 3/task/newsgroups},
	pages = {2237--2278},
	title = {{MedLDA: Maximum Margin Supervised Topic Models}},
	url = {http://jmlr.csail.mit.edu/papers/volume13/zhu12a/zhu12a.pdf},
	volume = {13},
	year = {2012}
}
@article{Hardt2016,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1610.02413},
	author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
	eprint = {arXiv:1610.02413},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/6374-equality-of-opportunity-in-supervised-learning.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {Nips},
	title = {{Equality of Opportunity in Supervised Learning}},
	year = {2016}
}
@article{Subramanian,
	author = {Subramanian, Anant and Pruthi, Danish and Jhamtani, Harsh and Berg-kirkpatrick, Taylor and Hovy, Eduard},
	file = {:E$\backslash$:/PhD/Papedrs/17433-76784-1-PB.pdf:pdf},
	keywords = {Natural Language Processing and Knowledge Represen},
	mendeley-groups = {11Thesis/Interpretability/Word Vectors/Post-processing,11Thesis/Interpretability/GAN's and VAE},
	pages = {4921--4928},
	title = {{SPINE : SParse Interpretable Neural Embeddings}}
}
@article{Matias,
	author = {Matias, J and Kivikangas, J Matias and Ekman, Inger and Chanel, Guillaume and J{\"{a}}rvel{\"{a}}, Simo and Cowley, Ben and Henttonen, Pentti and Ravaja, Niklas},
	file = {:E$\backslash$:/Downloads/Work/unige{\_}80604{\_}attachment01.pdf:pdf},
	keywords = {after that,and recent work,conveniently,details the most pertinent,method and the,practical use of it,the previous research section,theory behind the psychophysiological},
	number = {3},
	title = {{Article A review of the use of psychophysiological methods in game research activity Review on psychophysiological methods in game research}},
	volume = {3}
}
@article{Lei2016,
	abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
	archivePrefix = {arXiv},
	arxivId = {1606.04155},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	eprint = {1606.04155},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lei, Barzilay, Jaakkola - 2016 - Rationalizing Neural Predictions.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Rationalizing Neural Predictions}},
	url = {http://arxiv.org/abs/1606.04155},
	year = {2016}
}
@article{Mnih2008,
	abstract = {Neural probabilistic language models (NPLMs) have been shown to be competi- tive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non- hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1},
	author = {Mnih, Andriy and Hinton, Geoffrey E.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih, Hinton - 2008 - A Scalable Hierarchical Distributed Language Model.pdf:pdf},
	isbn = {9781605609492},
	journal = {Advances in Neural Information Processing Systems},
	pages = {1--8},
	title = {{A Scalable Hierarchical Distributed Language Model.}},
	url = {http://discovery.ucl.ac.uk/63249/},
	year = {2008}
}
@article{Li2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1503.00185v5},
	author = {Li, Jiwei and Luong, Minh-thang and Jurafsky, Dan and Hovy, Eduard},
	eprint = {arXiv:1503.00185v5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - When Are Tree Structures Necessary for Deep Learning of.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	title = {{When Are Tree Structures Necessary for Deep Learning of}},
	year = {2014}
}
@article{Zhou2015,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1511.08630v2},
	author = {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C M},
	eprint = {arXiv:1511.08630v2},
	file = {:E$\backslash$:/Downloads/Work/1511.08630.pdf:pdf},
	title = {{A C-LSTM Neural Network for Text Classification}},
	year = {2015}
}
@article{Rudinger2017,
	abstract = {We introduce the notion of a multi-vector sentence representation based on a " one vector per proposition " philosophy, which we term skip-prop vectors. By representing each predicate-argument structure in a complex sentence as an individual vector, skip-prop is (1) a response to empirical evidence that single-vector sentence representations degrade with sentence length, and (2) a repre-sentation that maintains a semantically useful level of granularity. We demonstrate the feasibility of training skip-prop vectors, introducing a method adapted from skip-thought vectors, and compare skip-prop with " one vector per sentence " and " one vector per token " approaches.},
	author = {Rudinger, Rachel and Duh, Kevin and {Van Durme}, Benjamin},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudinger, Duh, Van Durme - 2017 - Skip-Prop Representing Sentences with One Vector Per Proposition.pdf:pdf},
	journal = {Iwcs},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Skip-Prop: Representing Sentences with One Vector Per Proposition}},
	year = {2017}
}
@article{Grosenick2008,
	abstract = {{\textless}para{\textgreater} Despite growing interest in applying machine learning to neuroimaging analyses, few studies have gone beyond classifying sensory input to directly predicting behavioral output. With spatial resolution on the order of millimeters and temporal r...},
	author = {Grosenick, Logan and Greer, Stephanie and Knutson, Brian},
	doi = {10.1109/TNSRE.2008.926701},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grosenick, Greer, Knutson - 2008 - Interpretable Classifiers for fMRI Improve Prediction of Purchases.pdf:pdf},
	isbn = {1558-0210 (Electronic)},
	issn = {15580210},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	keywords = {Accumbens,classification,discriminant,elastic net,frontal,functional magnetic resonance imaging (fMRI),human,insula,lasso,penalized discriminant analysis (PDA),prediction,purchasing,single-trial,sparse,spatiotemporal,support vector machine (SVM)},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {6},
	pages = {539--548},
	pmid = {19144586},
	title = {{Interpretable Classifiers for fMRI Improve Prediction of Purchases}},
	volume = {16},
	year = {2008}
}
@article{Besana1991,
	abstract = {Forty-six heroin abusers were hospitalized and treated with meperidine either alone or in association with clonidine. Meperidine was given orally in rapidly decreasing doses according to three different schedules. The majority of patients (87{\%}) successfully completed the detoxification program. The best meperidine starting posology was 200 mg four times daily, which allowed stoppage of the opioid treatment after gradual reduction of the daily dose in a mean time of 9.5 days. Association with clonidine was not proven to be useful. This study shows that meperidine can be effectively used in rapidly decreasing doses in the pharmacological detoxification treatment of hospitalized heroin addicts.},
	archivePrefix = {arXiv},
	arxivId = {1301.3781},
	author = {Besana, Carlo and Memoli, Massimo and Salvioni, Piero M. and Finazzi, Renato A. and Inversi, Felice and Rugarli, Claudio},
	doi = {10.3109/10826089109058901},
	eprint = {1301.3781},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/N13-1090.pdf:pdf},
	isbn = {0020-773X (Print)$\backslash$r0020-773X (Linking)},
	issn = {10826084},
	journal = {Substance Use and Misuse},
	keywords = {Clonidine,Detoxification,Drug addiction,Heroin misuse,Meperidine},
	number = {5},
	pages = {505--513},
	pmid = {1938007},
	title = {{Meperidine in detoxification of hospitalized heroin addicts}},
	url = {http://research.microsoft.com/en-},
	volume = {26},
	year = {1991}
}
@article{Chandrashekar2014,
	author = {Chandrashekar, Girish and Sahin, Ferat},
	doi = {10.1016/j.compeleceng.2013.11.024},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrashekar, Sahin - 2014 - A survey on feature selection methods q.pdf:pdf},
	issn = {0045-7906},
	journal = {Computers and Electrical Engineering},
	mendeley-groups = {Report/Features,Report},
	number = {1},
	pages = {16--28},
	publisher = {Elsevier Ltd},
	title = {{A survey on feature selection methods q}},
	url = {http://dx.doi.org/10.1016/j.compeleceng.2013.11.024},
	volume = {40},
	year = {2014}
}
@article{Krakovna2016a,
	abstract = {As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks, state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining a long short-term memory (LSTM) model with a hidden Markov model (HMM), a simpler and more transparent model. We add the HMM state probabilities to the output layer of the LSTM, and then train the HMM and LSTM either sequentially or jointly. The LSTM can make use of the information from the HMM, and fill in the gaps when the HMM is not performing well. A small hybrid model usually performs better than a standalone LSTM of the same size, especially on smaller data sets. We test the algorithms on text data and medical time series data, and find that the LSTM and HMM learn complementary information about the features in the text.},
	archivePrefix = {arXiv},
	arxivId = {1611.05934},
	author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
	eprint = {1611.05934},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krakovna, Doshi-Velez - 2016 - Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models(2).pdf:pdf},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	number = {Whi},
	title = {{Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}},
	url = {http://arxiv.org/abs/1611.05934},
	year = {2016}
}
@article{Tickle,
	author = {Tickle, Alan B and Diederich, Joachim},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tickle, Diederich - Unknown - From Trained Artificial Neural Networks.pdf:pdf},
	mendeley-groups = {Progress Report},
	title = {{From Trained Artificial Neural Networks}}
}
@article{Krakovna2016,
	abstract = {As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks, state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining a long short-term memory (LSTM) model with a hidden Markov model (HMM), a simpler and more transparent model. We add the HMM state probabilities to the output layer of the LSTM, and then train the HMM and LSTM either sequentially or jointly. The LSTM can make use of the information from the HMM, and fill in the gaps when the HMM is not performing well. A small hybrid model usually performs better than a standalone LSTM of the same size, especially on smaller data sets. We test the algorithms on text data and medical time series data, and find that the LSTM and HMM learn complementary information about the features in the text.},
	archivePrefix = {arXiv},
	arxivId = {1611.05934},
	author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
	eprint = {1611.05934},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krakovna, Doshi-Velez - 2016 - Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models(2).pdf:pdf},
	mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
	number = {Whi},
	title = {{Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}},
	url = {http://arxiv.org/abs/1611.05934},
	year = {2016}
}
@article{Maas2011,
	abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
	author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	doi = {978-1-932432-87-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:pdf},
	isbn = {9781932432879},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Datasets,!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {142--150},
	title = {{Learning Word Vectors for Sentiment Analysis}},
	year = {2011}
}
@article{Ager2012,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/243{\_}Paper.pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@book{Bird,
	author = {Bird, Steven and Klein, Ewan and Loper, Edward and Bird, Steven and Klein, Ewan and Loper, Edward and Bird, Copyright Steven and Klein, Ewan and Loper, Edward},
	file = {:D$\backslash$:/PhD/Code/ThesisPipeline/ThesisPipeline/papers/nltk.pdf:pdf},
	isbn = {9780596516499},
	title = {{Natural Language Processing with Python}}
}
@article{Severyn2015,
	abstract = {This paper describes our deep learning system for sentiment anal-ysis of tweets. The main contribution of this work is a new model for initializing the parameter weights of the convolutional neural network, which is crucial to train an accurate model while avoid-ing the need to inject any additional features. Briefly, we use an unsupervised neural language model to train initial word embed-dings that are further tuned by our deep learning model on a distant supervised corpus. At a final stage, the pre-trained parameters of the network are used to initialize the model. We train the latter on the supervised training data recently made available by the official system evaluation campaign on Twitter Sentiment Analysis orga-nized by Semeval-2015. A comparison between the results of our approach and the systems participating in the challenge on the of-ficial test sets, suggests that our model could be ranked in the first two positions in both the phrase-level subtask A (among 11 teams) and on the message-level subtask B (among 40 teams). This is an important evidence on the practical value of our solution.},
	author = {Severyn, Aliaksei and Moschitti, Alessandro},
	doi = {10.1145/2766462.2767830},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Severyn, Moschitti - 2015 - Twitter Sentiment Analysis with Deep Convolutional Neural Networks.pdf:pdf},
	isbn = {9781450336215},
	journal = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '15},
	keywords = {convolutional neural networks,twitter sentiment analysis},
	mendeley-groups = {Progress Report},
	pages = {959--962},
	title = {{Twitter Sentiment Analysis with Deep Convolutional Neural Networks}},
	url = {http://dl.acm.org/citation.cfm?doid=2766462.2767830},
	year = {2015}
}
@article{Zhang,
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-chun},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/Zhang{\_}Interpretable{\_}Convolutional{\_}Neural{\_}CVPR{\_}2018{\_}paper.pdf:pdf},
	pages = {8827--8836},
	title = {{Interpretable Convolutional Neural Networks}}
}
@article{Westerveld,
	author = {Westerveld, Thijs and Vries, Arjen De and Jong, Franciska De},
	file = {:E$\backslash$:/Downloads/Work/9783540728948-c1.pdf:pdf},
	title = {{6 Generative Probabilistic Models}}
}
@inproceedings{Gladkova2016,
	abstract = {This paper presents an analysis of exist- ing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evalua- tions is “interpretability” of word embed- dings: a “good” embedding produces re- sults that make sense in terms of tradi- tional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of dis- tributional meaning representations. We argue for a shift from abstract ratings of word embedding “quality” to exploration of their strengths and weaknesses},
	author = {Gladkova, Anna and Drozd, Aleksandr},
	booktitle = {Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP},
	doi = {10.18653/v1/W16-2507},
	isbn = {9781945626142},
	mendeley-groups = {Report/Features,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	pages = {36--42},
	title = {{Intrinsic Evaluations of Word Embeddings: What Can We Do Better?}},
	year = {2016}
}
@article{Merity2017a,
	abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
	archivePrefix = {arXiv},
	arxivId = {1708.02182},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	eprint = {1708.02182},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merity, Keskar, Socher - 2017 - Regularizing and Optimizing LSTM Language Models.pdf:pdf},
	mendeley-groups = {!Paper 3/Language models},
	title = {{Regularizing and Optimizing LSTM Language Models}},
	url = {http://arxiv.org/abs/1708.02182},
	year = {2017}
}
@article{Johnson2016,
	abstract = {One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson {\&} Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.},
	archivePrefix = {arXiv},
	arxivId = {1602.02373},
	author = {Johnson, Rie and Zhang, Tong},
	eprint = {1602.02373},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson, Zhang - 2016 - Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings.pdf:pdf},
	isbn = {9781510829008},
	issn = {1938-7228},
	mendeley-groups = {!Paper 3/task/Large Movie Review},
	title = {{Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings}},
	url = {http://arxiv.org/abs/1602.02373},
	volume = {48},
	year = {2016}
}
@article{Sundararajan2017,
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	archivePrefix = {arXiv},
	arxivId = {1703.01365},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	eprint = {1703.01365},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundararajan, Taly, Yan - 2017 - Axiomatic Attribution for Deep Networks.pdf:pdf},
	issn = {1938-7228},
	mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3},
	title = {{Axiomatic Attribution for Deep Networks}},
	url = {http://arxiv.org/abs/1703.01365},
	year = {2017}
}
@article{Liu2017,
	abstract = {In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.},
	archivePrefix = {arXiv},
	arxivId = {1705.09207},
	author = {Liu, Yang and Lapata, Mirella},
	eprint = {1705.09207},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Lapata - 2017 - Learning Structured Text Representations(2).pdf:pdf},
	mendeley-groups = {!Paper 3/Structured LSTMs},
	title = {{Learning Structured Text Representations}},
	url = {http://arxiv.org/abs/1705.09207},
	year = {2017}
}
@article{Richards2001,
	abstract = {This paper describes the analysis of a database of diabetic patients' clinical records and death certificates. The objective of the study was to find rules that describe associations between observations made of patients at their first visit to the hospital and early mortality. Pre-processing was carried out and a knowledge discovery in databases (KDD) package, developed by the Lanner Group and the University of East Anglia, was used for rule induction using simulated annealing. The most significant discovered rules describe an association that was not generally known or accepted by the medical community, however, recent independent studies confirm their validity. ?? 2001 Elsevier Science B.V.},
	author = {Richards, G. and Rayward-Smith, V. J. and S??nksen, P. H. and Carey, S. and Weng, C.},
	doi = {10.1016/S0933-3657(00)00110-X},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Richards et al. - 2001 - Data mining for indicators of early mortality in a database of clinical records.pdf:pdf},
	isbn = {0933-3657},
	issn = {09333657},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Data mining,Diabetes,Neuropathy,Rule induction},
	mendeley-groups = {Report/Medical domain},
	number = {3},
	pages = {215--231},
	pmid = {11377148},
	title = {{Data mining for indicators of early mortality in a database of clinical records}},
	volume = {22},
	year = {2001}
}
@article{Ager2012a,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/Ager-CoNLL18 (1).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Binns2018,
	author = {Binns, Reuben and Kleek, Max Van and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
	file = {:E$\backslash$:/Downloads/Work/Binns2018Reducing{\_}PrePrint.pdf:pdf},
	isbn = {9781450356206},
	title = {{‘ It ' s Reducing a Human Being to a Percentage '; Perceptions of Justice in Algorithmic Decisions}},
	year = {2018}
}
@article{Arjovsky2017,
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	archivePrefix = {arXiv},
	arxivId = {1701.04862},
	author = {Arjovsky, Martin and Bottou, L{\'{e}}on},
	doi = {10.2507/daaam.scibook.2010.27},
	eprint = {1701.04862},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arjovsky, Bottou - 2017 - Towards Principled Methods for Training Generative Adversarial Networks.pdf:pdf},
	isbn = {1584880309},
	issn = {17269687},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	title = {{Towards Principled Methods for Training Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1701.04862},
	year = {2017}
}
@article{Gladkova2016,
	abstract = {This paper presents an analysis of exist- ing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evalua- tions is “interpretability” of word embed- dings: a “good” embedding produces re- sults that make sense in terms of tradi- tional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of dis- tributional meaning representations. We argue for a shift from abstract ratings of word embedding “quality” to exploration of their strengths and weaknesses},
	author = {Gladkova, Anna and Drozd, Aleksandr},
	doi = {10.18653/v1/W16-2507},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gladkova, Drozd - 2016 - Intrinsic Evaluations of Word Embeddings What Can We Do Better.pdf:pdf},
	isbn = {9781945626142},
	journal = {Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP},
	mendeley-groups = {Report/Features},
	number = {August},
	pages = {36--42},
	title = {{Intrinsic Evaluations of Word Embeddings: What Can We Do Better?}},
	year = {2016}
}
@article{Vembu2011,
	abstract = {Label ranking is a complex prediction task where the goal is to map instances to a total order over a finite set of predefined labels. An interesting aspect of this problem is that it subsumes several supervised learning problems, such as multiclass prediction, multilabel classification, and hierarchical classification. Unsurprisingly, there exists a plethora of label ranking algorithms in the literature due, in part, to this versatile nature of the problem. In this paper, we survey these algorithms.},
	author = {Vembu, Shankar and G{\"{a}}rtner, Thomas},
	doi = {10.1007/978-3-642-14125-6_3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vembu, G{\"{a}}rtner - 2011 - Label ranking algorithms A survey.pdf:pdf},
	isbn = {9783642141249},
	journal = {Preference Learning},
	pages = {45--64},
	title = {{Label ranking algorithms: A survey}},
	year = {2011}
}
@article{Ethayarajh2018,
	abstract = {A surprising property of word vectors is that vector algebra can often be used to solve word analogies. However, it is unclear why - and when - linear operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a rigorous explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has often conjectured that linear structures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel theoretical justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, providing rigorous justification for its use in capturing word dissimilarity.},
	archivePrefix = {arXiv},
	arxivId = {1810.04882},
	author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
	eprint = {1810.04882},
	file = {:D$\backslash$:/Downloads/Play/1810.04882.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	title = {{Towards Understanding Linear Word Analogies}},
	url = {http://arxiv.org/abs/1810.04882},
	year = {2018}
}
@article{Ager2018,
	author = {Ager, Thomas and Schockaert, Steven and Ager, Thomas and Schockaert, Steven and Ager, Thomas and Schockaert, Steven and Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}List{\_}of{\_}Supervisor{\_}Meetings{\_}Form.pdf:pdf},
	pages = {2018},
	title = {{PGR Review : Supervisor Meetings}},
	year = {2018}
}
@article{Reed2017,
	author = {Reed, S and Kalchbrenner, N and Bapst, V and Botvinick, M and Freitas, N De and Deepmind, Google},
	file = {:E$\backslash$:/PhD/Papedrs/Generating Interpretable Images with Controllable Structure.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {1--13},
	title = {{G ENERATING I NTERPRETABLE I MAGES WITH C ONTROLLABLE S TRUCTURE}},
	year = {2017}
}
@article{Bechavod2017,
	abstract = {We present a regularization-inspired approach for reducing bias in learned classifiers. In particular, we focus on binary classification tasks over individuals from two populations, where, as our criterion for fairness, we wish to achieve similar false positive rates in both populations, and similar false negative rates in both populations. As a proof of concept, we implement our approach and empirically evaluate its ability to achieve both fairness and accuracy, using the COMPAS scores data for prediction of recidivism.},
	archivePrefix = {arXiv},
	arxivId = {1707.00044},
	author = {Bechavod, Yahav and Ligett, Katrina},
	eprint = {1707.00044},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechavod, Ligett - 2017 - Learning Fair Classifiers A Regularization-Inspired Approach.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	pages = {6--10},
	title = {{Learning Fair Classifiers: A Regularization-Inspired Approach}},
	url = {http://arxiv.org/abs/1707.00044},
	year = {2017}
}
@article{Duch2001a,
	abstract = {A new methodology of extraction, optimization, and application of sets of logical rules is described. Neural networks are used for initial rule extraction, local or global minimization procedures for optimization, and Gaussian uncertainties of measurements are assumed during application of logical rules. Algorithms for extraction of logical rules from data with real-valued features require determination of linguistic variables or membership functions. Contest-dependent membership functions for crisp and fuzzy linguistic variables are introduced and methods of their determination described. Several neural and machine learning methods of logical rule extraction generating initial rules are described, based on constrained multilayer perceptron, networks with localized transfer functions or on separability criteria for determination of linguistic variables. A tradeoff between accuracy/simplicity is explored at the rule extraction stage and between rejection/error level at the optimization stage. Gaussian uncertainties of measurements are assumed during application of crisp logical rules, leading to "soft trapezoidal" membership functions and allowing to optimize the linguistic variables using gradient procedures. Numerous applications of this methodology to benchmark and real-life problems are reported and very simple crisp logical rules for many datasets provided.},
	author = {Duch, W??odzis??aw and Adamczak, Rafat and Gra??bczewski, Krzysztof},
	doi = {10.1109/72.914524},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duch, Adamczak, Grabczewski - 2001 - A new methodology of extraction, optimization and application of crisp and fuzzy logical rules.pdf:pdf},
	isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Backpropagation,Data mining,Decision trees,Feature selection,Fuzzy systems,Logical rule-based systems,Neural networks},
	mendeley-groups = {Progress Report},
	number = {2},
	pages = {277--306},
	pmid = {18244384},
	title = {{A new methodology of extraction, optimization and application of crisp and fuzzy logical rules}},
	volume = {12},
	year = {2001}
}
@article{Conll,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/243{\_}file{\_}Submission (1).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Words2002,
	author = {Words, Additional Key},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Words - 2002 - Cumulated Gain-based Evaluation of IR Techniques.pdf:pdf},
	mendeley-groups = {Report/Clustering/properties},
	number = {4},
	pages = {422--446},
	title = {{Cumulated Gain-based Evaluation of IR Techniques}},
	volume = {20},
	year = {2002}
}
@article{Jiang2003,
	abstract = {A new approach of constructing andtraining neural networks for pattern classi{\$}cation is proposed. Data clusters are generated andtrainedsequentially basedon distinct local subsets of the training data. Obtainedclusters are then usedto construct a feed-forwardnetwork, which is further trainedusing standardalgorithms operating on the global training set. The network obtained using this approach e6ectively inherits the knowledge from the local training procedure before improving on its generalization ability through the subsequent global training. Various experiments demonstrate the superiority of this approach over competing methods.},
	author = {Jiang, Xudong and {Kam Sie Wah}, Alvin Harvey},
	doi = {10.1016/S0031-3203(02)00087-0},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Kam Sie Wah - 2003 - Constructing and training feed-forward neural networks for pattern classification.pdf:pdf},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Classification,Clustering,Generalization,Local andglobal training,Neural networks},
	pages = {853--867},
	title = {{Constructing and training feed-forward neural networks for pattern classification}},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320302000870},
	volume = {36},
	year = {2003}
}
@article{Gardenfors2014,
	author = {G{\"{a}}rdenfors, Peter},
	doi = {10.1007/978-1-4020-9877-2},
	file = {:E$\backslash$:/Downloads/Work/Conceptual{\_}Spaces.pdf:pdf},
	isbn = {9781402098772},
	number = {October 2009},
	title = {{Conceptual spaces}},
	year = {2014}
}
@article{BurelGregoire2018,
	abstract = {Social media posts tend to provide valuable reports during crises. However, this information can be hidden in large amounts of unrelated documents. Providing tools that automatically identify relevant posts, event types (e.g., hurricane, floods, etc.) and information categories (e.g., reports on affected individuals, donations and volunteering, etc.) in social media posts is vital for their efficient handling and consumption. We introduce the Crisis Event Extraction Service (CREES), an open-source web API that automatically classifies posts during crisis situations. The API provides annotations for crisis-related documents, event types and information categories through an easily deployable and accessible web API that can be integrated into multiple platform and tools. The annotation service is backed by Convolutional Neural Networks (CNNs) and validated against traditional machine learning models. Results show that the CNN-based API results can be relied upon when dealing with specific crises with the benefits associated with the usage word embeddings.},
	author = {{Burel, Gr{\'{e}}goire} and Alani, Harith},
	file = {:E$\backslash$:/gburel{\_}iscram18.pdf:pdf},
	title = {{Crisis Event Extraction Service ( CREES ) - Automatic Detection and Classification of Crisis-related Content on Social Media on Social Media}},
	year = {2018}
}
@article{Hayes2017,
	abstract = {Shared expectations and mutual understanding are critical facets of teamwork. Achieving these in human-robot collaborative contexts can be especially challenging, as humans and robots are unlikely to share a common language to convey intentions, plans, or justifications. Even in cases where human co-workers can inspect a robot's control code, and particularly when statistical methods are used to encode control policies, there is no guarantee that meaningful insights into a robot's behavior can be derived or that a human will be able to efficiently isolate the behaviors relevant to the interaction. We present a series of algorithms and an accompanying system that enables robots to autonomously synthesize policy descriptions and respond to both general and targeted queries by human collaborators. We demonstrate applicability to a variety of robot controller types including those that utilize conditional logic, tabular reinforcement learning, and deep reinforcement learning, synthesizing informative policy descriptions for collaborators and facilitating fault diagnosis by non-experts.},
	author = {Hayes, Bradley and Shah, Julie A.},
	doi = {10.1145/2909824.3020233},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hayes, Shah - 2017 - Improving Robot Controller Transparency Through Autonomous Policy Explanation.pdf:pdf},
	isbn = {9781450343367},
	issn = {21672148},
	journal = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17},
	mendeley-groups = {Annotated/Overarching Interpretability},
	pages = {303--312},
	title = {{Improving Robot Controller Transparency Through Autonomous Policy Explanation}},
	url = {http://dl.acm.org/citation.cfm?doid=2909824.3020233},
	year = {2017}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Ganesha.docx:docx},
	title = {{Ganesha}}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - arffs.groovy:groovy},
	title = {arffs}
}

@incollection{Zhang2012,
title = {Large-Scale Sparse Principal Component Analysis with Application to Text Data},
author = {Youwei Zhang and Laurent E. Ghaoui},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {532--539},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4337-large-scale-sparse-principal-component-analysis-with-application-to-text-data.pdf}
}
@article{Ager2012h,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (12).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Management2011,
	author = {Management, Spatial and Naukowe, Bogucki Wydawnictwo},
	doi = {10.2478/v10117-011-0021-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Management, Naukowe - 2011 - COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ... COMPARISON OF VALUES OF P.pdf:pdf},
	isbn = {9788362662623},
	mendeley-groups = {Report/Features},
	number = {2},
	title = {{COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ... COMPARISON OF VALUES OF PEARSON ' S AND SPEARMAN ' S CORRELATION COEFFICIENTS ON THE SAME SETS OF DATA}},
	volume = {30},
	year = {2011}
}
@book{Joreskog,
	author = {J{\"{o}}reskog, Karl G and Wallentin, Fan Y},
	file = {:E$\backslash$:/Downloads/Work/Multivariate Analysis with LISREL.pdf:pdf},
	isbn = {9783319331522},
	title = {{Springer Series in Statistics Multivariate Analysis with LISREL}}
}
@article{Letham2010,
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. We introduce a generative model called the Bayesian List Machine for fitting decision lists, a type of interpretable classifier, to data. We use the model to predict stroke in atrial fibrillation patients, and produce predictive models that are simple enough to be understood by patients yet significantly outperform the medical scoring systems currently in use.},
	author = {Letham, Benjamin and Rudin, Cynthia and Mccormick, Tyler H and Madigan, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2010 - An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis.pdf:pdf},
	isbn = {9781577356288},
	journal = {AAAI Technical Report WS-13-17},
	keywords = {AAAI Technical Report WS-13-17},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {609},
	pages = {65--67},
	title = {{An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis}},
	year = {2010}
}
@article{Paul2008,
	author = {Paul, Michael and Girju, Roxana},
	file = {:E$\backslash$:/Downloads/Work/1730-8153-1-PB.pdf:pdf},
	keywords = {Technical Papers -- Machine Learning},
	pages = {545--550},
	title = {{A Two-Dimensional Topic-Aspect Model for Discovering Multi-Faceted Topics}},
	year = {2008}
}
@misc{Hatzilygeroudis2010,
	abstract = {Neurules are a kind of integrated rules integrating neurocomputing and production rules. Each neurule is represented as an adaline unit. Thus, the corresponding neurule base consists of a number of autonomous adaline units (neurules). In this paper, we present the construction process and the inference mechanism of neurules and explore their generalization capabilities. The construction process, which also implements corresponding learning algorithm, creates neurules from a given empirical data set. The inference mechanism of neurules is integrated in its nature; it combines neurocomputing with symbolic processes. It is also interactive, i.e., it interacts with the user asking him/her to provide values for some variables necessary to carry on inference. As shown via experiments, the neurules' integrated inference mechanism is more efficient than the inference mechanism used in connectionist expert systems. Furthermore, neurules generalize much better than their constituent neural component (adaline unit) and are comparable to the backpropagation neural net (BPNN).},
	author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
	booktitle = {IEEE Transactions on Knowledge and Data Engineering},
	doi = {10.1109/TKDE.2010.79},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatzilygeroudis, Prentzas - 2010 - Integrated rule-based learning and inference.htm:htm},
	isbn = {1041-4347},
	issn = {10414347},
	keywords = {Neurosymbolic integration,integrated inference,neurocomputing,rule-based reasoning},
	number = {11},
	pages = {1549--1562},
	title = {{Integrated rule-based learning and inference}},
	volume = {22},
	year = {2010}
}
@article{Zhang2017a,
	abstract = {The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet to establish a direct multimodal mapping between medical images and diagnostic reports that can read images, generate diagnostic reports, retrieve images by symptom descriptions, and visualize attention, to provide justifications of the network diagnosis process. MDNet includes an image model and a language model. The image model is proposed to enhance multi-scale feature ensembles and utilization efficiency. The language model, integrated with our improved attention mechanism, aims to read and explore discriminative image feature descriptions from reports to learn a direct mapping from sentence words to image pixels. The overall network is trained end-to-end by using our developed optimization strategy. Based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we conduct sufficient experiments to demonstrate that MDNet outperforms comparative baselines. The proposed image model obtains state-of-the-art performance on two CIFAR datasets as well.},
	archivePrefix = {arXiv},
	arxivId = {1707.02485},
	author = {Zhang, Zizhao and Xie, Yuanpu and Xing, Fuyong and McGough, Mason and Yang, Lin},
	doi = {10.1109/CVPR.2017.378},
	eprint = {1707.02485},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - MDNet A Semantically and Visually Interpretable Medical Image Diagnosis Network.pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {1063-6919},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	title = {{MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network}},
	url = {http://arxiv.org/abs/1707.02485},
	year = {2017}
}
@article{Simonyan2013,
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	archivePrefix = {arXiv},
	arxivId = {1312.6034},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	eprint = {1312.6034},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	pages = {1--8},
	title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
	url = {http://arxiv.org/abs/1312.6034},
	year = {2013}
}
@article{Macqueen,
	author = {Macqueen, J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Macqueen - Unknown - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MULTIVARIATE OBSERVATIONS.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	number = {233},
	pages = {281--297},
	title = {{SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MULTIVARIATE OBSERVATIONS}},
	volume = {233}
}
@article{Mikolov2013,
	abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lowerMikolov, T., Chen, K., Corrado, G., {\&} Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Arxiv, 1-12. http://doi.org/10.1162/153244303322533223 computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities. 1},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1301.3781v3},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	doi = {10.1162/153244303322533223},
	eprint = {arXiv:1301.3781v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {Arxiv},
	mendeley-groups = {Annotated/Word Vectors,Report/Features,Interim Review,Report,Thesis/Word Vectors},
	pages = {1--12},
	pmid = {18244602},
	title = {{Efficient Estimation of Word Representations in Vector Space}},
	url = {http://arxiv.org/abs/1301.3781},
	year = {2013}
}
@article{Kalchbrenner2014,
	abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25{\%} error reduction in the last task with respect to the strongest baseline.},
	archivePrefix = {arXiv},
	arxivId = {1404.2188v1},
	author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
	doi = {10.3115/v1/P14-1062},
	eprint = {1404.2188v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalchbrenner, Grefenstette, Blunsom - 2014 - A Convolutional Neural Network for Modelling Sentences.pdf:pdf},
	isbn = {9781937284725},
	journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	mendeley-groups = {!Paper 3/task/Sentiment treebank,Categories},
	pages = {655--665},
	title = {{A Convolutional Neural Network for Modelling Sentences}},
	url = {http://aclweb.org/anthology/P14-1062},
	year = {2014}
}
@article{David1992,
	author = {David, Douglass R Cuttingl and Kargerl, R and Tukey, John W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/David, Kargerl, Tukey - 1992 - Scatter Gather Browsing A Cluster-based Large Document Approach Collections to Scatter Gather.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	title = {{Scatter / Gather : Browsing A Cluster-based Large Document Approach Collections to Scatter / Gather}},
	year = {1992}
}
@article{H.~Zou2006,
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimension- ality reduction.However,PCAsuffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results.We introduce a newmethod called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings.We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. Key},
	archivePrefix = {arXiv},
	arxivId = {1205.0121v2},
	author = {H.{\~{}}Zou and T.{\~{}}Hastie and R.{\~{}}Tibshirani and Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	doi = {10.1198/106186006X113430},
	eprint = {1205.0121v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H.{\~{}}Zou et al. - 2006 - Sparse principal component analysis.pdf:pdf},
	isbn = {106186006X},
	issn = {1061-8600},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {arrays,ca 94305,composition,d student in the,department of statistics at,edu,elastic net,email,gene expression,gene expression arrays,hui zou is a,hzou,lasso,multivariate analysis,ph,singular,singular value de-,stanford,stanford university,stat,thresholding,value decomposition},
	mendeley-groups = {Annotated/NMF,11Thesis,!Paper 3},
	number = {2},
	pages = {265--286},
	pmid = {21811560},
	title = {{Sparse principal component analysis}},
	volume = {15},
	year = {2006}
}
@article{Chen2012a,
	author = {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian and Sha, Fei},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2012 - Marginalized denoising autoencoders for domain adaptation.pdf:pdf},
	journal = {arXiv preprint arXiv:1206.4683},
	keywords = {boring formatting information, machine learning, I},
	title = {{Marginalized denoising autoencoders for domain adaptation}},
	year = {2012}
}
@article{ActuarialSocietyofSouthAfrica2016,
	author = {{Actuarial Society of South Africa}},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Actuarial Society of South Africa - 2016 - Student handbook 2016.pdf:pdf},
	pages = {1--51},
	title = {{Student handbook 2016}},
	url = {http://www.actuarialsociety.org.za/Portals/2/Documents/Education Office Documents/2016 Policies/2016 Student Handbook{\_}19022016.pdf},
	year = {2016}
}
@article{Bach2015a,
	abstract = {Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally considered `black box' predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher Vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification, (3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data. All experiments are performed on the PASCAL VOC 2007 data set.},
	archivePrefix = {arXiv},
	arxivId = {1512.00172},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
	doi = {10.1109/CVPR.2016.318},
	eprint = {1512.00172},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - Analyzing Classifiers Fisher Vectors and Deep Neural Networks.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {10636919},
	mendeley-groups = {Annotated/Artifacts in the data},
	title = {{Analyzing Classifiers: Fisher Vectors and Deep Neural Networks}},
	url = {http://arxiv.org/abs/1512.00172},
	year = {2015}
}
@article{Bostrom2011,
	author = {Bostrom, Nick},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/artificialintelligence.pdf:pdf},
	keywords = {artificial intelligence, ethics},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	pages = {1--20},
	title = {{The Ethics of Artificial Intelligence}},
	year = {2011}
}
@article{Chrupaa2015,
	abstract = {We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.},
	archivePrefix = {arXiv},
	arxivId = {1506.03694},
	author = {Chrupa{\l}a, Grzegorz and K{\'{a}}d{\'{a}}r, {\'{A}}kos and Alishahi, Afra},
	eprint = {1506.03694},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chrupa{\l}a, K{\'{a}}d{\'{a}}r, Alishahi - 2015 - Learning language through pictures.pdf:pdf},
	mendeley-groups = {Progress Report},
	number = {September},
	pages = {8--9},
	title = {{Learning language through pictures}},
	url = {http://arxiv.org/abs/1506.03694},
	year = {2015}
}
@article{Barocas2016,
	abstract = {Big data claims to be neutral. It isn't.Advocates of algorithmic techniques like data mining argue that they eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data mining can inherit the prejudices of prior decision-makers or reflect the widespread biases that persist in society at large. Often, the “patterns” it discovers are simply preexisting societal patterns of inequality and exclusion. Unthinking reliance on data mining can deny members of vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm's use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Article examines these concerns through the lens of American anti-discrimination law — more particularly, through Title VII's prohibition on discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining's victims would seem to lie in disparate impact doctrine. Case law and the EEOC's Uniform Guidelines, though, hold that a practice can be justified as a business necessity where its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. As a result, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others' discrimination against members of vulnerable groups, or flaws in the underlying data.Addressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, where the discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying anti-discrimination law: nondiscrimination and anti-subordination. Finding a solution to big data's disparate impact will require more than best efforts to stamp out prejudice and bias; it will require wholesale reexamination of the meanings of “discrimination” and “fairness.”},
	author = {Barocas, Solon and Selbst, Andrew},
	doi = {http://dx.doi.org/10.15779/Z38BG31},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barocas, Selbst - 2016 - Big Data ' s Disparate Impact.pdf:pdf},
	issn = {9780262327343},
	journal = {California law review},
	mendeley-groups = {Annotated/Overarching Interpretability,11Thesis/Interpretability/Discrimination},
	number = {1},
	pages = {671--729},
	title = {{Big Data ' s Disparate Impact}},
	url = {https://ssrn.com/abstract=2477899},
	volume = {104},
	year = {2016}
}
@article{Li2016a,
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
	archivePrefix = {arXiv},
	arxivId = {1506.01066},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	eprint = {1506.01066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Visualizing and Understanding Neural Models in NLP.pdf:pdf},
	journal = {Naacl},
	keywords = {Neural Network,Visualization},
	pages = {1--10},
	title = {{Visualizing and Understanding Neural Models in NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	year = {2016}
}
@article{Chen2016,
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	file = {:E$\backslash$:/PhD/Papedrs/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted},
	number = {Nips},
	title = {{InfoGAN : Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	year = {2016}
}
@article{Zeiler2014,
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	archivePrefix = {arXiv},
	arxivId = {1311.2901},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	doi = {10.1007/978-3-319-10590-1_53},
	eprint = {1311.2901},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks arXiv1311.2901v3 cs.CV 28 Nov 2013.pdf:pdf},
	isbn = {978-3-319-10589-5},
	issn = {978-3-319-10589-5},
	journal = {Computer Vision-ECCV 2014},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {818--833},
	pmid = {26353135},
	title = {{Visualizing and Understanding Convolutional Networks arXiv:1311.2901v3 [cs.CV] 28 Nov 2013}},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53{\%}5Cnhttp://arxiv.org/abs/1311.2901{\%}5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
	volume = {8689},
	year = {2014}
}
@article{Lai1990,
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 1990 - Recurrent Convolutional Neural Networks for Text Classification.pdf:pdf},
	keywords = {NLP and Machine Learning Track},
	mendeley-groups = {Annotated/Representation Learning,!Paper 3/task/newsgroups},
	pages = {2267--2273},
	title = {{Recurrent Convolutional Neural Networks for Text Classification}},
	year = {1990}
}
@article{Bordes2012,
	abstract = {Open-text (or open-domain) semantic parsers are designed to interpret any state- ment in natural language by inferring a corresponding meaning representation (MR). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose here a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words, which are mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (WordNet and ConceptNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and word-sense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.},
	author = {Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio, Yoshua},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes et al. - 2012 - Joint learning of words and meaning representations for open-text semantic parsing.pdf:pdf},
	issn = {15337928},
	journal = {International {\ldots}},
	mendeley-groups = {Progress Report},
	pages = {127--135},
	title = {{Joint learning of words and meaning representations for open-text semantic parsing}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}BordesGWB12.pdf},
	volume = {22},
	year = {2012}
}
@article{Kayande2009,
	abstract = {Model-based decision support systems (DSS) improve performance in many contexts that are data-rich, uncertain, and require repetitive decisions. But such DSS are often not designed to help users understand and internalize the underlying factors driving DSS recommendations. Users then feel uncertain about DSS recommendations, leading them to possibly avoid using the system. We argue that a DSS must be designed to induce an alignment of a decision maker's mental model with the decision model embedded in the DSS. Such an alignment requires effort from the decision maker and guidance from the DSS. We experimentally evaluate two DSS design characteristics that facilitate such alignment: (i) feedback on the upside potential for performance improvement and (ii) feedback on corrective actions to improve decisions. We show that, in tandem, these two types of DSS feedback induce decision makers to align their mental models with the decision model, a process we call deep learning, whereas individually these two types of feedback have little effect on deep learning. We also show that deep learning, in turn, improves user evaluations of the DSS. We discuss how our findings could lead to DSS design improvements and better returns on DSS investments. [ABSTRACT FROM AUTHOR] Copyright of Information Systems Research is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	author = {Kayande, Ujwal and {De Bruyn}, Arnaud and Lilien, Gary L. and Rangaswamy, Arvind and van Bruggen, Gerrit H.},
	doi = {10.1287/isre.1080.0198},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kayande et al. - 2009 - How incorporating feedback mechanisms in a DSS affects DSS evaluations.pdf:pdf},
	isbn = {10477047},
	issn = {10477047},
	journal = {Information Systems Research},
	keywords = {DSS design,Decision support systems,Evaluations,Feedback,Learning,Mental models},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {4},
	pages = {527--546},
	title = {{How incorporating feedback mechanisms in a DSS affects DSS evaluations}},
	volume = {20},
	year = {2009}
}
@article{Letham2015,
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if {\ldots} then. . . statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1511.01644v1},
	author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
	doi = {10.1214/15-AOAS848},
	eprint = {arXiv:1511.01644v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2015 - Interpretable classifiers using rules and bayesian analysis Building a better stroke prediction model.pdf:pdf},
	isbn = {9781577356288},
	issn = {19417330},
	journal = {Annals of Applied Statistics},
	keywords = {Bayesian analysis,Classification,Interpretability},
	mendeley-groups = {Annotated/Rule-based classiifers,Report},
	number = {3},
	pages = {1350--1371},
	title = {{Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model}},
	volume = {9},
	year = {2015}
}
@article{Vellido2012a,
	abstract = {Peer Reviewed},
	author = {Vellido, Alfredo and Martin-Guerroro, J D and Lisboa, P},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vellido, Martin-Guerroro, Lisboa - 2012 - Making machine learning models interpretable.pdf:pdf},
	isbn = {9782874190490},
	journal = {20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
	mendeley-groups = {Report},
	number = {April},
	pages = {163--172},
	title = {{Making machine learning models interpretable}},
	year = {2012}
}
@article{Lee2012,
abstract = {Linear rankSVM is one of the widely used methods for learning to rank. Although its performance may be inferior to nonlinear methods such as kernel rankSVM and gradient boosting decision trees, linear rankSVM is useful to quickly produce a baseline model. Furthermore, following its recent development for classification, linear rankSVM may give competitive performance for large and sparse data. A great deal of works have studied linear rankSVM. The focus is on the computational efficiency when the number of preference pairs is large. In this letter, we systematically study existing works, discuss their advantages and disadvantages, and propose an efficient algorithm. We discuss different implementation issues and extensions with detailed experiments. Finally, we develop a robust linear rankSVM tool for public use. {\textcopyright} 2014 Massachusetts Institute of Technology.},
author = {Lee, Ching Pei and Lin, Chih Jen},
doi = {10.1162/NECO_a_00571},
file = {:E$\backslash$:/ranksvml2.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
keywords = {large-scale,learning,learning to rank,linear model,ranking support vector machines},
number = {4},
pages = {781--817},
title = {{Large-scale linear rankSVM}},
volume = {26},
year = {2014}
}

@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - vpetite.txt:txt},
	title = {vpetite}
}
@article{Yan2015,
	author = {Yan, Yan and Yin, Xu-cheng and Li, Sujian and Yang, Mingyuan and Hao, Hong-wei},
	file = {:E$\backslash$:/Downloads/Work/650527.pdf:pdf},
	title = {{Learning Document Semantic Representation with Hybrid Deep Belief Network}},
	volume = {2015},
	year = {2015}
}
@article{Marshal2016,
	author = {Marshal, David and Lai, Yukun and Marshal, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marshal, Lai, Marshal - 2016 - Part 1 PhD progress review(2).pdf:pdf},
	title = {{Part 1 : PhD progress review}},
	year = {2016}
}
@article{Kusner2010,
	author = {Kusner, Matt J and Weinberger, Kilian Q and Louis, St and Edu, Kilian Wustl},
	file = {:E$\backslash$:/kusnerb15.pdf:pdf},
	title = {{From Word Embeddings To Document Distances}},
	year = {2010}
}
@article{Deep2016,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1206.4683},
	author = {Deep, E Harnessing and Networks, Neural},
	eprint = {arXiv:1206.4683},
	file = {:E$\backslash$:/Downloads/Work/Reference List.pdf:pdf},
	isbn = {2200000006},
	pages = {787--795},
	title = {{Reference List}},
	volume = {3800},
	year = {2016}
}
@article{Yang2017,
	abstract = {We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.},
	archivePrefix = {arXiv},
	arxivId = {1711.03953},
	author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
	eprint = {1711.03953},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2017 - Breaking the Softmax Bottleneck A High-Rank RNN Language Model.pdf:pdf},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
	pages = {1--18},
	title = {{Breaking the Softmax Bottleneck: A High-Rank RNN Language Model}},
	url = {http://arxiv.org/abs/1711.03953},
	year = {2017}
}
@article{Murdoch2017,
	abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
	archivePrefix = {arXiv},
	arxivId = {1702.02540},
	author = {Murdoch, W. James and Szlam, Arthur},
	doi = {10.5121/ijci.2015.4221},
	eprint = {1702.02540},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Szlam - 2017 - Automatic Rule Extraction from Long Short Term Memory Networks.pdf:pdf},
	issn = {23208430},
	mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/task/Interpretable LSTMs,!Paper 3/task,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank},
	number = {2016},
	pages = {1--12},
	title = {{Automatic Rule Extraction from Long Short Term Memory Networks}},
	url = {http://arxiv.org/abs/1702.02540},
	year = {2017}
}
@article{Towell1993,
	author = {Towell, G and Shavlik, J W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik - 1993 - The Extraction of Refined Rules From Knowledge Based Neural Networks.pdf:pdf},
	journal = {Machine Learning},
	keywords = {integrated learning},
	number = {1},
	pages = {71--101},
	title = {{The Extraction of Refined Rules From Knowledge Based Neural Networks}},
	volume = {13},
	year = {1993}
}
@article{Caster,
	author = {Caster, The Magic},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 9 - The Magic Caster of Destroy.pdf:pdf},
	title = {{No Title}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Completed Annual Progress Review Form (1).pdf:pdf},
	title = {{Part {\%} 1 {\%}:{\%} PhD {\%} progress {\%} review {\%}}}
}
@article{Read2013,
	abstract = {abstract Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs compara-tive experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
	author = {Read, Jesse},
	doi = {10.4018/978-1-60566-058-5.ch021},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2013 - Multi-label Classification.pdf:pdf},
	isbn = {978-1-4244-1065-1},
	issn = {1548-3924},
	mendeley-groups = {Report/Multi-label},
	pages = {2002--2004},
	title = {{Multi-label Classification}},
	year = {2013}
}
@article{OldenD.A.2002a,
	author = {{Olden  D. A.}, J D Y Jackson},
	doi = {10.1016/S0304-3800(02)00064-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olden D. A. - 2002 - Illuminating the black box a ramdomization approach for understanding variable contributions in artificial neuronal.pdf:pdf},
	issn = {03043800},
	journal = {Ecological Modelling},
	keywords = {connection weights, sensitivity analysis, neural i},
	mendeley-groups = {Literature Review},
	pages = {135--150},
	title = {{Illuminating the "black box": a ramdomization approach for understanding variable contributions in artificial neuronal networks.}},
	volume = {154},
	year = {2002}
}
@article{Lacoste-Julien2008,
	abstract = {Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in finding  a reduced dimensionality representation.  Specifically, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood.  By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification task and show how our model can identify shared topics across classes as well as class-dependent topics.},
	author = {Lacoste-Julien, Simon and Sha, Fei and Jordan, Michael I.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lacoste-Julien, Sha, Jordan - 2008 - DiscLDA Discriminative Learning for Dimensionality Reduction and Classification.pdf:pdf},
	isbn = {9781605609492},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {Computational, Information-Theoretic Learning with,Information Retrieval {\&} Textual Information Access,Learning/Statistics {\&} Optimisation},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,!Paper 3/task/newsgroups},
	pages = {1--8},
	title = {{DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification}},
	url = {http://eprints.pascal-network.org/archive/00004292/},
	volume = {21},
	year = {2008}
}
@article{Burel2018,
	author = {Burel, Gr{\'{e}}goire and Alani, Harith},
	file = {:E$\backslash$:/73af4bf3b35e194724dc64e784dabe30c689.pdf:pdf},
	keywords = {api,convolutional neural networks,deep learning,event detection,word embeddings},
	mendeley-groups = {11Thesis/Applications},
	number = {May},
	title = {{Crisis Event Extraction Service ( CREES ) - Automatic Detection and Classification of Crisis-related Content on Social Media}},
	year = {2018}
}
@article{Schockaert2016,
	author = {Schockaert, Supervisor Steven and Ager, Thomas},
	file = {:E$\backslash$:/Downloads/Work/report1 (1).pdf:pdf},
	pages = {1--3},
	title = {{Thomas Ager : 9 Month Report}},
	year = {2016}
}
@article{Marchant2009,
	abstract = {The biomisation method is used to reconstruct Latin American vegetation at 6000±500 and 18 000±1000 radiocarbon years before present ({\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP) from pollen data. Tests using modern pollen data from 381 samples derived from 287 locations broadly reproduce potential natural vegetation. The strong temperature gradient associated with the Andes is recorded by a transition from high altitude cool grass/shrubland and cool mixed forest to mid-altitude cool temperate rain forest, to tropical dry, seasonal and rain forest at low altitudes. Reconstructed biomes from a number of sites do not match the potential vegetation due to local factors such as human impact, methodological artefacts and mechanisms of pollen representivity of the parent vegetation. At 6000±500 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP 255 samples are analysed from 127 sites. Differences between the modern and the 6000±500 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP reconstruction are comparatively small; change relative to the modern reconstruction are mainly to biomes characteristic of drier climate in the north of the region with a slight more mesic shift in the south. Cool temperate rain forest remains dominant in western South America. In northwestern South America a number of sites record transitions from tropical seasonal forest to tropical dry forest and tropical rain forest to tropical seasonal forest. Sites in Central America show a change in biome assignment, but to more mesic vegetation, indicative of greater plant available moisture, e.g. on the Yucat{\`{a}}n peninsula sites record warm evergreen forest, replacing tropical dry forest and warm mixed forest presently recorded. At 18 000±1000 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP 61 samples from 34 sites record vegetation reflecting a generally cool and dry environment. Cool grass/shrubland is prevalent in southeast Brazil whereas Amazonian sites record tropical dry forest, warm temperate rain forest and tropical seasonal forest. Southernmost South America is dominated by cool grass/shrubland, a single site retains cool temperate rain forest indicating that forest was present at some locations at the LGM. Some sites in Central Mexico and lowland Colombia remain unchanged in the biome assignments of warm mixed forest and tropical dry forest respectively, although the affinities that these sites have to different biomes do change between 18 000±1000 {\textless}sup{\textgreater}14{\textless}/sup{\textgreater}C yr BP and present. The "unresponsive" nature of these sites results from their location and the impact of local edaphic influence. [ABSTRACT FROM AUTHOR] Copyright of Climate of the Past is the property of Copernicus Gesellschaft mbH and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	author = {Marchant, R and Cleef, A and Harrison, S P and Hooghiemstra, H and Markgraf, V and van Boxel, J and Ager, T and Almeida, L and Anderson, R and Baied, C and Behling, H and Berrio, J C and Burbridge, R and Bjorck, S and Byrne, R and Bush, M and Duivenvoorden, J and Flenley, J and {De Oliveira}, P and van Geel, B},
	journal = {Climate of the Past},
	keywords = {BIOTIC communities -- Research,CARBON,CLIMATIC changes -- Research,CLIMATOLOGY -- Research,ISOTOPES,LATIN America,RADIOCARBON dating,VEGETATION {\&} climate},
	number = {4},
	pages = {725--767},
	publisher = {Copernicus Gesellschaft mbH},
	title = {{Pollen-based biome reconstructions for Latin America at 0, 6000 and 18000 radiocarbon years ago}},
	url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=eih{\&}AN=47908619{\&}site=ehost-live},
	volume = {5},
	year = {2009}
}
@article{Ager2012f,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (8).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Strobelt2018,
	abstract = {Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.},
	archivePrefix = {arXiv},
	arxivId = {1606.07461},
	author = {Strobelt, Hendrik and Gehrmann, Sebastian and Pfister, Hanspeter and Rush, Alexander M.},
	doi = {10.1109/TVCG.2017.2744158},
	eprint = {1606.07461},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobelt et al. - 2018 - LSTMVis A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks.pdf:pdf},
	isbn = {1077-2626 VO - PP},
	issn = {10772626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {LSTM,Machine Learning,Recurrent Neural Networks,Visualization},
	mendeley-groups = {!Paper 3,!Paper 3/Understanding LSTMs},
	number = {1},
	pages = {667--676},
	pmid = {28866526},
	title = {{LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks}},
	volume = {24},
	year = {2018}
}
@inproceedings{Chen2014,
	abstract = {Distributional semantics and frame semantics are two representative views on language understanding in the statistical world and the linguistic world, respectively. In this paper, we combine the best of two worlds to automatically induce the semantic slots for spoken dialogue systems. Given a collection of unlabeled audio files, we exploit continuous-valued word embeddings to augment a probabilistic frame-semantic parser that identifies key semantic slots in an unsupervised fashion. In experiments, our results on a real-world spoken dialogue dataset show that the distributional word representations significantly improve the adaptation of FrameNet-style parses of ASR decodings to the target semantic space; that comparing to a state-of-the-art baseline, a 13{\%} relative average precision improvement is achieved by leveraging word vectors trained on two 100-billion words datasets; and that the proposed technology can be used to reduce the costs for designing task-oriented spoken dialogue systems.},
	author = {Chen, Yun Nung and Wang, William Yang and Rudnicky, Alexander I.},
	booktitle = {2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings},
	doi = {10.1109/SLT.2014.7078639},
	isbn = {9781479971299},
	keywords = {Distributional semantics,Frame semantics,Unsupervised slot induction},
	mendeley-groups = {Report/Features},
	pages = {584--589},
	title = {{Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems}},
	year = {2014}
}
@article{Liang2017a,
	abstract = {This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1703.03055},
	author = {Liang, Xiaodan and Lin, Liang and Shen, Xiaohui and Feng, Jiashi and Yan, Shuicheng and Xing, Eric P.},
	doi = {10.1109/CVPR.2017.234},
	eprint = {1703.03055},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2017 - Interpretable Structure-Evolving LSTM(2).pdf:pdf},
	issn = {1703.03055},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	number = {61622214},
	pages = {1010--1019},
	title = {{Interpretable Structure-Evolving LSTM}},
	url = {http://arxiv.org/abs/1703.03055},
	year = {2017}
}
@article{Bologna,
	author = {Bologna, Guido and Pellegrini, Christian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna, Pellegrini - Unknown - Accurate Decomposition of Standard MLP Classification Responses into Symbolic Rules.pdf:pdf},
	mendeley-groups = {Progress Report},
	title = {{Accurate Decomposition of Standard MLP Classification Responses into Symbolic Rules}}
}
@article{Ding2015,
	abstract = {We propose a deep learning method for event- driven stock market prediction. First, events are extracted from news text, and represented as dense vectors, trained using a novel neural tensor net- work. Second, a deep convolutional neural network is used to model both short-term and long-term in- fluences of events on stock price movements. Ex- perimental results show that our model can achieve nearly 6{\%} improvements on S{\&}P 500 index predic- tion and individual stock prediction, respectively, compared to state-of-the-art baseline methods. In addition, market simulation results show that our system is more capable of making profits than pre- viously reported systems trained on S{\&}P 500 stock historical data.},
	author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2015 - Deep learning for event-driven stock prediction.pdf:pdf},
	isbn = {9781577357384},
	issn = {10450823},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	keywords = {Technical Papers — Web Mining},
	number = {Ijcai},
	pages = {2327--2333},
	title = {{Deep learning for event-driven stock prediction}},
	volume = {2015-Janua},
	year = {2015}
}
@article{Ros2017,
	author = {Ros, Andrew Slavin and Doshi-velez, Finale},
	file = {:E$\backslash$:/Downloads/Work/17337-76626-1-PB.pdf:pdf},
	keywords = {Humans and Artificial Intelligence Track},
	pages = {1660--1669},
	title = {{Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients}},
	year = {2017}
}

@article{Towell1990,
	author = {Towell, Geoffrey G and Shavlik, Jude W and Noordeweir, Michiel O},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik, Noordeweir - 1990 - Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks.pdf:pdf},
	journal = {Proceedings of the Eighth National Conference on Artificial Intelligence},
	pages = {861--866},
	title = {{Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks}},
	year = {1990}
}
@article{Rubin2012,
	abstract = {Machine learning approaches to multi-label document classification have to date largely relied on discriminative modeling techniques such as support vector machines. A drawback of these approaches is that performance rapidly drops off as the total number of labels and the number of labels per document increase. This problem is amplified when the label frequencies exhibit the type of highly skewed distributions that are often observed in real-world datasets. In this paper we investigate a class of generative statistical topic models for multi-label documents that associate individual word tokens with different labels. We investigate the advantages of this approach relative to discriminative models, particularly with respect to classification problems involving large numbers of relatively rare labels. We compare the performance of generative and discriminative approaches on document labeling tasks ranging from datasets with several thousand labels to datasets with tens of labels. The experimental results indicate that probabilistic generative models can achieve competitive multi-label classification performance compared to discriminative methods, and have advantages for datasets with many labels and skewed label frequencies.},
	archivePrefix = {arXiv},
	arxivId = {1107.2462},
	author = {Rubin, Timothy N. and Chambers, America and Smyth, Padhraic and Steyvers, Mark},
	doi = {10.1007/s10994-011-5272-5},
	eprint = {1107.2462},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubin et al. - 2012 - Statistical topic models for multi-label document classification.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Dependency-LDA,Document modeling,Graphical models,LDA,Multi-label classification,Probabilistic generative models,Text classification,Topic models},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	number = {1-2},
	pages = {157--208},
	title = {{Statistical topic models for multi-label document classification}},
	volume = {88},
	year = {2012}
}
@article{Nogueira2011,
	author = {Nogueira, T.M. and Camargo, H.a. and Rezende, S.O.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Camargo, Rezende - 2011 - Fuzzy Rules for Document Classification to Improve Information Retrieval.pdf:pdf},
	journal = {Mirlabs.Org},
	keywords = {fuzzy clustering,imprecision,information retrieval,text categorization,text mining,uncertainty},
	mendeley-groups = {Annotated/Decision Trees},
	pages = {210--217},
	title = {{Fuzzy Rules for Document Classification to Improve Information Retrieval}},
	url = {http://www.mirlabs.org/ijcisim/regular{\_}papers{\_}2011/Paper25.pdf},
	volume = {3},
	year = {2011}
}
@misc{Srivastava2014,
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
	archivePrefix = {arXiv},
	arxivId = {1102.4807},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	booktitle = {Journal of Machine Learning Research (JMLR)},
	doi = {10.1214/12-AOS1000},
	eprint = {1102.4807},
	isbn = {1532-4435},
	issn = {15337928},
	keywords = {deep learning,model combination,neural networks,regularization},
	pages = {1929--1958},
	title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
	volume = {15},
	year = {2014}
}
@article{Han2010a,
	abstract = {Due to the visually polysemous barrier, videos and images may be annotated by multiple tags. Discovering the correlations among different tags can significantly help predicting precise labels for videos and images. Many of recent studies toward multi-label learning construct a linear subspace embed- ding with encoded multi-label information, such that data points sharing many common labels tend to be close to each other in the embedded subspace. Motivated by the advances of compressive sensing research, a sparse representation that selects a compact subset to describe the input data can be more discriminative. In this paper, we propose a sparse multi-label learning method to circumvent the visually polysemous barrier of multiple tags. Our approach learns a multi-label encoded sparse linear embedding space from a related dataset, and maps the target data into the learned new representation space to achieve better annotation performance. Instead of using l1-norm penalty (lasso) to induce sparse representation, we propose to formulate the multi-label learning as a penalized least squares optimization problem with elastic-net penalty. By casting the video concept detection and image annotation tasks into a sparse multi-label transfer learning framework in this paper, ridge regression, lasso, elastic net, and the multi-label extended sparse discriminant analysis methods are, respectively, well explored and compared},
	author = {Han, Yahong and Wu, Fei and Zhuang, Yueting and He, Xiaofei},
	doi = {10.1109/TCSVT.2010.2057015},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2010 - Multi-label transfer learning with sparse representation.pdf:pdf},
	issn = {10518215},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Image annotation,multi-label learning,sparse representation,transfer learning,video concept detection},
	number = {8},
	pages = {1110--1121},
	title = {{Multi-label transfer learning with sparse representation}},
	volume = {20},
	year = {2010}
}
@article{Nguyen2014,
	author = {Nguyen, Tien T and Maxwell, Pik-mai Hui F and Loren, Harper and Joseph, Terveen},
	file = {:E$\backslash$:/PhD/Papedrs/nej1.pdf:pdf},
	isbn = {9781450327442},
	keywords = {content diversity,experience,filter bubble,longitudinal data,offerings,recommender system,sonalized product and information,tag-genome,they play a,user},
	mendeley-groups = {11Thesis/Interpretability/GAN's and VAE},
	pages = {677--686},
	title = {{Exploring the Filter Bubble : The Effect of Using Recommender Systems on Content Diversity}},
	year = {2014}
}
@article{Donahue2011,
	abstract = {Traditional supervised visual learning simply asks annotators {\&}{\#}x201C;what{\&}{\#}x201D; label an image should have. We propose an approach for image classification problems requiring subjective judgment that also asks {\&}{\#}x201C;why{\&}{\#}x201D;, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the first, the annotator highlights the spatial region of interest he found most influential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to refine the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices.},
	author = {Donahue, Jeff and Grauman, Kristen},
	doi = {10.1109/ICCV.2011.6126394},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue, Grauman - 2011 - Annotator rationales for visual recognition.pdf:pdf},
	isbn = {9781457711015},
	issn = {1550-5499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	mendeley-groups = {Report/Explaining predictions,Annotated},
	number = {Iccv},
	pages = {1395--1402},
	title = {{Annotator rationales for visual recognition}},
	year = {2011}
}
@article{Plikynas2004a,
	author = {Plikynas, Darius},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plikynas - 2004 - Decision rules extraction from neural network A modified pedagogical approach.pdf:pdf},
	journal = {Information Technology and Control},
	keywords = {decisions reasonong,information extraction,neural networks},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {31},
	pages = {53--59},
	title = {{Decision rules extraction from neural network: A modified pedagogical approach}},
	url = {http://itc.ktu.lt/itc31/Plikyn31.pdf},
	volume = {2},
	year = {2004}
}
@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Fine-Tuning Vector Space Representations for Interpretable Text Classification.pdf:pdf},
	title = {{Fine-Tuning Vector Space Representations for Interpretable Text Classification}}
}
@article{Ager2012e,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (7).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Tian2011,
	abstract = {We use the term "index predictor" to denote a score that consists of K binary rules such as "age {\textgreater} 60" or "blood pressure {\textgreater} 120 mm Hg." The index predictor is the sum of these binary scores, yielding a value from 0 to K. Such indices as often used in clinical studies to stratify population risk: They are usually derived from subject area considerations. In this paper, we propose a fast data-driven procedure for automatically constructing such indices for linear, logistic, and Cox regression models. We also extend the procedure to create indices for detecting treatment-marker interactions. The methods are illustrated on a study with protein biomarkers as well as a large microarray gene expression study.},
	author = {Tian, Lu and Tibshirani, Robert},
	doi = {10.1093/biostatistics/kxq047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Tibshirani - 2011 - Adaptive index models for marker-based risk stratification.pdf:pdf},
	isbn = {1468-4357 (Electronic)$\backslash$r1465-4644 (Linking)},
	issn = {14654644},
	journal = {Biostatistics},
	keywords = {Degree of freedom,Index predictor,International prognostic index},
	mendeley-groups = {Annotated/Applications/Medical},
	number = {1},
	pages = {68--86},
	pmid = {20663850},
	title = {{Adaptive index models for marker-based risk stratification}},
	volume = {12},
	year = {2011}
}
@article{Hailesilassie2016,
	author = {Hailesilassie, Tameru},
	file = {:E$\backslash$:/PhD/1610.05267.pdf:pdf},
	keywords = {- artificial neural network,decompositional,deep neural network,eclectic,pedagogical,rule extraction},
	number = {7},
	pages = {376--381},
	title = {{Rule Extraction Algorithm for Deep Neural Networks:}},
	volume = {14},
	year = {2016}
}
@article{Zhang2014,
	abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
	author = {Zhang, Min Ling and Zhou, Zhi Hua},
	doi = {10.1109/TKDE.2013.39},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - 2014 - A review on multi-label learning algorithms.pdf:pdf},
	isbn = {1041-4347},
	issn = {10414347},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Multi-label learning,algorithm adaptation,label correlations-problem transformation},
	mendeley-groups = {Progress Report},
	number = {8},
	pages = {1819--1837},
	title = {{A review on multi-label learning algorithms}},
	volume = {26},
	year = {2014}
}
@article{Chuang2012,
	abstract = {Topic models aid analysis of text corpora by identifying la- tent topics based on co-occurring words. Real-world de- ployments of topic models, however, often require intensive expert verification and model refinement. In this paper we present Termite, a visual analysis tool for assessing topic model quality. Termite uses a tabular layout to promote comparison of terms both within and across latent topics. We contribute a novel saliency measure for selecting relevant terms and a seriation algorithm that both reveals clustering structure and promotes the legibility of related terms. In a series of examples, we demonstrate how Termite allows analysts to identify coherent and significant themes.},
	author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
	doi = {10.1145/2254556.2254572},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chuang, Manning, Heer - 2012 - Termite Visualization Techniques for Assessing Textual Topic Models.pdf:pdf},
	isbn = {9781450312875},
	journal = {Proceedings of the International Working Conference on Advanced Visual Interfaces - AVI '12},
	keywords = {seriation,text visualization,topic models},
	pages = {74},
	title = {{Termite : Visualization Techniques for Assessing Textual Topic Models}},
	url = {http://dl.acm.org/citation.cfm?doid=2254556.2254572},
	year = {2012}
}
@article{Socher2013,
	abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
	archivePrefix = {arXiv},
	arxivId = {1512.03385},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
	doi = {10.1371/journal.pone.0073791},
	eprint = {1512.03385},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank(2).pdf:pdf},
	isbn = {9781937284978},
	issn = {1932-6203},
	journal = {Proceedings of the {\ldots}},
	mendeley-groups = {!Paper 3/task/Sentiment treebank},
	number = {October},
	pages = {1631--1642},
	pmid = {24086296},
	title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
	url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
	year = {2013}
}
@misc{Cambria2013a,
	abstract = {The way people express their opinions has radically changed in the past few years thanks to the advent of online collaborative media. The distillation of knowledge from this huge amount of unstructured information can be a key factor for marketers who want to create an identity for their product or brand in the minds of their customers. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. Existing approaches to opinion mining, in fact, are still far from being able to infer the cognitive and affective information associated with natural language as they mainly rely on knowledge bases that are too limited to efficiently process text at concept-level. In this context, standard clustering techniques have been previously employed on an affective common-sense knowledge base in attempt to discover how different natural language concepts are semantically and affectively related to each other and, hence, to accordingly mine on-line opinions. In this work, a novel cognitive model based on the combined use of multi-dimensional scaling and artificial neural networks is exploited for better modelling the way multi-word expressions are organised in a brain-like universe of natural language concepts. The integration of a biologically inspired paradigm with standard principal component analysis helps to better grasp the non-linearities of the resulting vector space and, hence, improve the affective common-sense reasoning capabilities of the system. ?? 2012 Elsevier B.V. All rights reserved.},
	author = {Cambria, Erik and Mazzocco, Thomas and Hussain, Amir},
	booktitle = {Biologically Inspired Cognitive Architectures},
	doi = {10.1016/j.bica.2013.02.003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cambria, Mazzocco, Hussain - 2013 - Application of multi-dimensional scaling and artificial neural networks for biologically inspired op.htm:htm},
	isbn = {1467351644},
	issn = {2212683X},
	keywords = {AI,ANN,Cognitive modelling,NLP,Sentic computing},
	pages = {41--53},
	title = {{Application of multi-dimensional scaling and artificial neural networks for biologically inspired opinion mining}},
	volume = {4},
	year = {2013}
}
@article{Zafar2017,
	abstract = {The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness--given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design convex margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.},
	archivePrefix = {arXiv},
	arxivId = {1707.00010},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1707.00010},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf:pdf},
	journal = {arXiv:1707.00010 [cs, stat]},
	keywords = {Algorithmic fairness,Machine learning},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{From Parity to Preference-based Notions of Fairness in Classification}},
	url = {http://arxiv.org/abs/1707.00010},
	year = {2017}
}
@article{Schnabel2015,
	abstract = {We present a comprehensive study of eval- uation methods for unsupervised embed- ding techniques that obtain meaningful representations ofwords from text. Differ- ent evaluations result in different orderings of embedding methods, calling into ques- tion the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods re- duce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.},
	author = {Schnabel, Tobias and Labutov, Igor and Mimno, David and Joachims, Thorsten},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schnabel et al. - 2015 - Evaluation methods for unsupervised word embeddings.pdf:pdf},
	isbn = {9781941643327},
	journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	keywords = {Distributional semantics,Evaluation techniques},
	mendeley-groups = {Report/Features},
	number = {September},
	pages = {298--307},
	pmid = {1847047},
	title = {{Evaluation methods for unsupervised word embeddings}},
	year = {2015}
}
@article{Palangi2017,
abstract = {We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In our application of TPRN, internal representations-learned by end-to-end optimization in a deep neural network performing a textual question-answering (QA) task-can be interpreted using basic concepts from linguistic theory. No performance penalty need be paid for this increased interpretability: the proposed model performs comparably to a state-of-the-art system on the SQuAD QA task. The internal representation which is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Fine-grained analysis reveals specific correspondences between the learned roles and parts of speech as assigned by a standard tagger (Toutanova et al. 2003), and finds several discrepancies in the model's favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means to build representations using symbols and roles, with an inductive bias favoring use of these in an approximately discrete manner.},
archivePrefix = {arXiv},
arxivId = {1705.08432},
author = {Palangi, Hamid and Smolensky, Paul and He, Xiaodong and Deng, Li},
eprint = {1705.08432},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palangi et al. - 2017 - Question-Answering with Grammatically-Interpretable Representations.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
pages = {5350--5357},
title = {{Question-answering with grammatically-interpretable representations}},
url = {http://arxiv.org/abs/1705.08432},
year = {2018}
}

@article{Laine2016,
	abstract = {In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63{\%} to 12.89{\%} in CIFAR-10 with 4000 labels and from 18.44{\%} to 6.83{\%} in SVHN with 500 labels.},
	archivePrefix = {arXiv},
	arxivId = {1610.02242},
	author = {Laine, Samuli and Aila, Timo},
	eprint = {1610.02242},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laine, Aila - 2016 - Temporal Ensembling for Semi-Supervised Learning.pdf:pdf},
	mendeley-groups = {Progress Report},
	number = {November 2016},
	pages = {1--9},
	title = {{Temporal Ensembling for Semi-Supervised Learning}},
	url = {http://arxiv.org/abs/1610.02242},
	year = {2016}
}
@article{Iyyer2015a,
	author = {Iyyer, M and Manjunatha, V},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iyyer, Manjunatha - 2015 - Deep unordered composition rivals syntactic methods for text classification.pdf:pdf},
	journal = {Proceedings of the 53rd  {\ldots}},
	mendeley-groups = {Literature Review},
	title = {{Deep unordered composition rivals syntactic methods for text classification}},
	url = {https://www.cs.colorado.edu/{~}jbg/docs/2015{\_}acl{\_}dan.pdf},
	year = {2015}
}
@article{Burges2005,
	author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
	doi = {10.1145/1102351.1102363},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:pdf},
	isbn = {1595931805},
	issn = {00243205},
	journal = {Icml 2005},
	keywords = {Learning to Rank,RankNet},
	mendeley-groups = {Progress Report},
	pages = {89--96},
	pmid = {16483612},
	title = {{Learning to rank using gradient descent}},
	year = {2005}
}
@article{Mascharka,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1803.05268v1},
	author = {Mascharka, David},
	eprint = {arXiv:1803.05268v1},
	title = {{Transparency by Design : Closing the Gap Between Performance and Interpretability in Visual Reasoning}}
}
@article{Cao2015,
	author = {Cao, Tru and Lim, Ee Peng and Zhou, Zhi Hua and Ho, Tu Bao and Cheung, David and Motoda, Hiroshi},
	doi = {10.1007/978-3-319-18038-0},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2015 - Advances in knowledge discovery and data mining 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam,.pdf:pdf},
	isbn = {9783319180373},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Annotated/Document representation},
	pages = {212--225},
	title = {{Advances in knowledge discovery and data mining: 19th pacific-asia conference, PAKDD 2015 Ho Chi Minh City, Vietnam, May 19-22, 2015 proceedings, part I}},
	volume = {9077},
	year = {2015}
}
@article{Bhatia2015,
	abstract = {Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classification-based methods.},
	archivePrefix = {arXiv},
	arxivId = {1509.01599},
	author = {Bhatia, Parminder and Ji, Yangfeng and Eisenstein, Jacob},
	eprint = {1509.01599},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatia, Ji, Eisenstein - 2015 - Better Document-level Sentiment Analysis from RST Discourse Parsing.pdf:pdf},
	isbn = {9781941643327},
	mendeley-groups = {!Paper 3/Structured LSTMs,!Paper 3/task/Sentiment treebank},
	title = {{Better Document-level Sentiment Analysis from RST Discourse Parsing}},
	url = {http://arxiv.org/abs/1509.01599},
	year = {2015}
}
@article{Leek2014,
	author = {Leek, Jeffrey T},
	doi = {10.1038/nrg2825.Tackling},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leek - 2014 - NIH Public Access.pdf:pdf},
	mendeley-groups = {Annotated/Artifacts in the data},
	number = {10},
	pages = {1--15},
	title = {{NIH Public Access}},
	volume = {11},
	year = {2014}
}
@article{Zhao,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1906.06719v1},
	author = {Zhao, Shenjian},
	eprint = {arXiv:1906.06719v1},
	file = {:E$\backslash$:/PhD/Papedrs/1906.06719.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Fixing Gaussian Mixture VAEs for Interpretable Text Generation}}
}
@article{Melis2017,
	abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
	archivePrefix = {arXiv},
	arxivId = {1707.05589},
	author = {Melis, G{\'{a}}bor and Dyer, Chris and Blunsom, Phil},
	eprint = {1707.05589},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melis, Dyer, Blunsom - 2017 - On the State of the Art of Evaluation in Neural Language Models.pdf:pdf},
	isbn = {9781604562170},
	mendeley-groups = {!Paper 3/Language models},
	pages = {1--10},
	title = {{On the State of the Art of Evaluation in Neural Language Models}},
	url = {http://arxiv.org/abs/1707.05589},
	year = {2017}
}
@article{Lewis1993,
author = {Lewis, David D. and Jones, Karen Sp\"{a}rck},
title = {Natural Language Processing for Information Retrieval},
year = {1996},
issue_date = {January 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/234173.234210},
doi = {10.1145/234173.234210},
journal = {Commun. ACM},
month = jan,
pages = {92-101},
numpages = {10}
}

@article{Science2018,
	author = {Science, Computer and Monitoring, Progress and Report, Interim Review},
	file = {:E$\backslash$:/Downloads/Work/InterimReviewForm (2).pdf:pdf},
	title = {{Interim Report Student Details Schockaert Podraig Corcoran October 2015 October 2018 December 2016 Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis Obtaining and improving properties , rankings and rules from neural networks that proc}},
	year = {2018}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Multivariate Data Analysis 7th Edition.pdf:pdf},
	title = {{Multivariate Data Analysis 7th Edition.pdf}}
}
@article{Zhao2018,
	abstract = {Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.},
	archivePrefix = {arXiv},
	arxivId = {1809.01496},
	author = {Zhao, Jieyu and Zhou, Yichao and Li, Zeyu and Wang, Wei and Chang, Kai-Wei},
	eprint = {1809.01496},
	file = {:D$\backslash$:/Downloads/Play/1809.01496.pdf:pdf},
	issn = {0029-2303 (Print)},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis/Interpretability/Discrimination},
	pmid = {3270028},
	title = {{Learning Gender-Neutral Word Embeddings}},
	url = {http://arxiv.org/abs/1809.01496},
	year = {2018}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCE04062018.pdf:pdf},
	title = {{CCE04062018 (1).pdf}}
}
@article{Bechberger,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.04825v1},
	author = {Bechberger, Lucas},
	eprint = {arXiv:1706.04825v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bechberger - Unknown - Neural Representations.pdf:pdf},
	mendeley-groups = {Annotated/Conceptual Spaces and Neural Networks,Annotated/Generative Adversarial Nets},
	title = {{Neural Representations}}
}
@article{Fu1998a,
	author = {Fu, Limin},
	doi = {10.1109/72.712152},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu - 1998 - A neural-network model for learning domain rules based on its activation function characteristics.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Certainty factor,Generalization,Machine learning,Neural network,Rule learning,Sample complexity,VC-dimension},
	number = {5},
	pages = {787--795},
	title = {{A neural-network model for learning domain rules based on its activation function characteristics}},
	volume = {9},
	year = {1998}
}
@article{Curry,
	author = {Curry, Edward and Buitelaar, Paul},
	file = {:C$\backslash$:/Users/Workk/Documents/preprint{\_}nldb{\_}{\_}commonsense{\_}2014.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	pages = {1--12},
	title = {{A Distributional Semantics Approach for Selective Reasoning on Commonsense Graph Knowledge Bases}}
}
@article{Peng2015a,
	abstract = {Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information.},
	archivePrefix = {arXiv},
	arxivId = {1506.07220},
	author = {Peng, Yangtuo and Jiang, Hui},
	eprint = {1506.07220},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Jiang - 2015 - Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks.pdf:pdf},
	mendeley-groups = {Literature Review,Interim Review},
	pages = {5},
	title = {{Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks}},
	url = {http://arxiv.org/abs/1506.07220},
	year = {2015}
}
@article{Santos2015,
	abstract = {Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1504.06580v2},
	author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
	eprint = {arXiv:1504.06580v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos, Xiang, Zhou - 2015 - Classifying Relations by Ranking with Convolutional Neural Networks.pdf:pdf},
	isbn = {9781941643723},
	journal = {Acl-2015},
	keywords = {Bing Xiang,Bowen Zhou,Cicero Nogueira dos Santos},
	mendeley-groups = {Progress Report},
	number = {3},
	pages = {626--634},
	title = {{Classifying Relations by Ranking with Convolutional Neural Networks}},
	url = {http://arxiv.org/pdf/1504.06580.pdf},
	year = {2015}
}
@article{Koppula,
	archivePrefix = {arXiv},
	arxivId = {1802.03816},
	author = {Koppula, Skanda and Sim, Khe Chai and Chin, Kean},
	eprint = {1802.03816},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koppula, Sim, Chin - Unknown - Understanding Recurrent Neural State Using Memory Signatures.pdf:pdf},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	title = {{Understanding Recurrent Neural State Using Memory Signatures}}
}
@article{Version2017,
	author = {Version, Document},
	file = {:E$\backslash$:/Member-SIGIR.pdf:pdf},
	isbn = {9781450350228},
	keywords = {entity embedding,entity rank-,list completion,maximum margin},
	title = {{Kent Academic Repository}},
	year = {2017}
}
@article{Zhang2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.03850v3},
	author = {Zhang, Yizhe and Gan, Zhe and Fan, Kai and Chen, Zhi and Henao, Ricardo and Shen, Dinghan and Carin, Lawrence},
	eprint = {arXiv:1706.03850v3},
	file = {:E$\backslash$:/PhD/Papedrs/1706.03850.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Adversarial Feature Matching for Text Generation}},
	year = {2017}
}
@article{Liang2017,
	abstract = {This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1703.03055},
	author = {Liang, Xiaodan and Lin, Liang and Shen, Xiaohui and Feng, Jiashi and Yan, Shuicheng and Xing, Eric P.},
	doi = {10.1109/CVPR.2017.234},
	eprint = {1703.03055},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2017 - Interpretable Structure-Evolving LSTM(2).pdf:pdf},
	issn = {1703.03055},
	mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
	number = {61622214},
	pages = {1010--1019},
	title = {{Interpretable Structure-Evolving LSTM}},
	url = {http://arxiv.org/abs/1703.03055},
	year = {2017}
}
@article{Hu2016,
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	archivePrefix = {arXiv},
	arxivId = {1603.06318},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	doi = {10.18653/v1/P16-1228},
	eprint = {1603.06318},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules(2).pdf:pdf},
	journal = {Acl},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1--18},
	title = {{Harnessing Deep Neural Networks with Logic Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	year = {2016}
}
@article{Bharti2015,
	author = {Bharti, Kusum Kumari and Singh, Pramod Kumar},
	doi = {10.1016/j.eswa.2014.11.038},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bharti, Singh - 2015 - Expert Systems with Applications Hybrid dimension reduction by integrating feature selection with feature extract.pdf:pdf},
	issn = {0957-4174},
	journal = {Expert Systems With Applications},
	mendeley-groups = {Report/Features},
	number = {6},
	pages = {3105--3114},
	publisher = {Elsevier Ltd},
	title = {{Expert Systems with Applications Hybrid dimension reduction by integrating feature selection with feature extraction method for text clustering}},
	url = {http://dx.doi.org/10.1016/j.eswa.2014.11.038},
	volume = {42},
	year = {2015}
}
@article{Rolfe2013,
	abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
	archivePrefix = {arXiv},
	arxivId = {1301.3775},
	author = {Rolfe, Jason Tyler and LeCun, Yan},
	eprint = {1301.3775},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rolfe, LeCun - 2013 - Discriminative Recurrent Sparse Auto-Encoders.pdf:pdf},
	journal = {CoRR},
	pages = {15},
	title = {{Discriminative Recurrent Sparse Auto-Encoders}},
	url = {http://arxiv.org/abs/1301.3775},
	year = {2013}
}
@article{Blitzer2007,
	author = {Blitzer, J and Dredze, M and Pereira, F},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blitzer, Dredze, Pereira - 2007 - Biographies, Bollywood, boom-boxes and blenders Domain adaptation for sentiment classification.pdf:pdf},
	journal = {Proc. Assoc. Comput. Linguist. (ACL},
	title = {{Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification}},
	year = {2007}
}
@article{Isinkaye2015,
	author = {Isinkaye, F O},
	doi = {10.1016/j.eij.2015.06.005},
	file = {:D$\backslash$:/PhD/PGR/1-s2.0-S1110866515000341-main.pdf:pdf},
	keywords = {collaborative filtering,content-based filtering,hybrid filtering technique},
	pages = {261--273},
	publisher = {Ministry of Higher Education and Scientific Research},
	title = {{Recommendation systems : Principles , methods and evaluation}},
	year = {2015}
}
@article{,
	file = {:E$\backslash$:/0395.pdf:pdf},
	pages = {0--16},
	title = {{Q RTS2UWV X Ya ` cbed fhgiSpU q r UWYtsiupfPbvVafGw4x q R fPb y  W  ` C Y q  g Y  Uv  Ya ` C  qtg  SpYaU  fhg ` Spfh  q  u k {\textcopyright} l  Cm npo ¤ fqpifj r3sut ¤ v v ¥ vxw v2y {\{} z ¡ ¤ | {\}}!  D z ¤ z ¥ {\S} ¦ s 0 {\~{}} |¦ v ¨ 5 {\#}}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 9 - The Magic Caster of Destroy (v1.2).pdf:pdf},
	title = {{Overlord Volume 9 - The Magic Caster of Destroy (v1.2).pdf}}
}
@article{Schockaert2015a,
	author = {Schockaert, Steven and Lee, Jae Hee},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert, Lee - 2015 - Qualitative reasoning about directions in semantic spaces.pdf:pdf},
	isbn = {9781577357384},
	issn = {10450823},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	pages = {3207--3213},
	title = {{Qualitative reasoning about directions in semantic spaces}},
	volume = {2015-Janua},
	year = {2015}
}
@article{Author2015a,
	archivePrefix = {arXiv},
	arxivId = {1506.03694},
	author = {Author, First and Author, Second and Author, Third},
	eprint = {1506.03694},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Author, Author - 2015 - Learning language through pictures.pdf:pdf},
	number = {2013},
	pages = {1--2},
	title = {{Learning language through pictures}},
	url = {http://arxiv.org/pdf/1506.03694v2.pdf},
	year = {2015}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCF06092019.pdf:pdf},
	title = {{CCF06092019.pdf}}
}
@article{Ruggieri2009,
	author = {Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/tkdd.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	title = {{Data Mining for Discrimination Discovery}},
	volume = {V},
	year = {2009}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 1606.03657.pdf.pdf:pdf},
	title = {1606.03657.pdf}
}
@article{Kim2017,
	abstract = {Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate weighting scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others.},
	author = {Kim, Han Kyul and Kim, Hyunjoong and Cho, Sungzoon},
	doi = {10.1016/j.neucom.2017.05.046},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kim, Cho - 2017 - Bag-of-concepts Comprehending document representation through clustering words in distributed representation.pdf:pdf},
	issn = {18728286},
	journal = {Neurocomputing},
	keywords = {Bag-of-concepts,Interpretable document representation,Word2vec clustering},
	mendeley-groups = {Annotated/Word Vectors,11Thesis/Interpretability,11Thesis/Interpretability/Representations,11Thesis},
	pages = {336--352},
	title = {{Bag-of-concepts: Comprehending document representation through clustering words in distributed representation}},
	volume = {266},
	year = {2017}
}
@article{Volumee,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 6 - Men in The Kingdom [Part 02].pdf:pdf},
	title = {{No Title}}
}
@article{Szafron2004,
	abstract = {Proteome Analyst (PA) (http://www.cs.ualberta.ca/{\~{}}bioinfo/PA/) is a publicly available, high-throughput, web-based system for predicting various properties of each protein in an entire proteome. Using machine-learned classifiers, PA can predict, for example, the GeneQuiz general function and Gene Ontology (GO) molecular function of a protein. In addition, PA is currently the most accurate and most comprehensive system for predicting subcellular localization, the location within a cell where a protein performs its main function. Two other capabilities of PA are notable. First, PA can create a custom classifier to predict a new property, without requiring any programming, based on labeled training data (i.e. a set of examples, each with the correct classification label) provided by a user. PA has been used to create custom classifiers for potassium-ion channel proteins and other general function ontologies. Second, PA provides a sophisticated explanation feature that shows why one prediction is chosen over another. The PA system produces a Na{\"{i}}ve Bayes classifier, which is amenable to a graphical and interactive approach to explanations for its predictions; transparent predictions increase the user's confidence in, and understanding of, PA.},
	author = {Szafron, Duane and Lu, Paul and Greiner, Russell and Wishart, David S. and Poulin, Brett and Eisner, Roman and Lu, Zhiyong and Anvik, John and Macdonell, Cam and Fyshe, Alona and Meeuwis, David},
	doi = {10.1093/nar/gkh485},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szafron et al. - 2004 - Proteome Analyst Custom predictions with explanations in a web-based tool for high-throughput proteome annotatio.pdf:pdf},
	isbn = {1362-4962 (Electronic)},
	issn = {03051048},
	journal = {Nucleic Acids Research},
	mendeley-groups = {Report/Biologicla domain},
	number = {WEB SERVER ISS.},
	pages = {365--371},
	pmid = {15215412},
	title = {{Proteome Analyst: Custom predictions with explanations in a web-based tool for high-throughput proteome annotations}},
	volume = {32},
	year = {2004}
}
@article{Jurman2016,
	author = {Jurman, Nicholas},
	file = {:C$\backslash$:/Users/Workk/Documents/Research{\_}Student{\_}Travel{\_}Application{\_}completed.pdf:pdf},
	title = {{N / a}},
	year = {2016}
}
@article{Blundell2015,
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	archivePrefix = {arXiv},
	arxivId = {1505.05424},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	eprint = {1505.05424},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
	isbn = {9781510810587},
	mendeley-groups = {!Paper 3/Bayesian Networks},
	title = {{Weight Uncertainty in Neural Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	volume = {37},
	year = {2015}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Ganesha.docx:docx},
	title = {{Ganesha}}
}
@misc{Cao2015,
	abstract = {We develop a Ranking framework upon Recursive Neural Networks (R2N2) to rank sentences for multi-document sum- marization. It formulates the sentence ranking task as a hi- erarchical regression process, which simultaneously mea- sures the salience of a sentence and its constituents (e.g., phrases) in the parsing tree. This enables us to draw on word-level to sentence-level supervisions derived from refer- ence summaries. In addition, recursive neural networks are used to automatically learn ranking features over the tree, with hand-crafted feature vectors of words as inputs. Hier- archical regressions are then conducted with learned features concatenating raw features. Ranking scores of sentences and words are utilized to effectively select informative and non- redundant sentences to generate summaries. Experiments on the DUC 2001, 2002 and 2004 multi-document summariza- tion datasets show that R2N2 outperforms state-of-the-art ex- tractive summarization approaches. Introduction},
	archivePrefix = {arXiv},
	arxivId = {1509.00685},
	author = {Cao, Ziqiang and Wei, Furu and Dong, Li and Li, Sujian and Zhou, Ming},
	booktitle = {Aaai},
	doi = {10.1162/153244303322533223},
	eprint = {1509.00685},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2015 - Ranking with recursive neural networks and its application to multi-document summarization.pdf:pdf},
	isbn = {9781577357018},
	issn = {19909772},
	keywords = {NLP and Knowledge Representation Track},
	mendeley-groups = {Progress Report},
	pages = {2153--2159},
	pmid = {18244602},
	title = {{Ranking with recursive neural networks and its application to multi-document summarization}},
	year = {2015}
}
@article{Ager2012b,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}CaptionFix.pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Graves2013,
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	archivePrefix = {arXiv},
	arxivId = {1308.0850},
	author = {Graves, Alex},
	doi = {10.1145/2661829.2661935},
	eprint = {1308.0850},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
	isbn = {2000201075},
	issn = {18792782},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pages = {1--43},
	pmid = {23459267},
	title = {{Generating Sequences With Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	year = {2013}
}
@article{Koc,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1807.07279v2},
	author = {Koc, Aykut and Senel, Lutfi Kerem and Utlu, Ihsan and Ozaktas, Haldun M},
	eprint = {arXiv:1807.07279v2},
	file = {:E$\backslash$:/PhD/Papedrs/1807.07279.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {1--11},
	title = {{Imparting Interpretability to Word Embeddings while Preserving Semantic Structure}}
}
@article{Kim2017,
	abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
	archivePrefix = {arXiv},
	arxivId = {1702.00887},
	author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
	eprint = {1702.00887},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2017 - Structured Attention Networks.pdf:pdf},
	issn = {1702.00887},
	mendeley-groups = {!Paper 3/Structured LSTMs},
	pages = {1--21},
	title = {{Structured Attention Networks}},
	url = {http://arxiv.org/abs/1702.00887},
	year = {2017}
}
@article{Herlocker2000,
	abstract = {Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.},
	archivePrefix = {arXiv},
	arxivId = {48},
	author = {Herlocker, Jonathan L and Konstan, Joseph a and Riedl, John},
	doi = {10.1145/358916.358995},
	eprint = {48},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herlocker, Konstan, Riedl - 2000 - Explaining collaborative filtering recommendations.pdf:pdf},
	isbn = {1581132220},
	issn = {00318655},
	journal = {Proceedings of the 2000 ACM conference on Computer supported cooperative work},
	keywords = {Explanations,GroupLens,MovieLens,collaborative filtering,recommender systems},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {241--250},
	pmid = {8234466},
	title = {{Explaining collaborative filtering recommendations}},
	url = {http://dl.acm.org/citation.cfm?id=358995},
	year = {2000}
}
@article{Srivastava2013a,
	abstract = {We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.},
	archivePrefix = {arXiv},
	arxivId = {abs/1309.6865},
	author = {Srivastava, Nitish and Salakhutdinov, Rr and Hinton, Ge},
	eprint = {1309.6865},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava, Salakhutdinov, Hinton - 2013 - Modeling Documents with Deep Boltzmann Machines.pdf:pdf},
	journal = {arXiv preprint arXiv: {\ldots}},
	mendeley-groups = {!Paper 3/task/newsgroups},
	primaryClass = {abs},
	title = {{Modeling Documents with Deep Boltzmann Machines}},
	url = {http://arxiv.org/abs/1309.6865{\%}5Cnhttp://www.arxiv.org/pdf/1309.6865.pdf},
	year = {2013}
}
@article{Tallec2017,
	abstract = {Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT.},
	archivePrefix = {arXiv},
	arxivId = {1705.08209},
	author = {Tallec, Corentin and Ollivier, Yann},
	eprint = {1705.08209},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tallec, Ollivier - 2017 - Unbiasing Truncated Backpropagation Through Time.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pages = {1--13},
	title = {{Unbiasing Truncated Backpropagation Through Time}},
	url = {http://arxiv.org/abs/1705.08209},
	year = {2017}
}
@article{Pazzani2007,
	author = {Pazzani, Michael J and Billsus, Daniel},
	file = {:E$\backslash$:/10.1.1.130.8327.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	pages = {325--341},
	title = {{Content-Based Recommendation Systems}},
	year = {2007}
}
@article{Doshi-Velez2017,
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	archivePrefix = {arXiv},
	arxivId = {1702.08608},
	author = {Doshi-Velez, Finale and Kim, Been},
	eprint = {1702.08608},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {Ml},
	pages = {1--13},
	title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	year = {2017}
}
@article{Gupta2015,
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1505.05192v1},
	author = {Gupta, Abhinav and Efros, Alexei a},
	doi = {10.1109/ICCV.2015.167},
	eprint = {arXiv:1505.05192v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta, Efros - 2015 - Unsupervised Visual Representation Learning by Context Prediction.pdf:pdf},
	isbn = {978-1-4673-8391-2},
	issn = {978-1-4673-8391-2},
	journal = {arXiv preprint},
	mendeley-groups = {Report/Features,11Thesis},
	pages = {1422--1430},
	pmid = {903},
	title = {{Unsupervised Visual Representation Learning by Context Prediction}},
	year = {2015}
}
@article{Lastname2011b,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (1).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Mathieu2016,
	abstract = {We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentanglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.},
	archivePrefix = {arXiv},
	arxivId = {1611.03383},
	author = {Mathieu, Michael and Zhao, Junbo and Sprechmann, Pablo and Ramesh, Aditya and LeCun, Yann},
	eprint = {1611.03383},
	file = {:D$\backslash$:/PhD/Code/ThesisPipeline/ThesisPipeline/papers/1611.03383.pdf:pdf},
	issn = {10495258},
	number = {Nips},
	title = {{Disentangling factors of variation in deep representations using adversarial training}},
	url = {http://arxiv.org/abs/1611.03383},
	year = {2016}
}
@article{Read2014a,
	abstract = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
	archivePrefix = {arXiv},
	arxivId = {1502.05988},
	author = {Read, Jesse and Perez-Cruz, Fernando},
	eprint = {1502.05988},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Perez-Cruz - 2014 - Deep Learning for Multi-label Classification.pdf:pdf},
	pages = {1--8},
	title = {{Deep Learning for Multi-label Classification}},
	url = {http://arxiv.org/abs/1502.05988},
	year = {2014}
}
@article{Collobert2000,
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling, achieving or exceeding state-of-the-art performance in each on four benchmark tasks. Our goal was to design a flexible architecture that can learn representations useful for the tasks, thus avoiding excessive task-specific feature engineering (and therefore disregarding a lot of prior knowledge). Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabelled training data. This work is then used as a basis for building a freely available tagging system with excellent performance while requiring minimal computational resources.},
	archivePrefix = {arXiv},
	arxivId = {1103.0398},
	author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	doi = {10.1145/2347736.2347755},
	eprint = {1103.0398},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2000 - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
	isbn = {1532-4435},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {Deep Learning,Natural Language Processing,Neural Networks},
	pages = {1--48},
	title = {{Natural Language Processing (almost) from Scratch}},
	volume = {1},
	year = {2000}
}
@article{Garc??a2009,
	abstract = {Classification in imbalanced domains is a recent challenge in data mining. We refer to imbalanced classification when data presents many examples from one class and few from the other class, and the less representative class is the one which has more interest from the point of view of the learning task. One of the most used techniques to tackle this problem consists in preprocessing the data previously to the learning process. This preprocessing could be done through under-sampling; removing examples, mainly belonging to the majority class; and over-sampling, by means of replicating or generating new minority examples. In this paper, we propose an under-sampling procedure guided by evolutionary algorithms to perform a training set selection for enhancing the decision trees obtained by the C4.5 algorithm and the rule sets obtained by PART rule induction algorithm. The proposal has been compared with other under-sampling and over-sampling techniques and the results indicate that the new approach is very competitive in terms of accuracy when comparing with over-sampling and it outperforms standard under-sampling. Moreover, the obtained models are smaller in terms of number of leaves or rules generated and they can considered more interpretable. The results have been contrasted through non-parametric statistical tests over multiple data sets. Crown Copyright ?? 2009.},
	author = {Garc??a, Salvador and Fern??ndez, Alberto and Herrera, Francisco},
	doi = {10.1016/j.asoc.2009.04.004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garca, Fernndez, Herrera - 2009 - Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with.pdf:pdf},
	isbn = {15684946},
	issn = {15684946},
	journal = {Applied Soft Computing Journal},
	keywords = {Data reduction,Decision trees,Evolutionary algorithms,Imbalanced classification,Rule induction,Training set selection},
	mendeley-groups = {Annotated/Rule-based classiifers},
	number = {4},
	pages = {1304--1314},
	title = {{Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with evolutionary training set selection over imbalanced problems}},
	volume = {9},
	year = {2009}
}
@article{Craven,
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - Unknown - Extracting Thee-Structured Representations of Thained Networks.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees},
	title = {{Extracting Thee-Structured Representations of Thained Networks}}
}
@article{Lastname2011c,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (2).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Interim Report Student Details Section A Student Self-Assessment A . 1 Thesis Title and Hypothesis A . 2 Overall Pr.pdf:pdf},
	title = {{Interim Report Student Details Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis A . 2 Overall Progress}}
}

@article{Kulkarni2015,
	abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
	archivePrefix = {arXiv},
	arxivId = {1503.03167},
	author = {Kulkarni, Td and Whitney, W},
	doi = {10.1063/1.4914407},
	eprint = {1503.03167},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network.pdf:pdf},
	issn = {10897550},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Progress Report},
	pages = {2539--2547},
	title = {{Deep Convolutional Inverse Graphics Network}},
	url = {http://arxiv.org/abs/1503.03167},
	year = {2015}
}
@article{Twomey1998,
	author = {Twomey, Janet M. and Smith, Alice E.},
	doi = {10.1109/5326.704579},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Twomey, Smith - 1998 - Bias and variance of validation methods for function approximation neural networks under conditions of sparse dat.pdf:pdf},
	issn = {10946977},
	journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
	keywords = {Function approximation,Neural network,Resampling,Validation},
	number = {3},
	pages = {417--430},
	title = {{Bias and variance of validation methods for function approximation neural networks under conditions of sparse data}},
	volume = {28},
	year = {1998}
}
@article{Blei2003,
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	archivePrefix = {arXiv},
	arxivId = {1111.6189v1},
	author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
	doi = {10.1162/jmlr.2003.3.4-5.993},
	eprint = {1111.6189v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:pdf},
	isbn = {9781577352815},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {993--1022},
	pmid = {21362469},
	title = {{Latent Dirichlet Allocation}},
	volume = {3},
	year = {2003}
}
@article{Abadi2016,
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	archivePrefix = {arXiv},
	arxivId = {1603.04467},
	author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	doi = {10.1038/nn.3331},
	eprint = {1603.04467},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
	isbn = {0010-0277},
	issn = {0270-6474},
	mendeley-groups = {!Paper 3},
	pmid = {16411492},
	title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
	url = {http://arxiv.org/abs/1603.04467},
	year = {2016}
}
@article{Lastname2011,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/feedback ch3.pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Yang2016,
	abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	archivePrefix = {arXiv},
	arxivId = {1606.02393},
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	doi = {10.18653/v1/N16-1174},
	eprint = {1606.02393},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2016 - Hierarchical Attention Networks for Document Classification.pdf:pdf},
	isbn = {9781941643914},
	journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Document representation,!Paper 3/Structured LSTMs,!Paper 3/task/Yelp,11Thesis/Document Representations},
	pages = {1480--1489},
	title = {{Hierarchical Attention Networks for Document Classification}},
	url = {http://aclweb.org/anthology/N16-1174},
	year = {2016}
}
@article{Martens2011,
	abstract = {This paper proposes a complete framework to assess the overall performance of classification models from a user perspective in terms of accuracy, comprehensibility, and justifiability. A review is provided of accuracy and comprehensibility measures, and a novel metric is introduced that allows one to measure the justifiability of classification models. Furthermore, taxonomy of domain constraints is introduced, and an overview of the existing approaches to impose constraints and include domain knowledge in data mining techniques is presented. Finally, justifiability metric is applied to a credit scoring and customer churn prediction case. ?? 2011 Elsevier B.V. All rights reserved.},
	author = {Martens, David and Vanthienen, Jan and Verbeke, Wouter and Baesens, Bart},
	doi = {10.1016/j.dss.2011.01.013},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2011 - Performance of classification models from a user perspective.pdf:pdf},
	isbn = {0167-9236},
	issn = {01679236},
	journal = {Decision Support Systems},
	keywords = {Classification,Comprehensibility,Data mining,Justifiability,Metrics},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {4},
	pages = {782--793},
	publisher = {Elsevier B.V.},
	title = {{Performance of classification models from a user perspective}},
	url = {http://dx.doi.org/10.1016/j.dss.2011.01.013},
	volume = {51},
	year = {2011}
}
@article{Setiono2008a,
	abstract = {In this paper, we present a recursive algorithm for extracting classification rules from feedforward neural networks (NNs) that have been trained on data sets having both discrete and continuous attributes. The novelty of this algorithm lies in the conditions of the extracted rules: the rule conditions involving discrete attributes are disjoint from those involving continuous attributes. The algorithm starts by first generating rules with discrete attributes only to explain the classification process of the NN. If the accuracy of a rule with only discrete attributes is not satisfactory, the algorithm refines this rule by recursively generating more rules with discrete attributes not already present in the rule condition, or by generating a hyperplane involving only the continuous attributes. We show that for three real-life credit scoring data sets, the algorithm generates rules that are not only more accurate but also more comprehensible than those generated by other NN rule extraction methods.},
	author = {Setiono, Rudy and Baesens, Bart and Mues, Christophe},
	doi = {10.1109/TNN.2007.908641},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Baesens, Mues - 2008 - Recursive neural network rule extraction for data with mixed attributes.pdf:pdf},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Continuous attributes,Credit scoring,Discrete attributes,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {2},
	pages = {299--307},
	pmid = {18269960},
	title = {{Recursive neural network rule extraction for data with mixed attributes}},
	volume = {19},
	year = {2008}
}
@article{Mitchell1997a,
	author = {Mitchell, Tom and Simon, Herb and Pomerleau, Dean},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell, Simon, Pomerleau - 1997 - Multitask Learning Rich Caruana 23 September1997.pdf:pdf},
	number = {September},
	title = {{Multitask Learning Rich Caruana 23 September1997}},
	year = {1997}
}
@article{Talley2011,
	author = {Talley, Edmund M and Newman, David and Mimno, David and Herr, Bruce W and Wallach, Hanna M and Burns, Gully A P C and Leenders, A G Miriam and Mccallum, Andrew},
	doi = {10.1038/nmeth.1619},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Talley et al. - 2011 - correspondEnce Database of NIH grants using machine-learned categories and graphical clustering Predicting protei.pdf:pdf},
	isbn = {2712009002},
	issn = {1548-7091},
	journal = {Nature Publishing Group},
	mendeley-groups = {Report/Clustering},
	number = {6},
	pages = {443--444},
	publisher = {Nature Publishing Group},
	title = {{correspondEnce Database of NIH grants using machine-learned categories and graphical clustering Predicting protein associations with long noncoding RNAs}},
	url = {http://dx.doi.org/10.1038/nmeth.1619},
	volume = {8},
	year = {2011}
}
@article{Methods2018,
	author = {Methods, Clustering and Methods, Scoring and Models, Classification and Representations, Vector Space},
	file = {:E$\backslash$:/Downloads/Work/thesis.pdf:pdf},
	number = {June},
	pages = {1--7},
	title = {{3 3.1 Directions in Unsupervised Vector Spaces}},
	year = {2018}
}
@article{F??rnkranz2008a,
	abstract = {Label ranking studies the problem of learning a mapping from instances to rankings over a predefined set of labels. Hitherto existing approaches to label ranking implicitly operate on an underlying (utility) scale which is not calibrated in the sense that it lacks a natural zero point. We propose a suitable extension of label ranking that incorporates the calibrated scenario and substantially extends the expressive power of these approaches. In particular, our extension suggests a conceptually novel technique for extending the common learning by pairwise comparison approach to the multilabel scenario, a setting previously not being amenable to the pairwise decomposition technique. The key idea of the approach is to introduce an artificial calibration label that, in each example, separates the relevant from the irrelevant labels. We show that this technique can be viewed as a combination of pairwise preference learning and the conventional relevance classification technique, where a separate classifier is trained to predict whether a label is relevant or not. Empirical results in the area of text categorization, image classification and gene analysis underscore the merits of the calibrated model in comparison to state-of-the-art multilabel learning methods.},
	author = {F??rnkranz, Johannes and H??llermeier, Eyke and {Loza Menc??a}, Eneldo and Brinker, Klaus},
	doi = {10.1007/s10994-008-5064-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frnkranz et al. - 2008 - Multilabel classification via calibrated label ranking.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Multi-label classification,Preference learning,Ranking},
	number = {2},
	pages = {133--153},
	title = {{Multilabel classification via calibrated label ranking}},
	volume = {73},
	year = {2008}
}
@article{Boureau2010,
	abstract = {Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter responses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be broken down into two steps: (1) a coding step, which performs a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pooling step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pooling schemes have been proposed in the literature. The goal of this paper is threefold. We seek to establish the relative importance of each step of mid-level feature extraction through a comprehensive cross evaluation of several types of coding modules (hard and soft vector quantization, sparse coding) and pooling schemes (by taking the average, or the maximum), which obtains state-of-the-art performance or better on several recognition benchmarks. We show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding. We provide theoretical and empirical insight into the remarkable performance of max pooling. By teasing apart components shared by modern mid-level feature extractors, our approach aims to facilitate the design of better recognition architectures.},
	author = {Boureau, Y. Lan and Bach, Francis and LeCun, Yann and Ponce, Jean},
	doi = {10.1109/CVPR.2010.5539963},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boureau et al. - 2010 - Learning mid-level features for recognition.pdf:pdf},
	isbn = {9781424469840},
	issn = {10636919},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	mendeley-groups = {Report/Features,Progress Report},
	pages = {2559--2566},
	title = {{Learning mid-level features for recognition}},
	year = {2010}
}
@article{Alshaikh2019,
	author = {Alshaikh, Rana and Schockaert, Steven},
	file = {:E$\backslash$:/PhD/Papedrs/K19-1013.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {131--139},
	title = {{Learning Conceptual Spaces with Disentangled Facets}},
	year = {2019}
}
@article{Skirpan2017,
	abstract = {In this paper, we argue for the adoption of a normative definition of fairness within the machine learning community. After characterizing this definition, we review the current literature of Fair ML in light of its implications. We end by suggesting ways to incorporate a broader community and generate further debate around how to decide what is fair in ML.},
	archivePrefix = {arXiv},
	arxivId = {1706.09976},
	author = {Skirpan, Michael and Gorelick, Micha},
	eprint = {1706.09976},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skirpan, Gorelick - 2017 - The Authority of Fair in Machine Learning.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{The Authority of "Fair" in Machine Learning}},
	url = {http://arxiv.org/abs/1706.09976},
	year = {2017}
}
@article{Kulkarni2015b,
	abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
	archivePrefix = {arXiv},
	arxivId = {1503.03167},
	author = {Kulkarni, Td and Whitney, W},
	doi = {10.1063/1.4914407},
	eprint = {1503.03167},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network(2).pdf:pdf},
	issn = {10897550},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {2539--2547},
	title = {{Deep Convolutional Inverse Graphics Network}},
	url = {http://arxiv.org/abs/1503.03167},
	year = {2015}
}
@article{Ghemawat2003,
	abstract = {We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.},
	author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
	journal = {ACM SIGOPS Operating Systems Review},
	keywords = {clustered storage,data storage,fault tolerance,scalability},
	number = {5},
	pages = {29},
	title = {{The Google file system}},
	volume = {37},
	year = {2003}
}
@article{Maas2011a,
	abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment in- formation as well as non-sentiment annota- tions. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
	author = {Maas, Andrew L and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	doi = {978-1-932432-87-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:pdf},
	isbn = {9781932432879},
	journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	mendeley-groups = {Annotated/Datasets,!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {142--150},
	title = {{Learning Word Vectors for Sentiment Analysis}},
	year = {2011}
}
@article{Kriegel2011,
	author = {Kriegel, Hans-peter and Kr, Peer},
	doi = {10.1002/widm.30},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kriegel, Kr - 2011 - Density-based clustering.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	number = {June},
	pages = {231--240},
	title = {{Density-based clustering}},
	volume = {1},
	year = {2011}
}
@article{Rivest1987,
	abstract = {This paper introduces a new representation for Boolean functions, called decision lists,  and shows that they are eciently learnable from examples. More precisely, this result  is established for $\backslash$k-DL" {\{} the set of decision lists with conjunctive clauses of size k at  each decision. Since k-DL properly includes other well-known techniques for representing  Boolean functions such as k-CNF (formulae in conjunctive normal form with at most k  literals per clause), k-DNF (formulae in disjunctive normal form with at most k literals  per term), and decision trees of depth k, our result strictly increases the set of functions  which are known to be polynomially learnable, in the sense of Valiant (1984). Our proof is  constructive: we present an algorithm which can eciently construct an element of k-DL  consistent with a given set of examples, if one exists.}},
	author = {Rivest, Ronald L.},
	doi = {10.1023/A:1022607331053},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivest - 1987 - Learning Decision Lists.pdf:pdf},
	issn = {15730565},
	journal = {Machine Learning},
	keywords = {Boolean formulae,Learning from examples,decision lists,polynomial-time identification},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	number = {3},
	pages = {229--246},
	title = {{Learning Decision Lists}},
	volume = {2},
	year = {1987}
}
@article{Alon2009,
	abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure. ?? 2009 Elsevier Inc. All rights reserved.},
	author = {Alon, Uri},
	journal = {Molecular Cell},
	number = {6},
	pages = {726--728},
	title = {{How To Choose a Good Scientific Problem}},
	volume = {35},
	year = {2009}
}
@article{Doe,
	author = {Doe, John and Storm, Rain},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 12 - The Paladin of the Holy Kingdom [Part 01].pdf:pdf},
	title = {{Translation : Nigel}}
}
@article{Shrikumar2017,
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
	archivePrefix = {arXiv},
	arxivId = {1704.02685},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	eprint = {1704.02685},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf},
	mendeley-groups = {Report},
	title = {{Learning Important Features Through Propagating Activation Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	year = {2017}
}
@article{Thrun1995,
	abstract = {Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neu-ral networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illus-trate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.},
	author = {Thrun, Sebastian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun - 1995 - Extracting Rules from Artificial Neural Networks with Distributed Representations(2).pdf:pdf},
	isbn = {1049-5258},
	journal = {Advances in Neural Information Processing Systems 7},
	mendeley-groups = {Progress Report},
	title = {{Extracting Rules from Artificial Neural Networks with Distributed Representations}},
	year = {1995}
}
@article{Devlin2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1908.10084v1},
	author = {Devlin, Bert and Liu, Roberta},
	eprint = {arXiv:1908.10084v1},
	file = {:E$\backslash$:/1908.10084.pdf:pdf},
	title = {{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}},
	year = {2014}
}
@article{Arras2016,
	abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
	archivePrefix = {arXiv},
	arxivId = {1612.07843},
	author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
	eprint = {1612.07843},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2016 - {\&}quotWhat is Relevant in a Text Document{\&}quot An Interpretable Machine Learning Approach.pdf:pdf},
	isbn = {1111111111},
	mendeley-groups = {Annotated/Explanations,!Paper 3/task/newsgroups},
	pages = {1--23},
	title = {{"What is Relevant in a Text Document?": An Interpretable Machine Learning Approach}},
	url = {http://arxiv.org/abs/1612.07843},
	year = {2016}
}

@article{Yang2014,
	abstract = {Abstract Genetic algorithms are among the most popular evolutionary algorithms in terms of the diversity of their applications. A vast majority of well-known optimization problems have been solved using genetic algorithms. In addition, genetic algorithms are population-based, and many modern evolutionary algorithms are directly based on genetic algorithms or have some strong similarities to them.},
	author = {Yang, Xin-She},
	doi = {10.1016/B978-0-12-416743-8.00005-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Man, Tang, Kwong - 1999 - Genetic Algorithms.pdf:pdf},
	isbn = {978-0-12-416743-8},
	issn = {0036-8733},
	journal = {Nature-Inspired Optimization Algorithms},
	keywords = {Evolutionary algorithm,Genetic algorithms,Genetic operators,Optimization},
	pages = {77--87},
	title = {{Genetic Algorithms}},
	url = {http://www.sciencedirect.com/science/article/pii/B9780124167438000051{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/B9780124167438000051},
	year = {2014}
}
@article{Joulin2016,
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
	archivePrefix = {arXiv},
	arxivId = {1607.01759},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	doi = {1511.09249v1},
	eprint = {1607.01759},
	file = {:E$\backslash$:/Downloads/Work/1607.01759.pdf:pdf},
	isbn = {9781577357384},
	issn = {10450823},
	mendeley-groups = {Annotated/Interpretable Classifiers,!Paper 3/Word vectors,!Paper 3/task/Yelp},
	pmid = {1000303116},
	title = {{Bag of Tricks for Efficient Text Classification}},
	url = {http://arxiv.org/abs/1607.01759},
	year = {2016}
}
@article{Marshal2016,
	author = {Marshal, David and Lai, Yukun and Marshal, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marshal, Lai, Marshal - 2016 - Part 1 PhD progress review.pdf:pdf},
	number = {October},
	title = {{Part 1 : PhD progress review}},
	year = {2016}
}
@article{Derrac2014,
	abstract = {Place types taxonomies tend to have a shallow structure, which limits their predictive value. Although existing place type taxonomies could in principle be refined, the result would inevitably be highly subjective and application-specific. Instead, in this paper, we propose a methodology to enrich place types taxonomies with a ternary betweenness relation derived from Flickr. In particular, we first construct a semantic space of place types by applying dimensionality reduction methods to tag co-occurrence data obtained from Flickr. Our hypothesis is that natural properties of place types should correspond to convex regions in this space. Specifically, knowing that places P 1,...,Pn have a given property, we could then induce that all places which are located in the convex hull of {\{}P1,...,P n{\}} in the semantic space are also likely to have this property. To avoid relying on computationally expensive convex hull algorithms, we propose to derive a ternary betweenness relation from the semantic space, and to approximate the convex hull at the symbolic level based on this relation. We present experimental results which support the usefulness of our approach. {\textcopyright} 2014 Springer International Publishing.},
	author = {Derrac, Joaqu{\'{i}}n and Schockaert, Steven},
	doi = {10.1007/978-3-319-04939-7_8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derrac, Schockaert - 2014 - Enriching taxonomies of place types using Flickr.pdf:pdf},
	isbn = {9783319049380},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {174--192},
	title = {{Enriching taxonomies of place types using Flickr}},
	volume = {8367 LNCS},
	year = {2014}
}
@article{Prior2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.11571v2},
	author = {Prior, Human-in-the-loop Interpretability and Ross, Andrew Slavin and Kim, Been and Gershman, Samuel J},
	eprint = {arXiv:1805.11571v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1805.11571.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	number = {1},
	pages = {1--13},
	title = {{Human-in-the-Loop Interpretability Prior}},
	year = {2018}
}
@article{Rezende2014,
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	archivePrefix = {arXiv},
	arxivId = {1401.4082},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1401.4082},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende, Mohamed, Wierstra - 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:pdf},
	isbn = {9781634393973},
	issn = {10495258},
	mendeley-groups = {!Paper 3/Bayesian Networks},
	pmid = {23459267},
	title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
	url = {http://arxiv.org/abs/1401.4082},
	year = {2014}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 10 - The Ruler of Conspiracy (v2.3).pdf:pdf},
	title = {{Overlord Volume 10 - The Ruler of Conspiracy (v2.3).pdf}}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - TalesOfTheDervishes (2).pdf.pdf:pdf},
	title = {{TalesOfTheDervishes (2).pdf}}
}
@article{Read2010,
	author = {Read, Jesse},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2010 - Scalable Multi-label Classification.pdf:pdf},
	mendeley-groups = {!Paper 3/task/newsgroups},
	title = {{Scalable Multi-label Classification}},
	volume = {1994},
	year = {2010}
}
@article{Lakhotia2018,
	author = {Lakhotia, Suyash and Bresson, Xavier},
	doi = {10.1109/CW.2018.00022},
	file = {:E$\backslash$:/PhD/Papedrs/08590017.pdf:pdf},
	isbn = {9781538673157},
	journal = {2018 International Conference on Cyberworlds (CW)},
	keywords = {-text classification,artificial,convolutional,feedforward neural networks,graph convolutional neural networks,machine learning,neural networks},
	mendeley-groups = {11Thesis/Neural network multi-l;abe},
	pages = {58--65},
	publisher = {IEEE},
	title = {{An Experimental Comparison of Text Classification Techniques}},
	year = {2018}
}
@article{Leike2017,
	abstract = {We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.},
	archivePrefix = {arXiv},
	arxivId = {1711.09883},
	author = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A. and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
	eprint = {1711.09883},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leike et al. - 2017 - AI Safety Gridworlds.pdf:pdf},
	mendeley-groups = {!Paper 3},
	title = {{AI Safety Gridworlds}},
	url = {http://arxiv.org/abs/1711.09883},
	year = {2017}
}
@article{Kumar2015,
	abstract = {{\textless}p{\textgreater} A framework for automated detection and classification of cancer from microscopic biopsy images using clinically significant and biologically interpretable features is proposed and examined. The various stages involved in the proposed methodology include enhancement of microscopic images, segmentation of background cells, features extraction, and finally the classification. An appropriate and efficient method is employed in each of the design steps of the proposed framework after making a comparative analysis of commonly used method in each category. For highlighting the details of the tissue and structures, the contrast limited adaptive histogram equalization approach is used. For the segmentation of background cells, {\textless}math id="M1"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}k{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -means segmentation algorithm is used because it performs better in comparison to other commonly used segmentation methods. In feature extraction phase, it is proposed to extract various biologically interpretable and clinically significant shapes as well as morphology based features from the segmented images. These include gray level texture features, color based features, color gray level texture features, Law's Texture Energy based features, Tamura's features, and wavelet features. Finally, the {\textless}math id="M2"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}K{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} -nearest neighborhood method is used for classification of images into normal and cancerous categories because it is performing better in comparison to other commonly used methods for this application. The performance of the proposed framework is evaluated using well-known parameters for four fundamental tissues (connective, epithelial, muscular, and nervous) of randomly selected 1000 microscopic biopsy images. {\textless}/p{\textgreater}},
	author = {Kumar, Rajesh and Srivastava, Rajeev and Srivastava, Subodh},
	doi = {10.1155/2015/457906},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Srivastava, Srivastava - 2015 - Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significan.pdf:pdf},
	issn = {2314-5129, 2314-5137},
	journal = {Journal of Medical Engineering},
	mendeley-groups = {Annotated/Applications/Medical},
	number = {2015},
	pages = {1--14},
	pmid = {21329180},
	title = {{Detection and Classification of Cancer from Microscopic Biopsy Images Using Clinically Significant and Biologically Interpretable Features}},
	url = {http://www.hindawi.com/journals/jme/2015/457906/},
	volume = {2015},
	year = {2015}
}
@article{Schockaert,
	author = {Schockaert, Supervisor Steven},
	file = {:E$\backslash$:/Downloads/Work/report1.pdf:pdf},
	pages = {1--3},
	title = {{Thomas Ager : 9 Month Report}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/POLICY AND PROCEDURE FOR THE CONDUCT OF RESEARCH DEGREE EXAMINATIONS.pdf:pdf},
	title = {{POLICY AND PROCEDURE FOR THE CONDUCT OF RESEARCH DEGREE}}
}
@article{Ager2012c,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}CaptionFix.pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Jiang2016,
	abstract = {This paper involves deriving high quality information from unstructured text data through the integration of rich document representations to improve machine learning text classification problems. Previous research has applied Neural Network Language Models (NNLMs) to document classification performance, and word vector representations have been used to measure semantics among text. Never have they been combined together and shown to have improved text classification performance. Our belief is that the inference and clustering abilities of word vectors coupled with the power of a neural network can create more accurate classification predictions. The first phase our work focused on word vector representations for classification purposes. This approach included analyzing two distinct text sources with pre-marked binary outcomes for classification, creating a benchmark metric, and comparing against word vector representations within the feature space as a classifier. The results showed promise, obtaining an area under the curve of 0.95 utilizing word vectors, relative to the benchmark case of 0.93. The second phase of the project focused on utilizing an extension of the neural network model used in phase one to represent a document in its entirety as opposed to being represented word by word. Preliminary results indicated a slight improvement over the baseline model of approximately 2-3 percent.},
	author = {Jiang, Suqi and Lewris, Jason and Voltmer, Michael and Wang, Hongning},
	doi = {10.1109/SIEDS.2016.7489319},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2016 - Integrating rich document representations for text classification.pdf:pdf},
	isbn = {9781509009701},
	journal = {2016 IEEE Systems and Information Engineering Design Symposium, SIEDS 2016},
	keywords = {Natural Language Processing,Text Classification,Text Mining,Word2vec},
	mendeley-groups = {Annotated/Document representation},
	pages = {303--308},
	title = {{Integrating rich document representations for text classification}},
	year = {2016}
}
@article{Andrews1995a,
	abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy. {\textcopyright} 1995.},
	author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
	doi = {10.1016/0950-7051(96)81920-4},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Diederich, Tickle - 1995 - Survey and critique of techniques for extracting rules from trained artificial neural networks.pdf:pdf},
	isbn = {09507051 (ISSN)},
	issn = {09507051},
	journal = {Knowledge-Based Systems},
	keywords = {fuzzy neural networks,inferencing,knowledge insertion,rule extraction,rule refinement},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {6},
	pages = {373--389},
	title = {{Survey and critique of techniques for extracting rules from trained artificial neural networks}},
	volume = {8},
	year = {1995}
}
@book{Bishop2006,
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {Bishop, Christopher M CM Christopher M.},
	booktitle = {Pattern Recognition},
	chapter = {Graphical},
	doi = {10.1117/1.2819119},
	editor = {Jordan, M and Kleinberg, J and Sch{\"{o}}lkopf, B},
	eprint = {0-387-31073-8},
	isbn = {978-0387310732},
	issn = {10179909},
	number = {4},
	pages = {738},
	pmid = {8943268},
	publisher = {Springer},
	series = {Information science and statistics},
	title = {{Pattern Recognition and Machine Learning}},
	url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf{\%}5Cnhttp://soic.iupui.edu/syllabi/semesters/4142/INFO{\_}B529{\_}Liu{\_}s.pdf{\%}5Cnhttp://www.library.wisc.edu/selectedtocs/bg0137.pdf},
	volume = {4},
	year = {2006}
}
@article{Sun2006,
	author = {Sun, Hongmao},
	doi = {10.1002/cmdc.200500047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun - 2006 - An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability.pdf:pdf},
	mendeley-groups = {Annotated/Applications/Scientific Discovery},
	pages = {315--322},
	title = {{An Accurate and Interpretable Bayesian Classification Model for Prediction of hERG Liability}},
	volume = {07110},
	year = {2006}
}
@article{Murdoch2018,
	abstract = {The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.},
	archivePrefix = {arXiv},
	arxivId = {1801.05453},
	author = {Murdoch, W. James and Liu, Peter J. and Yu, Bin},
	eprint = {1801.05453},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Liu, Yu - 2018 - Beyond Word Importance Contextual Decomposition to Extract Interactions from LSTMs.pdf:pdf},
	mendeley-groups = {!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank},
	pages = {1--14},
	title = {{Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs}},
	url = {http://arxiv.org/abs/1801.05453},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/CTAX1{\_}1214824-3{\_}09.pdf:pdf},
	pages = {1214824},
	title = {{COUNCIL TAX - Student Certificate TRETH CYNGOR - Tystysgrif Myfyriwr This certificate relates to the person , not the address /}},
	year = {2019}
}
@article{Franca2014a,
	abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph min-ing and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integratedBCP with a well-known neural-symbolic system, C-IL2 P, to perform learning from nu-merical vectors. C-IL2 P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, han-dles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary re-sults indicating that a reduction of more than 90{\{}{\%}{\}} of features can be achieved with a small loss of accuracy.},
	author = {Fran{\c{c}}a, Manoel V M and Zaverucha, Gerson and {D'Avila Garcez}, Artur S.},
	doi = {10.1007/s10994-013-5392-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}a, Zaverucha, D'Avila Garcez - 2014 - Fast relational learning using bottom clause propositionalization with artificial neural netw.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Artificial neural networks,Inductive logic programming,Neural-symbolic integration,Propositionalization,Relational learning},
	mendeley-groups = {Progress Report},
	number = {1},
	pages = {81--104},
	title = {{Fast relational learning using bottom clause propositionalization with artificial neural networks}},
	volume = {94},
	year = {2014}
}
@article{Bengio2013,
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	archivePrefix = {arXiv},
	arxivId = {1206.5538},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	doi = {10.1109/TPAMI.2013.50},
	eprint = {1206.5538},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2013 - Representation learning A review and new perspectives.pdf:pdf},
	isbn = {0162-8828 VO - 35},
	issn = {01628828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
	mendeley-groups = {Papers/Paper 1,Report},
	number = {8},
	pages = {1798--1828},
	pmid = {23787338},
	title = {{Representation learning: A review and new perspectives}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23459267{\%}5Cnhttp://arxiv.org/abs/1206.5538{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6472238},
	volume = {35},
	year = {2013}
}
@article{Ji2017,
	abstract = {We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.},
	archivePrefix = {arXiv},
	arxivId = {1702.01829},
	author = {Ji, Yangfeng and Smith, Noah},
	doi = {10.18653/v1/P17-1092},
	eprint = {1702.01829},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji, Smith - 2017 - Neural Discourse Structure for Text Categorization.pdf:pdf},
	isbn = {9781945626753},
	mendeley-groups = {!Paper 3/Structured LSTMs,!Paper 3/task/Yelp},
	title = {{Neural Discourse Structure for Text Categorization}},
	url = {http://arxiv.org/abs/1702.01829},
	year = {2017}
}
@article{Fares2017,
	author = {Fares, Murhaf and Kutuzov, Andrey and Oepen, Stephan and Velldal, Erik},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fares et al. - 2017 - Word vectors , reuse , and replicability Towards a community repository of large-text resources.pdf:pdf},
	mendeley-groups = {Report/Features},
	number = {May},
	pages = {271--276},
	title = {{Word vectors , reuse , and replicability : Towards a community repository of large-text resources}},
	year = {2017}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Frameworks{\_}for{\_}Properties{\_}Possible{\_}Worlds{\_}vs{\_}Conce.pdf:pdf},
	number = {January 1995},
	title = {{Frameworks for Properties : Possible Worlds vs . Conceptual Spaces Frameworks for Properties : Possible Worlds vs . Conceptual Spaces}},
	year = {2015}
}
@article{Xie2011,
	author = {Xie, Pengtao and Xing, Eric P},
	file = {:E$\backslash$:/Downloads/Work/naacl15.pdf:pdf},
	title = {{Incorporating Word Correlation Knowledge into Topic Modeling}},
	year = {2011}
}
@article{Baumel2007,
	author = {Baumel, Tal and Nassour-kassis, Jumana and Cohen, Raphael and Elhadad, Michael},
	file = {:E$\backslash$:/Downloads/Work/16881-75991-1-PB.pdf:pdf},
	pages = {409--416},
	title = {{Multi-Label Classification of Patient Notes : Case Study on ICD Code Assignment}},
	year = {2007}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.dropbox:dropbox},
	title = {{No Title}}
}
@article{Zaremba2014,
	abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99{\%} accuracy.},
	archivePrefix = {arXiv},
	arxivId = {1410.4615},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	doi = {10.1016/S0893-6080(96)00073-1},
	eprint = {1410.4615},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever - 2014 - Learning to Execute.pdf:pdf},
	isbn = {1410.4615},
	issn = {08936080},
	mendeley-groups = {!Paper 3/task/Sentiment treebank},
	pages = {1--25},
	title = {{Learning to Execute}},
	url = {http://arxiv.org/abs/1410.4615},
	year = {2014}
}
@article{Pca,
	author = {Pca, Obtain and Product, Amazon and Mds, Obtain and Mds, Obtain and Product, Amazon and Doc, Obtain},
	file = {:E$\backslash$:/Downloads/Work/Time plan (1) (1).pdf:pdf},
	pages = {10--11},
	title = {{Thomas Ager Time Plan for Thesis}}
}
@article{Mimno2011,
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Un-fortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional sub- spaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mimno et al. - 2011 - Optimizing semantic coherence in topic models.pdf:pdf},
	isbn = {9781937284114},
	issn = {1937284115},
	journal = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
	keywords = {topic coherence,topic models,topics evaluation},
	mendeley-groups = {Report/Clustering},
	number = {2},
	pages = {262--272},
	title = {{Optimizing semantic coherence in topic models}},
	year = {2011}
}
@article{Hauser2010,
	abstract = {The authors test methods, based on cognitively simple decision rules, that predict which products consumers select for their consideration sets. Drawing on qualitative research, the authors propose disjunctions-of- conjunctions (DOC) decision rules that generalize well-studied decision models, such as disjunctive, conjunctive, lexicographic, and subset conjunctive rules. They propose two machine-learning methods to estimate cognitively simple DOC rules. They observe consumers' consideration sets for global positioning systems for both calibration and validation data.They compare the proposed methods with both machine- learning and hierarchical Bayes methods, each based on five extant compensatory and noncompensatory rules. For the validation data, the cognitively simple DOC-based methods predict better than the ten benchmark methods on an information theoretic measure and on hit rates. The results are robust with respect to format by which consideration is measured, sample, and presentation of profiles. The article closes with an illustration of how DOC-based rules can affect managerial decisions.},
	author = {Hauser, John R and Toubia, Olivier and Evgeniou, Theodoros and Befurt, Rene and Dzyabura, Daria},
	doi = {10.1509/jmkr.47.3.485},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hauser et al. - 2010 - Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets.pdf:pdf},
	isbn = {00222437},
	issn = {0022-2437},
	journal = {Journal of Marketing Research},
	keywords = {cognitive,conjoint analysis,consideration sets,consumer,decision theory,disjunctions of conjunctions,heuristics,lexicography,machine learning,noncompensatory decisions,simplicity},
	mendeley-groups = {Annotated/Applications/Marketing},
	number = {3},
	pages = {485--496},
	pmid = {50522113},
	title = {{Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets}},
	volume = {47},
	year = {2010}
}
@article{Tang2015,
	abstract = {Document level sentiment classification remains a challenge: encoding the intrin- sic relations between sentences in the se- mantic meaning of a document. To ad- dress this, we introduce a neural network model to learn vector-based document rep- resentation in a unified, bottom-up fash- ion. The model first learns sentence rep- resentation with convolutional neural net- work or long short-term memory. After- wards, semantics of sentences and their relations are adaptively encoded in docu- ment representation with gated recurren- t neural network. We conduct documen- t level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimen- tal results show that: (1) our neural mod- el shows superior performances over sev- eral state-of-the-art algorithms; (2) gat- ed recurrent neural network dramatically outperforms standard recurrent neural net- work in document modeling for sentiment classification},
	archivePrefix = {arXiv},
	arxivId = {1508.04025},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	doi = {10.18653/v1/D15-1167},
	eprint = {1508.04025},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:pdf},
	isbn = {9781941643327},
	issn = {10495258},
	journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	mendeley-groups = {Annotated/Document representation,!Paper 3/task,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank,11Thesis/Document Representations},
	number = {September},
	pages = {1422--1432},
	title = {{Document Modeling with Gated Recurrent Neural Network for Sentiment Classification}},
	url = {http://aclweb.org/anthology/D15-1167},
	year = {2015}
}
@article{Rocktaschel2015,
	author = {Rockt{\"{a}}schel, Tim and Singh, Sameer and Riedel, Sebastian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rockt{\"{a}}schel, Singh, Riedel - 2015 - Injecting logical background knowledge into embeddings for relation extraction.pdf:pdf},
	journal = {Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	title = {{Injecting logical background knowledge into embeddings for relation extraction}},
	year = {2015}
}
@article{Kim2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1711.11279v5},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Jun, M L},
	eprint = {arXiv:1711.11279v5},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/1711.11279.pdf:pdf},
	title = {{Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCAV )}},
	year = {2017}
}
@article{Apte1994,
	abstract = {We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67{\%} recall/precision breakeven point to 80.5{\%}. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.},
	author = {Apt{\'{e}}, Chidanand and Damerau, Fred and Weiss, Sholom M.},
	doi = {10.1145/183422.183423},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Apt{\'{e}}, Damerau, Weiss - 1994 - Automated learning of decision rules for text categorization.pdf:pdf},
	isbn = {1046-8188},
	issn = {1046-8188},
	journal = {ACM Trans. Inf. Syst.},
	mendeley-groups = {Annotated/Decision Trees},
	number = {3},
	pages = {233--251},
	title = {{Automated learning of decision rules for text categorization}},
	url = {http://portal.acm.org/citation.cfm?id=183423{\&}dl=},
	volume = {12},
	year = {1994}
}
@article{Lundberg2016,
	abstract = {Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.},
	archivePrefix = {arXiv},
	arxivId = {1611.07478},
	author = {Lundberg, Scott and Lee, Su-In},
	eprint = {1611.07478},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Lee - 2016 - An unexpected unity among methods for interpreting model predictions.pdf:pdf},
	mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	number = {Nips},
	pages = {1--6},
	title = {{An unexpected unity among methods for interpreting model predictions}},
	url = {http://arxiv.org/abs/1611.07478},
	year = {2016}
}
@article{Craven1996,
	author = {Craven, Mark W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven - 1996 - EXTRACTING COMPREHENSIBLE MODELS By.pdf:pdf},
	isbn = {0591144956},
	journal = {Evaluation},
	title = {{EXTRACTING COMPREHENSIBLE MODELS By}},
	year = {1996}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/boardingPass.pdf:pdf},
	pages = {7},
	title = {{Ager / Thomas Mr}},
	year = {2018}
}
@article{Inan2016,
	abstract = {Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.},
	archivePrefix = {arXiv},
	arxivId = {1611.01462},
	author = {Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
	eprint = {1611.01462},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inan, Khosravi, Socher - 2016 - Tying Word Vectors and Word Classifiers A Loss Framework for Language Modeling.pdf:pdf},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
	pages = {1--13},
	title = {{Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling}},
	url = {http://arxiv.org/abs/1611.01462},
	year = {2016}
}
@article{Predic2010a,
	author = {Predic, N Y P and June, Meetup},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Predic, June - 2010 - Introduction to.pdf:pdf},
	isbn = {1441923349},
	pages = {1--8},
	title = {{Introduction to}},
	year = {2010}
}
@article{Blodgett2017,
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
	archivePrefix = {arXiv},
	arxivId = {1707.00061},
	author = {Blodgett, Su Lin and O'Connor, Brendan},
	eprint = {1707.00061},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blodgett, O'Connor - 2017 - Racial Disparity in Natural Language Processing A Case Study of Social Media African-American English.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English}},
	url = {http://arxiv.org/abs/1707.00061},
	year = {2017}
}
@article{Yang2018,
	author = {Yang, Zichao and Hu, Zhiting and Dyer, Chris and Xing, Eric P and Berg-kirkpatrick, Taylor},
	file = {:E$\backslash$:/PhD/Papedrs/7959-unsupervised-text-style-transfer-using-language-models-as-discriminators.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	number = {NeurIPS},
	pages = {1--12},
	title = {{Unsupervised Text Style Transfer using Language Models as Discriminators}},
	year = {2018}
}
@article{Siddharth2016,
	abstract = {We develop a framework for incorporating structured graphical models in the $\backslash$emph{\{}encoders{\}} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes.},
	archivePrefix = {arXiv},
	arxivId = {1611.07492},
	author = {Siddharth, N. and Paige, Brooks and Desmaison, Alban and {Van de Meent}, Jan-Willem and Wood, Frank and Goodman, Noah D. and Kohli, Pushmeet and Torr, Philip H. S.},
	eprint = {1611.07492},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siddharth et al. - 2016 - Inducing Interpretable Representations with Variational Autoencoders.pdf:pdf},
	journal = {arXiv preprint},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	number = {Nips 2016},
	title = {{Inducing Interpretable Representations with Variational Autoencoders}},
	url = {http://arxiv.org/abs/1611.07492},
	year = {2016}
}
@book{,
	file = {:E$\backslash$:/Downloads/Work/10290.pdf:pdf},
	isbn = {9780262039406},
	title = {{No Title}}
}
@article{Glorot2011a,
	author = {Glorot, X and Bordes, a and Bengio, Y},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain adaptation for large-scale sentiment classification A deep learning approach.pdf:pdf},
	keywords = {auto-encoders,deep-learning},
	mendeley-groups = {Papers/Paper 1,Interim Review,Report},
	number = {1},
	title = {{Domain adaptation for large-scale sentiment classification: A deep learning approach}},
	url = {http://eprints.pascal-network.org/archive/00008597/},
	year = {2011}
}
@article{Zenker2015,
	author = {Zenker, Frank},
	doi = {10.1007/978-3-319-15021-5},
	file = {:E$\backslash$:/GaerdenforsandZenker2014ACSEditorsintroduction{\_}final{\_}20141130.pdf:pdf},
	isbn = {9783319150215},
	number = {February 2017},
	title = {{Editors ' Introduction : Conceptual Spaces at Work Applications of Conceptual Spaces : The Case for Geometric Knowledge Representation Frank Zenker and Peter G{\"{a}}rdenfors ( Eds .)}},
	year = {2015}
}
@article{Garc??a2009,
	abstract = {The experimental analysis on the performance of a proposed method is a crucial and necessary task to carry out in a research. This paper is focused on the sta- tistical analysis of the results in the field of genetics-based machine Learning. It presents a study involving a set of techniques which can be used for doing a rigorous com- parison among algorithms, in terms of obtaining successful classification models. Two accuracy measures for multi- class problems have been employed: classification rate and Cohen's kappa. Furthermore, two interpretability measures have been employed: size of the rule set and number of antecedents. We have studied whether the samples of results obtained by genetics-based classifiers, using the performance measures cited above, check the necessary conditions for being analysed by means of parametrical tests. The results obtained state that the fulfillment of these conditions are problem-dependent and indefinite, which supports the use of non-parametric statistics in the experi- mental analysis. In addition, non-parametric tests can be satisfactorily employed for comparing generic classifiers over various data-sets considering any performance measure. According to these facts, we propose the use of the most powerful non-parametric statistical tests to carry out multiple comparisons. However, the statistical analysis conducted on interpretability must be carefully considered.},
	author = {Garc??a, S. and Fern??ndez, Alberto and Luengo, Julian and Herrera, F.},
	doi = {10.1007/s00500-008-0392-y},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garca et al. - 2009 - A study of statistical techniques and performance measures for genetics-based machine learning Accuracy and interp.pdf:pdf},
	issn = {14327643},
	journal = {Soft Computing},
	keywords = {Classification,Cohen's kappa,Genetic algorithms,Genetics-based machine learning,Interpretability,Non-parametric tests,Statistical tests},
	number = {10},
	pages = {959--977},
	title = {{A study of statistical techniques and performance measures for genetics-based machine learning: Accuracy and interpretability}},
	volume = {13},
	year = {2009}
}
@article{Larochelle2007,
	author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James},
	doi = {10.1145/1273496.1273556},
	file = {:E$\backslash$:/Downloads/Work/An{\_}empirical{\_}evaluation{\_}of{\_}deep{\_}architectures{\_}on{\_}p.pdf:pdf},
	number = {June 2014},
	title = {{An empirical evaluation of deep architectures on problems with many factors of variation An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation}},
	year = {2007}
}
@article{Corcoran2018a,
	author = {Corcoran, Podraig},
	file = {:E$\backslash$:/Downloads/Work/Completed Annual Progress Review Form.pdf:pdf},
	number = {October},
	title = {{Part 1 : PhD progress review}},
	year = {2018}
}
@article{Ba2016,
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	archivePrefix = {arXiv},
	arxivId = {1607.06450},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	eprint = {1607.06450},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf:pdf},
	isbn = {978-3-642-04273-7},
	issn = {1607.06450},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Layer Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	year = {2016}
}
@article{Cui,
	author = {Cui, Hang and Za{\"{i}}ane, Osmar R and Canada, T H},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui, Za{\"{i}}ane, Canada - Unknown - Hierarchical Structural Approach to Improving the Browsability of Web Search Engine Results.pdf:pdf},
	mendeley-groups = {Report/Clustering},
	pages = {1--5},
	title = {{Hierarchical Structural Approach to Improving the Browsability of Web Search Engine Results}}
}
@article{Andrews1995,
	abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy. {\textcopyright} 1995.},
	author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
	doi = {10.1016/0950-7051(96)81920-4},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Diederich, Tickle - 1995 - Survey and critique of techniques for extracting rules from trained artificial neural networks.pdf:pdf},
	isbn = {09507051 (ISSN)},
	issn = {09507051},
	journal = {Knowledge-Based Systems},
	keywords = {fuzzy neural networks,inferencing,knowledge insertion,rule extraction,rule refinement},
	number = {6},
	pages = {373--389},
	title = {{Survey and critique of techniques for extracting rules from trained artificial neural networks}},
	volume = {8},
	year = {1995}
}
@article{Concept2018,
	author = {Concept, Elative and Testing, Importance},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/tcav{\_}relative{\_}concept{\_}importance{\_}testing{\_}with{\_}linear{\_}concept{\_}activation{\_}vectors (1).pdf:pdf},
	number = {26},
	title = {{TCAV : R ELATIVE CONCEPT IMPORTANCE TESTING}},
	year = {2018}
}
@article{Hamilton2014,
	abstract = {The rise in prevalence of algorithmically curated feeds in online news and social media sites raises a new question for designers, critics, and scholars of media: how aware are users of the role of algorithms and filters in their news sources? This paper situates this problem within the history of design for interaction, with an emphasis on the contemporary challenges of studying, and designing for, the algorithmic "curation" of feeds. Such a problem presents particular challenges when, as is common, neither the user nor the researcher has access to the actual proprietary algorithms at work. Author},
	author = {Hamilton, Kevin and Karahalios, Karrie and Sandvig, Christian and Eslami, Motahhare},
	doi = {10.1145/2559206.2578883},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamilton et al. - 2014 - A path to understanding the effects of algorithm awareness.pdf:pdf},
	isbn = {9781450324748},
	journal = {Proceedings of the extended abstracts of the 32nd annual ACM conference on Human factors in computing systems - CHI EA '14},
	mendeley-groups = {Annotated/Psychology},
	pages = {631--642},
	title = {{A path to understanding the effects of algorithm awareness}},
	url = {http://dl.acm.org/citation.cfm?doid=2559206.2578883},
	year = {2014}
}
@article{Hara2016,
	abstract = {Tree ensembles such as random forests and boosted trees are renowned for their high prediction performance; however, their interpretability is critically limited. One way of interpreting a complex tree ensemble is to obtain its simplified representation, which is formalized as a model selection problem: Given a complex tree ensemble, we want to obtain the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm. Our approach has three appealing features: the prediction performance is maintained, the coverage is sufficiently large, and the computation is reasonably feasible. Our synthetic data experiment and real world data applications show that complicated tree ensembles are approximated reasonably as interpretable.},
	archivePrefix = {arXiv},
	arxivId = {1606.09066},
	author = {Hara, Satoshi and Hayashi, Kohei},
	eprint = {1606.09066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara, Hayashi - 2016 - Making Tree Ensembles Interpretable A Bayesian Model Selection Approach.pdf:pdf},
	mendeley-groups = {Report,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	number = {Whi},
	title = {{Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach}},
	url = {http://arxiv.org/abs/1606.09066},
	year = {2016}
}
@article{Martinc,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1902.00438v2},
	author = {Martinc, Matej and Kralj, Jan and Pollak, Senja},
	eprint = {arXiv:1902.00438v2},
	file = {:E$\backslash$:/1902.00438.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Representations,11Thesis},
	title = {{tax2vec : Constructing Interpretable Features from Taxonomies for Short Text Classification}}
}
@article{Code,
	author = {Code, Legal},
	file = {:E$\backslash$:/000000135948.pdf:pdf},
	title = {{Bag-of-Concepts : Comprehending Document Representation through Clustering Words in Distributed Representation Bag-of-Concepts : 단어에 대한 분산표상의}}
}
@article{Read2015,
	abstract = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
	archivePrefix = {arXiv},
	arxivId = {1503.09022},
	author = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
	eprint = {1503.09022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Hollm{\'{e}}n - 2015 - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
	keywords = {meta-labels,multi-label classification,neural net-,problem transformation},
	pages = {1--23},
	title = {{Multi-label Classification using Labels as Hidden Nodes}},
	url = {http://arxiv.org/abs/1503.09022},
	year = {2015}
}
@book{Carroll1976,
	abstract = {The authors' general objective was to demonstrate that three affective dimensions of meaning -- Evaluation, Potency, and Activity (E-P-A) -- are in fact pancultural. the study was done by 80 researchers in 20 different countries.},
	author = {Carroll, John B},
	booktitle = {The American Journal of Psychology},
	isbn = {9780252004261},
	keywords = {ACTIVITY,Dimension,affect,evaluation,meaning,potency,semantics,space,universals,word class},
	number = {1},
	pages = {172--178},
	title = {{Cross-Cultural Universals of Affective Meaning}},
	volume = {89},
	year = {1976}
}
@article{Huysmans2011,
	abstract = {An important objective of data mining is the development of predictive models. Based on a number of observations, a model is constructed that allows the analysts to provide classifications or predictions for new observations. Currently, most research focuses on improving the accuracy or precision of these models and comparatively little research has been undertaken to increase their comprehensibility to the analyst or end-user. This is mainly due to the subjective nature of 'comprehensibility', which depends on many factors outside the model, such as the user's experience and his/her prior knowledge. Despite this influence of the observer, some representation formats are generally considered to be more easily interpretable than others. In this paper, an empirical study is presented which investigates the suitability of a number of alternative representation formats for classification when interpretability is a key requirement. The formats under consideration are decision tables, (binary) decision trees, propositional rules, and oblique rules. An end-user experiment was designed to test the accuracy, response time, and answer confidence for a set of problem-solving tasks involving the former representations. Analysis of the results reveals that decision tables perform significantly better on all three criteria, while post-test voting also reveals a clear preference of users for decision tables in terms of ease of use. ?? 2010 Elsevier B.V. All rights reserved.},
	author = {Huysmans, Johan and Dejaeger, Karel and Mues, Christophe and Vanthienen, Jan and Baesens, Bart},
	doi = {10.1016/j.dss.2010.12.003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huysmans et al. - 2011 - An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models.pdf:pdf},
	isbn = {0167-9236},
	issn = {01679236},
	journal = {Decision Support Systems},
	keywords = {Classification,Comprehensibility,Data mining,Decision tables,Knowledge representation},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	number = {1},
	pages = {141--154},
	publisher = {Elsevier B.V.},
	title = {{An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models}},
	url = {http://dx.doi.org/10.1016/j.dss.2010.12.003},
	volume = {51},
	year = {2011}
}
@article{Speer,
	abstract = {Machine learning about language can be improved by sup-plying it with specific knowledge and sources of external in-formation. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embed-dings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowl-edge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a pur-pose. It is designed to represent the general knowledge in-volved in understanding language, improving natural lan-guage applications by allowing the application to better un-derstand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improve-ments on applications of word vectors, including solving SAT-style analogies.},
	author = {Speer, Robert and Chin, Joshua and Havasi, Catherine},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Speer, Chin, Havasi - Unknown - ConceptNet 5.5 An Open Multilingual Graph of General Knowledge.pdf:pdf},
	title = {{ConceptNet 5.5: An Open Multilingual Graph of General Knowledge}}
}
@article{Zhang2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.00614v2},
	author = {Zhang, Quanshi and Zhu, Song-chun},
	eprint = {arXiv:1802.00614v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1802.00614.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Visual},
	title = {{Visual Interpretability for Deep Learning: a Survey}},
	year = {2017}
}
@article{WenminLi,
	abstract = {Previous studies propose that associative classification has high$\backslash$nclassification accuracy and strong flexibility at handling unstructured$\backslash$ndata. However, it still suffers from the huge set of mined rules and$\backslash$nsometimes biased classification or overfitting since the classification$\backslash$nis based on only a single high-confidence rule. The authors propose a$\backslash$nnew associative classification method, CMAR, i.e., Classification based$\backslash$non Multiple Association Rules. The method extends an efficient frequent$\backslash$npattern mining method, FP-growth, constructs a class$\backslash$ndistribution-associated FP-tree, and mines large databases efficiently.$\backslash$nMoreover, it applies a CR-tree structure to store and retrieve mined$\backslash$nassociation rules efficiently, and prunes rules effectively based on$\backslash$nconfidence, correlation and database coverage. The classification is$\backslash$nperformed based on a weighted {\&}chi;2 analysis using multiple$\backslash$nstrong association rules. Our extensive experiments on 26 databases from$\backslash$nthe UCI machine learning database repository show that CMAR is$\backslash$nconsistent, highly effective at classification of various kinds of$\backslash$ndatabases and has better average classification accuracy in comparison$\backslash$nwith CBA and C4.5. Moreover, our performance study shows that the method$\backslash$nis highly efficient and scalable in comparison with other reported$\backslash$nassociative classification methods},
	author = {{Wenmin Li} and {Jiawei Han} and {Jian Pei}},
	doi = {10.1109/ICDM.2001.989541},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wenmin Li, Jiawei Han, Jian Pei - Unknown - CMAR accurate and efficient classification based on multiple class-association rules.pdf:pdf},
	isbn = {0-7695-1119-8},
	issn = {15504786},
	journal = {Proceedings 2001 IEEE International Conference on Data Mining},
	mendeley-groups = {Report},
	pages = {369--376},
	title = {{CMAR: accurate and efficient classification based on multiple class-association rules}},
	url = {http://ieeexplore.ieee.org/document/989541/}
}
@article{Fallis2013a,
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Fallis, A.G},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fallis - 2013 - No Title No Title(2).pdf:pdf},
	isbn = {9788578110796},
	issn = {1098-6596},
	journal = {Journal of Chemical Information and Modeling},
	keywords = {icle},
	number = {9},
	pages = {1689--1699},
	pmid = {25246403},
	title = {{No Title No Title}},
	volume = {53},
	year = {2013}
}
@article{Bandara2017,
	abstract = {With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks, and in particular Long Short-Term Memory (LSTM) networks have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context, when trained across all available time series. However, if the time series database is heterogeneous accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model using LSTMs on subgroups of similar time series, which are identified by time series clustering techniques. The proposed methodology is able to consistently outperform the baseline LSTM model, and it achieves competitive results on benchmarking datasets, in particular outperforming all other methods on the CIF2016 dataset.},
	archivePrefix = {arXiv},
	arxivId = {1710.03222},
	author = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
	eprint = {1710.03222},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bandara, Bergmeir, Smyl - 2017 - Forecasting Across Time Series Databases using Long Short-Term Memory Networks on Groups of Similar Ser.pdf:pdf},
	keywords = {big data forecasting,lstm,neural networks,rnn,time series clustering},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	number = {Ml},
	title = {{Forecasting Across Time Series Databases using Long Short-Term Memory Networks on Groups of Similar Series}},
	url = {http://arxiv.org/abs/1710.03222},
	volume = {1999},
	year = {2017}
}
@article{Karaletsos2015,
	abstract = {Representation learning systems typically rely on massive amounts of labeled data in order to be trained effectively. Recently, high-dimensional parametric models like convolutional neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Human-in-the-loop systems like crowdsourcing are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. We propose to combine generative unsupervised feature learning with learning from similarity orderings in order to learn models which take advantage of privileged information coming from the crowd. We use a fast variational algorithm to learn the model on standard datasets and demonstrate applicability to two image datasets, where classification is drastically improved. We show how triplet-samples of the crowd can supplement labels as a source of information to shape latent spaces with rich semantic information.},
	archivePrefix = {arXiv},
	arxivId = {1506.05011},
	author = {Karaletsos, Theofanis and Belongie, Serge and R{\"{a}}tsch, Gunnar},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1506.05011},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karaletsos, Belongie, R{\"{a}}tsch - 2015 - Bayesian representation learning with oracle constraints.pdf:pdf},
	journal = {Iclr},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Bayesian representation learning with oracle constraints}},
	url = {http://arxiv.org/abs/1506.05011},
	year = {2015}
}
@article{Sarikaya2014a,
	abstract = {Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.},
	author = {Sarikaya, Ruhi and Hinton, Geoffrey E. and Deoras, Anoop},
	doi = {10.1109/TASLP.2014.2303296},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarikaya, Hinton, Deoras - 2014 - Application of deep belief networks for natural language understanding.pdf:pdf},
	isbn = {2329-9290 VO  - 22},
	issn = {15587916},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	keywords = {Call-routing,DBN,Deep learning,Deep neural nets,Natural language understanding,RBM},
	number = {4},
	pages = {778--784},
	title = {{Application of deep belief networks for natural language understanding}},
	volume = {22},
	year = {2014}
}
@article{From2019,
	author = {From, E X T},
	file = {:E$\backslash$:/PhD/Papedrs/TopicGAN Unsupervised Text Generation from Explainable Latent Topics.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {1--10},
	title = {{T OPIC GAN : U NSUPERVISED T EXT}},
	year = {2019}
}
@article{Ross2017,
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	archivePrefix = {arXiv},
	arxivId = {1703.03717},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	eprint = {1703.03717},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Annotated/Explanations,!Paper 3/task/newsgroups},
	title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
	url = {http://arxiv.org/abs/1703.03717},
	year = {2017}
}
@article{Chen2014a,
	abstract = {Abstract Denoising auto - encoders (DAEs) have been successfully used to learn new representations for a wide range of machine learning tasks. During training, DAEs make many passes over the training dataset and reconstruct it from partial corruption generated ... $\backslash$n},
	author = {Chen, M and Weinberger, K and Sha, F and Bengio, Y},
	doi = {10.1007/s11222-007-9033-z},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Marginalized Denoising Auto-encoders for Nonlinear Representations.pdf:pdf},
	isbn = {9781634393973},
	issn = {0960-3174},
	journal = {Proceedings of The 31st {\ldots}},
	keywords = {ICML2014},
	title = {{Marginalized Denoising Auto-encoders for Nonlinear Representations}},
	url = {http://jmlr.org/proceedings/papers/v32/cheng14.pdf{\%}5Cnpapers3://publication/uuid/B8D413D0-9096-41DC-A234-EBFC9C94CBAB},
	volume = {32},
	year = {2014}
}
@article{Hechtlinger2016,
	abstract = {State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a "black box". In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing.},
	archivePrefix = {arXiv},
	arxivId = {1611.07634},
	author = {Hechtlinger, Yotam},
	eprint = {1611.07634},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hechtlinger - 2016 - Interpretation of Prediction Models Using the Input Gradient.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	number = {Nips},
	title = {{Interpretation of Prediction Models Using the Input Gradient}},
	url = {http://arxiv.org/abs/1611.07634},
	year = {2016}
}
@article{Lee2017,
	author = {Lee, Jaesong and Shin, Joong-hwi and Kim, Jun-seok and Corp, Naver},
	file = {:E$\backslash$:/Downloads/Work/D17-2021.pdf:pdf},
	pages = {121--126},
	title = {{Interactive Visualization and Manipulation of Attention-based Neural Machine Translation}},
	year = {2017}
}
@article{Jacobsson2005,
	abstract = {Rule extraction (RE) from recurrent neural networks (RNNs) refers to finding models of the underlying RNN, typically in the form of finite state machines, that mimic the network to a satisfactory degree while having the advantage of being more transparent. RE from RNNs can be argued to allow a deeper and more profound form of analysis of RNNs than other, more or less ad hoc methods. RE may give us understanding of RNNs in the intermediate levels between quite abstract theoretical knowledge of RNNs as a class of computing devices and quantitative performance evaluations of RNN instantiations. The development of techniques for extraction of rules from RNNs has been an active field since the early 1990s. This article reviews the progress of this development and analyzes it in detail. In order to structure the survey and evaluate the techniques, a taxonomy specifically designed for this purpose has been developed. Moreover, important open research issues are identified that, if addressed properly, possibly ca...},
	author = {Jacobsson, Henrik},
	doi = {10.1162/0899766053630350},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobsson - 2005 - Rule Extraction from Recurrent Neural Networks ATaxonomy and Review.pdf:pdf},
	isbn = {0899766053630350},
	issn = {0899-7667},
	journal = {Neural Computation},
	number = {6},
	pages = {1223--1263},
	title = {{Rule Extraction from Recurrent Neural Networks: ATaxonomy and Review}},
	volume = {17},
	year = {2005}
}
@article{Beltagy2013,
	author = {Beltagy, Islam and Chau, Cuong and Boleda, Gemma and Garrette, Dan and Erk, Katrin},
	file = {:C$\backslash$:/Users/Workk/Documents/S13-1002.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	pages = {11--21},
	title = {{Montague Meets Markov : Deep Semantics with Probabilistic Logical Form}},
	volume = {1},
	year = {2013}
}
@article{Kim2000,
	author = {Kim, D and Lee, J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Lee - 2000 - Handling continuous-valued attributes in decision tree with neural network modeling.pdf:pdf},
	isbn = {3-540-67602-3},
	journal = {Machine Learning: Ecml 2000},
	mendeley-groups = {Report/Decision Trees?,Papers/Paper 1,Progress Report,Report},
	pages = {211--219},
	title = {{Handling continuous-valued attributes in decision tree with neural network modeling}},
	volume = {1810},
	year = {2000}
}
@article{Zhang2010,
	abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
	author = {Zhang, Min-Ling and Zhang, Kun},
	doi = {10.1145/1835804.1835930},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
	isbn = {9781450300551},
	issn = {9781577355687},
	journal = {Kdd},
	mendeley-groups = {Progress Report},
	pages = {999--1007},
	title = {{Multi-label learning by exploiting label dependency}},
	url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
	year = {2010}
}
@article{Merity2017,
	abstract = {Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing. Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance. Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations. We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling. Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures. These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.},
	archivePrefix = {arXiv},
	arxivId = {1708.01009},
	author = {Merity, Stephen and McCann, Bryan and Socher, Richard},
	eprint = {1708.01009},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merity, McCann, Socher - 2017 - Revisiting Activation Regularization for Language RNNs.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Revisiting Activation Regularization for Language RNNs}},
	url = {http://arxiv.org/abs/1708.01009},
	year = {2017}
}
@article{Saaty2003,
	abstract = {In 1956, Miller [1] conjectured that there is an upper limit on our capacity to process information on simultaneously interacting elements with reliable accuracy and with validity. This limit is seven plus or minus two elements. He noted that the number 7 occurs in many aspects of life, from the seven wonders of the world to the seven seas and seven deadly sins. We demonstrate in this paper that in making preference judgments on pairs of elements in a group, as we do in the analytic hierarchy process (AHP), the number of elements in the group should be no more than seven. The reason is founded in the consistency of information derived from relations among the elements. When the number of elements increases past seven, the resulting increase in inconsistency is too small for the mind to single out the element that causes the greatest inconsistency to scrutinize and correct its relation to the other elements, and the result is confusion to the mind from the existing information. The AHP as a theory of measurement has a basic way to obtain a measure of inconsistency for any such set of pairwise judgments. When the number of elements is seven or less the inconsistency measurement is relatively large with respect to the number of elements involved; when the number is more it is relatively small. The most inconsistent judgment is easily determined in the first case and the individual providing the judgments can change it in an effort to improve the overall inconsistency. In the second case, as the inconsistency measurement is relatively small, improving inconsistency requires only small perturbations and the judge would be hard put to determine what that change should be, and how such a small change could be justified for improving the validity of the outcome. The mind is sufficiently sensitive to improve large inconsistencies but not small ones. And the implication of this is that the number of elements in a set should be limited to seven plus or minus two.},
	author = {Saaty, T.L. and Ozdemir, M.S.},
	doi = {10.1016/S0895-7177(03)90083-5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saaty, Ozdemir - 2003 - Why the magic number seven plus or minus two.pdf:pdf},
	isbn = {0895-7177},
	issn = {08957177},
	journal = {Mathematical and Computer Modelling},
	mendeley-groups = {Annotated/Psychology},
	number = {3},
	pages = {233--244},
	pmid = {8022966},
	title = {{Why the magic number seven plus or minus two}},
	volume = {38},
	year = {2003}
}
@article{Foster2011,
	author = {Foster, Jennifer and Wagner, Joachim and Roux, Joseph Le and Hogan, Stephen and Nivre, Joakim and Hogan, Deirdre and Genabith, Josef Van},
	file = {:E$\backslash$:/Downloads/Work/3912-16625-1-PB.pdf:pdf},
	pages = {20--25},
	title = {{{\#} hardtoparse : POS Tagging and Parsing the Twitterverse Evaluation of WSJ-Trained Resources}},
	year = {2011}
}
@article{Greff2017,
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({\$}\backslashapprox 15{\$} years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	archivePrefix = {arXiv},
	arxivId = {1503.04069},
	author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
	doi = {10.1109/TNNLS.2016.2582924},
	eprint = {1503.04069},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf:pdf},
	isbn = {9788578110796},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Functional ANalysis Of VAriance (fANOVA),long short-term memory (LSTM),random search,recurrent neural networks,sequence learning},
	mendeley-groups = {!Paper 3/Training LSTMs},
	number = {10},
	pages = {2222--2232},
	pmid = {25246403},
	title = {{LSTM: A Search Space Odyssey}},
	volume = {28},
	year = {2017}
}
@article{Smilkov2016,
	abstract = {Embeddings are ubiquitous in machine learning, appearing in recommender systems, NLP, and many other applications. Researchers and developers often need to explore the properties of a specific embedding, and one way to analyze embeddings is to visualize them. We present the Embedding Projector, a tool for interactive visualization and interpretation of embeddings.},
	archivePrefix = {arXiv},
	arxivId = {1611.05469},
	author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Vi{\'{e}}gas, Fernanda B. and Wattenberg, Martin},
	eprint = {1611.05469},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smilkov et al. - 2016 - Embedding Projector Interactive Visualization and Interpretation of Embeddings.pdf:pdf},
	mendeley-groups = {Report},
	number = {Nips},
	title = {{Embedding Projector: Interactive Visualization and Interpretation of Embeddings}},
	url = {http://arxiv.org/abs/1611.05469},
	year = {2016}
}
@article{Lavra??1999,
	abstract = {Widespread use of medical information systems and explosive growth of medical databases require traditional manual data analysis to be coupled with methods for efficient computer-assisted analysis. This paper presents selected data mining techniques that can be applied in medicine, and in particular some machine learning techniques including the mechanisms that make them better suited for the analysis of medical databases (derivation of symbolic rules, use of background knowledge, sensitivity and specificity of induced descriptions). The importance of the interpretability of results of data analysis is discussed and illustrated on selected medical applications.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Lavra??, Nada},
	doi = {10.1016/S0933-3657(98)00062-1},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavra - 1999 - Selected techniques for data mining in medicine.pdf:pdf},
	isbn = {0933-3657},
	issn = {09333657},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Data mining,Machine learning,Medical applications},
	mendeley-groups = {Report/Medical domain},
	number = {1},
	pages = {3--23},
	pmid = {10225344},
	title = {{Selected techniques for data mining in medicine}},
	volume = {16},
	year = {1999}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The-Ten-Principal-Upanishads.pdf.pdf:pdf},
	title = {{The-Ten-Principal-Upanishads.pdf}}
}
@article{Semeniuta2016,
	abstract = {This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to $\backslash$textit{\{}forward{\}} connections of feed-forward architectures or RNNs, we propose to drop neurons directly in $\backslash$textit{\{}recurrent{\}} connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for Long Short-Term Memory network, the most popular type of RNN cells. Our experiments on NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.},
	archivePrefix = {arXiv},
	arxivId = {1603.05118},
	author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
	eprint = {1603.05118},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Semeniuta, Severyn, Barth - 2016 - Recurrent Dropout without Memory Loss.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Recurrent Dropout without Memory Loss}},
	url = {http://arxiv.org/abs/1603.05118},
	year = {2016}
}
@article{Fu1994,
	abstract = {The neural network approach has proven useful for the development of artificial intelligence systems.  However, a disadvantage with this approach is that the knowledge embedded in the neural network is opaque.  In this paper, we show how to interpret neural network knowledge in sympobic form.  We lay down required definitions for this treatment, formulate the interpretation algorithm, and formally verigy its soundness.  The main result is a formalized relationship between a neural network and a rule-based system.  In addition, it has been demonstrated that the neural network generates rules of better performance than the decision tree approach in noisy conditions.},
	author = {Fu, LiMin},
	doi = {10.1109/21.299696},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu - 1994 - Rule generation from neural networks.pdf:pdf},
	issn = {00189472},
	journal = {IEEE Transactions on Systems, Man and Cybernetics},
	number = {8},
	pages = {1114--1124},
	title = {{Rule generation from neural networks}},
	volume = {24},
	year = {1994}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 4ae075de7bf9ea2cac01fe02e2920ee5c789.pdf.pdf:pdf},
	mendeley-groups = {Report},
	title = {4ae075de7bf9ea2cac01fe02e2920ee5c789.pdf}
}
@article{Trifonov,
  title={Learning and Evaluating Sparse Interpretable Sentence Embeddings},
  author={Valentin Trifonov and Octavian-Eugen Ganea and Anna Potapenko and Thomas Hofmann},
  booktitle={BlackboxNLP EMNLP},
  year={2018}
}

@article{TomasMikolovWen-tauYih2013,
	abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
	archivePrefix = {arXiv},
	arxivId = {1301.3781},
	author = {{Tomas Mikolov∗ , Wen-tau Yih}, Geoffrey Zweig},
	doi = {10.3109/10826089109058901},
	eprint = {1301.3781},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/rvecs.pdf:pdf},
	isbn = {9781937284473},
	issn = {9781937284473},
	journal = {Hlt-Naacl},
	mendeley-groups = {11Thesis},
	number = {June},
	pages = {746--751},
	pmid = {1938007},
	title = {{Linguistic Regularities in Continuous Space Word Representations}},
	url = {http://anthology.aclweb.org/N/N13/N13-1.pdf{\#}page=655},
	year = {2013}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 12 - The Paladin of the Holy Kingdom [Part 01] (v1.4) (1).pdf:pdf},
	title = {{Overlord Volume 12 - The Paladin of the Holy Kingdom [Part 01] (v1.4) (1).pdf}}
}
@misc{,
	title = {{{\~{}}1245963944HI}}
}
@article{Craven1993,
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
	journal = {Machine Learning: Proceedings of the Tenth International Conference},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	pages = {73--80},
	title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
	year = {1993}
}
@article{Fader2011,
	abstract = {Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the ReVerb Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TextRunner and woepos. More than 30{\%} of ReVerb's extractions are at precision 0.8 or higher---compared to virtually none for earlier systems. The paper concludes with a detailed analysis of ReVerb's errors, suggesting directions for future work.},
	author = {Fader, Anthony and Soderland, Stephen and Etzioni, Oren},
	doi = {10.1234/12345678},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fader, Soderland, Etzioni - 2011 - Identifying relations for open information extraction.pdf:pdf},
	isbn = {978-1-937284-11-4},
	issn = {1937284115},
	journal = {Proceedings of the Conference on {\ldots}},
	pages = {1535--1545},
	title = {{Identifying relations for open information extraction}},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145596{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2145596},
	year = {2011}
}
@article{Augasta2012a,
	abstract = {Though neural networks have achieved highest classification accuracy for many classification problems, the obtained results may not be interpretable as they are often considered as black box. To overcome this drawback researchers have developed many rule extraction algorithms. This paper has discussed on various rule extraction algorithms based on three different rule extraction approaches namely decompositional, pedagogical and eclectic. Also it evaluates the performance of those approaches by comparing different algorithms with these three approaches on three real datasets namely Wisconsin breast cancer, Pima Indian diabetes and Iris plants. 2012 IEEE.},
	author = {Augasta, M Gethsiyal and Kathirvalavakumar, T},
	doi = {10.1109/icprime.2012.6208380},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Augasta, Kathirvalavakumar - 2012 - Rule extraction from neural networks - A comparative study.pdf:pdf},
	isbn = {9781467310390},
	journal = {2012 International Conference on Pattern Recognition, Informatics and Medical Engineering, PRIME 2012, March 21, 2012 - March 23, 2012},
	keywords = {Biomedical engineering,Classification (of information),Data mining,Information science,Neural networks,Pattern recognition},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	pages = {404--408},
	title = {{Rule extraction from neural networks - A comparative study}},
	url = {http://dx.doi.org/10.1109/ICPRIME.2012.6208380},
	year = {2012}
}
@article{Soneson2014,
	abstract = {BACKGROUND: With the large amount of biological data that is currently publicly available, many investigators combine multiple data sets to increase the sample size and potentially also the power of their analyses. However, technical differences ("batch effects") as well as differences in sample composition between the data sets may significantly affect the ability to draw generalizable conclusions from such studies.$\backslash$n$\backslash$nFOCUS: The current study focuses on the construction of classifiers, and the use of cross-validation to estimate their performance. In particular, we investigate the impact of batch effects and differences in sample composition between batches on the accuracy of the classification performance estimate obtained via cross-validation. The focus on estimation bias is a main difference compared to previous studies, which have mostly focused on the predictive performance and how it relates to the presence of batch effects.$\backslash$n$\backslash$nDATA: We work on simulated data sets. To have realistic intensity distributions, we use real gene expression data as the basis for our simulation. Random samples from this expression matrix are selected and assigned to group 1 (e.g., 'control') or group 2 (e.g., 'treated'). We introduce batch effects and select some features to be differentially expressed between the two groups. We consider several scenarios for our study, most importantly different levels of confounding between groups and batch effects.$\backslash$n$\backslash$nMETHODS: We focus on well-known classifiers: logistic regression, Support Vector Machines (SVM), k-nearest neighbors (kNN) and Random Forests (RF). Feature selection is performed with the Wilcoxon test or the lasso. Parameter tuning and feature selection, as well as the estimation of the prediction performance of each classifier, is performed within a nested cross-validation scheme. The estimated classification performance is then compared to what is obtained when applying the classifier to independent data.},
	author = {Soneson, Charlotte and Gerster, Sarah and Delorenzi, Mauro},
	doi = {10.1371/journal.pone.0100335},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Soneson, Gerster, Delorenzi - 2014 - Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation.pdf:pdf},
	isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Annotated/Artifacts in the data},
	number = {6},
	pmid = {24967636},
	title = {{Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation}},
	volume = {9},
	year = {2014}
}
@article{Veale2018,
	abstract = {Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions — like taxation, justice, and child protection —-are now commonplace. How might design-ers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regard-ing challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning — absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications. NOTE 04/10/2017: This is a document currently under review. We'd be happy if you wanted to cite this — so please get in touch if that is the case. You can also circulate it to colleagues, but please don't publish it publicly. An earlier, shorter version of the paper is available that you can link to publicly: Veale, M (2017) Logics and practices of opacity and transparency in real-world applications of public sector machine learning. Presented as a talk at the 4th Workshop on Fairness, Accountability and Trans-parency in Machine Learning (FAT/ML 2017), Halifax, Nova Scotia, Canada. Available at:},
	archivePrefix = {arXiv},
	arxivId = {1802.01029},
	author = {Veale, Michael and Kleek, Max Van and Binns, Reuben},
	doi = {10.1145/3173574.3174014},
	eprint = {1802.01029},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale, Kleek, Binns - 2018 - Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Maki.pdf:pdf},
	isbn = {9781450356206},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
	title = {{Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making}},
	url = {http://},
	year = {2018}
}
@article{Conll,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/243{\_}file{\_}Submission.pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Bibal2018,
	author = {Bibal, Adrien and Marion, Rebecca and Fr{\'{e}}nay, Beno{\^{i}}t},
	file = {:C$\backslash$:/Users/Workk/Documents/es2018-89.pdf:pdf},
	isbn = {9782875870476},
	number = {April},
	pages = {25--27},
	title = {{Finding the Most Interpretable MDS Rotation for Sparse Linear Models based on External Features}},
	year = {2018}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Overview of literature.pdf:pdf},
	pages = {10--11},
	title = {{No Title}}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Bull.jar:jar},
	title = {{Bull}}
}
@article{Fyshe2015,
	abstract = {Vector Space Models (VSMs) of Semantics are useful tools for exploring the semantics of single words, and the composition of words to make phrasal meaning. While many meth- ods can estimate the meaning (i.e. vector) of a phrase, few do so in an interpretable way. We introduce a new method (CNNSE) that al- lows word and phrase vectors to adapt to the notion of composition. Our method learns a VSM that is both tailored to support a chosen semantic composition operation, and whose resulting features have an intuitive interpreta- tion. Interpretability allows for the exploration of phrasal semantics, which we leverage to an- alyze performance on a behavioral task. 1},
	author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - 2015 - A compositional and interpretable semantic space.pdf:pdf},
	isbn = {9781941643495},
	journal = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015)},
	mendeley-groups = {Progress Report,Interim Review,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
	pages = {32--41},
	title = {{A compositional and interpretable semantic space}},
	year = {2015}
}
@article{Gardenfors2014,
	author = {G{\"{a}}rdenfors, Peter},
	doi = {10.1007/978-1-4020-9877-2},
	file = {:C$\backslash$:/Users/Workk/Documents/Conceptual{\_}Spaces.pdf:pdf},
	isbn = {9781402098772},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {September},
	title = {{Conceptual spaces}},
	year = {2014}
}
@article{Conllb,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (3).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Bolukbasi2016,
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	archivePrefix = {arXiv},
	arxivId = {1607.06520},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	eprint = {1607.06520},
	file = {:D$\backslash$:/Downloads/Play/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf:pdf},
	isbn = {9781510838819},
	issn = {10495258},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {Nips},
	pages = {1--9},
	title = {{Debiasing Word Embedding}},
	url = {https://code.google.com/archive/p/word2vec/},
	year = {2016}
}
@article{Tai2015,
	abstract = {Because of their superior ability to pre-serve sequence information over time, Long Short-Term Memory (LSTM) net-works, a type of recurrent neural net-work with a more complex computational unit, have obtained strong results on a va-riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn-tactic properties that would naturally com-bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti-ment Treebank).},
	archivePrefix = {arXiv},
	arxivId = {1503.0075},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	doi = {10.1515/popets-2015-0023},
	eprint = {1503.0075},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tai, Socher, Manning - 2015 - Improved semantic representations from tree-structured long short-term memory networks.pdf:pdf},
	isbn = {9781941643723},
	issn = {9781941643723},
	journal = {Proceedings of ACL},
	pages = {1556--1566},
	pmid = {18267787},
	title = {{Improved semantic representations from tree-structured long short-term memory networks}},
	year = {2015}
}
@article{Schockaert2015,
	author = {Schockaert, S and Derrac, J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert, Derrac - 2015 - Commonsense Reasoning Based on Betweenness and Direction in Distributional Models.pdf:pdf},
	journal = {2015 AAAI Spring Symposium Series},
	mendeley-groups = {Categories/Commonsense Reasoning},
	number = {April},
	pages = {3--6},
	title = {{Commonsense Reasoning Based on Betweenness and Direction in Distributional Models}},
	url = {http://users.cs.cf.ac.uk/S.Schockaert/Publications{\_}files/AAAI-SS2015.pdf},
	year = {2015}
}
@article{Ticklea,
	author = {Tickle, Alan B and Andrews, Robert and Golea, Mostefa and Diederich, Joachim},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tickle et al. - Unknown - The truth is in there directions and challenges in extracting rules from trained artificial neural networks.pdf:pdf},
	journal = {Neural Networks},
	mendeley-groups = {Progress Report},
	title = {{The truth is in there : directions and challenges in extracting rules from trained artificial neural networks}}
}
@article{Schuhmacher2015,
	author = {Schuhmacher, Michael and Dietz, Laura and Ponzetto, Simone Paolo},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schuhmacher, Dietz, Ponzetto - 2015 - Ranking Entities for Web Queries Through Text and Knowledge.pdf:pdf},
	isbn = {9781450337946},
	keywords = {entities,information retrieval,knowledge bases},
	mendeley-groups = {Report/Rankings},
	pages = {1461--1470},
	title = {{Ranking Entities for Web Queries Through Text and Knowledge}},
	year = {2015}
}
@article{Krueger2016,
	abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.},
	archivePrefix = {arXiv},
	arxivId = {1606.01305},
	author = {Krueger, David and Maharaj, Tegan and Kram{\'{a}}r, J{\'{a}}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
	eprint = {1606.01305},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krueger et al. - 2016 - Zoneout Regularizing RNNs by Randomly Preserving Hidden Activations.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	pages = {1--11},
	title = {{Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations}},
	url = {http://arxiv.org/abs/1606.01305},
	year = {2016}
}
@article{Hill2014,
	author = {Hill, Harold},
	doi = {10.1068/p220887},
	file = {:E$\backslash$:/Downloads/Work/JohnstonHillCarmen92.pdf:pdf},
	number = {February 1993},
	title = {{Independent effects of lighting , orientation , and stereopsis on the hollow- face illusion}},
	year = {2014}
}
@article{Fallis2013,
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Fallis, A.G},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fallis - 2013 - No Title No Title.pdf:pdf},
	isbn = {9788578110796},
	issn = {1098-6596},
	journal = {Journal of Chemical Information and Modeling},
	keywords = {icle},
	number = {9},
	pages = {1689--1699},
	pmid = {25246403},
	title = {{No Title No Title}},
	volume = {53},
	year = {2013}
}
@article{Science,
	author = {Science, Computer and Progress, Student and Review, Interim Progress and Main, Supervisors},
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}Interim{\_}Progress{\_}Review{\_}Form.pdf:pdf},
	title = {{Interim Progress Report Student Details PhD progress review A . 1 Thesis Title and Hypothesis A . 2 Overall Progress}}
}
@article{Williamson2010,
	abstract = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric mixed membership model - each data point is modeled with a collection of components of different proportions. Though powerful, the HDP makes an assumption that the probability of a component being exhibited by a data point is positively correlated with its proportion within that data point. This might be an undesirable assumption. For example, in topic modeling, a topic (component) might be rare throughout the corpus but dominant within those documents (data points) where it occurs. We develop the IBP compound Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data prevalence and within-data proportion in a mixed membership model. The ICD combines properties from the HDP and the Indian buffet process (IBP), a Bayesian nonparametric prior on binary matrices. The ICD assigns a subset of the shared mixture components to each data point. This subset, the data point's ``focus'', is determined independently from the amount that each of its components contribute. We develop an ICD mixture model for text, the focused topic model (FTM), and show superior performance over the HDP-based topic model.},
	author = {Williamson, Sinead and Wang, Chong and Heller, Katherine A},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williamson, Wang, Heller - 2010 - The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling.pdf:pdf},
	isbn = {9781605589077},
	journal = {Icml},
	mendeley-groups = {Annotated/Interpretable representations,!Paper 3/task/newsgroups},
	pages = {1151--1158},
	title = {{The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling}},
	url = {http://www.icml2010.org/papers/397.pdf},
	year = {2010}
}
@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - CM1102 Web Applications Exercise 3 CSS for multi-column layout and drop down menus Objectives of the exercise.pdf:pdf},
	pages = {1--2},
	title = {{CM1102 Web Applications Exercise 3 : CSS for multi-column layout and drop down menus Objectives of the exercise :}}
}
@article{Volumea,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 2 - The Dark Knight.pdf:pdf},
	title = {{No Title}}
}
@article{Pcaa,
	author = {Pca, Obtain and Product, Amazon and Mds, Obtain and Mds, Obtain and Product, Amazon and Doc, Obtain and Pca, Obtain},
	file = {:E$\backslash$:/Downloads/Work/Time plan (2).pdf:pdf},
	pages = {12--14},
	title = {{Thomas Ager Time Plan for Thesis}}
}
@article{Mercado2016,
	abstract = {OBJECTIVE We investigated the effects of level of agent transparency on operator performance, trust, and workload in a context of human-agent teaming for multirobot management. BACKGROUND Participants played the role of a heterogeneous unmanned vehicle (UxV) operator and were instructed to complete various missions by giving orders to UxVs through a computer interface. An intelligent agent (IA) assisted the participant by recommending two plans-a top recommendation and a secondary recommendation-for every mission. METHOD A within-subjects design with three levels of agent transparency was employed in the present experiment. There were eight missions in each of three experimental blocks, grouped by level of transparency. During each experimental block, the IA was incorrect three out of eight times due to external information (e.g., commander's intent and intelligence). Operator performance, trust, workload, and usability data were collected. RESULTS Results indicate that operator performance, trust, and perceived usability increased as a function of transparency level. Subjective and objective workload data indicate that participants' workload did not increase as a function of transparency. Furthermore, response time did not increase as a function of transparency. CONCLUSION Unlike previous research, which showed that increased transparency resulted in increased performance and trust calibration at the cost of greater workload and longer response time, our results support the benefits of transparency for performance effectiveness without additional costs. APPLICATION The current results will facilitate the implementation of IAs in military settings and will provide useful data to the design of heterogeneous UxV teams.},
	author = {Mercado, Joseph E. and Rupp, Michael A. and Chen, Jessie Y. C. and Barnes, Michael J. and Barber, Daniel and Procci, Katelyn},
	doi = {10.1177/0018720815621206},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mercado et al. - 2016 - Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management.pdf:pdf},
	issn = {0018-7208},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	keywords = {address correspondence to joseph,agent teaming,army,e,human,human research and engineering,intelligent agent transparency,mercado,multi-uxv management,research laboratory,s,u},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {3},
	pages = {401--415},
	pmid = {26867556},
	title = {{Intelligent Agent Transparency in Human-Agent Teaming for Multi-UxV Management}},
	url = {http://journals.sagepub.com/doi/10.1177/0018720815621206},
	volume = {58},
	year = {2016}
}
@article{Dasigi2014a,
	abstract = {Automatically identifying anomalous newswire events is a hard problem. We discuss the com-plexity of the problem and introduce a novel technique to model events based on recursive neural networks to represent events as composition of their semantic arguments. Our model learns to differentiate between normal and anomalous events. We model anomaly detection as a binary classification problem and show that the model learns useful features to classify anomaly. We use headlines from the weird news category publicly available on newswire websites to extract anomalous training examples and those from Gigaword as normal examples. We evaluate the classifier on human annotated data and obtain an accuracy of 65.44{\%}. We also show that our model is at least as competent as the least competent human annotator in anomaly detection.},
	author = {Dasigi, Pradeep and Hovy, Eduard},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasigi, Hovy - 2014 - Modeling Newswire Events using Neural Networks for Anomaly Detection.pdf:pdf},
	isbn = {9781941643266},
	journal = {Coling-2014},
	pages = {1414--1422},
	title = {{Modeling Newswire Events using Neural Networks for Anomaly Detection}},
	year = {2014}
}
@article{Bader2004,
	abstract = {Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.},
	archivePrefix = {arXiv},
	arxivId = {cs/0408069},
	author = {Bader, Sebastian and Hitzler, Pascal and Hoelldobler, Steffen},
	eprint = {0408069},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bader, Hitzler, Hoelldobler - 2004 - The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challe.pdf:pdf},
	journal = {Network},
	keywords = {logic programs,neural networks,neural symbolic integra},
	mendeley-groups = {Categories/Logic},
	pages = {12},
	primaryClass = {cs},
	title = {{The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challenge for Artificial Intelligence}},
	url = {http://arxiv.org/abs/cs/0408069},
	year = {2004}
}
@article{Li2015,
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\{}$\backslash$em compositionality{\}}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\{}$\backslash$em salience{\}}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
	archivePrefix = {arXiv},
	arxivId = {1506.01066},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	doi = {10.18653/v1/N16-1082},
	eprint = {1506.01066},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Visualizing and Understanding Neural Models in NLP.pdf:pdf},
	isbn = {9781941643914},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Visualizing and Understanding Neural Models in NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	year = {2015}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/396ec311349a6b596a6467e141ee6f578bf21d48.html:html},
	title = {language{\_}modeling @ github.com},
	url = {https://github.com/sebastianruder/NLP-progress/blob/master/english/language{\_}modeling.md}
}
@article{Tang2014,
	abstract = {We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
	doi = {10.3115/1220575.1220648},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2014 - Learning Sentiment-Specific Word Embedding.pdf:pdf},
	isbn = {9781937284725},
	issn = {03029743},
	journal = {Acl},
	mendeley-groups = {Progress Report},
	pages = {1555--1565},
	pmid = {18487783},
	title = {{Learning Sentiment-Specific Word Embedding}},
	year = {2014}
}
@article{Liang2016,
	abstract = {By taking the semantic object parsing task as an exemplar application scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network, which is the generalization of LSTM from sequential data or multi-dimensional data to general graph-structured data. Particularly, instead of evenly and fixedly dividing an image to pixels or patches in existing multi-dimensional LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each arbitrary-shaped superpixel as a semantically consistent node, and adaptively construct an undirected graph for each image, where the spatial relations of the superpixels are naturally used as edges. Constructed on such an adaptive graph topology, the Graph LSTM is more naturally aligned with the visual patterns in the image (e.g., object boundaries or appearance similarities) and provides a more economical information propagation route. Furthermore, for each optimization step over Graph LSTM, we propose to use a confidence-driven scheme to update the hidden and memory states of nodes progressively till all nodes are updated. In addition, for each node, the forgets gates are adaptively learned to capture different degrees of semantic correlation with neighboring nodes. Comprehensive evaluations on four diverse semantic object parsing datasets well demonstrate the significant superiority of our Graph LSTM over other state-of-the-art solutions.},
	archivePrefix = {arXiv},
	arxivId = {1603.07063},
	author = {Liang, Xiaodan and Shen, Xiaohui and Feng, Jiashi and Lin, Liang and Yan, Shuicheng},
	doi = {10.1007/978-3-319-46448-0_8},
	eprint = {1603.07063},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2016 - Semantic object parsing with graph LSTM.pdf:pdf},
	isbn = {9783319464473},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Graph LSTM,Object parsing,Recurrent neural networks},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	pages = {125--143},
	pmid = {4520227},
	title = {{Semantic object parsing with graph LSTM}},
	volume = {9905 LNCS},
	year = {2016}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Bull.jar:jar},
	title = {{Bull}}
}
@article{Conlla,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (3).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@misc{Bologna2004,
	abstract = {Although many authors generated comprehensible models from individual networks, much less work has been done in the explanation of ensembles. DIMLP is a special neural network model from which rules are generated at the level of a single network and also at the level of an ensemble of networks. We applied ensembles of 25 DIMLP networks to several datasets of the public domain and a classification problem related to post-translational modifications of proteins. For the classification problems of the public domain, the average predictive accuracy of rulesets extracted from ensembles of neural networks was significantly better than the average predictive accuracy of rulesets generated from ensembles of decision trees. By varying the architectures of DIMLP networks we found that the average predictive accuracy of rules, as well as their complexity were quite stable. The comparison to other rule extraction techniques applied to neural networks showed that rules generated from DIMLP ensembles gave very good results. In the last problem related to bioinformatics, the best result obtained by ensembles of DIMLP networks was also significantly better than the best result obtained by ensembles of decision trees. Thus, although neural networks take much longer to train than decision trees and also rules are generated at a greater computational cost (however, still polynomial), at least for several classification problems it was worth using neural network ensembles, as extracted rules were more accurate, on average. The DIMLP software is available for PC-Linux under http://us.expasy.org/people/Guido.Bologna.html ?? 2004 Elsevier B.V. All rights reserved.},
	author = {Bologna, Guido},
	booktitle = {Journal of Applied Logic},
	doi = {10.1016/j.jal.2004.03.004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna - 2004 - Is it worth generating rules from neural network ensembles.htm:htm},
	isbn = {1570-8683},
	issn = {15708683},
	keywords = {Decision trees,Ensembles,Neural networks,Proteins,Rule extraction},
	number = {3},
	pages = {325--348},
	title = {{Is it worth generating rules from neural network ensembles?}},
	volume = {2},
	year = {2004}
}
@article{Karpathy2015,
	abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	archivePrefix = {arXiv},
	arxivId = {1506.02078},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	doi = {10.1007/978-3-319-10590-1_53},
	eprint = {1506.02078},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
	isbn = {978-3-319-10589-5},
	issn = {978-3-319-10589-5},
	mendeley-groups = {!Paper 3,!Paper 3/Understanding LSTMs},
	pages = {1--12},
	pmid = {26353135},
	title = {{Visualizing and Understanding Recurrent Networks}},
	url = {http://arxiv.org/abs/1506.02078},
	year = {2015}
}
@article{Rosales-p2018,
	author = {Rosales-p, Alejandro},
	doi = {10.1109/MCI.2018.2806997},
	file = {:E$\backslash$:/Downloads/Work/MC2ESVM.pdf:pdf},
	number = {April},
	title = {{MC2ESVM : Multiclass Classification Based on Cooperative Evolution of Support MC 2 ESVM : Multiclass Classification based on Cooperative Evolution of Support Vector Machines}},
	year = {2018}
}
@article{Shrikumar2017,
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
	archivePrefix = {arXiv},
	arxivId = {1704.02685},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	eprint = {1704.02685},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences(2).pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Learning Important Features Through Propagating Activation Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	volume = {1},
	year = {2017}
}
@article{XinYao1999a,
	abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
	archivePrefix = {arXiv},
	arxivId = {1108.1530},
	author = {{Xin Yao}},
	doi = {10.1109/5.784219},
	eprint = {1108.1530},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xin Yao - 1999 - Evolving artificial neural networks.pdf:pdf},
	isbn = {9780470287194},
	issn = {00189219},
	journal = {Proceedings of the IEEE},
	keywords = {evolutionary computation,intelligent systems,neu-},
	number = {9},
	pages = {1423--1447},
	pmid = {9821520},
	title = {{Evolving artificial neural networks}},
	volume = {87},
	year = {1999}
}
@article{Kuchaiev2017,
	abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 23.36.},
	archivePrefix = {arXiv},
	arxivId = {1703.10722},
	author = {Kuchaiev, Oleksii and Ginsburg, Boris},
	doi = {10.1109/CVPR.2016.90},
	eprint = {1703.10722},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuchaiev, Ginsburg - 2017 - Factorization tricks for LSTM networks.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {1664-1078},
	mendeley-groups = {!Paper 3/Language models,!Paper 3/Training LSTMs},
	pages = {1--6},
	pmid = {23554596},
	title = {{Factorization tricks for LSTM networks}},
	url = {http://arxiv.org/abs/1703.10722},
	year = {2017}
}
@article{Huang,
	author = {Huang, Sheng and Peng, Xueping and Niu, Zhendong and Wang, Kunshan},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - News Topic Detection based on Hierarchical Clustering and Named Entity.pdf:pdf},
	isbn = {9781612847290},
	keywords = {-news topic detection,agglomerative hierarchical,clustering,named entity,vector space model},
	mendeley-groups = {Report/Clustering},
	pages = {280--284},
	title = {{News Topic Detection based on Hierarchical Clustering and Named Entity}}
}
@article{Linegang2006,
	abstract = {The US Navy is funding the development of advanced automation systems to plan and execute unmanned vehicles missions, pushing towards a higher level of autonomy for automated planning systems. With effective systems, the human could play a role of mission manager and automation systems could perform mission planning and execution tasks with limited human involvement. Evaluations of the automation systems currently under development are identifying critical conflicts between human operator expectations and automated planning results. This paper presents a model of this human-automation interaction system and summarizes the resulting system design effort. This model provides a theory explaining the source of conflict between human and automation, and predicts that an ecological approach to display design would reduce that conflict. Based on that prediction, the paper describes initial results of an ecological approach to system analysis and design, intended to improve human-automation interaction for these types of advanced automation systems.},
	author = {Linegang, M. P. and Stoner, H. a. and Patterson, M. J. and Seppelt, B. D. and Hoffman, J. D. and Crittendon, Z. B. and Lee, John D.},
	doi = {10.1177/154193120605002304},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Linegang et al. - 2006 - Human-Automation Collaboration in Dynamic Mission Planning A Challenge Requiring an Ecological Approach.pdf:pdf},
	isbn = {10711813 (ISSN); 9780945289296 (ISBN)},
	issn = {1071-1813},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {23},
	pages = {2482--2486},
	title = {{Human-Automation Collaboration in Dynamic Mission Planning: A Challenge Requiring an Ecological Approach}},
	volume = {50},
	year = {2006}
}
@article{Peterson2016,
	abstract = {Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reach-ing or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representa-tions learned by these networks and human psychological rep-resentations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these fea-tures do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human sim-ilarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological exper-iments.},
	archivePrefix = {arXiv},
	arxivId = {1608.02164},
	author = {Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L},
	eprint = {1608.02164},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peterson, Abbott, Griffiths - 2016 - Adapting Deep Network Features to Capture Psychological Representations.pdf:pdf},
	keywords = {deep learning,neural networks,psychological,representations,similarity},
	mendeley-groups = {Report/Features,Progress Report},
	pages = {2363--2368},
	title = {{Adapting Deep Network Features to Capture Psychological Representations}},
	year = {2016}
}
@article{Sculley,
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Dennison, Dan},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/5656-hidden-technical-debt-in-machine-learning-systems.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Technical Debt},
	pages = {1--9},
	title = {{Hidden Technical Debt in Machine Learning Systems}}
}
@article{Hinton2015a,
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	archivePrefix = {arXiv},
	arxivId = {1503.02531},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	eprint = {1503.02531},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network.pdf:pdf},
	journal = {NIPS 2014 Deep Learning Workshop},
	mendeley-groups = {Literature Review,Interim Review},
	pages = {1--9},
	title = {{Distilling the Knowledge in a Neural Network}},
	url = {http://arxiv.org/abs/1503.02531},
	year = {2015}
}
@article{Evans2017,
	abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework ({\$}\backslashpartial{\$}ILP), which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
	archivePrefix = {arXiv},
	arxivId = {1711.04574},
	author = {Evans, Richard and Grefenstette, Edward},
	eprint = {1711.04574},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans, Grefenstette - 2017 - Learning Explanatory Rules from Noisy Data.pdf:pdf},
	mendeley-groups = {!Paper 3},
	pages = {1--64},
	title = {{Learning Explanatory Rules from Noisy Data}},
	url = {http://arxiv.org/abs/1711.04574},
	volume = {61},
	year = {2017}
}
@article{Ai2016,
	author = {Ai, Qingyao and Yang, Liu and Guo, Jiafeng and Croft, W Bruce},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ai et al. - 2016 - Analysis of the Paragraph Vector Model for Information Retrieval.pdf:pdf},
	isbn = {9781450344975},
	keywords = {language model,paragraph vector},
	mendeley-groups = {Annotated/Document representation},
	title = {{Analysis of the Paragraph Vector Model for Information Retrieval}},
	year = {2016}
}
@article{Craven1993,
	author = {{M.W. Craven}, J.W. Shavlik},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/M.W. Craven - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
	mendeley-groups = {Papers/Paper 1,Report},
	number = {4},
	pages = {434--441},
	title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
	volume = {41},
	year = {1993}
}
@article{Kovashka,
abstract = {We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query “black shoes”, the user might state, “Show me shoe images like these, but sportier.” Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently “whittle away” irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient—both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.},
archivePrefix = {arXiv},
arxivId = {1505.04141},
author = {Kovashka, Adriana and Parikh, Devi and Grauman, Kristen},
doi = {10.1007/s11263-015-0814-0},
eprint = {1505.04141},
file = {:E$\backslash$:/WhittleSearch{\_}Image{\_}search{\_}with{\_}relative.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Active selection,Content-based image search,Interactive image search,Relative attributes},
mendeley-groups = {11Thesis/Applications},
number = {2},
pages = {185--210},
title = {{WhittleSearch: Interactive Image Search with Relative Attribute Feedback}},
volume = {115},
year = {2015}
}
@article{Rankings,
	author = {Rankings, Phrase Direction and Score, Ndcg and Rankings, Cluster and Tree, Decision and Validation, Using Cross},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rankings et al. - Unknown - Cross - validation Process.pdf:pdf},
	pages = {7--8},
	title = {{Cross - validation Process}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCF06092019.pdf:pdf},
	title = {{CCF06092019 (1).pdf}}
}
@misc{Hruschka2006a,
	abstract = {Multilayer perceptrons adjust their internal parameters performing vector mappings from the input to the output space. Although they may achieve high classification accuracy, the knowledge acquired by such neural networks is usually incomprehensible for humans. This fact is a major obstacle in data mining applications, in which ultimately understandable patterns (like classification rules) are very important. Therefore, many algorithms for rule extraction from neural networks have been developed. This work presents a method to extract rules from multilayer perceptrons trained in classification problems. The rule extraction algorithm basically consists of two steps. First, a clustering genetic algorithm is applied to find clusters of hidden unit activation values. Then, classification rules describing these clusters, in relation to the inputs, are generated. The proposed approach is experimentally evaluated in four datasets that are benchmarks for data mining applications and in a real-world meteorological dataset, leading to interesting results. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	author = {Hruschka, Eduardo R. and Ebecken, Nelson F F},
	booktitle = {Neurocomputing},
	doi = {10.1016/j.neucom.2005.12.127},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hruschka, Ebecken - 2006 - Extracting rules from multilayer perceptrons in classification problems A clustering-based approach.htm:htm},
	issn = {09252312},
	keywords = {Clustering,Genetic algorithms,Rule extraction from neural networks},
	number = {1-3},
	pages = {384--397},
	title = {{Extracting rules from multilayer perceptrons in classification problems: A clustering-based approach}},
	volume = {70},
	year = {2006}
}
@article{Nguyenn,
	author = {Nguyenn, Viet},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 10 - The Ruler of Conspiracy.pdf:pdf},
	title = {{The Ruler of Conspiracy Translation : Nigel}}
}
@article{Blei2006,
	abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:0712.1486v1},
	author = {Blei, David M. and Lafferty, John D.},
	doi = {10.1145/1143844.1143859},
	eprint = {arXiv:0712.1486v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, Lafferty - 2006 - Correlated Topic Models.pdf:pdf},
	isbn = {1595933832},
	issn = {19326157},
	journal = {Advances in Neural Information Processing Systems 18},
	mendeley-groups = {Annotated/Topic models},
	pages = {147--154},
	pmid = {9013932},
	title = {{Correlated Topic Models}},
	url = {papers2://publication/uuid/1191CDB8-6BB3-4201-8EFB-6F7B8CBA0E8F},
	year = {2006}
}
@article{Dong2017,
	abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
	archivePrefix = {arXiv},
	arxivId = {1703.04096},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
	doi = {10.1109/CVPR.2017.110},
	eprint = {1703.04096},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2017 - Improving Interpretability of Deep Neural Networks with Semantic Information(2).pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {1063-6919},
	mendeley-groups = {!Paper 3/Creating Interpretable LSTMs},
	pages = {4306--4314},
	title = {{Improving Interpretability of Deep Neural Networks with Semantic Information}},
	url = {http://arxiv.org/abs/1703.04096},
	year = {2017}
}
@article{Turney2010,
	abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
	archivePrefix = {arXiv},
	arxivId = {1003.1141},
	author = {Turney, Peter D. and Pantel, Patrick},
	doi = {10.1613/jair.2934},
	eprint = {1003.1141},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turney, Pantel - 2010 - From frequency to meaning Vector space models of semantics.pdf:pdf},
	isbn = {1076-9757},
	issn = {10769757},
	journal = {Journal of Artificial Intelligence Research},
	mendeley-groups = {Report/Explaining predictions,Report/Clustering/properties},
	pages = {141--188},
	title = {{From frequency to meaning: Vector space models of semantics}},
	volume = {37},
	year = {2010}
}
@article{Cheng2016,
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	archivePrefix = {arXiv},
	arxivId = {1601.06733},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	eprint = {1601.06733},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Dong, Lapata - 2016 - Long Short-Term Memory-Networks for Machine Reading.pdf:pdf},
	title = {{Long Short-Term Memory-Networks for Machine Reading}},
	url = {http://arxiv.org/abs/1601.06733},
	year = {2016}
}
@article{Bloehdorn2006,
	abstract = {Recent work has shown improvements in text clustering and classification tasks by integrating conceptual features extracted from ontologies. In this paper we present text mining experiments in the medical domain in which the ontological structures used are acquired automatically in an unsupervised learning process from the text corpus in question. We compare results obtained using the automatically learned ontologies with those obtained using manually engineered ones. Our results show that both types of ontologies improve results on text clustering and classification tasks, whereby the automatically acquired ontologies yield a improvement competitive with the manually engineered ones.$\backslash$nER -},
	author = {Bloehdorn, S and Cimiano, P and Hotho, A},
	doi = {10.1007/3-540-31314-1_40},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bloehdorn, Cimiano, Hotho - 2006 - Learning Ontologies to Improve Text Clustering and Classification.pdf:pdf},
	isbn = {9783540313137},
	journal = {From Data and Information Analysis to Knowledge Engineering},
	number = {2005},
	pages = {334--341},
	title = {{Learning Ontologies to Improve Text Clustering and Classification}},
	year = {2006}
}
@article{Thrun1995,
	abstract = {Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neural networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illustrate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.},
	author = {Thrun, S.},
	doi = {10.1007/BFb0100473},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun - 1995 - Extracting rules from artificial neural networks with distributed representations.pdf:pdf},
	isbn = {1049-5258},
	issn = {16113349},
	journal = {Advances in Neural Information Processing Systems},
	pages = {505--512},
	title = {{Extracting rules from artificial neural networks with distributed representations}},
	year = {1995}
}
@article{Conlld,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (5).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Madotto2018,
	author = {Madotto, Andrea},
	file = {:E$\backslash$:/PhD/Papedrs/Interpreting Word Embeddings with Eigenvector Analysis.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Word Vectors,11Thesis/Interpretability/Word Vectors/Constraints,11Thesis/Interpretability/GAN's and VAE},
	number = {Nips},
	title = {{Interpreting Word Embeddings with Eigenvector Analysis}},
	year = {2018}
}
@article{Vincent2010,
	abstract = {This paper presents the findings from a small-scale experiment investigating the presentation of a synchronous remote electronic examination. It discusses the students' experiences of taking such an examination. The study confirms that the majority of participants found the experience at least as good as a conventional written examination. In addition, typing answers does not prevent students from producing answers in the time available. However, the pressure of time continues to be a major cause of anxiety for students. The paper discusses technical issues, particularly those related to the loss of communications during the 3-hour duration of the exam. Although software processes were available to save and restore students' answers throughout the examination, problems still occurred and more robust software is required.},
	author = {Thomas, Pete and Price, Blaine and Paine, Carina and Richards, Michael},
	doi = {10.1111/1467-8535.00290},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas et al. - 2002 - Remote electronic examinations student experiences.pdf:pdf},
	isbn = {0007-1013},
	issn = {0007-1013},
	journal = {Journal of Machine Learning Research},
	number = {3},
	pages = {3371--3408},
	title = {{Remote electronic examinations: student experiences}},
	url = {http://oro.open.ac.uk/2572/},
	volume = {11},
	year = {2002}
}
@article{Ribeiro,
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, " sufficient " conditions for predic-tions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by ex-plaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - Unknown - Anchors High-Precision Model-Agnostic Explanations.pdf:pdf},
	mendeley-groups = {!Paper 3},
	title = {{Anchors: High-Precision Model-Agnostic Explanations}}
}
@article{Schaul2010,
	author = {Schaul, Tom and Bayer, Justin and Wierstra, Daan and Sun, Yi},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaul et al. - 2010 - PyBrain.pdf:pdf},
	issn = {1532-4435},
	journal = {The Journal of Machine {\ldots}},
	keywords = {neural networks,optimization,python,reinforcement learning},
	pages = {743--746},
	title = {{PyBrain}},
	url = {http://dl.acm.org/citation.cfm?id=1756030},
	volume = {11},
	year = {2010}
}
@article{Dou2013,
	abstract = {Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system--HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.},
	author = {Dou, Wenwen and Yu, Li and Wang, Xiaoyu and Ma, Zhiqiang and Ribarsky, William},
	doi = {10.1109/TVCG.2013.162},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dou et al. - 2013 - HierarchicalTopics Visually exploring large text collections using topic hierarchies.pdf:pdf},
	isbn = {1077-2626},
	issn = {10772626},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Hierarchical topic representation,rose tree,topic modeling,visual analytics},
	mendeley-groups = {Annotated/Explanations},
	number = {12},
	pages = {2002--2011},
	pmid = {24051766},
	title = {{HierarchicalTopics: Visually exploring large text collections using topic hierarchies}},
	volume = {19},
	year = {2013}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/SCIP App Paper.pdf:pdf},
	pages = {1--27},
	title = {{No Title}}
}
@article{Erhan2014,
	abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
	archivePrefix = {arXiv},
	arxivId = {1312.2249},
	author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
	doi = {10.1109/CVPR.2014.276},
	eprint = {1312.2249},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan et al. - 2014 - Scalable Object Detection Using Deep Neural Networks.pdf:pdf},
	isbn = {978-1-4799-5118-5},
	issn = {10636919},
	journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
	mendeley-groups = {Progress Report},
	pages = {2155--2162},
	title = {{Scalable Object Detection Using Deep Neural Networks}},
	url = {http://arxiv.org/abs/1312.2249{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909673},
	year = {2014}
}
@article{Garcez2014,
	abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
	author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
	doi = {10.13140/2.1.1779.4243},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez et al. - 2014 - Neural-Symbolic Learning and Reasoning Contributions and Challenges.pdf:pdf},
	journal = {Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, Stanford},
	number = {November},
	pages = {18--21},
	title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
	year = {2014}
}
@article{Lipton2016,
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	archivePrefix = {arXiv},
	arxivId = {1606.03490},
	author = {Lipton, Zachary C.},
	eprint = {1606.03490},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
	mendeley-groups = {Annotated/Overarching Interpretability,Report/Explaining predictions,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {Whi},
	title = {{The Mythos of Model Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	year = {2016}
}
@article{Setiono2000,
	abstract = {An effective algorithm for extracting M-of-N rules from trained feedforward neural networks is proposed. Two components of the algorithm distinguish our method from previously proposed algorithms which extract symbolic rules from neural networks. First, we train a network where each input of the data can only have one of the two possible values, -1 or one. Second, we apply the hyperbolic tangent function to each connection from the input layer to the hidden layer of the network. By applying this squashing function, the activation values at the hidden units are effectively computed as the hyperbolic tangent (or the sigmoid) of the weighted inputs, where the weights have magnitudes that are equal one. By restricting the inputs and the weights to binary values either -1 or one, the extraction of M-of-N rules from the networks becomes trivial. We demonstrate the effectiveness of the proposed algorithm on several widely tested datasets. For datasets consisting of thousands of patterns with many attributes, the rules extracted by the algorithm are surprisingly simple and accurate.},
	author = {Setiono, Rudy},
	doi = {10.1109/72.839020},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono - 2000 - Extracting M-of-N rules from trained neural networks.pdf:pdf},
	isbn = {1045-9227},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	number = {2},
	pages = {512--519},
	pmid = {18249780},
	title = {{Extracting M-of-N rules from trained neural networks}},
	volume = {11},
	year = {2000}
}
@article{Dai,
	abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
	archivePrefix = {arXiv},
	arxivId = {1507.07998},
	author = {Dai, Andrew M and Olah, Christopher and Le, Quoc V.},
	eprint = {1507.07998},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Olah - Unknown - Document Embedding with Paragraph Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	pages = {1--8},
	title = {{Document Embedding with Paragraph Vectors}},
	url = {http://arxiv.org/abs/1507.07998},
	year = {2015}
}
@article{Kheder2014,
	author = {Kheder, Waad Ben and Matrouf, Driss and Bousquet, Pierre-Michel and Bonastre, Jean-Fran{\c{c}}ois and Ajili, Moez},
	doi = {10.1007/978-3-319-11397-5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kheder et al. - 2014 - Statistical Language and Speech Processing.pdf:pdf},
	isbn = {978-3-319-11396-8},
	journal = {Statistical Language and Speech Processing},
	keywords = {i-vectors},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {97--107},
	title = {{Statistical Language and Speech Processing}},
	url = {http://link.springer.com/10.1007/978-3-319-11397-5{\%}5Cnhttp://link.springer.com/content/pdf/10.1007/978-3-319-11397-5.pdf},
	volume = {8791},
	year = {2014}
}
@article{Cooijmans2016,
	abstract = {We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.},
	archivePrefix = {arXiv},
	arxivId = {1603.09025},
	author = {Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'{e}}sar and G{\"{u}}l{\c{c}}ehre, {\c{C}}ağlar and Courville, Aaron},
	doi = {10.1227/01.NEU.0000210260.55124.A4},
	eprint = {1603.09025},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cooijmans et al. - 2016 - Recurrent Batch Normalization.pdf:pdf},
	isbn = {9783319464657},
	issn = {16113349},
	mendeley-groups = {!Paper 3/Training LSTMs},
	number = {Section 3},
	pages = {1--13},
	pmid = {26774160},
	title = {{Recurrent Batch Normalization}},
	url = {http://arxiv.org/abs/1603.09025},
	year = {2016}
}
@article{Xu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1802.01345v3},
	author = {Xu, Jingjing and Ren, Xuancheng and Lin, Junyang and Sun, Xu},
	eprint = {arXiv:1802.01345v3},
	file = {:E$\backslash$:/PhD/Papedrs/1802.01345.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation}},
	year = {2017}
}
@article{Miyato2016,
	abstract = {Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.},
	archivePrefix = {arXiv},
	arxivId = {1605.07725},
	author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
	doi = {10.2507/daaam.scibook.2010.27},
	eprint = {1605.07725},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyato, Dai, Goodfellow - 2016 - Adversarial Training Methods for Semi-Supervised Text Classification.pdf:pdf},
	isbn = {2840601737},
	issn = {18766102},
	mendeley-groups = {!Paper 3/task/Large Movie Review},
	pages = {1--11},
	pmid = {19963286},
	title = {{Adversarial Training Methods for Semi-Supervised Text Classification}},
	url = {http://arxiv.org/abs/1605.07725},
	year = {2016}
}
@article{Declerck2000,
	author = {Declerck, Carolyn H and Boone, Christophe and Emonds, Griet},
	file = {:E$\backslash$:/Downloads/Work/0c0bc5c5.pdf:pdf},
	number = {0},
	title = {{When do people cooperate ? The neuroeconomics of prosocial decision making Faculty of Applied Economics}},
	volume = {32},
	year = {2000}
}
@article{Lastname2011d,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (3) workthru.pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Fyshe,
	author = {Fyshe, Alona and Wehbe, Leila and Talukdar, Partha and Murphy, Brian and Mitchell, Tom},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyshe et al. - Unknown - A Compositional and Interpretable Semantic Space.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations,Annotated/Word Vectors},
	title = {{A Compositional and Interpretable Semantic Space}}
}
@article{Korde2012,
	abstract = {As most information (over 80{\%}) is stored as text, text mining is believed to have a high commercial potential value. knowledge may be discovered from many sources of information; yet, unstructured texts remain the largest readily available source of knowledge .Text classification which classifies the documents according to predefined categories .In this paper we are tried to give the introduction of text classification, process of text classification as well as the overview of the classifiers and tried to compare the some existing classifier on basis of few criteria like time complexity, principal and performance.},
	author = {Korde, Vandana and Mahender, C Namrata},
	doi = {10.5121/ijaia.2012.3208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korde, Mahender - 2012 - Text Classification and Classifiers A Survey.pdf:pdf},
	issn = {09762191},
	journal = {International Journal of Artificial Intelligence {\&} Applications},
	keywords = {classifiers,text classification,text representation},
	mendeley-groups = {Annotated/Decision Trees},
	number = {2},
	pages = {85--99},
	title = {{Text Classification and Classifiers: A Survey}},
	url = {http://www.airccse.org/journal/ijaia/papers/3212ijaia08.pdf},
	volume = {3},
	year = {2012}
}
@article{,
	file = {:E$\backslash$:/PhD/Papedrs/nguyen{\_}umd{\_}0117E{\_}20313.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	title = {{Title of dissertation :}}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Multivariate Analysis of Ecological Data using CANOCO.pdf:pdf},
	title = {{Multivariate Analysis of Ecological Data using CANOCO.pdf}}
}
@article{Fischer2012a,
	abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1311.0966v3},
	author = {Fischer, Asja and Igel, Christian},
	doi = {10.1007/978-3-642-33275-3_2},
	eprint = {arXiv:1311.0966v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer, Igel - 2012 - An Introduction to Restricted Boltzmann Machines.pdf:pdf},
	isbn = {978-3-642-33274-6},
	issn = {1875-7855},
	journal = {Lecture Notes in Computer Science: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
	pages = {14--36},
	pmid = {24309266},
	title = {{An Introduction to Restricted Boltzmann Machines}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33275-3{\_}2},
	volume = {7441},
	year = {2012}
}
@article{Zaidan2007,
	abstract = {We propose a new framework for supervised machine learning. Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with “rationales.” When annotating an example, the human teacher will also highlight evidence supporting this annotation—thereby teaching the machine learner why the example belongs to the category. We provide some rationale-annotated data and present a learning method that exploits the rationales during training to boost performance signi?cantly on a sample task, namely sentiment classi?cation of movie reviews. We hypothesize that in some situations, providing rationales is a more fruitful use of an annotator's time than annotating more examples.},
	author = {Zaidan, O. and Zaidan, O. and Eisner, J. and Eisner, J. and Piatko, C. and Piatko, C.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaidan et al. - 2007 - Using “annotator rationales” to improve machine learning for text categorization.pdf:pdf},
	isbn = {9781932432657},
	journal = {Proceedings of NAACL-HLT},
	mendeley-groups = {Report/Explaining predictions,Annotated},
	number = {April},
	pages = {260--267},
	title = {{Using “annotator rationales” to improve machine learning for text categorization}},
	url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Using+Annotator+Rationales+to+Improve+Machine+Learning+for+Text+Categorization{\#}0},
	volume = {260},
	year = {2007}
}
@article{Chen,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1301.3618v2},
	author = {Chen, Danqi and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
	eprint = {arXiv:1301.3618v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - Neural Tensor Networks and Semantic Word Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	pages = {1--4},
	title = {{Neural Tensor Networks and Semantic Word Vectors}}
}
@article{Garcez2003a,
	author = {d'Avila Garcez, Artur S and Lamb, Luis C},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez, Lamb - 2003 - Reasoning about Time and Knowledge in neural-symbolic learning systems.pdf:pdf},
	isbn = {0262201526},
	issn = {10495258},
	journal = {Advances in neural information processing systems},
	title = {{Reasoning about Time and Knowledge in neural-symbolic learning systems}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2003{\_}CS03.pdf},
	year = {2003}
}
@article{Donoho2004,
	abstract = {We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that un- der certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sam- pling. For such databases there is a generative model in terms of “parts” and NMF correctly identifies the ”parts”. We show that our theoretical results are predictive of the performance of publishedNMFcode, by run- ning the published algorithms on one of our synthetic image articulation databases.},
	archivePrefix = {arXiv},
	arxivId = {1512.00567},
	author = {Donoho, Dl and Stodden, Vc},
	doi = {10.1.1.85.8157},
	eprint = {1512.00567},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho, Stodden - 2004 - When does non-negative matrix factorization give a correct decomposition into parts.pdf:pdf},
	isbn = {9780262201520},
	issn = {09501991},
	journal = {Proc. Advances in Neural Information Processing Systems 16},
	mendeley-groups = {Annotated/NMF},
	pages = {1141--1148},
	pmid = {11585793},
	title = {{When does non-negative matrix factorization give a correct decomposition into parts?}},
	url = {http://academiccommons.columbia.edu/catalog/ac:140175},
	year = {2004}
}
@article{Lakkaraju2017,
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	archivePrefix = {arXiv},
	arxivId = {1707.01154},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	eprint = {1707.01154},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju et al. - 2017 - Interpretable {\&} Explorable Approximations of Black Box Models.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	title = {{Interpretable {\&} Explorable Approximations of Black Box Models}},
	url = {http://arxiv.org/abs/1707.01154},
	year = {2017}
}
@article{Wisdom2016,
	abstract = {Recurrent neural networks (RNNs) are powerful and effective for processing sequential data. However, RNNs are usually considered "black box" models whose internal structure and learned parameters are not interpretable. In this paper, we propose an interpretable RNN based on the sequential iterative soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery problem, which models a sequence of correlated observations with a sequence of sparse latent vectors. The architecture of the resulting SISTA-RNN is implicitly defined by the computational structure of SISTA, which results in a novel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are perfectly interpretable as the parameters of a principled statistical model, which in this case include a sparsifying dictionary, iterative step size, and regularization parameters. In addition, on a particular sequential compressive sensing task, the SISTA-RNN trains faster and achieves better performance than conventional state-of-the-art black box RNNs, including long-short term memory (LSTM) RNNs.},
	archivePrefix = {arXiv},
	arxivId = {1611.07252},
	author = {Wisdom, Scott and Powers, Thomas and Pitton, James and Atlas, Les},
	eprint = {1611.07252},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wisdom et al. - 2016 - Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery.pdf:pdf},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	number = {Nips},
	pages = {1--8},
	title = {{Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery}},
	url = {http://arxiv.org/abs/1611.07252},
	year = {2016}
}
@article{Naacl2019,
	author = {Naacl, Anonymous},
	file = {:D$\backslash$:/Downloads/Work/naaclhlt2019{\_}latex{\_}{\_}Copy{\_}.pdf:pdf},
	pages = {1--9},
	title = {{No Title}},
	year = {2019}
}
@article{Corcoran2018,
	author = {Corcoran, Podraig},
	file = {:E$\backslash$:/Downloads/Work/AnnualProgressReview (2) (1).pdf:pdf},
	number = {October},
	title = {{Part 1 : PhD progress review}},
	year = {2018}
}
@article{Chen2016,
	abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	archivePrefix = {arXiv},
	arxivId = {1606.03657},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	eprint = {1606.03657},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
	issn = {10495258},
	mendeley-groups = {Annotated/Generative Adversarial Nets,Report,11Thesis,11Thesis/Interpretability/GAN's and VAE},
	title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	url = {http://arxiv.org/abs/1606.03657},
	year = {2016}
}
@article{Li2017,
	abstract = {An important goal in behaviour analytics is to connect disease state or genome variation with observable differences in behaviour. Despite advances in sensor technology and imaging, informative behaviour quantification remains challenging. The nematode worm C. elegans provides a unique opportunity to test analysis approaches because of its small size, compact nervous system, and the availability of large databases of videos of freely behaving animals with known genetic differences. Despite its relative simplicity, there are still no reports of generative models that can capture essential differences between even well-described mutant strains. Here we show that a multilayer recurrent neural network (RNN) can produce diverse behaviours that are difficult to distinguish from real worms' behaviour and that some of the artificial neurons in the RNN are interpretable and correlate with observable features such as body curvature, speed, and reversals. Although the RNN is not trained to perform classification, we find that artificial neuron responses provide features that perform well in worm strain classification.},
	author = {Li, Kezhi and Javer, Avelino and Keaveny, Eric E. and Brown, Andre E.X. and Buchanan, E Kelly and Linderman, Scott and Paninski, Liam},
	doi = {10.1101/222208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Recurrent Neural Networks with Interpretable Cells Predict and Classify Worm Behaviour.pdf:pdf},
	journal = {Doi.Org},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	number = {Nips},
	pages = {222208},
	title = {{Recurrent Neural Networks with Interpretable Cells Predict and Classify Worm Behaviour}},
	url = {https://www.biorxiv.org/content/early/2017/11/20/222208},
	year = {2017}
}
@article{Zhang2016,
	abstract = {We present a novel subset scan method to detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup, and identify the characteristics of this subgroup. This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias or regions of poor classifier fit. This allows consideration of not just subgroups of a priori interest or small dimensions, but the space of all possible subgroups of features. To address the difficulty of considering these exponentially many possible subgroups, we use subset scan and parametric bootstrap-based methods. Extending this method, we can penalize the complexity of the detected subgroup and also identify subgroups with high classification errors. We demonstrate these methods and find interesting results on the COMPAS crime recidivism and credit delinquency data.},
	archivePrefix = {arXiv},
	arxivId = {1611.08292},
	author = {Zhang, Zhe and Neill, Daniel B.},
	eprint = {1611.08292},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Neill - 2016 - Identifying Significant Predictive Bias in Classifiers.pdf:pdf},
	mendeley-groups = {Annotated/Fairness},
	number = {June},
	pages = {1--5},
	title = {{Identifying Significant Predictive Bias in Classifiers}},
	url = {http://arxiv.org/abs/1611.08292},
	year = {2016}
}
@article{Chih-WeiHsuChih-ChungChang2008,
	abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
	author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
	journal = {BJU international},
	number = {1},
	pages = {1396--400},
	title = {{A Practical Guide to Support Vector Classification}},
	url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
	volume = {101},
	year = {2008}
}
@article{Alvarez-melis2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.07538v2},
	author = {Alvarez-melis, David and Jaakkola, Tommi S},
	eprint = {arXiv:1806.07538v2},
	file = {:E$\backslash$:/PhD/Papedrs/New folder/1806.07538.pdf:pdf},
	number = {NeurIPS},
	title = {{Towards Robust Interpretability with Self-Explaining Neural Networks}},
	year = {2018}
}
@article{Sourek2015a,
	abstract = {We propose a method combining relational-logic representations with deep neural network learning. Domain-specific knowledge is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structure of given training or testing examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descend algorithm. Notable relational concepts can be discovered by interpreting shared hidden layer weights corresponding to the rules. Experiments on 78 relational learning benchmarks demonstrate the favorable performance of the method.},
	archivePrefix = {arXiv},
	arxivId = {1508.05128},
	author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
	eprint = {1508.05128},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sourek et al. - 2015 - Lifted Relational Neural Networks.pdf:pdf},
	journal = {CoRR},
	keywords = {lifted models,neural networks,relational learning},
	pages = {1--21},
	title = {{Lifted Relational Neural Networks}},
	url = {http://arxiv.org/abs/1508.05128},
	volume = {abs/1508.0},
	year = {2015}
}
@article{Kim2018,
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/kim18d.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Uncategorized},
	title = {{Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCAV )}},
	year = {2018}
}
@article{Model2018,
	author = {Model, Autoregressive Exogenous and Lu, Yao},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Model, Lu - 2018 - INTERPRETABLE LSTM NEURAL NETWORK FOR.pdf:pdf},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	pages = {1--7},
	title = {{INTERPRETABLE LSTM NEURAL NETWORK FOR}},
	year = {2018}
}
@article{Chih-WeiHsuChih-ChungChang2008,
	abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
	doi = {10.1177/02632760022050997},
	eprint = {0-387-31073-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chih-Wei Hsu, Chih-Chung Chang - 2008 - A Practical Guide to Support Vector Classification.pdf:pdf},
	isbn = {013805326X},
	issn = {1464-410X},
	journal = {BJU international},
	number = {1},
	pages = {1396--400},
	pmid = {18190633},
	title = {{A Practical Guide to Support Vector Classification}},
	url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
	volume = {101},
	year = {2008}
}
@article{Zhang2010a,
	abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
	author = {Zhang, Min-Ling and Zhang, Kun},
	doi = {10.1145/1835804.1835930},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
	isbn = {9781450300551},
	issn = {9781577355687},
	journal = {Kdd},
	mendeley-groups = {Progress Report},
	pages = {999--1007},
	title = {{Multi-label learning by exploiting label dependency}},
	url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
	year = {2010}
}
@article{Kenton1953,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{Miller2017,
	abstract = {In his seminal book The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity [2004, Sams Indianapolis, IN, USA], Alan Cooper ar-gues that a major reason why software is of-ten poorly designed (from a user perspective) is that programmers are in charge of design de-cisions, rather than interaction designers. As a result, programmers design software for them-selves, rather than for their target audience; a phenomenon he refers to as the 'inmates run-ning the asylum'. This paper argues that ex-plainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But ex-plainable AI is more likely to succeed if re-searchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science; and if evalu-ation of these models is focused more on people than on technology. From a light scan of litera-ture, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
	author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller, Howe, Sonenberg - 2017 - Explainable AI Beware of Inmates Running the Asylum.pdf:pdf},
	journal = {IJCAI - Workshop on Explainable AI},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability},
	title = {{Explainable AI: Beware of Inmates Running the Asylum}},
	year = {2017}
}
@article{Setiono1997b,
	abstract = {We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.},
	author = {Setiono, Rudy and Liu, Huan},
	doi = {10.1016/S0925-2312(97)00038-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Liu - 1997 - Neurolinear From neural networks to oblique decision rules(2).pdf:pdf},
	isbn = {3-540-62858-4},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Discretization,Oblique-rule,Pruning,Rule extraction},
	number = {1},
	pages = {1--24},
	title = {{Neurolinear: From neural networks to oblique decision rules}},
	volume = {17},
	year = {1997}
}
@article{Melamud2016,
	abstract = {We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.},
	archivePrefix = {arXiv},
	arxivId = {1601.00893},
	author = {Melamud, Oren and McClosky, David and Patwardhan, Siddharth and Bansal, Mohit},
	eprint = {1601.00893},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melamud et al. - 2016 - The Role of Context Types and Dimensionality in Learning Word Embeddings.pdf:pdf},
	isbn = {9781941643914},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Sentiment treebank},
	title = {{The Role of Context Types and Dimensionality in Learning Word Embeddings}},
	url = {http://arxiv.org/abs/1601.00893},
	year = {2016}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/CCE04062018.pdf:pdf},
	title = {{CCE04062018.pdf}}
}
@article{Volumed,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 5 - Men in The Kingdom [Part 01].pdf:pdf},
	title = {{No Title}}
}
@article{Narayanan2018,
	abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
	archivePrefix = {arXiv},
	arxivId = {1802.00682},
	author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
	eprint = {1802.00682},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine Learning Systems An Evaluation of the Human-Interpretabilit.pdf:pdf},
	pages = {1--21},
	title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
	url = {http://arxiv.org/abs/1802.00682},
	year = {2018}
}
@article{Vandewiele2016,
	abstract = {Models obtained by decision tree induction techniques excel in being interpretable.However, they can be prone to overfitting, which results in a low predictive performance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial. To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques.},
	archivePrefix = {arXiv},
	arxivId = {1611.05722},
	author = {Vandewiele, Gilles and Janssens, Olivier and Ongenae, Femke and {De Turck}, Filip and {Van Hoecke}, Sofie},
	eprint = {1611.05722},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vandewiele et al. - 2016 - GENESIM genetic extraction of a single, interpretable model.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees,Report},
	number = {Nips},
	title = {{GENESIM: genetic extraction of a single, interpretable model}},
	url = {http://arxiv.org/abs/1611.05722},
	year = {2016}
}
@article{Xie2013,
	abstract = {Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets demonstrate the effectiveness of our model.},
	archivePrefix = {arXiv},
	arxivId = {1309.6874},
	author = {Xie, Pengtao and Xing, Eric P},
	eprint = {1309.6874},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Xing - 2013 - Integrating Document Clustering and Topic Modeling.pdf:pdf},
	journal = {Proceedings of the 29th conference on uncertainty in artificial intelligence},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
	pages = {694--703},
	title = {{Integrating Document Clustering and Topic Modeling}},
	url = {http://www.cs.cmu.edu/{~}pengtaox/papers/uai2013paper.pdf},
	year = {2013}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 13 - The Paladin of the Holy Kingdom [Part 02] (v2.1).pdf:pdf},
	title = {{Overlord Volume 13 - The Paladin of the Holy Kingdom [Part 02] (v2.1).pdf}}
}
@article{Malioutov2013,
	abstract = {We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean 'sensing' matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting. We show competitive classification accuracy using the proposed approach. Copyright 2013 by the author(s).},
	author = {Malioutov, D M and Varshney, K R},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malioutov, Varshney - 2013 - Exact rule learning via Boolean compressed sensing.pdf:pdf},
	journal = {30th International Conference on Machine Learning, ICML 2013},
	keywords = {Classification accuracy; Exact recoveries; Group,Learning systems; Signal reconstruction,Recovery},
	mendeley-groups = {Annotated/Interpretable Classifiers},
	number = {PART 3},
	pages = {1802--1810},
	title = {{Exact rule learning via Boolean compressed sensing}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84897531301{\&}partnerID=40{\&}md5=896c81fac3b62d2c5b6e9f3cf3a5a96d},
	year = {2013}
}
@article{Ustun2014,
	abstract = {We present an integer programming framework to build accurate and interpretable discrete linear classification models. Unlike existing approaches, our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice. To this end, our framework can produce models that are fully optimized for accuracy, by minimizing the 0--1 classification loss, and that address multiple aspects of interpretability, by incorporating a range of discrete constraints and penalty functions. We use our framework to produce models that are difficult to create with existing methods, such as scoring systems and M-of-N rule tables. In addition, we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction. We show that discrete linear classifiers can attain the training accuracy of any other linear classifier, and provide an Occam's Razor type argument as to why the use of small discrete coefficients can provide better generalization. We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis.},
	archivePrefix = {arXiv},
	arxivId = {1405.4047},
	author = {Ustun, Berk and Rudin, Cynthia},
	eprint = {1405.4047},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ustun, Rudin - 2014 - Methods and Models for Interpretable Linear Classification.pdf:pdf},
	journal = {arXiv},
	mendeley-groups = {Annotated/Interpretable Classifiers,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	pages = {1--57},
	title = {{Methods and Models for Interpretable Linear Classification}},
	url = {http://arxiv.org/abs/1405.4047},
	year = {2014}
}

@article{Conll,
	author = {Conll, Anonymous},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Martins2013,
	abstract = {We present fast, accurate, direct non-projective dependency parsers with third-order features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-of- the-art accuracies for the largest datasets (English, Czech, and German).},
	author = {Martins, Andre and Almeida, Miguel and Smith, Noah A},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martins, Almeida, Smith - 2013 - Turning on the Turbo Fast Third-Order Non-Projective Turbo Parsers.pdf:pdf},
	isbn = {9781937284510},
	journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	pages = {617--622},
	title = {{Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers}},
	url = {http://www.aclweb.org/anthology/P13-2109{\%}5Cnhttp://www.cs.cmu.edu/{~}nasmith/papers/martins+almeida+smith.acl13.pdf},
	year = {2013}
}
@article{Senin2013,
	author = {Senin, Pavel and Malinchik, Sergey},
	doi = {10.1109/ICDM.2013.52},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Senin, Malinchik - 2013 - SAX-VSM Interpretable time series classification using sax and vector space model.pdf:pdf},
	isbn = {978-0-7695-5108-1},
	issn = {15504786},
	journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
	keywords = {classification algorithms,time series analysis},
	mendeley-groups = {Report},
	number = {0704},
	pages = {1175--1180},
	title = {{SAX-VSM: Interpretable time series classification using sax and vector space model}},
	volume = {298},
	year = {2013}
}
@article{Po,
	author = {Po, Huang and Teaching, Zen and Po, Huang},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Po, Teaching, Po - Unknown - “ Up to now , you have refuted everything which has been said . You have done nothing to point out the true.pdf:pdf},
	title = {{“ Up to now , you have refuted everything which has been said . You have done nothing to point out the true Dharma to us .”}}
}
@article{,
	file = {:E$\backslash$:/AAAI{\_}2020{\_}{\_}{\_}hierarchical{\_}feature{\_}directions.pdf:pdf},
	mendeley-groups = {11Thesis/Rana},
	title = {{Hierarchical Linear Disentanglement of Entity Embeddings}},
	year = {2015}
}
@article{Emnlp2018,
	author = {Emnlp, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/EMNLP2018{\_}{\_}{\_}Tom.pdf:pdf},
	pages = {1--8},
	title = {{Fine-Tuning Vector Space Representations for Interpretable Text Classification}},
	year = {2018}
}
@article{Burges1998,
	abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
	archivePrefix = {arXiv},
	arxivId = {1111.6189v1},
	author = {Burges, CJC Christopher J C},
	doi = {10.1023/A:1009715923555},
	eprint = {1111.6189v1},
	isbn = {0818672404},
	issn = {13845810},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
	number = {2},
	pages = {121--167},
	pmid = {5207842081938259593},
	title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
	url = {http://www.springerlink.com/index/Q87856173126771Q.pdf{\%}5Cnhttp://link.springer.com/article/10.1023/A:1009715923555},
	volume = {2},
	year = {1998}
}
@article{Utgoff2002,
	abstract = {We explore incremental assimilation of new knowledge by sequential learning. Of particular interest is how a network of many knowledge layers can be constructed in an on-line manner, such that the learned units represent building blocks of knowledge that serve to compress the overall representation and facilitate transfer. We motivate the need for many layers of knowledge, and we advocate sequential learning as an avenue for promoting the construction of layered knowledge structures. Finally, our novel STL algorithm demonstrates a method for simultaneously acquiring and organizing a collection of concepts and functions as a network from a stream of unstructured information.},
	author = {Utgoff, Paul E and Stracuzzi, David J},
	doi = {10.1109/DEVLRN.2002.1011824},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Utgoff, Stracuzzi - 2002 - Many-layered learning.pdf:pdf},
	isbn = {0-7695-1459-6},
	issn = {0899-7667},
	journal = {Neural computation},
	number = {10},
	pages = {2497--2529},
	pmid = {12396572},
	title = {{Many-layered learning.}},
	volume = {14},
	year = {2002}
}
@article{Dai2015,
	abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
	author = {Dai, Andrew M and Le, Quoc V},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
	mendeley-groups = {!Paper 3/task/Large Movie Review,!Paper 3/task/Sentiment treebank,!Paper 3/task/newsgroups},
	pages = {1--10},
	pmid = {414454},
	title = {{Semi-supervised Sequence Learning}},
	url = {http://arxiv.org/abs/1511.01432},
	year = {2015}
}
@article{Ekstrand,
	author = {Ekstrand, Michael D and Kluver, Daniel and Harper, F Maxwell and Konstan, Joseph A},
	file = {:E$\backslash$:/PhD/Papedrs/MultiRecs-Author.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/GAN's and VAE},
	title = {{Letting Users Choose Recommender Algorithms : An Experimental Study}}
}
@article{Lee1999,
	abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1408.1149},
	author = {Lee, D D and Seung, H S},
	doi = {10.1038/44565},
	eprint = {arXiv:1408.1149},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Seung - 1999 - Learning the parts of objects by non-negative matrix factorization.pdf:pdf},
	isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
	issn = {0028-0836},
	journal = {Nature},
	keywords = {Algorithms,Face,Humans,Learning,Models, Neurological,Perception,Perception: physiology,Semantics},
	mendeley-groups = {Annotated/NMF},
	number = {6755},
	pages = {788--91},
	pmid = {10548103},
	title = {{Learning the parts of objects by non-negative matrix factorization.}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10548103},
	volume = {401},
	year = {1999}
}
@article{Faruqui2015,
	abstract = {Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks.},
	archivePrefix = {arXiv},
	arxivId = {1506.02004},
	author = {Faruqui, Manaal and Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah A.},
	eprint = {1506.02004},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui et al. - 2015 - Sparse Overcomplete Word Vector Representations.pdf:pdf},
	isbn = {9781941643723},
	journal = {Acl-2015},
	mendeley-groups = {Annotated/Word Vectors,Progress Report,Interim Review,Annotated/NMF,11Thesis/Interpretability/Word Vectors/Post-processing,11Thesis/Interpretability/GAN's and VAE,Thesis/Retrofitting Word Vectors},
	pages = {1491--1500},
	title = {{Sparse Overcomplete Word Vector Representations}},
	url = {http://homes.cs.washington.edu/{~}nasmith/papers/faruqui+tsvetkov+yogatama+dyer+smith.acl15.pdf},
	year = {2015}
}
@article{Sellam2018,
	author = {Sellam, Thibault and Lin, Kevin and Huang, Ian Yiran and Vondrick, Carl and Research, Google and Wu, Eugene},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sellam et al. - 2018 - {\&}quot I Like the Way You Think! {\&}quot Inspecting the Internal Logic of Recurrent Neural Networks.pdf:pdf},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	pages = {1--3},
	title = {{" I Like the Way You Think! " Inspecting the Internal Logic of Recurrent Neural Networks}},
	url = {http://sellam.me/assets/papers/sellam-sysML.pdf},
	year = {2018}
}
@article{Chung,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1412.3555v1},
	author = {Chung, Junyoung},
	eprint = {arXiv:1412.3555v1},
	file = {:E$\backslash$:/Downloads/Work/1412.3555.pdf:pdf},
	pages = {1--9},
	title = {{Gated Recurrent Neural Networks on Sequence Modeling arXiv : 1412 . 3555v1 [ cs . NE ] 11 Dec 2014}}
}
@article{Xu2003,
	abstract = {In this paper, we propose a novel document clustering method based on the non-negative factorization of the term- document matrix of the given document corpus. In the la- tent semantic space derived by the non-negative matrix fac- torization (NMF), each axis captures the base topic of a par- ticular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value. Our experimental evalua- tions show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clus- tering methods not only in the easy and reliable derivation of document clustering results, but also in document clus- tering accuracies.},
	archivePrefix = {arXiv},
	arxivId = {1410.0993},
	author = {Xu, Wei and Liu, Xin and Gong, Yihong},
	doi = {10.1145/860484.860485},
	eprint = {1410.0993},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Liu, Gong - 2003 - Document clustering based on non-negative matrix factorization.pdf:pdf},
	isbn = {1581136463},
	issn = {01635840},
	journal = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval - SIGIR '03},
	keywords = {document clustering,non-negative matrix factorization},
	mendeley-groups = {Annotated/NMF},
	pages = {267},
	title = {{Document clustering based on non-negative matrix factorization}},
	url = {http://portal.acm.org/citation.cfm?doid=860435.860485},
	year = {2003}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - S000437020800101X.htm:htm},
	title = {{S000437020800101X}}
}
@article{Ou2007,
	abstract = {Multi-class pattern classification has many applications including text document classification, speech recognition, object recognition, etc. Multi-class pattern classification using neural networks is not a trivial extension from two-class neural networks. This paper presents a comprehensive and competitive study in multi-class neural learning with focuses on issues including neural network architecture, encoding schemes, training methodology and training time complexity. Our study includes multi-class pattern classification using either a system of multiple neural networks or a single neural network, and modeling pattern classes using one-against-all, one-against-one, one-against-higher-order, and P-against-Q. We also discuss implementations of these approaches and analyze training time complexity associated with each approach. We evaluate six different neural network system architectures for multi-class pattern classification along the dimensions of imbalanced data, large number of pattern classes, large vs. small training data through experiments conducted on well-known benchmark data. ?? 2006 Pattern Recognition Society.},
	author = {Ou, Guobin and Murphey, Yi Lu},
	doi = {10.1016/j.patcog.2006.04.041},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ou, Murphey - 2007 - Multi-class pattern classification using neural networks.pdf:pdf},
	isbn = {0769521282},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Machine learning,Multi-class classification,Neural networks,Pattern recognition},
	number = {1},
	pages = {4--18},
	title = {{Multi-class pattern classification using neural networks}},
	volume = {40},
	year = {2007}
}
@article{Lastname2011e,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (4).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Bottou2012a,
	abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
	author = {Bottou, Leon},
	doi = {10.1007/978-3-642-35289-8_25},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
	isbn = {978-3-642-35288-1},
	issn = {2045-2322},
	journal = {Neural Networks: Tricks of the Trade},
	number = {1},
	pages = {421--436},
	pmid = {25382349},
	title = {{Stochastic Gradient Descent Tricks}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
	volume = {1},
	year = {2012}
}
@article{Aldarwish2017,
	author = {Aldarwish, Maryam Mohammed},
	doi = {10.1109/ISADS.2017.41},
	file = {:E$\backslash$:/07940253.pdf:pdf},
	isbn = {9781509040421},
	keywords = {2015 is almost 2,5,a statistic made by,billion and it is,emarketer,has shown the number,network sites,of,sns,social,social media users in,support vector machine,svm,ugc,user generated content},
	pages = {282--285},
	title = {{Posts}},
	year = {2017}
}
@article{Huynh2011a,
	abstract = {The production of relatively large and opaque weight matrices by error backpropagation learning has inspired substantial research on how to extract symbolic human-readable rules from trained networks. While considerable progress has been made, the results at present are still relatively limited, in part due to the large numbers of symbolic rules that can be generated. Most past work to address this issue has focused on progressively more powerful methods for rule extraction (RE) that try to minimize the number of weights and/or improve rule expressiveness. In contrast, here we take a different approach in which we modify the error backpropagation training process so that it learns a different hidden layer representation of input patterns than would normally occur. Using five publicly available datasets, we show via computational experiments that the modified learning method helps to extract fewer rules without increasing individual rule complexity and without decreasing classification accuracy. We conclude that modifying error backpropagation so that it more effectively separates learned pattern encodings in the hidden layer is an effective way to improve contemporary RE methods.},
	author = {Huynh, Thuan Q. and Reggia, James A.},
	doi = {10.1109/TNN.2010.2094205},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huynh, Reggia - 2011 - Guiding hidden layer representations for improved rule extraction from neural networks.pdf:pdf},
	isbn = {1941-0093 (Electronic)$\backslash$n1045-9227 (Linking)},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Hidden layer representation,neural networks,penalty function,rule extraction},
	number = {2},
	pages = {264--275},
	pmid = {21138801},
	title = {{Guiding hidden layer representations for improved rule extraction from neural networks}},
	volume = {22},
	year = {2011}
}
@misc{,
	mendeley-groups = {Report},
	title = {{2001-Li-ICDM}}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Thomas Ager Thesis Structure (1).pdf:pdf},
	pages = {4--6},
	title = {{Thomas Ager Thesis Structure}}
}
@article{Boz,
	author = {Boz, Olcay and Ave, Laurel},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boz, Ave - Unknown - Decision Tree DecText - Decision Tree Extractor.pdf:pdf},
	mendeley-groups = {Annotated/Decision Trees},
	pages = {1--7},
	title = {{Decision Tree DecText - Decision Tree Extractor}}
}
@article{Gal2015,
	abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
	archivePrefix = {arXiv},
	arxivId = {1512.05287},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	doi = {10.1201/9781420049176},
	eprint = {1512.05287},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal, Ghahramani - 2015 - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:pdf},
	isbn = {9789537619084},
	issn = {0302-9743},
	mendeley-groups = {!Paper 3/Training LSTMs,!Paper 3/task/Sentiment treebank},
	number = {Nips},
	pmid = {21803542},
	title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1512.05287},
	year = {2015}
}
@article{Radford2017,
	abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
	archivePrefix = {arXiv},
	arxivId = {1704.01444},
	author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
	eprint = {1704.01444},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford, Jozefowicz, Sutskever - 2017 - Learning to Generate Reviews and Discovering Sentiment.pdf:pdf},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Interpretable LSTMs,!Paper 3/task/Large Movie Review,!Paper 3/task/Yelp,!Paper 3/task/Sentiment treebank,!Paper 3/Understanding LSTMs},
	title = {{Learning to Generate Reviews and Discovering Sentiment}},
	url = {http://arxiv.org/abs/1704.01444},
	year = {2017}
}
@article{Garg2017,
	abstract = {Word embeddings use vectors to represent words such that the geometry between vectors captures semantic relationship between the words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding can be leveraged to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 years of text data with the U.S. Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures global social shifts -- e.g., the women's movement in the 1960s and Asian immigration into the U.S -- and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a powerful new intersection between machine learning and quantitative social science.},
	archivePrefix = {arXiv},
	arxivId = {1711.08412},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	doi = {10.1073/pnas.1720347115},
	eprint = {1711.08412},
	file = {:D$\backslash$:/Downloads/Play/E3635.full.pdf:pdf},
	isbn = {1720347115},
	issn = {0027-8424},
	mendeley-groups = {11Thesis/Interpretability/Discrimination},
	number = {16},
	pmid = {29615513},
	title = {{Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes}},
	url = {http://arxiv.org/abs/1711.08412{\%}0Ahttp://dx.doi.org/10.1073/pnas.1720347115},
	volume = {115},
	year = {2017}
}
@article{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
	title = {{No Title}}
}
@article{Le2014,
	abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
	archivePrefix = {arXiv},
	arxivId = {1405.4053},
	author = {Le, Qv and Mikolov, Tomas},
	doi = {10.1145/2740908.2742760},
	eprint = {1405.4053},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
	isbn = {9781634393973},
	issn = {10495258},
	journal = {International Conference on Machine Learning - ICML 2014},
	mendeley-groups = {Progress Report,Interim Review,11Thesis},
	pages = {1188--1196},
	pmid = {9377276},
	title = {{Distributed Representations of Sentences and Documents}},
	url = {http://arxiv.org/abs/1405.4053},
	volume = {32},
	year = {2014}
}
@article{Hu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1703.00955v4},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
	eprint = {arXiv:1703.00955v4},
	file = {:E$\backslash$:/PhD/Papedrs/1703.00955.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted},
	title = {{Toward Controlled Generation of Text}},
	year = {2017}
}
@book{Primer,
	author = {Primer, A},
	file = {:E$\backslash$:/Downloads/Work/Multivariate Statistical Methods{\_} A Primer, Fourth Edition (2017) Bryan Manly.pdf:pdf},
	isbn = {9781498728966},
	title = {{No Title}}
}
@article{Srivastava2014a,
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
	archivePrefix = {arXiv},
	arxivId = {1102.4807},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	doi = {10.1214/12-AOS1000},
	eprint = {1102.4807},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
	isbn = {1532-4435},
	issn = {15337928},
	journal = {Journal of Machine Learning Research (JMLR)},
	keywords = {deep learning,model combination,neural networks,regularization},
	pages = {1929--1958},
	title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
	volume = {15},
	year = {2014}
}
@article{Y.2015a,
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1312.6184v5},
	author = {Y., Lecun and Y., Bengio and G., Hinton},
	doi = {10.1038/nature14539},
	eprint = {arXiv:1312.6184v5},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Y., Y., G. - 2015 - Deep learning.pdf:pdf},
	isbn = {3135786504},
	issn = {0028-0836},
	journal = {Nature},
	number = {7553},
	pages = {436--444},
	pmid = {26017442},
	title = {{Deep learning}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930630277{\&}partnerID=40{\&}md5=befeefa64ddca265c713cf81f4e2fc54},
	volume = {521},
	year = {2015}
}
@article{Reading2008,
	author = {Reading, Additional},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reading - 2008 - Reproduced with permission of the copyright owner . Further reproduction prohibited without permission .pdf:pdf},
	journal = {Pediatric Infectious Disease},
	number = {3},
	title = {{Reproduced with permission of the copyright owner . Further reproduction prohibited without permission .}},
	volume = {34},
	year = {2008}
}
@article{Jeawak2019,
	author = {Jeawak, Shelan S},
	file = {:E$\backslash$:/Downloads/Work/Shelan{\_}Thesis.pdf:pdf},
	title = {{Exploiting Flickr Meta-Data for Predicting Environmental Features School of Computer Science {\&} Informatics}},
	year = {2019}
}
@article{Mao2014,
	abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1410.1090v1},
	author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
	eprint = {arXiv:1410.1090v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
	journal = {arXiv:1410.1090 [cs]},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Explain Images with Multimodal Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1410.1090{\%}5Cnhttp://www.arxiv.org/pdf/1410.1090.pdf},
	year = {2014}
}
@article{Stubbs2007,
	abstract = {The use of robots, especially autonomous mobile robots, to support work is expected to increase over the next few decades. However, little empirical research examines how users form mental models of robots, how they collaborate with them, and what factors contribute to the success or failure of human-robot collaboration. A two-year observational study of a collaborative human-robot system suggests that the factors disrupting the creation of common ground for interactive communication change at different levels of robot autonomy. Our observations of users collaborating with the remote robot showed differences in how the users reached common ground with the robot in terms of an accurate, shared understanding of the robot's context, planning, and actions - a process called grounding. We focus on how the types and levels of robot autonomy affect grounding. We also examine the challenges a highly autonomous system presents to people's ability to maintain a shared mental model of the robot},
	author = {Stubbs, Kristen and Wettergreen, David and Hinds, Pamela J.},
	doi = {10.1109/MIS.2007.21},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stubbs, Wettergreen, Hinds - 2007 - Autonomy and common ground in human-robot interaction A field study.pdf:pdf},
	isbn = {1541-1672 VO  - 22},
	issn = {15411672},
	journal = {IEEE Intelligent Systems},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {2},
	pages = {42--50},
	title = {{Autonomy and common ground in human-robot interaction: A field study}},
	volume = {22},
	year = {2007}
}
@article{Setiono1997c,
	abstract = {We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.},
	author = {Setiono, Rudy and Liu, Huan},
	doi = {10.1016/S0925-2312(97)00038-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Liu - 1997 - Neurolinear From neural networks to oblique decision rules.pdf:pdf},
	isbn = {3-540-62858-4},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Discretization,Oblique-rule,Pruning,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {1},
	pages = {1--24},
	title = {{Neurolinear: From neural networks to oblique decision rules}},
	volume = {17},
	year = {1997}
}
@article{Faruqui2015,
	abstract = {Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninter-pretable components whose relationship to the categories of traditional lexical seman-tic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguis-tic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9{\%} sparse. We analyze their performance on state-of-the-art eval-uation methods for distributional models of word vectors and find they are competi-tive to standard distributional approaches.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1506.05230v1},
	author = {Faruqui, Manaal and Dyer, Chris},
	doi = {10.3115/v1/P15-2076},
	eprint = {arXiv:1506.05230v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui, Dyer - 2015 - Non-distributional Word Vector Representations.pdf:pdf},
	isbn = {9781941643730},
	journal = {Acl-2015},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {464--469},
	title = {{Non-distributional Word Vector Representations}},
	year = {2015}
}
@article{Saad2007b,
	abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., {\&} Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373-389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e. calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity-complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
	author = {Saad, Emad W. and Wunsch, Donald C.},
	doi = {10.1016/j.neunet.2006.07.005},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saad, Wunsch - 2007 - Neural network explanation using inversion.pdf:pdf},
	isbn = {0893-6080},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Evolutionary algorithm,Explanation capability of neural networks,Hyperplanes,Inversion,Neural network explanation,Pedagogical,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {1},
	pages = {78--93},
	pmid = {17029713},
	title = {{Neural network explanation using inversion}},
	volume = {20},
	year = {2007}
}
@article{Hoyer2004,
	abstract = {Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of `sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.},
	archivePrefix = {arXiv},
	arxivId = {cs/0408058},
	author = {Hoyer, Patrik O.},
	doi = {10.1109/ICMLC.2011.6016966},
	eprint = {0408058},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoyer - 2004 - Non-negative matrix factorization with sparseness constraints.pdf:pdf},
	isbn = {0780395174},
	issn = {1532-4435},
	keywords = {data-adaptive representations,non-negative matrix factorization,sparseness},
	mendeley-groups = {Annotated/NMF},
	pages = {1457--1469},
	pmid = {1000253614},
	primaryClass = {cs},
	title = {{Non-negative matrix factorization with sparseness constraints}},
	url = {http://arxiv.org/abs/cs/0408058},
	volume = {5},
	year = {2004}
}
@article{Karaletsos2015a,
	abstract = {Representation learning systems typically rely on massive amounts of labeled data in order to be trained effectively. Recently, high-dimensional parametric models like convolutional neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Human-in-the-loop systems like crowdsourcing are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. We propose to combine generative unsupervised feature learning with learning from similarity orderings in order to learn models which take advantage of privileged information coming from the crowd. We use a fast variational algorithm to learn the model on standard datasets and demonstrate applicability to two image datasets, where classification is drastically improved. We show how triplet-samples of the crowd can supplement labels as a source of information to shape latent spaces with rich semantic information.},
	archivePrefix = {arXiv},
	arxivId = {1506.05011},
	author = {Karaletsos, Theofanis and Belongie, Serge and R{\"{a}}tsch, Gunnar},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1506.05011},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karaletsos, Belongie, R{\"{a}}tsch - 2015 - Bayesian representation learning with oracle constraints.pdf:pdf},
	journal = {Iclr},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Bayesian representation learning with oracle constraints}},
	url = {http://arxiv.org/abs/1506.05011},
	year = {2015}
}
@article{Gilpin,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.00069v2},
	author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	eprint = {arXiv:1806.00069v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1806.00069.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	title = {{Explaining Explanations : An Approach to Evaluating Interpretability of Machine Learning}}
}
@article{Freitas2010,
	author = {Freitas, AA and Wieser, DC and Apweiler, R},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas, Wieser, Apweiler - 2010 - On the importance of comprehensible classification models for protein function prediction.pdf:pdf},
	journal = {IEEE/ACM Transactions on},
	mendeley-groups = {Annotated/Applications/Scientific Discovery,Report/Biologicla domain},
	number = {1},
	pages = {172--182},
	title = {{On the importance of comprehensible classification models for protein function prediction}},
	url = {http://dl.acm.org/citation.cfm?id=1719290},
	volume = {7},
	year = {2010}
}
@article{Wachter2017,
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	archivePrefix = {arXiv},
	arxivId = {1711.00399},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	doi = {10.2139/ssrn.3063289},
	eprint = {1711.00399},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wachter, Mittelstadt, Russell - 2017 - Counterfactual Explanations without Opening the Black Box Automated Decisions and the GDPR.pdf:pdf},
	issn = {1556-5068},
	mendeley-groups = {!Paper 3/Justifying Interpretability},
	number = {1},
	pages = {1--47},
	title = {{Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR}},
	url = {http://arxiv.org/abs/1711.00399},
	year = {2017}
}

@article{Baehrens2010,
	abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
	archivePrefix = {arXiv},
	arxivId = {0912.1128},
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Muller, Klaus-Robert},
	eprint = {0912.1128},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baehrens et al. - 2010 - How to Explain Individual Classification Decisions.pdf:pdf},
	isbn = {1532-4435},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {Ames mutagenicity,black box model,explaining,kernel methods,nonlinear},
	mendeley-groups = {Report/Explaining predictions},
	pages = {1803--1831},
	title = {{How to Explain Individual Classification Decisions}},
	volume = {11},
	year = {2010}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - ScienceDirect - Artificial Intelligence Symbolic knowledge extraction from trained neural networks A sound approach.pdf:pdf},
	title = {{ScienceDirect - Artificial Intelligence : Symbolic knowledge extraction from trained neural networks: A sound approach}},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370200000771}
}
@article{Diao2014,
	abstract = {Recommendation and review sites offer a wealth of infor-mation beyond ratings. For instance, on IMDb users leave reviews, commenting on different aspects of a movie (e.g. actors, plot, visual effects), and expressing their sentiments (positive or negative) on these aspects in their reviews. This suggests that uncovering aspects and sentiments will allow us to gain a better understanding of users, movies, and the process involved in generating ratings. The ability to answer questions such as " Does this user care more about the plot or about the special effects? " or " What is the quality of the movie in terms of acting? " helps us to understand why certain ratings are generated. This can be used to provide more meaningful recommendations. In this work we propose a probabilistic model based on collaborative filtering and topic modeling. It allows us to capture the interest distribution of users and the content distribution for movies; it provides a link between inter-est and relevance on a per-aspect basis and it allows us to differentiate between positive and negative sentiments on a per-aspect basis. Unlike prior work our approach is entirely unsupervised and does not require knowledge of the aspect specific ratings or genres for inference. We evaluate our model on a live copy crawled from IMDb. Our model offers superior performance by joint modeling. Moreover, we are able to address the cold start problem — by utilizing the information inherent in reviews our model demonstrates improvement for new users and movies.},
	author = {Diao, Qiming and Qiu, Minghui and Wu, Chao-Yuan and Smola, Alexander J. and Jiang, Jing and Wang, Chong},
	doi = {10.1145/2623330.2623758},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Diao et al. - 2014 - Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS).pdf:pdf},
	isbn = {9781450329569},
	journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Yelp},
	pages = {193--202},
	title = {{Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)}},
	url = {http://dl.acm.org/citation.cfm?doid=2623330.2623758},
	year = {2014}
}
@article{Abdollahi2018,
	author = {Abdollahi, Behnoush and Nasraoui, Olfa},
	doi = {10.1007/978-3-319-90403-0},
	file = {:E$\backslash$:/Downloads/Work/behnoush-book-chapter5.pdf:pdf},
	isbn = {9783319904030},
	number = {June},
	pages = {0--15},
	title = {{Transparency in Fair Machine Learning : the Case of Explainable Recommender Chapter 1 Transparency in Fair Machine Learning : the Case of Explainable Recommender Systems}},
	year = {2018}
}
@article{Kiros,
	author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros et al. - Unknown - Skip-Thought Vectors.pdf:pdf},
	mendeley-groups = {Annotated/Representation Learning},
	number = {786},
	pages = {1--9},
	title = {{Skip-Thought Vectors}}
}
@article{GethsiyalAugasta2012a,
	abstract = {Artificial neural networks often achieve high classification accuracy$\backslash$nrates, but they are considered as black boxes due to their lack of$\backslash$nexplanation capability. This paper proposes the new rule extraction$\backslash$nalgorithm RxREN to overcome this drawback. In pedagogical approach the$\backslash$nproposed algorithm extracts the rules from trained neural networks for$\backslash$ndatasets with mixed mode attributes. The algorithm relies on reverse$\backslash$nengineering technique to prune the insignificant input neurons and to$\backslash$ndiscover the technological principles of each significant input neuron$\backslash$nof neural network in classification. The novelty of this algorithm lies$\backslash$nin the simplicity of the extracted rules and conditions in rule are$\backslash$ninvolving both discrete and continuous mode of attributes.$\backslash$nExperimentation using six different real datasets namely iris, wbc,$\backslash$nhepatitis, pid, ionosphere and creditg show that the proposed algorithm$\backslash$nis quite efficient in extracting smallest set of rules with high$\backslash$nclassification accuracy than those generated by other neural network$\backslash$nrule extraction methods.},
	author = {{Gethsiyal Augasta}, M. and Kathirvalavakumar, T.},
	doi = {10.1007/s11063-011-9207-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gethsiyal Augasta, Kathirvalavakumar - 2012 - Reverse engineering the neural networks for rule extraction in classification problems(2).pdf:pdf},
	issn = {13704621},
	journal = {Neural Processing Letters},
	keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	number = {2},
	pages = {131--150},
	title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
	volume = {35},
	year = {2012}
}
@article{Chaney2012,
	abstract = {Managing large collections of documents is an important$\backslash$nproblem for many areas of science, industry, and$\backslash$nculture. Probabilistic topic modeling offers a promising$\backslash$nsolution. Topic modeling is an unsupervised machine$\backslash$nlearning method that learns the underlying themes in$\backslash$na large collection of otherwise unorganized documents.$\backslash$nThis discovered structure summarizes and organizes the$\backslash$ndocuments. However, topic models are high-level statistical$\backslash$ntools—a user must scrutinize numerical distributions$\backslash$nto understand and explore their results. In this$\backslash$npaper, we present a method for visualizing topic models.$\backslash$nOur method creates a navigator of the documents,$\backslash$nallowing users to explore the hidden structure that a$\backslash$ntopic model discovers. These browsing interfaces reveal$\backslash$nmeaningful patterns in a collection, helping end-users$\backslash$nexplore and understand its contents in new ways. We$\backslash$nprovide open source software of our method.},
	author = {Chaney, Ajb and Blei, Dm},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaney, Blei - 2012 - Visualizing Topic Models.pdf:pdf},
	isbn = {9781577355564},
	journal = {Icwsm},
	pages = {419--422},
	title = {{Visualizing Topic Models.}},
	year = {2012}
}
@article{Hsu2017,
	author = {Hsu, Wei-ning and Zhang, Yu and Glass, James},
	file = {:E$\backslash$:/PhD/Papedrs/6784-unsupervised-learning-of-disentangled-and-interpretable-representations-from-sequential-data.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	number = {Nips},
	title = {{Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data}},
	year = {2017}
}
@article{Binns2017,
	abstract = {The internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and are subject to the idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic content moderation systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants.},
	archivePrefix = {arXiv},
	arxivId = {1707.01477},
	author = {Binns, Reuben and Veale, Michael and {Van Kleek}, Max and Shadbolt, Nigel},
	doi = {10.1007/978-3-319-67256-4_32},
	eprint = {1707.01477},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Binns et al. - 2017 - Like trainer, like bot Inheritance of bias in algorithmic content moderation.pdf:pdf},
	isbn = {9783319672557},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Algorithmic accountability,Discussion platforms,Freedom of speech,Machine learning,Online abuse},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Discrimination},
	pages = {405--415},
	title = {{Like trainer, like bot? Inheritance of bias in algorithmic content moderation}},
	volume = {10540 LNCS},
	year = {2017}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - arffs.groovy:groovy},
	title = {arffs}
}
@article{Cambria2013,
	abstract = {The way people express their opinions has radically changed in the past few years thanks to the advent of online collaborative media. The distillation of knowledge from this huge amount of unstructured information can be a key factor for marketers who want to create an identity for their product or brand in the minds of their customers. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. Existing approaches to opinion mining, in fact, are still far from being able to infer the cognitive and affective information associated with natural language as they mainly rely on knowledge bases that are too limited to efficiently process text at concept-level. In this context, standard clustering techniques have been previously employed on an affective common-sense knowledge base in attempt to discover how different natural language concepts are semantically and affectively related to each other and, hence, to accordingly mine on-line opinions. In this work, a novel cognitive model based on the combined use of multi-dimensional scaling and artificial neural networks is exploited for better modelling the way multi-word expressions are organised in a brain-like universe of natural language concepts. The integration of a biologically inspired paradigm with standard principal component analysis helps to better grasp the non-linearities of the resulting vector space and, hence, improve the affective common-sense reasoning capabilities of the system. ?? 2012 Elsevier B.V. All rights reserved.},
	author = {Cambria, Erik and Mazzocco, Thomas and Hussain, Amir},
	doi = {10.1016/j.bica.2013.02.003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cambria, Mazzocco, Hussain - 2013 - Application of multi-dimensional scaling and artificial neural networks for biologically inspired op.pdf:pdf},
	isbn = {1467351644},
	issn = {2212683X},
	journal = {Biologically Inspired Cognitive Architectures},
	keywords = {AI,ANN,Cognitive modelling,NLP,Sentic computing},
	pages = {41--53},
	publisher = {Elsevier B.V.},
	title = {{Application of multi-dimensional scaling and artificial neural networks for biologically inspired opinion mining}},
	url = {http://dx.doi.org/10.1016/j.bica.2013.02.003},
	volume = {4},
	year = {2013}
}
@article{Ager2012d,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}CaptionFix.pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Schockaert,
	author = {Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schockaert - Unknown - Lexical inference as a spatial reasoning problem.pdf:pdf},
	title = {{Lexical inference as a spatial reasoning problem}}
}
@article{Bau2017,
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
	archivePrefix = {arXiv},
	arxivId = {1704.05796},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	doi = {10.1109/CVPR.2017.354},
	eprint = {1704.05796},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1704.05796.pdf:pdf},
	isbn = {9781538604571},
	issn = {1530-1567},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Representations},
	pages = {3319--3327},
	pmid = {12882599},
	title = {{Network dissection: Quantifying interpretability of deep visual representations}},
	volume = {2017-Janua},
	year = {2017}
}
@article{Ananny2016,
	abstract = {Models for understanding and holding systems accountable have long rested upon ideals and logics of transparency. Being able to see a system is sometimes equated with being able to know how it works and govern it—a pattern that recurs in recent work about transparency and computational systems. But can “black boxes' ever be opened, and if so, would that ever be sufficient? In this article, we critically interrogate the ideal of transparency, trace some of its roots in scientific and sociotechnical epistemological cultures, and present 10 limitations to its application. We specifically focus on the inadequacy of transparency for understanding and governing algorithmic systems and sketch an alternative typology of algorithmic accountability grounded in constructive engagements with the limitations of transparency ideals.},
	author = {Ananny, Mike and Crawford, Kate},
	doi = {10.1177/1461444816676645},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ananny, Crawford - 2016 - Seeing without knowing Limitations of the transparency ideal and its application to algorithmic accountability.pdf:pdf},
	issn = {1461-4448},
	journal = {New Media {\&} Society},
	keywords = {accountability,algorithms,critical infrastructure studies,platform governance},
	mendeley-groups = {!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
	pages = {146144481667664},
	title = {{Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability}},
	url = {http://journals.sagepub.com/doi/10.1177/1461444816676645},
	year = {2016}
}
@book{Edwards2017,
	abstract = {ABSTRACT This article reflects the kinds of situations and spaces where people and algorithms meet. In what situations do people become aware of algorithms? How do they experience and make sense of these algorithms, given their often hidden and invisible nature? To what extent does an awareness of algorithms affect people's use of these platforms, if at all? To help answer these questions, this article examines people's personal stories about the Facebook algorithm through tweets and interviews with 25 ordinary users. To understand the spaces where people and algorithms meet, this article develops the notion of the algorithmic imaginary. It is argued that the algorithmic imaginary - ways of thinking about what algorithms are, what they should be and how they function - is not just productive of different moods and sensations but plays a generative role in moulding the Facebook algorithm itself. Examining how algorithms make people feel, then, seems crucial if we want to understand their social power.},
	author = {Edwards, Lilian and Veale, Michael},
	booktitle = {SSRN Electronic Journal},
	doi = {10.2139/ssrn.2972855},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards, Veale - 2017 - Slave to the Algorithm Why a Right to Explanationn is Probably Not the Remedy You are Looking for.pdf:pdf},
	isbn = {3540445668},
	issn = {1556-5068},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability},
	pages = {1--65},
	title = {{Slave to the Algorithm? Why a Right to Explanationn is Probably Not the Remedy You are Looking for}},
	url = {https://www.ssrn.com/abstract=2972855},
	volume = {2017},
	year = {2017}
}
@article{Lakkaraju2016,
	author = {Lakkaraju, Himabindu and Bach, Stephen and Leskovec, Jure},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju, Bach, Leskovec - 2016 - Interpretable Decision Sets A Joint Framework for Description and Prediction.pdf:pdf},
	journal = {The 22th {\{}ACM{\}} {\{}SIGKDD{\}} International Conference on Knowledge Discovery and Data Mining, {\{}KDD{\}} '16, San Fransisco, CA, USA, August, 2016 International Conference on Knowledge Discovery and Data Mining, {\{}{\{}{\}}KDD{\{}{\}}{\}} '16, San Fransisco, CA, USA, August, 2016},
	title = {{Interpretable Decision Sets: A Joint Framework for Description and Prediction}},
	volume = {1},
	year = {2016}
}
@article{Volumeb,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 3 - The Bloody Valkyrie.pdf:pdf},
	title = {{No Title}}
}
@article{Le2014,
	author = {Le, Quoc and Mikolov, Tomas and Com, Tmikolov Google},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov, Com - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
	mendeley-groups = {Annotated/Document representation,!Paper 3/task/Sentiment treebank},
	title = {{Distributed Representations of Sentences and Documents}},
	volume = {32},
	year = {2014}
}
@article{Keil2006,
	abstract = {The study of explanation, while related to intuitive theories, concepts, and mental models, offers important new perspectives on high-level thought. Explanations sort themselves into several distinct types corresponding to patterns of causation, content domains, and explanatory stances, all of which have cognitive consequences. Although explanations are necessarily incomplete—often dramatically so in laypeople—those gaps are difficult to discern. Despite such gaps and the failure to recognize them fully, people do have skeletal explanatory senses, often implicit, of the causal structure of the world. They further leverage those skeletal understandings by knowing how to access additional explanatory knowledge in other minds and by being particularly adept at using situational support to build explanations on the fly in real time. Across development and cultures, there are differences in preferred explanatory schemes, but rarely are any kinds of schemes completely unavailable to a group},
	archivePrefix = {arXiv},
	arxivId = {NIHMS150003},
	author = {Keil, Frank C.},
	doi = {10.1146/annurev.psych.57.102904.190100},
	eprint = {NIHMS150003},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keil - 2006 - Explanation and Understanding.pdf:pdf},
	isbn = {0066-4308 1545-2085},
	issn = {0066-4308},
	journal = {Annual Review of Psychology},
	keywords = {abstract the study of,and mental models,causality,cognition,cognitive development,concepts,domain specificity,expla-,explanation,illusions of,knowing,offers important new perspectives,on high-level thought,stances,theories,while related to intuitive},
	mendeley-groups = {Report/Explaining predictions},
	number = {1},
	pages = {227--254},
	pmid = {16318595},
	title = {{Explanation and Understanding}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.57.102904.190100},
	volume = {57},
	year = {2006}
}
@article{Kamkarhaghighi2017,
	author = {Kamkarhaghighi, Mehran and Makrehchi, Masoud},
	doi = {10.1016/j.eswa.2017.08.021},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamkarhaghighi, Makrehchi - 2017 - Content Tree Word Embedding for document representation.pdf:pdf},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Word embedding,Content tree,Word2Vec,GloVe,Sentime},
	mendeley-groups = {Annotated/Document representation},
	pages = {241--249},
	publisher = {Elsevier Ltd},
	title = {{Content Tree Word Embedding for document representation}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417417305596},
	volume = {90},
	year = {2017}
}
@article{Craven1993,
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1993 - Learning Symbolic Rules Using Artificial Neural Networks.pdf:pdf},
	journal = {Machine Learning: Proceedings of the Tenth International Conference},
	pages = {73--80},
	title = {{Learning Symbolic Rules Using Artificial Neural Networks}},
	year = {1993}
}
@article{Guidotti2018,
	archivePrefix = {arXiv},
	arxivId = {1802.01933},
	author = {Guidotti, Riccardo and Monreale, Anna and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	eprint = {1802.01933},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guidotti et al. - 2018 - A Survey Of Methods For Explaining Black Box Models.pdf:pdf},
	keywords = {KDD Lab,survey},
	mendeley-groups = {!Paper 3},
	pages = {1--45},
	title = {{A Survey Of Methods For Explaining Black Box Models}},
	year = {2018}
}
@article{Mitchell2015,
	abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that per-formance on this task can be related to orthogonality within the space. Explic-itly designing such structure into a neu-ral network model results in represen-tations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within En-glish Wikipedia text to enable this de-composition can produce substantial im-provements on semantic-similarity, pos-induction and word-analogy tasks.},
	author = {Mitchell, Jeff and Steedman, Mark},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/semsynacl2015{\_}final.pdf:pdf},
	isbn = {9781941643723},
	journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis/Interpretability/Visual},
	pages = {1301--1310},
	title = {{Orthogonality of Syntax and Semantics within Distributional Spaces}},
	url = {http://www.aclweb.org/anthology/P15-1126},
	year = {2015}
}
@article{Tai2015a,
	abstract = {Because of their superior ability to pre-serve sequence information over time, Long Short-Term Memory (LSTM) net-works, a type of recurrent neural net-work with a more complex computational unit, have obtained strong results on a va-riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn-tactic properties that would naturally com-bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti-ment Treebank).},
	archivePrefix = {arXiv},
	arxivId = {1503.0075},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	doi = {10.1515/popets-2015-0023},
	eprint = {1503.0075},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tai, Socher, Manning - 2015 - Improved semantic representations from tree-structured long short-term memory networks.pdf:pdf},
	isbn = {9781941643723},
	issn = {9781941643723},
	journal = {Proceedings of ACL},
	mendeley-groups = {Progress Report,!Paper 3/task/Interpretable LSTMs,!Paper 3/task/Sentiment treebank},
	pages = {1556--1566},
	pmid = {18267787},
	title = {{Improved semantic representations from tree-structured long short-term memory networks}},
	year = {2015}
}
@book{Washington,
	author = {Washington, Allyn J},
	file = {:E$\backslash$:/Downloads/Work/Basic Technical Mathematics with Calculus 11th ed - Allyn J. Washington, Richard Evans (Pearson, 2018).pdf:pdf},
	isbn = {9780134437736},
	title = {{Mathematics with Calculus}}
}

@article{Windows2014,
	abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Windows, Microsoft and Os, Mac and When, Considerable Points and Wei, Yanhao and Yildirim, Pinar and den Bulte, Christophe and Dellarocas, Chris and Weekly, The and Weekly, I C T Issues and {W. E. Henley} and Vyas, Shilpan Dineshkumar and Uk, The and Trend, Security and Trend, Finance Security and Technology, Banker and Insights, Financial and Longtop, About and Technologies, Financial and Tan, Arhnue and Darby, Sime and Corp, I O I and Loong, Kim and SW공학센터 and Skan, Julian and Lumb, Richard and Masood, Samad and Conway, Sean K. and Shue, Kelly and Service, Social Networking and Server, Web and Module, Server and Security, Financial and Scheme, Lending and Sample, Char and Schaffer, Kim and Report, Survey and Report, Industry Issue and Pos, Mobile and Permissions, App and Experience, Web and Links, App and Payments, Mobile and Support, Fingerprint and Store, Play and Paper, Working and Online, Fast Identity and Okten, Cagla and Osili, Una Okonkwo and November, Security Focus and Name, Last and Name, First and Training, Online and Training, Practical and Darin, C and Training, Rank Online and Kimberly, M and Deepa, G and Board, Ethics and Principal, Enter and Primary, Investigator and Systems, Food and Study, Emu Behaviour and Co-investigator, New and Mohamad, Rosli and Building, Accountancy and Ismail, Noor Azizi and March, Security Focus and Lin, Mingfeng and Prabhala, N.R. and Viswanathan, Siva and Lee, Kwanghoe and Park, Sean and Lee, Jihye Jenna and Park, Sean and Law, Fintech and Straight, R Jason and Vice, Senior and Privacy, Chief and Straight, Unitedlex and Douglass, Duncan B and Avery, B Y Christopher and Fanger, Gwen and Douglass, Duncan B and KPMG and Kempe, David and Kleinberg, Jon and Tardos, {\'{E}}va and Karma, Credit and Issues, Special and Issue, Market and Internet, Secure and Service, Payment and Insight, L G Business and Indicators, Hot and Huang, Cheng-Lung and Chen, Mu-Chen and Wang, Chieh-Jen and Group, Alibaba and Go, Rnaseq P F and Go, Rnaseq P F and Go, David Down and Go, David Down and Go, Rnaseq P F and From, Industry Overview and Freedman, Seth and Jin, Ginger and Forgot, Email Password and Foust, By Dean and February, Aaron Pressman and Corp, Fair Isaac and Fair, Bill and Isaac, Earl and Fellowes, Matt and Isaac, Fair and Fico, The and Isaac, Fair and Sanders, Anthony B and Bank, Deutsche and York, New and Street, Wall and Fico, The and For, Everything and Home, T H E and Finance, Segye and February, Security Focus and {Eroglu S., Toprak S., Urgan O, MD, Ozge E. Onur, MD, Arzu Denizbasi, MD, Haldun Akoglu, MD, Cigdem Ozpolat, MD, Ebru Akoglu}, Md and {Ernst {\&} Young} and Economics, Size South Mountain and Duarte, Jefferson and Siegel, Stephan and Young, Lance and Debnath, Souvik and Ganguly, Niloy and Mitra, Pabitra and Data, Big and Dapp, Thomas S. and Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R. A. and Holmberg, S. D. and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, SVD Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise and Contributor, Tom Groenfeldt and Conference, Mobile Security and Call, Secure Monitor and Compass, Gyro and Cho, Byungchul and Park, Jong-man and Chavan, Jayshree and Chat, We and Channel, Omni and Challenge, Business and Brief, Keri and Brief, Keri and Block, Bitcoin and Economics, Size South Mountain and Big, Using and Based, Data and Analysis, Network and Bankitx, A and Bank, Challenger and Bank, Challenger and Bank, Challenger and Bankers, British and Street, High and Bank, British Business and Authority, Financial Conduct and Accenture and Ioffe, Sergey and Szegedy, Christian},
	doi = {10.1007/s13398-014-0173-7.2},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Windows et al. - 2014 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
	isbn = {9780874216561},
	issn = {0717-6163},
	journal = {Uma {\'{e}}tica para quantos?},
	keywords = {12,2007,3,Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\&} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cornway,Corporate Finance,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,Industrial Organization,J.,JUVENTUD,Lumb,MODIFICACIONES CORPORALES,Male,Masood,Motivation,Movement,Public,R.,Risk-Taking,S.,S.K.,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Skan,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\&} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,advantages,aesthetics,and e-banking,and on cor-,anomaly detection,as none were found,authentication,autoinjury and health,body,business model,candidate,classification,collaboration,competition,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,credit access,credit financing,credit score,credit scoring,critical success factors,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,customer satisfaction,customer scoring,data mining,decision tree,department of economics at,e-,e- banking,e-banking,e-commerce,e-payment,e-trading,electronic communication and computation,emergency,endogenous tie,epidural,esth{\'{e}}tique,est{\'{e}}tica,feature sim-,finance includes e-payment,financial fervices technology,financial services innovation,find any reports of,fintech,fintech analysis,fintech start-ups,functions,genetic programming,global fintech comparison,high resolution images,if neuraxial anes-,in practice,indonesia,information technology,ing with neuraxial anesthesia,internet bank,internet primary bank,jarunee wonglimpiyarat,jeunesse,jibc december 2007,juvenile cultures,juventud,limitations,luation of non-urgent visits,m-commerce,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,multimodal biometric,needle through a,nes corporales,network security,networks,neural networks,no,patents analysis,perforaci{\'{o}}n corporal,piel,professor of marketing,professor of marketing at,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,recommender system,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,smart cards,social network analysis,social networks,social status,spinal,strategic,strategy,support vector machine,sustainable reconstruction,sydney fintech,sydney start-ups,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,the university of pennsylvania,the wharton school of,to a busy urban,traditional banking services,unimodal biometric,university of pennsylvania,vol,was reviewed to see,youth},
	mendeley-groups = {!Paper 3/Training LSTMs,!Paper 3/Training Feedforward/CNN},
	number = {2},
	pages = {81--87},
	pmid = {15003161},
	title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
	url = {http://arxiv.org/abs/1502.03167{\%}5Cnhttp://www.americanbanker.com/issues/179{\_}124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/15003161{\%}5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1},
	volume = {XXXIII},
	year = {2014}
}
@article{Lee,
	author = {Lee, Hsin-ying and Tseng, Hung-yu and Huang, Jia-bin and Singh, Maneesh and Tech, Virginia},
	file = {:E$\backslash$:/Hsin-Ying{\_}Lee{\_}Diverse{\_}Image-to-Image{\_}Translation{\_}ECCV{\_}2018{\_}paper.pdf:pdf},
	pages = {1--17},
	title = {{Diverse Image-to-Image Translation via Disentangled Representations}}
}
@article{Chemiavsky1991a,
	abstract = {properties for software complexity measures are explored. It is shown that a collection of properties suggested by Weyuker are inadequate for determining the quality of a software complexity measure},
	author = {Chemiavsky, John C and Smith, Carl H},
	doi = {10.1109/32.106988},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chemiavsky, Smith - 1991 - Concise Papers.pdf:pdf},
	isbn = {0098-5589},
	issn = {00985589},
	journal = {Foundations},
	number = {9144263},
	pages = {636--638},
	pmid = {158},
	title = {{Concise Papers}},
	volume = {17},
	year = {1991}
}
@book{Moewes,
	author = {Moewes, Christian and N{\"{u}}rnberger, Andreas},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/The safe and interpretable machine learning part.pdf:pdf},
	isbn = {9783642323775},
	mendeley-groups = {11Thesis/Interpretability/Safety},
	title = {{in Intelligent Data Analysis}}
}
@article{Zhang2016,
	abstract = {We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their component sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions.},
	archivePrefix = {arXiv},
	arxivId = {1605.04469},
	author = {Zhang, Ye and Marshall, Iain and Wallace, Byron C.},
	eprint = {1605.04469},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Marshall, Wallace - 2016 - Rationale-Augmented Convolutional Neural Networks for Text Classification.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions},
	title = {{Rationale-Augmented Convolutional Neural Networks for Text Classification}},
	url = {http://arxiv.org/abs/1605.04469},
	year = {2016}
}
@article{Kim2015,
	abstract = {We present the Mind the Gap Model (MGM), an approach for interpretable fea- ture extraction and selection. By placing interpretability criteria directly into the model, we allowfor the model to both optimize parameters related to interpretabil- ity and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and dis- ease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.},
	author = {Kim, Been and Shah, Julie and Doshi-Velez, Finale},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Shah, Doshi-Velez - 2015 - Mind the Gap A Generative Approach to Interpretable Feature Selection and Extraction.pdf:pdf},
	issn = {10495258},
	journal = {Nips},
	mendeley-groups = {Annotated/Interpretable representations},
	pages = {1--9},
	title = {{Mind the Gap : A Generative Approach to Interpretable Feature Selection and Extraction}},
	year = {2015}
}
@article{Chatfield2014,
	abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. A particularly significant one is data augmentation, which achieves a boost in performance in shallow methods analogous to that observed with CNN-based methods. Finally, we are planning to provide the configurations and code that achieve the state-of-the-art performance on the PASCAL VOC Classification challenge, along with alternative configurations trading-off performance, computation speed and compactness.},
	archivePrefix = {arXiv},
	arxivId = {1405.3531},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	doi = {10.5244/C.28.6},
	eprint = {1405.3531},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatfield et al. - 2014 - Return of the Devil in the Details Delving Deep into Convolutional Nets.pdf:pdf},
	isbn = {1-901725-52-9},
	issn = {1-901725-52-9},
	journal = {arXiv preprint arXiv: {\ldots}},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1--11},
	title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
	url = {http://arxiv.org/abs/1405.3531},
	year = {2014}
}
@article{Martens,
	author = {Martens, David},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens - Unknown - Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary.pdf:pdf},
	journal = {Knowledge Creation Diffusion Utilization},
	mendeley-groups = {Annotated/Applications/Financial Engineering},
	pages = {1--2},
	title = {{Building Acceptable Classification Models for Financial Engineering Applications Thesis Summary}}
}
@article{Kingma2015,
	abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
	archivePrefix = {arXiv},
	arxivId = {1506.02557},
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
	doi = {10.1016/S0733-8619(03)00096-3},
	eprint = {1506.02557},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Salimans, Welling - 2015 - Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
	isbn = {1506.02557},
	issn = {10495258},
	mendeley-groups = {!Paper 3/Bayesian Networks},
	number = {Mcmc},
	pages = {1--9},
	pmid = {15062530},
	title = {{Variational Dropout and the Local Reparameterization Trick}},
	url = {http://arxiv.org/abs/1506.02557},
	year = {2015}
}
@article{Machina,
	author = {Machina, Deus Ex and Doe, John},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 11 - The Craftsman of Dwarf.pdf:pdf},
	title = {{The Craftsmen of Dwarf Translation : Nigel}}
}
@article{Caruana2015,
	abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
	author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
	doi = {10.1145/2783258.2788613},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caruana et al. - 2015 - Intelligible Models for HealthCare.pdf:pdf},
	isbn = {9781450336642},
	journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
	keywords = {additive models,classification,healthcare,intelligibility,interaction detection,logistic regression,risk prediction},
	mendeley-groups = {Report/Explaining predictions},
	pages = {1721--1730},
	title = {{Intelligible Models for HealthCare}},
	url = {http://dl.acm.org/citation.cfm?id=2783258.2788613},
	year = {2015}
}
@article{Volumef,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 8 - The Two Leaders.pdf:pdf},
	title = {{No Title}}
}
@article{Johansson2009,
	abstract = {Some data mining problems require predictive models to be not only accurate but also comprehensible. Comprehensibility enables human inspection and understanding of the model, making it possible to trace why individual predictions are made. Since most high-accuracy techniques produce opaque models, accuracy is, in practice, regularly sacrificed for comprehensibility. One frequently studied technique, often able to reduce this accuracy vs. comprehensibility tradeoff, is rule extraction, i.e., the activity where another, transparent, model is generated from the opaque. In this paper, it is argued that techniques producing transparent models, either directly from the dataset, or from an opaque model, could benefit from using an oracle guide. In the experiments, genetic programming is used to evolve decision trees, and a neural network ensemble is used as the oracle guide. More specifically, the datasets used by the genetic programming when evolving the decision trees, consist of several different combinations of the original training data and ldquooracle datardquo, i.e., training or test data instances, together with corresponding predictions from the oracle. In total, seven different ways of combining regular training data with oracle data were evaluated, and the results, obtained on 26 UCI datasets, clearly show that the use of an oracle guide improved the performance. As a matter of fact, trees evolved using training data only had the worst test set accuracy of all setups evaluated. Furthermore, statistical tests show that two setups, both using the oracle guide, produced significantly more accurate trees, compared to the setup using training data only.},
	author = {Johansson, Ulf and Niklasson, Lars},
	doi = {10.1109/CIDM.2009.4938655},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johansson, Niklasson - 2009 - Evolving decision trees using oracle guides.pdf:pdf},
	isbn = {9781424427659},
	journal = {2009 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2009 - Proceedings},
	mendeley-groups = {Report/Medical domain},
	pages = {238--244},
	title = {{Evolving decision trees using oracle guides}},
	year = {2009}
}
@article{Guillame-Bert2010,
	abstract = {Artificial Neural Networks have previously been applied in neuro-symbolic learning to learn ground logic program rules. However, there are few results of learning relations using neuro-symbolic learning. This paper presents the system PAN, which can learn relations. The inputs to PAN are one or more atoms, representing the conditions of a logic rule, and the output is the conclusion of the rule. The symbolic inputs may include functional terms of arbitrary depth and arity, and the output may include terms constructed from the input functors. Symbolic inputs are encoded as an integer using an invertible encoding function, which is used in reverse to extract the output terms. The main advance of this system is a convention to allow construction of Artificial Neural Networks able to learn rules with the same power of expression as first order definite clauses. The system is tested on three examples and the results are discussed.},
	author = {Guillame-Bert, M. and Broda, K. and d'Avila Garcez, a.},
	doi = {10.1109/IJCNN.2010.5596491},
	isbn = {978-1-4244-6916-1},
	issn = {1098-7576},
	journal = {Neural Networks (IJCNN), The 2010 International Joint Conference on},
	pages = {18--23},
	title = {{First-order logic learning in Artificial Neural Networks}},
	year = {2010}
}
@article{Ager2012,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Submission{\_}243{\_}LabelFix (2).pdf:pdf},
	mendeley-groups = {Temp},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - self.txt:txt},
	title = {self}
}
@article{Gupta2015,
	author = {Gupta, Abhijeet and Boleda, Gemma and Baroni, Marco and Pad, Sebastian},
	file = {:C$\backslash$:/Users/Workk/Documents/EMNLP002.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {September},
	pages = {12--21},
	title = {{Distributional vectors encode referential attributes}},
	year = {2015}
}
@article{Li2014,
	author = {Li, Li and Zhang, Longkai and Wang, Houfeng},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Wang - 2014 - Muli-label Text Categorization with Hidden Components.pdf:pdf},
	journal = {Emnlp},
	pages = {1816--1821},
	title = {{Muli-label Text Categorization with Hidden Components}},
	year = {2014}
}
@article{Lastname2011f,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (6).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Freitas2013,
	abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
	author = {Freitas, Alex A.},
	doi = {10.1145/2594473.2594475},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas - 2013 - Comprehensible Classification Models - a position paper.pdf:pdf},
	isbn = {1931-0145},
	issn = {19310145},
	journal = {ACM SIGKDD Explorations Newsletter},
	keywords = {bayesian network classifiers,decision table,decision tree,monotonicity constraint,nearest neighbors,rule induction},
	mendeley-groups = {Report/Just about interpretability,Annotated/Overarching Interpretability,Report,11Thesis/Interpretability,11Thesis/Interpretability/General},
	number = {1},
	pages = {1--10},
	title = {{Comprehensible Classification Models - a position paper}},
	url = {http://dl.acm.org.miman.bib.bth.se/citation.cfm?id=2594475},
	volume = {15},
	year = {2013}
}
@article{Sourek2015,
	abstract = {We propose a method combining relational-logic representations with deep neural network learning. Domain-specific knowledge is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structure of given training or testing examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descend algorithm. Notable relational concepts can be discovered by interpreting shared hidden layer weights corresponding to the rules. Experiments on 78 relational learning benchmarks demonstrate the favorable performance of the method.},
	archivePrefix = {arXiv},
	arxivId = {1508.05128},
	author = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
	eprint = {1508.05128},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sourek et al. - 2015 - Lifted Relational Neural Networks.pdf:pdf},
	journal = {CoRR},
	keywords = {lifted models,neural networks,relational learning},
	mendeley-groups = {Progress Report},
	pages = {1--21},
	title = {{Lifted Relational Neural Networks}},
	url = {http://arxiv.org/abs/1508.05128},
	volume = {abs/1508.0},
	year = {2015}
}
@book{Bishop2006a,
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {Bishop, Christopher M},
	booktitle = {Pattern Recognition},
	doi = {10.1117/1.2819119},
	eprint = {0-387-31073-8},
	isbn = {9780387310732},
	issn = {10179909},
	number = {4},
	pages = {738},
	pmid = {8943268},
	title = {{Pattern Recognition and Machine Learning}},
	url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
	volume = {4},
	year = {2006}
}
@article{Exner2012,
	abstract = {In this paper, we describe an end-to-end system that automatically extracts RDF triples describing entity relations and properties from unstructured text. This system is based on a pipeline of text processing modules that includes a semantic parser and a coreference solver. By using coreference chains, we group entity actions and properties described in different sentences and convert them{\textless}br/{\textgreater}{\textless}br{\textgreater}$\backslash$r$\backslash$ninto entity triples. We applied our system to over 114,000 Wikipedia articles and we could extract more than 1,000,000 triples. Using an ontology-mapping system that we bootstrapped using existing DBpedia triples, we mapped 189,000 extracted triples onto the DBpedia namespace. These extracted entities are availableonline in the N-Triple format.},
	author = {Exner, Peter and Nugues, Pierre},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Exner, Nugues - 2012 - Entity extraction From unstructured text to dbpedia rdf triples.pdf:pdf},
	issn = {16130073},
	journal = {CEUR Workshop Proceedings},
	mendeley-groups = {Report},
	number = {Iswc},
	pages = {58--69},
	title = {{Entity extraction: From unstructured text to dbpedia rdf triples}},
	volume = {906},
	year = {2012}
}
@article{Ross2017a,
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	archivePrefix = {arXiv},
	arxivId = {1703.03717},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	eprint = {1703.03717},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
	mendeley-groups = {!Paper 3/task/newsgroups},
	title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
	url = {http://arxiv.org/abs/1703.03717},
	year = {2017}
}
@article{Giraud-Carrier1998,
	author = {Giraud-Carrier, Christophe},
	doi = {10.1002/9781119990413.ch1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giraud-Carrier - 1998 - Beyond predictive accuracy what.pdf:pdf},
	isbn = {9780470749838},
	journal = {Proceedings of the ECML-98 Workshop on Upgrading Learning to Meta-Level: Model Selection and Data Transformation},
	pages = {78--85},
	title = {{Beyond predictive accuracy: what?}},
	year = {1998}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/POLICY AND PROCEDURE FOR THE APPOINTMENT OF RESEARCH DEGREE EXAMINING BOARDS (VIVA EXAMINATION).pdf:pdf},
	pages = {1--3},
	title = {{POLICY AND PROCEDURE FOR THE APPOINTMENT OF RESEARCH DEGREE}}
}
@article{Collobert2000,
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling, achieving or exceeding state-of-the-art performance in each on four benchmark tasks. Our goal was to design a flexible architecture that can learn representations useful for the tasks, thus avoiding excessive task-specific feature engineering (and therefore disregarding a lot of prior knowledge). Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabelled training data. This work is then used as a basis for building a freely available tagging system with excellent performance while requiring minimal computational resources.},
	archivePrefix = {arXiv},
	arxivId = {1103.0398},
	author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	doi = {10.1145/2347736.2347755},
	eprint = {1103.0398},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2000 - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
	isbn = {1532-4435},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {Deep Learning,Natural Language Processing,Neural Networks},
	mendeley-groups = {Literature Review},
	pages = {1--48},
	title = {{Natural Language Processing (almost) from Scratch}},
	volume = {1},
	year = {2000}
}
@article{Mao2014a,
	abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1410.1090v1},
	author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
	eprint = {arXiv:1410.1090v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
	journal = {arXiv:1410.1090 [cs]},
	mendeley-groups = {Progress Report},
	pages = {1--9},
	title = {{Explain Images with Multimodal Recurrent Neural Networks}},
	url = {http://arxiv.org/abs/1410.1090{\%}5Cnhttp://www.arxiv.org/pdf/1410.1090.pdf},
	year = {2014}
}
@article{Hornik1993a,
	abstract = {We show that standard feedforward networks with as few as a single hidden layer can uniformly approximate continuous functions on compacta provided that the activation function $\psi$ is locally Riemann integrable and nonpolynomial, and have universal Lp ($\mu$) approximation capabilities for finite and compactly supported input environment measures $\mu$ provided that $\psi$ is locally bounded and nonpolynomial. In both cases, the input-to-hidden weights and hidden layer biases can be constrained to arbitrarily small sets; if in addition $\psi$ is locally analytic a single universal bias will do.},
	author = {Hornik, Kurt},
	doi = {10.1016/S0893-6080(09)80018-X},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik - 1993 - Some new results on neural network approximation.pdf:pdf},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {-universal approximation capabilit ies,feedfor wa rd networks,small weight sets,universal bias},
	number = {8},
	pages = {1069--1072},
	publisher = {Pergamon Press Ltd.},
	title = {{Some new results on neural network approximation}},
	url = {http://www.sciencedirect.com/science/article/pii/S089360800980018X},
	volume = {6},
	year = {1993}
}
@article{Amato2009,
	author = {Amato, Claudia and Fanizzi, Nicola and Fazzinga, Bettina},
	file = {:C$\backslash$:/Users/Workk/Documents/Combining{\_}Semantic{\_}Web{\_}Search{\_}with{\_}the{\_}Power{\_}of{\_}In.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces},
	number = {June 2014},
	title = {{Combining Semantic Web Search with the Power of Inductive Reasoning . Combining Semantic Web Search with the Power of Inductive Reasoning}},
	year = {2009}
}
@article{Fern2014,
	author = {Fern, Manuel and Cernadas, Eva},
	file = {:C$\backslash$:/Users/Workk/Documents/delgado14a.pdf:pdf},
	pages = {3133--3181},
	title = {{Do we Need Hundreds of Classifiers to Solve Real World Classification Problems ?}},
	volume = {15},
	year = {2014}
}
@article{Nguyen2015,
	author = {Nguyen, Dat Quoc and Billingsley, Richard and Du, Lan and Johnson, Mark},
	file = {:E$\backslash$:/Downloads/Work/582-1696-1-PB.pdf:pdf},
	pages = {299--313},
	title = {{Improving Topic Models with Latent Feature Word Representations}},
	volume = {3},
	year = {2015}
}
@article{Zhu2015,
	abstract = {The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we pro-pose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hier-archies, e.g., language or image parse structures. We leverage the models for semantic composi-tion to understand the meaning of text, a funda-mental problem in natural language understand-ing, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that with-out considering the structures.},
	author = {Zhu, Xiaodan and Sobhani, Parinaz and Guo, Hongyu},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Sobhani, Guo - 2015 - Long Short-Term Memory Over Recursive Structures.pdf:pdf},
	isbn = {9781510810587},
	journal = {International Conference on Machine Learning (ICML)},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs,!Paper 3/task/Sentiment treebank},
	pages = {1604--1612},
	title = {{Long Short-Term Memory Over Recursive Structures}},
	volume = {37},
	year = {2015}
}
@article{Glorot2011,
	abstract = {The exponential increase in the availability of online reviews and recommendations makes sentiment classi cation an interesting topic in academic and industrial research. Reviews can span so many di erent domains that it is dicult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classi ers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classi ers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain Adaptation for Large-Scale Sentiment Classification A Deep Learning Approach(3).pdf:pdf},
	isbn = {978-1-4503-0619-5},
	journal = {Proceedings of the 28th International Conference on Machine Learning},
	number = {1},
	pages = {513--520},
	title = {{Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach}},
	url = {http://www.icml-2011.org/papers/342{\_}icmlpaper.pdf},
	year = {2011}
}
@article{Kitaev2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1805.01052v1},
	author = {Kitaev, Nikita and Klein, Dan},
	eprint = {arXiv:1805.01052v1},
	file = {:E$\backslash$:/1805.01052.pdf:pdf},
	mendeley-groups = {11Thesis},
	title = {{Constituency Parsing with a Self-Attentive Encoder}},
	year = {2017}
}
@article{Franca2014,
	abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph min-ing and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integratedBCP with a well-known neural-symbolic system, C-IL2 P, to perform learning from nu-merical vectors. C-IL2 P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, han-dles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary re-sults indicating that a reduction of more than 90{\{}{\%}{\}} of features can be achieved with a small loss of accuracy.},
	author = {Fran{\c{c}}a, Manoel V M and Zaverucha, Gerson and {D'Avila Garcez}, Artur S.},
	doi = {10.1007/s10994-013-5392-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}a, Zaverucha, D'Avila Garcez - 2014 - Fast relational learning using bottom clause propositionalization with artificial neural netw.pdf:pdf},
	isbn = {0885-6125},
	issn = {08856125},
	journal = {Machine Learning},
	keywords = {Artificial neural networks,Inductive logic programming,Neural-symbolic integration,Propositionalization,Relational learning},
	mendeley-groups = {Progress Report},
	number = {1},
	pages = {81--104},
	title = {{Fast relational learning using bottom clause propositionalization with artificial neural networks}},
	volume = {94},
	year = {2014}
}
@article{Lenci2008,
	author = {Lenci, Alessandro},
	file = {:E$\backslash$:/Downloads/Work/Distributional{\_}semantics{\_}in{\_}linguistic{\_}and{\_}cogniti.pdf:pdf},
	number = {January},
	title = {{Distributional semantics in linguistic and cognitive research Distributional semantics in linguistic and cognitive research}},
	year = {2008}
}
@article{Craven1994,
	abstract = {Concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to...},
	author = {Craven, Mark W and Shavlik, Jude W},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1994 - Using sampling and queries to extract rules from trained neural networks.pdf:pdf},
	journal = {Proc.$\backslash$ Intl.$\backslash$ Conf.$\backslash$ Machine Learning},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {37--45},
	title = {{Using sampling and queries to extract rules from trained neural networks}},
	year = {1994}
}
@article{Rothe2016,
	author = {Rothe, Sascha and Processing, Language},
	file = {:C$\backslash$:/Users/Workk/Documents/P16-2083.pdf:pdf},
	mendeley-groups = {11Thesis/Semantic Relations {\&} Conceptual Spaces,11Thesis},
	pages = {512--517},
	title = {{Word Embedding Calculus in Meaningful Ultradense Subspaces}},
	year = {2016}
}
@article{Blei2010,
	abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
	archivePrefix = {arXiv},
	arxivId = {1003.0783},
	author = {Blei, David M. and McAuliffe, Jon D.},
	doi = {10.1002/asmb.540},
	eprint = {1003.0783},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, McAuliffe - 2010 - Supervised Topic Models.pdf:pdf},
	isbn = {160560352X},
	issn = {15241904},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {1--8},
	title = {{Supervised Topic Models}},
	url = {http://arxiv.org/abs/1003.0783},
	year = {2010}
}
@article{Kindermans2017,
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
	archivePrefix = {arXiv},
	arxivId = {1711.00867},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"{u}}tt, Kristof T. and D{\"{a}}hne, Sven and Erhan, Dumitru and Kim, Been},
	doi = {10.1016/j.jns.2003.09.014},
	eprint = {1711.00867},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kindermans et al. - 2017 - The (Un)reliability of saliency methods.pdf:pdf},
	isbn = {0022-510X (Print)},
	issn = {0022510X},
	mendeley-groups = {!Paper 3/Interpretability General},
	pages = {1--12},
	pmid = {14706220},
	title = {{The (Un)reliability of saliency methods}},
	url = {http://arxiv.org/abs/1711.00867},
	year = {2017}
}
@article{Hatzilygeroudis2004,
	abstract = {In this paper, we first present and compare existing categorization schemes for neuro-symbolic$\backslash$napproaches. We then stress the point that not all hybrid neuro-symbolic approaches can be$\backslash$naccommodated by existing categories. Such a case is rule-based neuro-symbolic approaches that propose$\backslash$na unified knowledge representation scheme suitable for use in expert systems. That kind of integrated$\backslash$nschemes have the two component approaches tightly and indistinguishably integrated, offer an$\backslash$ninteractive inference engine and can provide explanations. Therefore, we introduce a new category of$\backslash$nneuro-symbolic integrations, namely {\{}{\^{a}}{\}}€˜representational integrations{\{}{\^{a}}{\}}€™. Furthermore, two sub-categories of$\backslash$nrepresentational integrations are distinguished, based on which of the two component approaches of the$\backslash$nintegrations is given pre-eminence. Representative approaches as well as advantages and disadvantages$\backslash$nof both sub-categories are discussed.},
	author = {Hatzilygeroudis, Ioannis and Prentzas, Jim},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatzilygeroudis, Prentzas - 2004 - Neuro-symbolic approaches for knowledge representation in expert systems.pdf:pdf},
	journal = {International Journal of Hybrid Intelligent Systems},
	keywords = {connectionist expert systems,neuro-symbolic integrations,rule-based expert systems},
	mendeley-groups = {Progress Report},
	number = {3, 4},
	pages = {111--126},
	title = {{Neuro-symbolic approaches for knowledge representation in expert systems}},
	url = {http://dl.acm.org/citation.cfm?id=1232821},
	volume = {1},
	year = {2004}
}
@article{Pazzani2000,
	author = {Pazzani, Michael J},
	doi = {10.1109/5254.850821},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pazzani - 2000 - Knowledge discovery from data.pdf:pdf},
	issn = {1094-7167},
	journal = {Intelligent systems and their applications, IEEE},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {2},
	pages = {10--12},
	title = {{Knowledge discovery from data?}},
	volume = {15},
	year = {2000}
}
@article{Cernovsky2015,
	author = {Cernovsky, Zack},
	doi = {10.1080/0092623X.2015.1070779},
	file = {:E$\backslash$:/Downloads/Work/FetishPreferences{\_}aspublished{\_}Sept2015.pdf:pdf},
	number = {July},
	title = {{Fetishistic Preferences of Clients as Ranked by a Sex Worker}},
	year = {2015}
}
@article{Edunov2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1808.09381v2},
	author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David and Park, Menlo and Brain, Google and View, Mountain},
	eprint = {arXiv:1808.09381v2},
	file = {:E$\backslash$:/1808.09381.pdf:pdf},
	mendeley-groups = {11Thesis/State of the art},
	title = {{Understanding Back-Translation at Scale}},
	year = {2018}
}
@article{Goodfellow2014,
	abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2661v1},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	doi = {10.1017/CBO9781139058452},
	eprint = {arXiv:1406.2661v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
	isbn = {1406.2661},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems 27},
	mendeley-groups = {Annotated/Representation Learning,Annotated/Generative Adversarial Nets},
	pages = {2672--2680},
	pmid = {1000183096},
	title = {{Generative Adversarial Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	year = {2014}
}
@article{Zilke2016,
abstract = {Neural network classifiers are known to be able to learn very accurate models. In the recent past, researchers have even been able to train neural networks with multiple hidden layers (deep neural networks) more effectively and efficiently. However, the major downside of neural networks is that it is not trivial to understand the way how they derive their classification decisions. To solve this problem, there has been research on extracting better understandable rules from neural networks. However, most authors focus on nets with only one single hidden layer. The present paper introduces a new decompositional algorithm - DeepRED - that is able to extract rules from deep neural networks. The evaluation of the proposed algorithm shows its ability to outperform a pedagogical baseline on several tasks, including the successful extraction of rules from a neural network realizing the XOR function.},
author = {Zilke, Jan Ruben and Menc{\'{i}}a, Eneldo Loza and Janssen, Frederik},
doi = {10.1007/978-3-319-46307-0_29},
file = {:E$\backslash$:/PhD/DS16DeepRED.pdf:pdf},
isbn = {9783319463063},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {457--473},
title = {{DeepRED - Rule extraction from deep neural networks}},
volume = {9956 LNAI},
year = {2016}
}

@article{Chorowski2015a,
	abstract = {People can understand complex structures if they relate to more isolated yet understandable concepts. Despite this fact, popular pattern recognition tools, such as decision tree or production rule learners, produce only flat models which do not build intermediate data representations. On the other hand, neural networks typically learn hierarchical but opaque models. We show how constraining neurons' weights to be nonnegative improves the interpretability of a network's operation. We analyze the proposed method on large data sets: the MNIST digit recognition data and the Reuters text categorization data. The patterns learned by traditional and constrained network are contrasted to those learned with principal component analysis and nonnegative matrix factorization.},
	author = {Chorowski, Jan and Zurada, Jacek M.},
	doi = {10.1109/TNNLS.2014.2310059},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chorowski, Zurada - 2015 - Learning understandable neural networks with nonnegative weight constraints.pdf:pdf},
	isbn = {2162-237X VO - 26},
	issn = {21622388},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Multilayer perceptron,pattern analysis,supervised learning,white-box models.},
	mendeley-groups = {Annotated/Word Vectors},
	number = {1},
	pages = {62--69},
	title = {{Learning understandable neural networks with nonnegative weight constraints}},
	volume = {26},
	year = {2015}
}
@article{Yeh2016,
	author = {Yeh, Chih-kuan and Wu, Wei-chieh and Ko, Wei-jen and Wang, Yu-chiang Frank},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeh et al. - 2016 - Learning Deep Latent Spaces for Multi-Label Classification.pdf:pdf},
	mendeley-groups = {Progress Report,Interim Review},
	title = {{Learning Deep Latent Spaces for Multi-Label Classification}},
	year = {2016}
}
@article{Ross2017,
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	archivePrefix = {arXiv},
	arxivId = {1703.03717},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	eprint = {1703.03717},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross, Hughes, Doshi-Velez - 2017 - Right for the Right Reasons Training Differentiable Models by Constraining their Explanations.pdf:pdf},
	mendeley-groups = {!Paper 3/task/newsgroups},
	title = {{Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations}},
	url = {http://arxiv.org/abs/1703.03717},
	year = {2017}
}
@article{Thiagarajan2016,
	abstract = {With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior [1]. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted.},
	archivePrefix = {arXiv},
	arxivId = {1611.07429},
	author = {Thiagarajan, Jayaraman J. and Kailkhura, Bhavya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan},
	eprint = {1611.07429},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiagarajan et al. - 2016 - TreeView Peeking into Deep Neural Networks Via Feature-Space Partitioning.pdf:pdf},
	journal = {30th Conference on Neural Information Processing Systems (NIPS 2016)},
	mendeley-groups = {Annotated/Explanations,Annotated/Decision Trees},
	number = {Nips},
	title = {{TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning}},
	url = {http://arxiv.org/abs/1611.07429},
	year = {2016}
}
@article{Lai2016,
	abstract = {We analyze three critical components of word embedding training: the model, the corpus, and the training parameters. We systematize existing neural-network-based word embedding algorithms and compare them using the same corpus. We evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks and using it to initialize neural networks. We also provide several simple guidelines for training word embeddings. First, we discover that corpus domain is more important than corpus size. We recommend choosing a corpus in a suitable domain for the desired task, after that, using a larger corpus yields better results. Second, we find that faster models provide sufficient performance in most cases, and more complex models can be used if the training corpus is sufficiently large. Third, the early stopping metric for iterating should rely on the development set of the desired task rather than the validation loss of training embedding.},
	archivePrefix = {arXiv},
	arxivId = {1507.05523},
	author = {Lai, Siwei and Liu, Kang and He, Shizhu and Zhao, Jun},
	doi = {10.1109/MIS.2016.45},
	eprint = {1507.05523},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 2016 - How to generate a good word embedding.pdf:pdf},
	issn = {15411672},
	journal = {IEEE Intelligent Systems},
	keywords = {distributed representation,intelligent systems,neural network,word embedding},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Large Movie Review,!Paper 3/task/Sentiment treebank},
	number = {6},
	pages = {5--14},
	title = {{How to generate a good word embedding}},
	volume = {31},
	year = {2016}
}
@article{Google,
	abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep re-current architecture that combines recent advances in com-puter vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target de-scription sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descrip-tions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1411.4555v2},
	author = {Google, Oriol Vinyals and Google, Alexander Toshev and Google, Samy Bengio and Google, Dumitru Erhan},
	doi = {10.1109/CVPR.2015.7298935},
	eprint = {arXiv:1411.4555v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Google et al. - Unknown - Show and Tell A Neural Image Caption Generator.pdf:pdf},
	isbn = {9781467369640},
	issn = {9781467369640},
	title = {{Show and Tell: A Neural Image Caption Generator}}
}
@article{Lastname2011a,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/feedback ch25.pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Volumec,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 4 - The Lizardmen Heroes.pdf:pdf},
	title = {{No Title}}
}
@article{Santos2015a,
	abstract = {Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1504.06580v2},
	author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
	eprint = {arXiv:1504.06580v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos, Xiang, Zhou - 2015 - Classifying Relations by Ranking with Convolutional Neural Networks.pdf:pdf},
	isbn = {9781941643723},
	journal = {Acl-2015},
	keywords = {Bing Xiang,Bowen Zhou,Cicero Nogueira dos Santos},
	mendeley-groups = {Progress Report},
	number = {3},
	pages = {626--634},
	title = {{Classifying Relations by Ranking with Convolutional Neural Networks}},
	url = {http://arxiv.org/pdf/1504.06580.pdf},
	year = {2015}
}
@article{Ding2014,
	author = {Ding, Shifei and Jia, Hongjie and Chen, Jinrong and Jin, Fengxiang},
	doi = {10.1007/s10462-012-9313-7},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2014 - Granular neural networks.pdf:pdf},
	isbn = {0269-2821},
	issn = {02692821},
	journal = {Artificial Intelligence Review},
	keywords = {Fuzzy neural networks,Granular neural networks,Rough neural networks},
	number = {3},
	pages = {373--384},
	title = {{Granular neural networks}},
	volume = {41},
	year = {2014}
}
@article{Galloway1982,
	abstract = {A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic similarity, pos-induction and word-analogy tasks.},
	author = {Galloway, Patricia},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/P15-1126.pdf:pdf},
	journal = {ALLC Journal},
	keywords = {*Diachronic Linguistics (di1),*French (fr2),*Poetry (pl2),*Statistical Analysis of Style (st3),5710: poetics/literary theory; poetics,article,cluster analysis, typology, Lai de l'Ombre manuscr},
	mendeley-groups = {11Thesis/Interpretability/Visual},
	number = {1},
	pages = {1--8},
	title = {{Clustering Variants in the Lai de l'Ombre Manuscripts: Techniques and Principles}},
	url = {http://search.proquest.com/docview/85463650?accountid=8330{\%}5Cnhttp://library.anu.edu.au:4550/resserv?genre=article{\&}issn={\&}title=ALLC+Journal{\&}volume=3{\&}issue=1{\&}date=1982-04-01{\&}atitle=Clustering+Variants+in+the+Lai+de+l'Ombre+Manuscripts:+Techniques+and+Princ},
	volume = {3},
	year = {1982}
}
@article{Miller1956,
	abstract = {First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Miller, George A.},
	doi = {10.1037/h0043158},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1956 - The magical number seven, plus or minus two some limits on our capacity for processing information.pdf:pdf},
	isbn = {0198568770;},
	issn = {1939-1471},
	journal = {Psychological Review},
	mendeley-groups = {Annotated/Psychology},
	number = {2},
	pages = {81--97},
	pmid = {8022966},
	title = {{The magical number seven, plus or minus two: some limits on our capacity for processing information.}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0043158},
	volume = {63},
	year = {1956}
}
@article{Garcez2014,
	abstract = {The goal of neural-symbolic computation is to integrate ro- bust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in par- ticular deep neural networks, forms of representation learn- ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
	author = {Garcez, Avila and Besold, Tarek R and Raedt, Luc De and F{\"{o}}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"{u}}hnberger, Kai-uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
	doi = {10.13140/2.1.1779.4243},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez et al. - 2014 - Neural-Symbolic Learning and Reasoning Contributions and Challenges(2).pdf:pdf},
	journal = {Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, Stanford},
	pages = {18--21},
	title = {{Neural-Symbolic Learning and Reasoning : Contributions and Challenges}},
	year = {2014}
}
@article{Sun1998,
	abstract = {This paper presents a novel learning model CLARION, which is a hybrid model based on the two-level approach proposed by Sun. The model integrates neural, reinforcement, and symbolic learning methods to perform on-line, bottom-up learning (i.e., learning that goes from neural to symbolic representations). The model utilizes both procedural and declarative knowledge (in neural and symbolic representations, respectively), tapping into the synergy of the two types of processes. It was applied to deal with sequential decision tasks. Experiments and analyzes in various ways are reported that shed light on the advantages of the model.},
	author = {Sun, Ron and Peterson, Todd},
	doi = {10.1109/72.728364},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Peterson - 1998 - Autonomous learning of sequential tasks Experiments and analyzes.pdf:pdf},
	isbn = {0364-0213},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Hybrid models,Markov decision process,Multistrategy learning,Navigation,Reinforcement learning,Rule extraction,Sequential decision making},
	number = {6},
	pages = {1217--1234},
	pmid = {18255804},
	title = {{Autonomous learning of sequential tasks: Experiments and analyzes}},
	volume = {9},
	year = {1998}
}
@article{Wu2013,
	abstract = {Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker's training data for synthesizing the converted speech. Both objective and subjective evaluations indicate that our proposed method outperforms JD-GMM and conventional unit selection methods. Copyright {\textcopyright} 2013 ISCA.},
	archivePrefix = {arXiv},
	arxivId = {1502.01710},
	author = {Wu, Zhizheng and Virtanen, Tuomas and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou},
	doi = {10.1063/1.4906785},
	eprint = {1502.01710},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2013 - character-level-convolutional-networks-for-text-classification.pdf:pdf},
	isbn = {0123456789},
	issn = {19909772},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	keywords = {Multi-frame exemplar,Temporal information,Unit selection,Voice conversion},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Yelp},
	pages = {3057--3061},
	pmid = {25246403},
	title = {character-level-convolutional-networks-for-text-classification},
	year = {2013}
}
@article{Bowman2015,
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	archivePrefix = {arXiv},
	arxivId = {1511.06349},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	eprint = {1511.06349},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:pdf},
	mendeley-groups = {Annotated/Generative Adversarial Nets},
	title = {{Generating Sentences from a Continuous Space}},
	url = {http://arxiv.org/abs/1511.06349},
	year = {2015}
}
@article{Higgins2017,
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	file = {:E$\backslash$:/9fcd6616f4c4e9985552865eb0d2383e4fd55a26.pdf:pdf},
	mendeley-groups = {11Thesis/Disentanglement},
	pages = {1--22},
	title = {{$\beta$ -VAE : L EARNING B ASIC V ISUAL C ONCEPTS WITH A C ONSTRAINED V ARIATIONAL F RAMEWORK}},
	year = {2017}
}
@article{Guarini2013,
	author = {Guarini, Marcello},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guarini - 2013 - Scholarship at UWindsor Case Classification , Similarities , Spaces of Reasons , and Coherences C ASE C LASSIFICATION ,.pdf:pdf},
	pages = {187--201},
	title = {{Scholarship at UWindsor Case Classification , Similarities , Spaces of Reasons , and Coherences C ASE C LASSIFICATION , S IMILARITIES ,}},
	year = {2013}
}
@article{Zhanga,
	author = {Zhang, Lei and Corporation, Linkedin},
	file = {:E$\backslash$:/1801.07883.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	title = {{Deep Learning for Sentiment Analysis : A Survey}}
}
@article{Escalante2018,
	author = {Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jacques, Julio and Bar{\'{o}}, Xavier and Ayache, Stephane and Viegas, Evelyne and G{\"{u}}{\c{c}}l{\"{u}}t{\"{u}}rk, Yağmur and G{\"{u}}{\c{c}}l{\"{u}}, Umut and Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jacques, Julio and Madadi, Meysam and Escalante, Hugo Jair and Guyon, Isabelle and Escalera, Sergio and Jr, Julio Jacques},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Escalante et al. - 2018 - Design of an Explainable Machine Learning Challenge for Video Interviews To cite this version HAL Id hal-016.pdf:pdf},
	mendeley-groups = {!Paper 3/Interpreting Vision},
	title = {{Design of an Explainable Machine Learning Challenge for Video Interviews To cite this version : HAL Id : hal-01668386 Design of an Explainable Machine Learning Challenge for Video Interviews}},
	year = {2018}
}
@article{Andrews2002,
	abstract = {This paper describes RULEX, a technique for providing an explanation component for local cluster (LC) neural networks. RULEX extracts symbolic rules from the weights of a trained LC net. LC nets are a special class of multilayer perceptrons that use sigmoid functions to generate localised functions. LC nets are well suited to both function approximation and discrete classification tasks. The restricted LC net is constrained in such a way that the local functions are 'axis parallel' thus facilitating rule extraction. This paper presents results for the LC net on a wide variety of benchmark problems and shows that RULEX produces comprehensible, accurate rules that exhibit a high degree of fidelity with the LC network from which they were extracted. ?? 2002 Elsevier Science B.V. All rights reserved.},
	author = {Andrews, Robert and Geva, Shloma},
	doi = {10.1016/S0925-2312(01)00577-X},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Geva - 2002 - Rule extraction from local cluster neural nets.pdf:pdf},
	issn = {09252312},
	journal = {Neurocomputing},
	keywords = {Knowledge extraction,Local response networks,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Progress Report,Report},
	number = {August 2002},
	pages = {1--20},
	title = {{Rule extraction from local cluster neural nets}},
	volume = {47},
	year = {2002}
}
@article{GethsiyalAugasta2012,
	abstract = {Artificial neural networks often achieve high classification accuracy$\backslash$nrates, but they are considered as black boxes due to their lack of$\backslash$nexplanation capability. This paper proposes the new rule extraction$\backslash$nalgorithm RxREN to overcome this drawback. In pedagogical approach the$\backslash$nproposed algorithm extracts the rules from trained neural networks for$\backslash$ndatasets with mixed mode attributes. The algorithm relies on reverse$\backslash$nengineering technique to prune the insignificant input neurons and to$\backslash$ndiscover the technological principles of each significant input neuron$\backslash$nof neural network in classification. The novelty of this algorithm lies$\backslash$nin the simplicity of the extracted rules and conditions in rule are$\backslash$ninvolving both discrete and continuous mode of attributes.$\backslash$nExperimentation using six different real datasets namely iris, wbc,$\backslash$nhepatitis, pid, ionosphere and creditg show that the proposed algorithm$\backslash$nis quite efficient in extracting smallest set of rules with high$\backslash$nclassification accuracy than those generated by other neural network$\backslash$nrule extraction methods.},
	author = {{Gethsiyal Augasta}, M. and Kathirvalavakumar, T.},
	doi = {10.1007/s11063-011-9207-8},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gethsiyal Augasta, Kathirvalavakumar - 2012 - Reverse engineering the neural networks for rule extraction in classification problems.pdf:pdf},
	issn = {13704621},
	journal = {Neural Processing Letters},
	keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Report/Rule-learning for neural networks,Report},
	number = {2},
	pages = {131--150},
	title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
	volume = {35},
	year = {2012}
}
@article{Barnes2017,
	abstract = {There has been a good amount of progress in sentiment analysis over the past 10 years, including the proposal of new methods and the creation of benchmark datasets. In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task. Accordingly, it is hard to understand how well a certain model generalizes across different tasks and datasets. In this paper, we contribute to this situation by comparing several models on six different benchmarks, which belong to different domains and additionally have different levels of granularity (binary, 3-class, 4-class and 5-class). We show that Bi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are particularly good at fine-grained sentiment tasks (i. e., with more than two classes). Incorporating sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the SenTube datasets.},
	archivePrefix = {arXiv},
	arxivId = {1709.04219},
	author = {Barnes, Jeremy and Klinger, Roman and im Walde, Sabine Schulte},
	eprint = {1709.04219},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnes, Klinger, Walde - 2017 - Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets.pdf:pdf},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Large Movie Review},
	pages = {2--12},
	title = {{Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets}},
	url = {http://arxiv.org/abs/1709.04219},
	year = {2017}
}
@article{Character,
	author = {Character, Translated and Psychic, Sheets},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 13 - The Paladin of the Holy Kingdom [Part 02] (Outdated) .pdf:pdf},
	title = {{Translation : Nigel}}
}
@article{Garrette2014,
	abstract = {First-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, weighted knowledge, for example regarding word meaning. This paper describes a mapping between predicates of logical form and points in a vector space. This mapping is then used to project distributional inferences to inference rules in logical form. We then describe first steps of an approach that uses this mapping to recast first-order semantics into the probabilistic models that are part of Statistical Relational AI. Specifically, we show how Discourse Representation Structures can be combined with distributional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as negation and factivity as well as weighted information on word meaning in context.},
	author = {Garrette, Dan and Erk, Katrin and Mooney, Raymond},
	doi = {10.1007/978-94-007-7284-7_3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garrette, Erk, Mooney - 2014 - A Formal Approach to Linking Logical Form and Vector-Space Lexical Semantics.pdf:pdf},
	isbn = {978-94-007-7283-0},
	journal = {Computing Meaning SE - 3},
	mendeley-groups = {Progress Report},
	pages = {27--48},
	title = {{A Formal Approach to Linking Logical Form and Vector-Space Lexical Semantics}},
	url = {http://link.springer.com/chapter/10.1007/978-94-007-7284-7{\_}3{\%}5Cnhttp://dx.doi.org/10.1007/978-94-007-7284-7{\_}3},
	volume = {47},
	year = {2014}
}
@article{Ager,
	author = {Ager, Thomas},
	file = {:E$\backslash$:/Downloads/Work/Ager.pdf:pdf},
	title = {{Neural Networks}}
}
@article{Read2015a,
	abstract = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
	archivePrefix = {arXiv},
	arxivId = {1503.09022},
	author = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
	eprint = {1503.09022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read, Hollm{\'{e}}n - 2015 - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
	keywords = {meta-labels,multi-label classification,neural net-,problem transformation},
	mendeley-groups = {Interim Review},
	pages = {1--23},
	title = {{Multi-label Classification using Labels as Hidden Nodes}},
	url = {http://arxiv.org/abs/1503.09022},
	year = {2015}
}
@article{Lake2015,
	abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	archivePrefix = {arXiv},
	arxivId = {10.1126/science.aab3050},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tnenbaum, Joshua B.},
	doi = {10.1126/science.aab3050},
	eprint = {science.aab3050},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tnenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
	isbn = {0036-8075},
	issn = {0036-8075},
	journal = {Science},
	mendeley-groups = {Progress Report,Interim Review},
	number = {6266},
	pages = {1332--1338},
	pmid = {26659050},
	primaryClass = {10.1126},
	title = {{Human-level concept learning through probabilistic program induction}},
	url = {https://www.sciencemag.org/content/350/6266/1332.full.pdf},
	volume = {350},
	year = {2015}
}
@article{Than,
	author = {Than, Khoat and Ho, Tu Bao},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Than, Ho - Unknown - Fully Sparse Topic ModelsPKDD 2012.pdf.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations,Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models},
	pages = {1--16},
	title = {{Fully Sparse Topic Models[PKDD 2012].pdf}}
}
@article{Wang,
	abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn sim-ilarity metric directly from images. It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sam-pling algorithm is proposed to learn the model with dis-tributed asynchronized stochastic gradient. Extensive ex-periments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
	archivePrefix = {arXiv},
	arxivId = {1404.4661},
	author = {Wang, Jiang and Song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jingbin and Philbin, James and Chen, Bo and Wu, Ying},
	doi = {10.1109/CVPR.2014.180},
	eprint = {1404.4661},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Learning Fine-grained Image Similarity with Deep Ranking.pdf:pdf},
	isbn = {9781479951178},
	issn = {10636919},
	mendeley-groups = {Report/Features,Progress Report},
	title = {{Learning Fine-grained Image Similarity with Deep Ranking}}
}
@article{Ager2018a,
	author = {Ager, Thomas},
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}Training{\_}Needs{\_}Assessment{\_}Form.pdf:pdf},
	pages = {3},
	title = {{PGR Review : Training Needs Assessment Training {\&} Support}},
	year = {2018}
}
@article{Zhu2014,
	abstract = {Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max- margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the “augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time efficiency. The classification performance is also improved over competitors on binary, multi- class and multi-label classification tasks.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1310.2816v1},
	author = {Zhu, Jun and Chen, Ning and Perkins, Hugh and Zhang, Bo},
	eprint = {arXiv:1310.2816v1},
	file = {:E$\backslash$:/Downloads/Work/zhu14a.pdf:pdf},
	isbn = {1532-4435},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {Gibbs classifiers,max-margin learning,regularized Bayesian inference,supervised topic models,support vector machines},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	pages = {1073--1110},
	title = {{Gibbs max-margin topic models with data augmentation}},
	volume = {15},
	year = {2014}
}
@article{Miller2017a,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.07269v1},
	author = {Miller, Tim},
	eprint = {arXiv:1706.07269v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 2017 - Explanation in Artificial Intelligence Insights from the Social Sciences.pdf:pdf},
	keywords = {explainability,explainable ai,explanation,interpretability,transparency},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/General},
	title = {{Explanation in Artificial Intelligence : Insights from the Social Sciences}},
	year = {2017}
}
@article{Amodei2016,
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	archivePrefix = {arXiv},
	arxivId = {1606.06565},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
	doi = {1606.06565},
	eprint = {1606.06565},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
	isbn = {0387310738},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability/Safety},
	pages = {1--29},
	title = {{Concrete Problems in AI Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	year = {2016}
}
@article{VanLinh2017,
	abstract = {{\textcopyright} 2016, Springer-Verlag London.As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises-Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental results showed the advantages of our approach in terms of separability, interpretability and effectiveness in classification task of datasets with high dimension and complex distribution. Our method is highly competitive with state-of-the-art approaches.},
	author = {{Van Linh}, Ngo and Anh, Nguyen Kim and Than, Khoat and Dang, Chien Nguyen},
	doi = {10.1007/s10115-016-0956-6},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Linh et al. - 2017 - An effective and interpretable method for document classification.pdf:pdf},
	issn = {02193116},
	journal = {Knowledge and Information Systems},
	keywords = {Bayesian nonparametrics,Classification,Variational inference,Von Mises-Fisher distribution},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Representations},
	number = {3},
	pages = {763--793},
	title = {{An effective and interpretable method for document classification}},
	volume = {50},
	year = {2017}
}
@article{Li2014,
	abstract = {Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the '' forced topic'' problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.},
	author = {Li, Ximing and Ouyang, Jihong and Lu, You and Zhou, Xiaotang and Tian, Tian},
	doi = {10.1007/s10791-014-9244-9},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - Group topic model organizing topics into groups.pdf:pdf},
	issn = {15737659},
	journal = {Information Retrieval},
	keywords = {Document clustering,Group,Latent Dirichlet allocation,Online learning,Topic modeling,Variational inference},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
	number = {1},
	pages = {1--25},
	title = {{Group topic model: organizing topics into groups}},
	volume = {18},
	year = {2014}
}
@article{Maire1999a,
	abstract = {The core problem of rule-extraction from feed-forward networks is an inversion problem. In this article, we solve this inversion problem by backpropagating unions of polyhedra. We obtain as a by-product a new rule-extraction technique for which the fidelity of the extracted rules can be made arbitrarily high.},
	author = {Maire, F.},
	doi = {10.1016/S0893-6080(99)00013-1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maire - 1999 - Rule-extraction by backpropagation of polyhedra.pdf:pdf},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Artificial neural network,Inversion,Polyhedron,Rule-extraction},
	number = {4-5},
	pages = {717--725},
	pmid = {12662679},
	title = {{Rule-extraction by backpropagation of polyhedra}},
	volume = {12},
	year = {1999}
}
@article{Setiono2009,
	abstract = {We address an important issue in knowledge discovery using neural networks that has been left out in a recent article "Knowledge discovery using a neural network simultaneous optimization algorithm on a real world classification problem" by Sexton et al. [R.S. Sexton, S. McMurtrey, D.J. Cleavenger, Knowledge discovery using a neural network simultaneous optimization algorithm on a real world classification problem, European Journal of Operational Research 168 (2006) 1009-1018]. This important issue is the generation of comprehensible rule sets from trained neural networks. In this note, we present our neural network rule extraction algorithm that is very effective in discovering knowledge embedded in a neural network. This algorithm is particularly appropriate in applications where comprehensibility as well as accuracy are required. For the same data sets used by Sexton et al. our algorithm produces accurate rule sets that are concise and comprehensible, and hence helps validate the claim that neural networks could be viable alternatives to other data mining tools for knowledge discovery. ?? 2007 Elsevier B.V. All rights reserved.},
	author = {Setiono, Rudy and Baesens, Bart and Mues, Christophe},
	doi = {10.1016/j.ejor.2007.09.022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Setiono, Baesens, Mues - 2009 - A note on knowledge discovery using neural networks and its application to credit card screening.pdf:pdf},
	isbn = {0377-2217},
	issn = {03772217},
	journal = {European Journal of Operational Research},
	keywords = {Credit screening,Knowledge discovery,Neural networks,Rule extraction},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	number = {1},
	pages = {326--332},
	title = {{A note on knowledge discovery using neural networks and its application to credit card screening}},
	volume = {192},
	year = {2009}
}
@article{Ho2006,
	abstract = {An accurate classifier with linguistic interpretability using a small number of relevant genes is beneficial to microarray data analysis and development of inexpensive diagnostic tests. Several frequently used techniques for designing classifiers of microarray data, such as support vector machine, neural networks, k-nearest neighbor, and logistic regression model, suffer from low interpretabilities. This paper proposes an interpretable gene expression classifier (named iGEC) with an accurate and compact fuzzy rule base for microarray data analysis. The design of iGEC has three objectives to be simultaneously optimized: maximal classification accuracy, minimal number of rules, and minimal number of used genes. An "intelligent" genetic algorithm IGA is used to efficiently solve the design problem with a large number of tuning parameters. The performance of iGEC is evaluated using eight commonly-used data sets. It is shown that iGEC has an accurate, concise, and interpretable rule base (1.1 rules per class) on average in terms of test classification accuracy (87.9{\%}), rule number (3.9), and used gene number (5.0). Moreover, iGEC not only has better performance than the existing fuzzy rule-based classifier in terms of the above-mentioned objectives, but also is more accurate than some existing non-rule-based classifiers. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
	author = {Ho, Shinn Ying and Hsieh, Chih Hung and Chen, Hung Ming and Huang, Hui Ling},
	doi = {10.1016/j.biosystems.2006.01.002},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ho et al. - 2006 - Interpretable gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis.pdf:pdf},
	issn = {03032647},
	journal = {BioSystems},
	keywords = {Fuzzy classifier,Gene expression,Intelligent genetic algorithm,Microarray data analysis,Pattern recognition},
	mendeley-groups = {Report},
	number = {3},
	pages = {165--176},
	pmid = {16490299},
	title = {{Interpretable gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis}},
	volume = {85},
	year = {2006}
}
@article{Mikolov2013,
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	archivePrefix = {arXiv},
	arxivId = {1310.4546},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	doi = {10.1162/jmlr.2003.3.4-5.951},
	eprint = {1310.4546},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
	isbn = {2150-8097},
	issn = {10495258},
	journal={arXiv preprint arXiv:1310.4546},
	year={2013}
	mendeley-groups = {Annotated/Word Vectors,Interim Review},
	pages = {1--9},
	pmid = {903},
	title = {{Distributed Representations of Words and Phrases and their Compositionality}}
}
@article{Guo2014,
	abstract = {Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presented approaches can be integrated into most of the classical linear models in NLP. Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features, among which the distributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score},
	author = {Guo, Jiang and Che, Wanxiang and Wang, Haifeng and Liu, Ting},
	isbn = {9781937284961},
	journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	mendeley-groups = {Report/Features},
	number = {2005},
	pages = {110--120},
	pmid = {1685741},
	title = {{Revisiting Embedding Features for Simple Semi-supervised Learning}},
	url = {http://www.aclweb.org/anthology/D14-1012.pdf},
	year = {2014}
}
@article{Towell1990a,
	author = {Towell, Geoffrey G and Shavlik, Jude W and Noordeweir, Michiel O},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Towell, Shavlik, Noordeweir - 1990 - Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks.pdf:pdf},
	journal = {Proceedings of the Eighth National Conference on Artificial Intelligence},
	pages = {861--866},
	title = {{Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks}},
	year = {1990}
}
@misc{,
	file = {:E$\backslash$:/Downloads/Work/RambachP1979{\_}AjiKan.pdf:pdf},
	title = {{RambachP1979{\_}AjiKan.pdf}}
}
@article{Oquab2014,
	abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large-scale visual recognition challenge (ILSVRC2012). The suc-cess of CNNs is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification meth-ods. Learning CNNs, however, amounts to estimating mil-lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi-ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep-resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
	author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
	doi = {10.1109/CVPR.2014.222},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab et al. - 2014 - Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks.pdf:pdf},
	isbn = {9781479951178},
	issn = {10636919},
	journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1717--1724},
	title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
	year = {2014}
}
@article{Bastani2017,
	abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
	archivePrefix = {arXiv},
	arxivId = {1706.09773},
	author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
	eprint = {1706.09773},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim, Bastani - 2017 - Interpretability via Model Extraction.pdf:pdf},
	keywords = {()},
	mendeley-groups = {Annotated/Fairness},
	title = {{Interpretability via Model Extraction}},
	url = {http://arxiv.org/abs/1706.09773},
	year = {2017}
}
@article{Ekstrand2014,
	author = {Ekstrand, Michael D and Harper, F Maxwell and Willemsen, Martijn C and Konstan, Joseph A},
	file = {:E$\backslash$:/PhD/Papedrs/listcmp.pdf:pdf},
	isbn = {9781450326681},
	keywords = {12,21,algorithms with comparable accuracy,movie recommendation domain,of,recommender systems,those differences in the,to map out some,user study,we present},
	mendeley-groups = {11Thesis/Interpretability/General},
	title = {{User Perception of Differences in Recommender Algorithms}},
	year = {2014}
}
@article{Dong2017a,
	abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
	archivePrefix = {arXiv},
	arxivId = {1703.04096},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
	doi = {10.1109/CVPR.2017.110},
	eprint = {1703.04096},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2017 - Improving Interpretability of Deep Neural Networks with Semantic Information(2).pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {1063-6919},
	mendeley-groups = {!Paper 3/task/Interpretable LSTMs},
	pages = {4306--4314},
	title = {{Improving Interpretability of Deep Neural Networks with Semantic Information}},
	url = {http://arxiv.org/abs/1703.04096},
	year = {2017}
}
@article{Zhang2010,
	author = {Zhang, Yin and Jin, Rong},
	doi = {10.1007/s13042-010-0001-0},
	file = {:E$\backslash$:/Downloads/Work/Understanding{\_}bag-of-words{\_}model{\_}A{\_}statistical{\_}fra.pdf:pdf},
	number = {December},
	pages = {0--16},
	title = {{Understanding bag-of-words model : A statistical framework Understanding Bag-of-Words Model : A Statistical Framework}},
	year = {2010}
}
@article{Bouchacourt2014,
	author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
	file = {:E$\backslash$:/16521-76678-1-PB.pdf:pdf},
	keywords = {Machine Learning Applications Track},
	number = {iid},
	pages = {2095--2102},
	title = {{Multi-Level Variational Autoencoder : Learning Disentangled Representations from Grouped Observations}},
	year = {2014}
}
@article{Gong2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1809.06858v1},
	author = {Gong, Chengyue},
	eprint = {arXiv:1809.06858v1},
	file = {:E$\backslash$:/1809.06858.pdf:pdf},
	mendeley-groups = {11Thesis/State of the art},
	number = {Nips},
	pages = {1--15},
	title = {{FRAGE : Frequency-Agnostic Word Representation}},
	volume = {1},
	year = {2018}
}
@article{FenTan2016,
	abstract = {Ensembles of decision trees have good prediction accuracy but suffer from a lack of interpretability. We propose a new approach for interpreting tree ensembles by finding prototypes in tree space, utilizing the naturally-learned similarity measure from the tree ensemble. Demonstrating the method on random forests, we show that the method benefits from two unique aspects of tree ensembles by leveraging tree structure to sequentially find prototypes, and utilizing the naturally-learned similarity measure from the tree ensemble. The method provides good prediction accuracy when found prototypes are used in nearest-prototype classifiers, while us-ing fewer prototypes than competitor methods. We are investigating the sensitivity of the method to different prototype-finding procedures and demonstrating it on higher-dimensional data.},
	archivePrefix = {arXiv},
	arxivId = {1611.07115},
	author = {{Fen Tan}, Hui and Hooker, Giles J and Wells, Martin T},
	eprint = {1611.07115},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fen Tan, Hooker, Wells - 2016 - Tree Space Prototypes Another Look at Making Tree Ensembles Interpretable.pdf:pdf},
	journal = {Nips},
	mendeley-groups = {Annotated/Decision Trees,!Paper 3/task/newsgroups,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	title = {{Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable}},
	year = {2016}
}
@article{Funahashi1989,
	author = {Funahashi, K.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Funahashi - 1989 - On the Approximate Realisation of Continuous Mappings by Neural Networks.pdf:pdf},
	journal = {Neural Networks},
	keywords = {--neural network,alization,back propagation,continuous mapping,hidden layer,output function,re-,sigmoid function,unit},
	number = {3},
	pages = {183--192},
	title = {{On the Approximate Realisation of Continuous Mappings by Neural Networks}},
	volume = {2},
	year = {1989}
}
@article{Giatsoglou2017,
	abstract = {Sentiment analysis and opinion mining are valuable for extraction of useful subjective information out of text documents. These tasks have become of great importance, especially for business and marketing professionals, since online posted products and services reviews impact markets and consumers shifts. This work is motivated by the fact that automating retrieval and detection of sentiments expressed for certain products and services embeds complex processes and pose research challenges, due to the textual phenomena and the language specific expression variations. This paper proposes a fast, flexible, generic methodology for sentiment detection out of textual snippets which express people's opinions in different languages. The proposed methodology adopts a machine learning approach with which textual documents are represented by vectors and are used for training a polarity classification model. Several documents' vector representation approaches have been studied, including lexicon-based, word embedding-based and hybrid vectorizations. The competence of these feature representations for the sentiment classification task is assessed through experiments on four datasets containing online user reviews in both Greek and English languages, in order to represent high and weak inflection language groups. The proposed methodology requires minimal computational resources, thus, it might have impact in real world scenarios where limited resources is the case.},
	author = {Giatsoglou, Maria and Vozalis, Manolis G. and Diamantaras, Konstantinos and Vakali, Athena and Sarigiannidis, George and Chatzisavvas, Konstantinos Ch},
	doi = {10.1016/j.eswa.2016.10.043},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giatsoglou et al. - 2017 - Sentiment analysis leveraging emotions and word embeddings.pdf:pdf},
	isbn = {0957-4174},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Hybrid vectorization,Machine learning,Multilingual sentiment analysis,Online user reviews,Text analysis,Vector representation},
	mendeley-groups = {!Paper 3/task,!Paper 3/task/Sentiment treebank},
	pages = {214--224},
	publisher = {Elsevier Ltd},
	title = {{Sentiment analysis leveraging emotions and word embeddings}},
	url = {http://dx.doi.org/10.1016/j.eswa.2016.10.043},
	volume = {69},
	year = {2017}
}
@article{Sun2016,
	abstract = {Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-the-art document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.},
	archivePrefix = {arXiv},
	arxivId = {1603.07603},
	author = {Sun, Fei and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Cheng, Xueqi},
	eprint = {1603.07603},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2016 - Semantic Regularities in Document Representations.pdf:pdf},
	mendeley-groups = {Annotated/Interpretable representations},
	title = {{Semantic Regularities in Document Representations}},
	url = {http://arxiv.org/abs/1603.07603},
	year = {2016}
}
@book{,
	file = {:E$\backslash$:/Downloads/Work/Methods of Multivariate Analysis 3rd Ed.pdf:pdf},
	isbn = {9780470178966},
	title = {{METHODS OF MULTIVARIATE ANALYSIS}}
}
@article{G.Towell1993a,
	author = {{G. Towell}, Geoffrey and {W. Shavlik}, Jude},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G. Towell, W. Shavlik - 1993 - Interpretation of artificial neural networks Mapping knowledge based neural networks into rules.pdf:pdf},
	journal = {Advances in Neural Information Processing Systems},
	pages = {977--984},
	title = {{Interpretation of artificial neural networks: Mapping knowledge based neural networks into rules}},
	volume = {5},
	year = {1993}
}
@article{Li2016,
	abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
	archivePrefix = {arXiv},
	arxivId = {1612.08220},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	eprint = {1612.08220},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Monroe, Jurafsky - 2016 - Understanding Neural Networks through Representation Erasure.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,!Paper 3/Explaining LSTMs,!Paper 3,!Paper 3/task/Sentiment treebank},
	title = {{Understanding Neural Networks through Representation Erasure}},
	url = {http://arxiv.org/abs/1612.08220},
	year = {2016}
}
@article{Chen2014c,
	abstract = {We have provided a model and framework as a foundation for transparent interfaces via our Situation Awareness-based Agent Transparency (SAT) model. In this report we discuss the implications of agent transparency for operator trust and workload; we also review potential user interface designs (information visualization and displaying uncertainty information) to support agent transparency. Finally, we provide examples of transparent interface design efforts currently ongoing at the U.S. Army Research Laboratory's Human Research and Engineering Directorate under the Autonomy Research Pilot Initiative.},
	author = {Chen, Jessie Y. C. and Procci, Katelyn and Boyce, Michael and Wright, Julia and Garcia, Andre and Barnes, Michael J.},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2014 - Situation Awareness-Based Agent Transparency.pdf:pdf},
	isbn = {ARL-TR-6905},
	journal = {US Army Research Laboratory},
	keywords = {autonomous systems,human-robot interaction,situation awareness (SA),transparency,trust},
	mendeley-groups = {Annotated/Overarching Interpretability},
	number = {April},
	pages = {1--29},
	title = {{Situation Awareness-Based Agent Transparency}},
	year = {2014}
}
@article{Zhu2016,
	abstract = {Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference.},
	archivePrefix = {arXiv},
	arxivId = {1602.07428},
	author = {Zhu, Jun and Song, Jiaming and Chen, Bei},
	doi = {10.1.1.160.2072},
	eprint = {1602.07428},
	file = {:E$\backslash$:/Downloads/Work/1602.07428.pdf:pdf},
	isbn = {978-1-4503-1285-1},
	issn = {13669516},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Supervised Topic Models},
	number = {1},
	title = {{Max-Margin Nonparametric Latent Feature Models for Link Prediction}},
	url = {http://arxiv.org/abs/1602.07428},
	volume = {6},
	year = {2016}
}
@article{Grosenick2008,
	author = {Grosenick, Logan and Greer, Stephanie and Knutson, Brian},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grosenick, Greer, Knutson - 2008 - Interpretable classi ers for FMRI improve prediction of purchases.pdf:pdf},
	journal = {Analysis},
	mendeley-groups = {Report},
	number = {Xx},
	pages = {1--10},
	title = {{Interpretable classi ers for FMRI improve prediction of purchases}},
	volume = {X},
	year = {2008}
}
@article{Wu,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1609.08144v2},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	eprint = {arXiv:1609.08144v2},
	file = {:E$\backslash$:/1609.08144.pdf (7.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	pages = {1--23},
	title = {{Google ' s Neural Machine Translation System : Bridging the Gap between Human and Machine Translation}}
}
@article{Zhou2004,
	abstract = {In the research of rule extraction from neural networks, fidelity describes how well the rules mimic the behavior of a neural network while accuracy describes how well the rules can be generalized. This paper identifies the fidelity-accuracy dilemma. It argues to distinguish rule extraction using neural networks and rule extraction for neural networks according to their differing goals, where fidelity and accuracy should be excluded from the rule quality evaluation framework, respectively.},
	author = {Zhou, Zhi-Hua},
	doi = {10.1007/BF02944803},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou - 2004 - Rule Extraction Using Neural Networks or for Neural Networks ×.pdf:pdf},
	issn = {1000-9000},
	journal = {J. Comput. Sci. {\&} Technol.},
	keywords = {accuracy,fidelity,machine learning,neural networks,rule extraction},
	pages = {249--253},
	title = {{Rule Extraction: Using Neural Networks or for Neural Networks? ×}},
	url = {http://ac.els-cdn.com/S1877750313000185/1-s2.0-S1877750313000185-main.pdf?{\_}tid=f08c3f6c-e35d-11e4-9abc-00000aacb35f{\&}acdnat=1429095513{\_}7903f6ce12ea46c9f359f585fc91609a},
	volume = {19},
	year = {2004}
}
@article{Saad2007a,
	abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., {\&} Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373-389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e. calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity-complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
	author = {Saad, Emad W. and Wunsch, Donald C.},
	doi = {10.1016/j.neunet.2006.07.005},
	isbn = {0893-6080},
	issn = {08936080},
	journal = {Neural Networks},
	keywords = {Evolutionary algorithm,Explanation capability of neural networks,Hyperplanes,Inversion,Neural network explanation,Pedagogical,Rule extraction},
	number = {1},
	pages = {78--93},
	pmid = {17029713},
	title = {{Neural network explanation using inversion}},
	volume = {20},
	year = {2007}
}
@article{Veale2017,
	abstract = {Machine learning systems are increasingly used to sup-port public sector decision-making across a variety of sectors. Given concerns around accountability in these domains, and amidst accusations of intentional or unintentional bias, there have been increased calls for transparency of these technologies. Few, however, have considered how logics and practices concerning transparency have been understood by those involved in the machine learning systems already being piloted and deployed in public bodies today. This short pa-per distils insights about transparency on the ground from interviews with 27 such actors, largely public ser-vants and relevant contractors, across 5 OECD coun-tries. Considering transparency and opacity in relation to trust and buy-in, better decision-making, and the avoidance of gaming, it seeks to provide useful insights for those hoping to develop socio-technical approaches to transparency that might be useful to practitioners on-the-ground.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1706.09249v2},
	author = {Veale, Michael},
	eprint = {arXiv:1706.09249v2},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veale - 2017 - Logics and practices of transparency and opacity in real-world applications of public sector machine learning.pdf:pdf},
	keywords = {()},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{Logics and practices of transparency and opacity in real-world applications of public sector machine learning}},
	year = {2017}
}
@article{Karwath2002,
	abstract = {BACKGROUND: The inference of homology between proteins is a key problem in molecular biology The current best approaches only identify approximately 50{\%} of homologies (with a false positive rate set at 1/1000).$\backslash$n$\backslash$nRESULTS: We present Homology Induction (HI), a new approach to inferring homology. HI uses machine learning to bootstrap from standard sequence similarity search methods. First a standard method is run, then HI learns rules which are true for sequences of high similarity to the target (assumed homologues) and not true for general sequences, these rules are then used to discriminate sequences in the twilight zone. To learn the rules HI describes the sequences in a novel way based on a bioinformatic knowledge base, and the machine learning method of inductive logic programming. To evaluate HI we used the PDB40D benchmark which lists sequences of known homology but low sequence similarity. We compared the HI methodology with PSI-BLAST alone and found HI performed significantly better. In addition, Receiver Operating Characteristic (ROC) curve analysis showed that these improvements were robust for all reasonable error costs. The predictive homology rules learnt by HI by can be interpreted biologically to provide insight into conserved features of homologous protein families.$\backslash$n$\backslash$nCONCLUSIONS: HI is a new technique for the detection of remote protein homology--a central bioinformatic problem. HI with PSI-BLAST is shown to outperform PSI-BLAST for all error costs. It is expect that similar improvements would be obtained using HI with any sequence similarity method.},
	author = {Karwath, Andreas and King, Ross D},
	doi = {10.1186/1471-2105-3-11},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karwath, King - 2002 - Homology induction the use of machine learning to improve sequence similarity searches.pdf:pdf},
	isbn = {10.1186/1471-2105-3-11},
	issn = {1471-2105},
	journal = {BMC bioinformatics},
	keywords = {Algorithms,Animals,Artificial Intelligence,Computational Biology,Computational Biology: methods,Databases, Protein,Fungal Proteins,Fungal Proteins: genetics,Internet,Mice,Oryza sativa,Plant Proteins,Plant Proteins: genetics,Predictive Value of Tests,Retroviridae Proteins,Retroviridae Proteins: genetics,Sensitivity and Specificity,Sequence Alignment,Sequence Alignment: methods,Sequence Homology, Amino Acid},
	mendeley-groups = {Report/Biologicla domain},
	number = {1},
	pages = {11},
	pmid = {11972320},
	title = {{Homology induction: the use of machine learning to improve sequence similarity searches.}},
	url = {http://www.biomedcentral.com/1471-2105/3/11},
	volume = {3},
	year = {2002}
}
@article{Bai2018,
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.},
	archivePrefix = {arXiv},
	arxivId = {1803.01271},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	eprint = {1803.01271},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Kolter, Koltun - 2018 - An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	year = {2018}
}
@article{Glorot2011,
	author = {Glorot, X and Bordes, a and Bengio, Y},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain adaptation for large-scale sentiment classification A deep learning approach(2).pdf:pdf},
	keywords = {auto-encoders,deep-learning},
	mendeley-groups = {Literature Review,Progress Report},
	number = {1},
	title = {{Domain adaptation for large-scale sentiment classification: A deep learning approach}},
	url = {http://eprints.pascal-network.org/archive/00008597/},
	year = {2011}
}
@article{Thomas2018,
	author = {Thomas, Student},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas - 2018 - Interim Report Student Details October 2015 October 2018 Section A Student Self-Assessment A . 1 Thesis Title and Hypot.pdf:pdf},
	title = {{Interim Report Student Details October 2015 October 2018 Section A : Student Self-Assessment A . 1 Thesis Title and Hypothesis Fine-tuning vector spaces by improving directions A . 2 Overall Progress}},
	year = {2018}
}
@article{Seera2014a,
	abstract = {In this paper, a hybrid intelligent system that consists of the Fuzzy Min-Max neural network, the Classification and Regression Tree, and the Random Forest model is proposed, and its efficacy as a decision support tool for medical data classification is examined. The hybrid intelligent system aims to exploit the advantages of the constituent models and, at the same time, alleviate their limitations. It is able to learn incrementally from data samples (owing to Fuzzy Min-Max neural network), explain its predicted outputs (owing to the Classification and Regression Tree), and achieve high classification performances (owing to Random Forest). To evaluate the effectiveness of the hybrid intelligent system, three benchmark medical data sets, viz., Breast Cancer Wisconsin, Pima Indians Diabetes, and Liver Disorders from the UCI Repository of Machine Learning, are used for evaluation. A number of useful performance metrics in medical applications which include accuracy, sensitivity, specificity, as well as the area under the Receiver Operating Characteristic curve are computed. The results are analyzed and compared with those from other methods published in the literature. The experimental outcomes positively demonstrate that the hybrid intelligent system is effective in undertaking medical data classification tasks. More importantly, the hybrid intelligent system not only is able to produce good results but also to elucidate its knowledge base with a decision tree. As a result, domain users (i.e., medical practitioners) are able to comprehend the prediction given by the hybrid intelligent system; hence accepting its role as a useful medical decision support tool. ?? 2013 Elsevier Ltd. All rights reserved.},
	author = {Seera, Manjeevan and Lim, Chee Peng},
	doi = {10.1016/j.eswa.2013.09.022},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seera, Lim - 2014 - A hybrid intelligent system for medical data classification.pdf:pdf},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Classification and regression tree,Fuzzy Min-Max neural network,Hybrid intelligent systems,Medical decision support,Random forest},
	mendeley-groups = {Annotated/Decision Trees},
	number = {5},
	pages = {2239--2249},
	publisher = {Elsevier Ltd},
	title = {{A hybrid intelligent system for medical data classification}},
	url = {http://dx.doi.org/10.1016/j.eswa.2013.09.022},
	volume = {41},
	year = {2014}
}
@article{Systems,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1610.01256v2},
	author = {Systems, Cyber-physical and Products, Data},
	eprint = {arXiv:1610.01256v2},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/1610.01256.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Safety},
	pages = {1--20},
	title = {{On the Safety of Machine Learning : Cyber-Physical Systems , Decision Sciences , and Data Products}}
}
@article{Goodman2016,
	abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
	archivePrefix = {arXiv},
	arxivId = {1606.08813},
	author = {Goodman, Bryce and Flaxman, Seth},
	eprint = {1606.08813},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
	isbn = {978-0-674-36827-9},
	journal = {2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)},
	keywords = {machine learning},
	mendeley-groups = {Annotated/Overarching Interpretability,!Paper 3/Justifying Interpretability,11Thesis/Interpretability,11Thesis/Interpretability/Explanation},
	number = {Whi},
	pages = {26--30},
	title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
	url = {http://arxiv.org/abs/1606.08813},
	year = {2016}
}
@article{Selvaraju2016,
	abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
	archivePrefix = {arXiv},
	arxivId = {1611.07450},
	author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
	eprint = {1611.07450},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
	mendeley-groups = {Report/Explaining predictions,Annotated/Explanations},
	pages = {1--4},
	title = {{Grad-CAM: Why did you say that?}},
	url = {http://arxiv.org/abs/1611.07450},
	year = {2016}
}
@article{Vilnis2015,
	abstract = {Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.},
	archivePrefix = {arXiv},
	arxivId = {1412.6623},
	author = {Vilnis, Luke and McCallum, Andrew},
	eprint = {1412.6623},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vilnis, McCallum - 2015 - Word Representations via Gaussian Embedding.pdf:pdf},
	journal = {Iclr},
	mendeley-groups = {Progress Report},
	pages = {12},
	title = {{Word Representations via Gaussian Embedding}},
	url = {http://arxiv.org/abs/1412.6623},
	year = {2015}
}
@article{Jameel,
  title={Member: Max-margin based embeddings for entity retrieval},
author={Jameel, Shoaib and Bouraoui, Zied and Schockaert, Steven},
booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages={783--792},
year={2017}
}
@article{Hu2016,
	abstract = {Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.},
	archivePrefix = {arXiv},
	arxivId = {1603.06318},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	eprint = {1603.06318},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2016 - Harnessing Deep Neural Networks with Logic Rules.pdf:pdf},
	journal = {arXiv preprint},
	mendeley-groups = {Papers/Paper 1,Literature Review,Annotated/Rule-based classiifers,Report},
	pages = {1--18},
	title = {{Harnessing Deep Neural Networks with Logic Rules}},
	url = {http://arxiv.org/abs/1603.06318},
	year = {2016}
}
@article{Oquab2015,
	author = {Oquab, Maxime},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab - 2015 - Is object localization for free - Weakly-supervised learning with convolutional neural networks To cite this version Is.pdf:pdf},
	isbn = {9781467369640},
	mendeley-groups = {Progress Report},
	number = {iii},
	title = {{Is object localization for free ? - Weakly-supervised learning with convolutional neural networks To cite this version : Is object localization for free ? - Weakly-supervised learning with convolutional neural networks}},
	year = {2015}
}
@article{Emnlp-ijcnlp,
	author = {Emnlp-ijcnlp, Anonymous},
	file = {:E$\backslash$:/CoNLL{\_}facets-2.pdf:pdf},
	mendeley-groups = {11Thesis/Rana},
	pages = {1--9},
	title = {{No Title}}
}
@article{Conllc,
	author = {Conll, Anonymous},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (4).pdf:pdf},
	pages = {1--10},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}}
}
@article{Narayanan2018,
	abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
	archivePrefix = {arXiv},
	arxivId = {1802.00682},
	author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
	eprint = {1802.00682},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine Learning Systems An Evaluation of the Human-Interpretabilit.pdf:pdf},
	mendeley-groups = {!Paper 3,11Thesis/Interpretability,11Thesis/Interpretability/Explanation,11Thesis/Interpretability/General},
	pages = {1--21},
	title = {{How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation}},
	url = {http://arxiv.org/abs/1802.00682},
	year = {2018}
}
@article{Pfenning2001,
	abstract = {We reconsider the foundations of modal logic, following Martin-L{\"{o}}f's methodology of distinguishing judgments from propositions. We give constructive meaning explanations for necessity and possibility, which yields a simple and uniform system of natural deduction for intuitionistic modal logic that does not exhibit anomalies found in other proposals. We also give a new presentation of lax logic and find that the lax modality is already expressible using possibility and necessity. Through a computational interpretation of proofs in modal logic we further obtain a new formulation of Moggi's monadic metalanguage.$\backslash$n},
	author = {Pfenning, Frank and Davies, Rowan},
	doi = {10.1017/S0960129501003322},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfenning, Davies - 2001 - A judgmental reconstruction of modal logic.pdf:pdf},
	isbn = {0960129501003},
	issn = {0960-1295},
	journal = {Mathematical Structures in Computer Science},
	mendeley-groups = {Progress Report},
	number = {4},
	pages = {511--540},
	title = {{A judgmental reconstruction of modal logic}},
	url = {http://www.journals.cambridge.org/abstract{\_}S0960129501003322},
	volume = {11},
	year = {2001}
}
@article{Verbeke2011,
	abstract = {Customer churn prediction models aim to detect customers with a high propensity to attrite. Predictive accuracy, comprehensibility, and justifiability are three key aspects of a churn prediction model. An accurate model permits to correctly target future churners in a retention marketing campaign, while a comprehensible and intuitive rule-set allows to identify the main drivers for customers to churn, and to develop an effective retention strategy in accordance with domain knowledge. This paper provides an extended overview of the literature on the use of data mining in customer churn prediction modeling. It is shown that only limited attention has been paid to the comprehensibility and the intuitiveness of churn prediction models. Therefore, two novel data mining techniques are applied to churn prediction modeling, and benchmarked to traditional rule induction techniques such as C4.5 and RIPPER. Both AntMiner+ and ALBA are shown to induce accurate as well as comprehensible classification rule-sets. AntMiner+ is a high performing data mining technique based on the principles of Ant Colony Optimization that allows to include domain knowledge by imposing monotonicity constraints on the final rule-set. ALBA on the other hand combines the high predictive accuracy of a non-linear support vector machine model with the comprehensibility of the rule-set format. The results of the benchmarking experiments show that ALBA improves learning of classification techniques, resulting in comprehensible models with increased performance. AntMiner+ results in accurate, comprehensible, but most importantly justifiable models, unlike the other modeling techniques included in this study. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
	author = {Verbeke, Wouter and Martens, David and Mues, Christophe and Baesens, Bart},
	doi = {10.1016/j.eswa.2010.08.023},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbeke et al. - 2011 - Building comprehensible customer churn prediction models with advanced rule induction techniques.pdf:pdf},
	isbn = {0957-4174},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {ALBA,Ant Colony Optimization,Churn prediction,Classification,Comprehensible rule induction,Data mining},
	mendeley-groups = {Annotated/Applications/Marketing},
	number = {3},
	pages = {2354--2364},
	publisher = {Elsevier Ltd},
	title = {{Building comprehensible customer churn prediction models with advanced rule induction techniques}},
	url = {http://dx.doi.org/10.1016/j.eswa.2010.08.023},
	volume = {38},
	year = {2011}
}
@article{Agera,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders.pdf:pdf},
	title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Invaders,
	author = {Invaders, The and Tomb, Large},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 7 - The Invaders of the Large Tomb.pdf:pdf},
	title = {{No Title}}
}
@misc{,
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Zhou- Extracting symbolic rules from trained neural.url:url},
	title = {{Zhou- Extracting symbolic rules from trained neural}}
}
@article{Johnson2015,
	abstract = {Convolutional neural network (CNN) is a neu-ral network that can make use of the inter-nal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embed-ding of small text regions for use in classifi-cation. In addition to a straightforward adap-tation of CNN from image to text, a sim-ple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.},
	archivePrefix = {arXiv},
	arxivId = {1412.1058},
	author = {Johnson, Rie and Zhang, Tong},
	eprint = {1412.1058},
	file = {:C$\backslash$:/Users/Workk/Documents/1412.1058.pdf:pdf},
	isbn = {9781941643495},
	journal = {Naacl},
	mendeley-groups = {11Thesis},
	number = {2011},
	pages = {103--112},
	title = {{Effective Use of Word Order for Text Categorization with Convolutional Neural Networks}},
	url = {http://arxiv.org/abs/1412.1058{\%}5Cnhttp://arxiv.org/abs/1412.1058v1},
	year = {2015}
}
@article{Li2014a,
	author = {Li, Li and Zhang, Longkai and Wang, Houfeng},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Wang - 2014 - Muli-label Text Categorization with Hidden Components.pdf:pdf},
	journal = {Emnlp},
	mendeley-groups = {Interim Review},
	pages = {1816--1821},
	title = {{Muli-label Text Categorization with Hidden Components}},
	year = {2014}
}
@article{Lastname2011g,
	author = {Lastname, Name M},
	file = {:E$\backslash$:/Downloads/Work/thesis (7).pdf:pdf},
	number = {July},
	title = {{Title line 1 Title line 2 of the requirement for the degree of Doctor of Philosophy Name M . Lastname Cardiff University}},
	year = {2011}
}
@article{Berger,
	author = {Berger, Mark J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - Unknown - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
	pages = {1--8},
	title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}}
}
@article{Zhai2016,
	abstract = {In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.},
	archivePrefix = {arXiv},
	arxivId = {1512.04466},
	author = {Zhai, Shuangfei and Zhang, Zhongfei},
	eprint = {1512.04466},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhai, Zhang - 2016 - Semisupervised Autoencoder for Sentiment Analysis.pdf:pdf},
	isbn = {9781577357605},
	journal = {Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016)},
	keywords = {Technical Papers: Machine Learning Applications},
	mendeley-groups = {Progress Report,Interim Review},
	pages = {1394--1400},
	title = {{Semisupervised Autoencoder for Sentiment Analysis}},
	url = {http://arxiv.org/abs/1512.04466},
	volume = {13902},
	year = {2016}
}
@article{Pedregosa2012,
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	archivePrefix = {arXiv},
	arxivId = {1201.0490},
	author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
	doi = {10.1007/s13398-014-0173-7.2},
	eprint = {1201.0490},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedregosa et al. - 2012 - Scikit-learn Machine Learning in Python.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	mendeley-groups = {Annotated/Software},
	pages = {2825--2830},
	pmid = {1000044560},
	title = {{Scikit-learn: Machine Learning in Python}},
	url = {http://arxiv.org/abs/1201.0490},
	volume = {12},
	year = {2012}
}
@article{Burges2005a,
	author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
	doi = {10.1145/1102351.1102363},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:pdf},
	isbn = {1595931805},
	issn = {00243205},
	journal = {Icml 2005},
	keywords = {Learning to Rank,RankNet},
	mendeley-groups = {Progress Report},
	pages = {89--96},
	pmid = {16483612},
	title = {{Learning to rank using gradient descent}},
	year = {2005}
}
@article{Luo2015,
abstract = {Word embeddings encode semantic meanings of words into low-dimension word vectors. In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors. Nonnegative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http://github.com/skTim/OIWE.},
author = {Luo, Hongyin and Luan, Zhiyuan and Huanbo, Liuv and Sun, Maosong},
doi = {10.18653/v1/d15-1196},
file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2015 - Online Learning of Interpretable Word Embeddings.pdf:pdf},
isbn = {9781941643327},
journal = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {Annotated/Word Vectors,11Thesis/Interpretability/Word Vectors/Constraints},
number = {September},
pages = {1687--1692},
title = {{Online learning of interpretable word embeddings}},
year = {2015}
}

@article{Manly2017,
	author = {Manly, Bryan F J and Alberto, Jorge A Navarro},
	doi = {10.18637/jss.v078.b03},
	file = {:E$\backslash$:/Downloads/Work/v78b03 (1).pdf:pdf},
	isbn = {9781498728966},
	number = {June},
	title = {{Journal of Statistical Software}},
	volume = {78},
	year = {2017}
}
@article{Wood,
	author = {Wood, Matthew and Rana, Omer and C, Eran Peer and C, Nathan Melly and C, Ryan Codrai and C, Matt Wood and C, Dom Routley and C, Mahima Dalal and C, Jamie Harkins and C, Jeremy Yee and C, Zara Siddique},
	pages = {1--4},
	title = {{Role in team}}
}
@article{VanAssche2008,
	abstract = {The purpose of this column is to stimulate discussion among nurses regarding the importance of nursing theory-guided practice. The use of metaphor may shed light on defining nursing by its own terms. The time has come for nursing to recognize its worth as an autonomous discipline and own its contributions.},
	author = {{Van Assche}, Anneleen and Blockeel, Hendrik},
	doi = {10.1007/978-3-540-78469-2_26},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Assche, Blockeel - 2008 - Seeing the forest through the trees learning a comprehensible model from a first order ensemble.pdf:pdf},
	isbn = {3540784683},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Comprehensibility,Ensembles,First order decision trees},
	mendeley-groups = {Report/Biologicla domain,Report/Medical domain},
	pages = {269--279},
	pmid = {22228521},
	title = {{Seeing the forest through the trees learning a comprehensible model from a first order ensemble}},
	volume = {4894 LNAI},
	year = {2008}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/PGR{\_}Research{\_}Plan{\_}Form.pdf:pdf},
	title = {{Make codebase workable on ARCCA Complete Chapter 3 Complete Chapter 4 Complete Chapter 5 Code for CONLL paper Additional experimental results Journal paper}},
	year = {2018}
}
@article{Bach2015,
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
	doi = {10.1371/journal.pone.0130140},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.pdf:pdf},
	isbn = {10.1371/journal.pone.0130140},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {Report},
	number = {7},
	pages = {1--46},
	pmid = {26161953},
	title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
	volume = {10},
	year = {2015}
}
@article{Hullermeier2008,
	abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison (RPC), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (weighted) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare RPC to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that RPC is superior in terms of computational efficiency, and at least competitive in terms of accuracy. ?? 2008 Elsevier B.V. All rights reserved.},
	author = {H{\"{u}}llermeier, Eyke and F{\"{u}}rnkranz, Johannes and Cheng, Weiwei and Brinker, Klaus},
	doi = {10.1016/j.artint.2008.08.002},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{u}}llermeier et al. - 2008 - Label ranking by learning pairwise preferences.pdf:pdf},
	isbn = {0004-3702},
	issn = {00043702},
	journal = {Artificial Intelligence},
	keywords = {Constraint classification,Pairwise classification,Preference learning,Ranking},
	mendeley-groups = {Annotated/Ranking,Progress Report},
	number = {16-17},
	pages = {1897--1916},
	title = {{Label ranking by learning pairwise preferences}},
	volume = {172},
	year = {2008}
}
@article{Erhan2010a,
	abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1206.5538v1},
	author = {Erhan, Dumitru and Courville, Aaron and Vincent, Pascal},
	doi = {10.1145/1756006.1756025},
	eprint = {arXiv:1206.5538v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan, Courville, Vincent - 2010 - Why Does Unsupervised Pre-training Help Deep Learning.pdf:pdf},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	mendeley-groups = {Papers/Paper 1,Report/Features,Progress Report,Report},
	number = {2007},
	pages = {625--660},
	title = {{Why Does Unsupervised Pre-training Help Deep Learning ?}},
	url = {http://portal.acm.org/citation.cfm?id=1756025},
	volume = {11},
	year = {2010}
}
@article{Park2017,
abstract = {Vector representation of words improves performance in various NLP tasks, but the high-dimensional word vectors are very difficult to interpret. We apply several rotation algorithms to the vector representation of words to improve the interpretability. Unlike previous approaches that induce sparsity, the rotated vectors are interpretable while preserving the expressive performance of the original vectors. Furthermore, any pre-built word vector representation can be rotated for improved interpretability. We apply rotation to skip-grams and glove and compare the expressive power and interpretability with the original vectors and the sparse overcomplete vectors. The results show that the rotated vectors outperform the original and the sparse overcomplete vectors for interpretability and expressiveness tasks.},
author = {Park, Sungjoon and Bak, Jin Yeong and Oh, Alice},
doi = {10.18653/v1/d17-1041},
file = {:E$\backslash$:/PhD/Papedrs/e86c35f9f72edcec37ac747a6da19cd7f489.pdf:pdf},
isbn = {9781945626838},
journal = {EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
mendeley-groups = {11Thesis/Interpretability/Word Vectors/Post-processing,11Thesis/Interpretability/GAN's and VAE},
pages = {401--411},
title = {{Rotated word vector representations and their interpretability}},
year = {2017}
}

@article{Panchenko2016,
	abstract = {Word sense embeddings represent a word sense as a low-dimensional numeric vector. While this representation is potentially useful for NLP applications, its interpretability is inherently limited. We propose a simple technique that improves interpretability of sense vectors by mapping them to synsets of a lexical resource. Our experiments with AdaGram sense embeddings and BabelNet synsets showthat it is possible to retrieve synsets that correspond to automatically learned sense vectors with Precision of 0.87, Recall of 0.42 andAUC of 0.78.},
	author = {Panchenko, Alexander},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Panchenko - 2016 - Best of Both Worlds Making Word Sense Embeddings Interpretable.pdf:pdf},
	journal = {the 10th edition of the Language Resources and Evaluation Conference (LREC 2016)},
	keywords = {adagram,babelnet,lexical semantics,sense matching,word sense embeddings,wordnet},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {2649--2655},
	title = {{Best of Both Worlds: Making Word Sense Embeddings Interpretable}},
	url = {http://www.lrec-conf.org/proceedings/lrec2016/pdf/625{\_}Paper.pdf},
	year = {2016}
}
@article{Lau2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1607.05368v1},
	author = {Lau, Jey Han and Baldwin, Timothy},
	eprint = {arXiv:1607.05368v1},
	file = {:C$\backslash$:/Users/Workk/Documents/1607.05368.pdf:pdf},
	mendeley-groups = {11Thesis},
	title = {{Practical Insights into Document Embedding Generation}},
	year = {2014}
}
@article{Levy2015,
	author = {Levy, Omer},
	file = {:E$\backslash$:/Downloads/Work/570-1656-1-PB.pdf:pdf},
	pages = {211--225},
	title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
	volume = {3},
	year = {2015}
}
@article{Bai2004,
	author = {Bai, Xue and Padman, Rema and Airoldi, Edoardo},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai, Padman, Airoldi - 2004 - Sentiment Extraction from Unstructured Text using Tabu Search-Enhanced Markov Blanket.pdf:pdf},
	journal = {Proceedings of the International Workshop on Mining for and from the Semantic Web},
	keywords = {bayesian models,opinion,semantic learning,sematic orientation,sentiments},
	mendeley-groups = {Report},
	number = {July},
	pages = {24--35},
	title = {{Sentiment Extraction from Unstructured Text using Tabu Search-Enhanced Markov Blanket}},
	url = {http://ra.adm.cs.cmu.edu/anon/home/ftp/usr/ftp/isri2004/CMU-ISRI-04-127.pdf},
	year = {2004}
}
@article{Dosilovic2018,
	author = {Do{\v{s}}ilovi{\'{c}}, Filip Karlo and Br{\v{c}}i{\'{c}}, Mario and Hlupi{\'{c}}, Nikica},
	file = {:D$\backslash$:/PhD/Thesis/pdf downloads/dsdc{\_}11{\_}4754.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/Visual,11Thesis/Interpretability/Explanation},
	pages = {232--237},
	title = {{232 Mipro 2018/Ds-Dc}},
	year = {2018}
}
@article{Grodzicki2008a,
	abstract = {This paper considers the multilabel classification problem, which$\backslash$nis a generalization of traditional two-class or multi-class classification$\backslash$nproblem. In multilabel classification a set of labels (categories)$\backslash$nis given and each training instance is associated with a subset of$\backslash$nthis label-set. The task is to output the appropriate subset of labels$\backslash$n(generally of unknown size) for a given, unknown testing instance.$\backslash$nSome improvements to the existing neural network multilabel classification$\backslash$nalgorithm, named BP-MLL, are proposed here. The modifications concern$\backslash$nthe form of the global error function used in BP-MLL. The modified$\backslash$nclassification system is tested in the domain of functional genomics,$\backslash$non the yeast genome data set. Experimental results show that proposed$\backslash$nmodifications visibly improve the performance of the neural network$\backslash$nbased multilabel classifier. The results are statistically significant.$\backslash$n漏 2008 Springer-Verlag Berlin Heidelberg.},
	author = {Grodzicki, Rafa{\l} and Ma{\'{n}}dziuk, Jacek and Wang, Lipo},
	doi = {10.1007/978-3-540-87700-4_41},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grodzicki, Ma{\'{n}}dziuk, Wang - 2008 - Improved multilabel classification with neural networks.pdf:pdf},
	isbn = {3540876995},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Backpropagation,Bioinformatics,Functional genomics,Learning system,Multilabel,Neural network},
	number = {2},
	pages = {409--416},
	title = {{Improved multilabel classification with neural networks}},
	volume = {5199 LNCS},
	year = {2008}
}
@article{Rosenfield2016,
	author = {Rosenfield, Mark},
	file = {:E$\backslash$:/Downloads/Work/CVSOIPpaper.pdf:pdf},
	number = {January},
	title = {{Computer vision syndrome (a.k.a. digital eye strain)}},
	year = {2016}
}
@article{Zhao2017,
	author = {Zhao, Rui and Mao, Kezhi},
	doi = {10.1109/TFUZZ.2017.2690222},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Mao - 2017 - Fuzzy Bag-of-Words Model for Document Representation.pdf:pdf},
	issn = {1063-6706},
	journal = {IEEE Transactions on Fuzzy Systems},
	mendeley-groups = {Annotated/Document representation,!Paper 3/task/newsgroups},
	number = {8},
	pages = {1--1},
	title = {{Fuzzy Bag-of-Words Model for Document Representation}},
	url = {http://ieeexplore.ieee.org/document/7891009/},
	volume = {14},
	year = {2017}
}
@article{Vellido2012,
	abstract = {This article investigates methods for the accurate and robust differentiation of metastases from glioblastomas on the basis of single-voxel (1)H MRS information. Single-voxel (1)H MR spectra from a total of 109 patients (78 glioblastomas and 31 metastases) from the multicenter, international INTERPRET database, plus a test set of 40 patients (30 glioblastomas and 10 metastases) from three different centers in the Barcelona (Spain) metropolitan area, were analyzed using a robust method for feature (spectral frequency) selection coupled with a linear-in-the-parameters single-layer perceptron classifier. For the test set, a parsimonious selection of five frequencies yielded an area under the receiver operating characteristic curve of 0.86, and an area under the convex hull of the receiver operating characteristic curve of 0.91. Moreover, these accurate results for the discrimination between glioblastomas and metastases were obtained using a small number of frequencies that are amenable to metabolic interpretation, which should ease their use as diagnostic markers. Importantly, the prediction can be expressed as a simple formula based on a linear combination of these frequencies. As a result, new cases could be straightforwardly predicted by integrating this formula into a computer-based medical decision support system. This work also shows that the combination of spectra acquired at different TEs (short TE, 20-32 ms; long TE, 135-144 ms) is key to the successful discrimination between glioblastomas and metastases from single-voxel (1)H MRS.},
	author = {Vellido, A. and Romero, E. and Juli??-Sap??, M. and Maj??s, C. and Moreno-Torres, ?? and Pujol, J. and Ar??s, C.},
	doi = {10.1002/nbm.1797},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vellido et al. - 2012 - Robust discrimination of glioblastomas from metastatic brain tumors on the basis of single-voxel 1H MRS.pdf:pdf},
	isbn = {1099-1492 (Electronic)$\backslash$n0952-3480 (Linking)},
	issn = {09523480},
	journal = {NMR in Biomedicine},
	keywords = {Feature selection,Glioblastomas,High-grade malignant tumors,Medical decision support system,Metastases,Pattern recognition,SV 1H MRS},
	mendeley-groups = {Report},
	number = {6},
	pages = {819--828},
	pmid = {22081447},
	title = {{Robust discrimination of glioblastomas from metastatic brain tumors on the basis of single-voxel 1H MRS}},
	volume = {25},
	year = {2012}
}
@article{Bergera,
	author = {Berger, Mark J},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - Unknown - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
	mendeley-groups = {Interim Review},
	pages = {1--8},
	title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}}
}
@article{Duchi2011,
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1103.4296v1},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	doi = {10.1109/CDC.2012.6426698},
	eprint = {arXiv:1103.4296v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
	isbn = {9780982252925},
	issn = {15324435},
	journal = {Journal of Machine Learning Research},
	keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
	mendeley-groups = {Annotated/Software},
	pages = {2121--2159},
	pmid = {2868127},
	title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	volume = {12},
	year = {2011}
}
@article{Wainer2017,
	author = {Wainer, Jacques and Cawley, Gavin},
	file = {:D$\backslash$:/PhD/PGR/16-174.pdf:pdf},
	keywords = {bootstrap,cross-validation,hyperparameters,k-fold,resampling,svm},
	pages = {1--35},
	title = {{Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters}},
	volume = {18},
	year = {2017}
}
@article{Ager,
	author = {Ager, Thomas},
	mendeley-groups = {Temp},
	title = {{Annual Review Year 2 : Obtaining interpretable classifiers from text-based neural networks}}
}
@article{Sculley2010,
	author = {Sculley, D},
	file = {:E$\backslash$:/Downloads/Work/fastkmeans.pdf:pdf},
	isbn = {9781605587998},
	mendeley-groups = {11Thesis},
	pages = {4--5},
	title = {{Web-Scale K-Means Clustering}},
	year = {2010}
}
@article{Elazmeh2007,
	abstract = {The paper presents ongoing issues, challenges, and difficulties we face in applying machine learning methods to retrospectively collected clinical data. The objective of our research is to build a reliable prediction model for early assessment of emergency pediatric asthma exacerbations. This predictive model should be able to distinguish between patients with mild or moderate/severe asthma attacks at a medically acceptable level of performance. Our real-life data set presents us with some difficult challenges which we communicate in this paper. Our approach to overcoming some of these difficulties is to use external expert knowledge to aid with classification by decomposing the classification problem into a two-tier concept, where concepts can be explicitly described in terms of the external knowledge source. Such an approach also has the advantage of significantly reducing the size of the training set required. Copyright {\textcopyright} 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	author = {Elazmeh, W.a and Matwin, S.a and O'Sullivan, D.b and Michalowski, W.b and Farion, K.c},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elazmeh et al. - 2007 - Insights from predicting pediatric asthma exacerbations from retrospective clinical data.pdf:pdf},
	isbn = {9781577353324},
	journal = {AAAI Workshop - Technical Report},
	keywords = {Artificial intelligence,Classification (of infor,Clinical data,Evaluation methods,Expert knowledg,Learning systems},
	mendeley-groups = {Report/Medical domain},
	pages = {10--15},
	title = {{Insights from predicting pediatric asthma exacerbations from retrospective clinical data}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-51849160749{\&}partnerID=40{\&}md5=345e9c0a3f9c793b5c69b25dfab7d721},
	volume = {WS-07-05},
	year = {2007}
}
@article{Yin2017,
	abstract = {Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
	archivePrefix = {arXiv},
	arxivId = {1702.01923},
	author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"{u}}tze, Hinrich},
	eprint = {1702.01923},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin et al. - 2017 - Comparative Study of CNN and RNN for Natural Language Processing.pdf:pdf},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Comparative Study of CNN and RNN for Natural Language Processing}},
	url = {http://arxiv.org/abs/1702.01923},
	year = {2017}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/Thesis plan.pdf:pdf},
	pages = {13--15},
	title = {{Solution to the TF-IDF problem of no-context, utilizes nnets 1.}}
}
@article{Wallach2008,
	abstract = {This thesis introduces new methods for statistically modelling text using topic models. Topic models have seen many successes in recent years, and are used in a variety of applications, including analysis of news articles, topic-based search interfaces and navigation tools for digital libraries. Despite these recent successes, the field of topic modelling is still relatively new and there remains much to be explored. One notice- able absence from most of the previous work on topic modelling is consideration of language and document structurefrom low-level structures, including word order and syntax, to higher-level structures, such as relationships between documents. The focus of this thesis is therefore structured topic modelsmodels that combine latent topics with information about document structure, ranging from local sen- tence structure to inter-document relationships. These models draw on techniques from Bayesian statistics, including hierarchical Dirichlet distributions and processes, Pitman-Yor processes, and Markov chain Monte Carlo methods. Several methods for estimating the parameters of Dirichlet-multinomial distributions are also compared. The main contribution of this thesis is the introduction of three structured topic mod- els. The first is a topic-based language model. This model captures both word order and latent topics by extending a Bayesian topic model to incorporate n-gram statistics. A bigram version of the new model does better at predicting future words than either a topic model or a trigram language model. It also provides interpretable topics. The second model arises from a Bayesian reinterpretation of a classic generative de- pendency parsing model. The new model demonstrates that parsing performance can be substantially improved by a careful choice of prior and by sampling hyperparame- ters. Additionally, the generative nature of the model facilitates the inclusion of latent state variables, which act as specialised part-of-speech tags or syntactic topics. The third is a model that captures high-level relationships between documents. This model uses nonparametric Bayesian priors and Markov chain Monte Carlo methods to infer topic-based document clusters. The model assigns a higher probability to un- seen test documents than either a clustering model without topics or a Bayesian topic model without document clusters. The model can be extended to incorporate author information, resulting in finer-grained clusters and better predictive performance.},
	author = {Wallach, Hanna M},
	file = {:E$\backslash$:/Downloads/Work/1eefe0e5c69d6e14840e2d2b30f62a09a28b.pdf:pdf},
	journal = {Doctor},
	mendeley-groups = {Annotated/Topic models/Unsupervised Topic Models},
	number = {2001},
	pages = {136},
	title = {{Structured Topic Models for Language}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.2537{\&}rep=rep1{\&}type=pdf},
	year = {2008}
}
@article{Noack,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1912.03430v1},
	author = {Noack, Adam and Ahern, Isaac},
	eprint = {arXiv:1912.03430v1},
	file = {:E$\backslash$:/1912.03430.pdf:pdf},
	title = {{Does Interpretability of Neural Networks Imply Adversarial Robustness?}}
}
@article{Mcauley2013,
	author = {Mcauley, Julian},
	isbn = {9781450324090},
	journal = {RecSys '13 Proceedings of the 7th ACM conference on Recommender systems},
	keywords = {recommender systems,topic models},
	mendeley-groups = {Annotated/Datasets},
	pages = {165--172},
	title = {{Hidden Factors and Hidden Topics : Understanding Rating Dimensions with Review Text}},
	year = {2013}
}
@article{Abe2013,
	author = {Abe, Shigeo},
	doi = {10.1109/21.362960},
	file = {:E$\backslash$:/Downloads/Work/90000209.pdf:pdf},
	number = {February 1995},
	title = {{Fuzzy rules extraction directly from numerical data for function Title University Repository : Kernel Fuzzy rules extraction directly from numerical data for function approximation}},
	year = {2013}
}
@article{Joachims1998,
	abstract = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.},
	author = {Joachims, Thorsten},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims - 1998 - 1 Introduction 2 Text Categorization 3 Support Vector Machines.pdf:pdf},
	isbn = {3540644172},
	journal = {Machine Learning},
	number = {LS-8 Report 23},
	pages = {137--142},
	title = {{1 Introduction 2 Text Categorization 3 Support Vector Machines}},
	url = {http://www.springerlink.com/index/drhq581108850171.pdf},
	volume = {1398},
	year = {1998}
}
@article{Read2010,
	author = {Read, Jesse},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Read - 2010 - Scalable Multi-label Classification(2).pdf:pdf},
	title = {{Scalable Multi-label Classification}},
	year = {2010}
}
@article{Ehrentraut2018,
	author = {Ehrentraut, Claudia and Ekholm, Markus and Dalianis, Hercules},
	doi = {10.1177/1460458216656471},
	file = {:E$\backslash$:/1460458216656471.pdf:pdf},
	mendeley-groups = {11Thesis/Applications},
	title = {{Detecting hospital-acquired infections : A document classification approach using support vector machines and gradient tree boosting}},
	year = {2018}
}
@article{Burges1998a,
	abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
	archivePrefix = {arXiv},
	arxivId = {1111.6189v1},
	author = {Burges, C.J.C. J Christopher J C},
	doi = {10.1023/A:1009715923555},
	eprint = {1111.6189v1},
	isbn = {0818672404},
	issn = {13845810},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
	number = {2},
	pages = {121--167},
	pmid = {5207842081938259593},
	title = {{A tutorial on support vector machines for pattern recognition}},
	url = {http://www.springerlink.com/index/Q87856173126771Q.pdf},
	volume = {2},
	year = {1998}
}
@article{,
	file = {:E$\backslash$:/Downloads/Work/max-margin-tutorial.pdf:pdf},
	title = {{No Title}}
}
@article{Yosinski2014,
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	archivePrefix = {arXiv},
	arxivId = {1411.1792},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	eprint = {1411.1792},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - How transferable are features in deep neural networks.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems 27 (Proceedings of NIPS)},
	mendeley-groups = {Progress Report,Interim Review,Report},
	pages = {1--9},
	title = {{How transferable are features in deep neural networks?}},
	url = {http://arxiv.org/abs/1411.1792},
	volume = {27},
	year = {2014}
}

@inproceedings{Ribeiro2016,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135-1144},
numpages = {10},
keywords = {explaining machine learning, interpretability, black box classifier, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{Rocktaschel2015,
	abstract = {While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.},
	archivePrefix = {arXiv},
	arxivId = {1509.06664},
	author = {Rockt{\"{a}}schel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Blunsom, Phil},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {1509.06664},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rockt{\"{a}}schel et al. - 2015 - Reasoning about Entailment with Neural Attention.pdf:pdf},
	isbn = {9781941643723},
	issn = {10450823},
	mendeley-groups = {!Paper 3/LSTM Types},
	number = {2015},
	pages = {1--9},
	pmid = {9377276},
	title = {{Reasoning about Entailment with Neural Attention}},
	url = {http://arxiv.org/abs/1509.06664},
	year = {2015}
}
@article{Johnson,
	author = {Johnson, Rie},
	file = {:E$\backslash$:/Downloads/Work/cnn-semi-nips15.pdf:pdf},
	pages = {1--9},
	title = {{Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding}}
}
@article{Bastani,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1705.08504v1},
	author = {Bastani, Osbert and Kim, Carolyn},
	eprint = {arXiv:1705.08504v1},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim - Unknown - Interpreting Blackbox Models via Model Extraction.pdf:pdf},
	mendeley-groups = {Annotated},
	title = {{Interpreting Blackbox Models via Model Extraction}}
}
@article{Ganin2015,
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	archivePrefix = {arXiv},
	arxivId = {1505.07818},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
	doi = {10.1088/1475-7516/2015/08/013},
	eprint = {1505.07818},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
	issn = {1475-7516},
	mendeley-groups = {Progress Report},
	pages = {1--35},
	title = {{Domain-Adversarial Training of Neural Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	volume = {17},
	year = {2015}
}
@article{Bengio2009,
	abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
	archivePrefix = {arXiv},
	arxivId = {submit/0500581},
	author = {Bengio, Yoshua},
	doi = {10.1561/2200000006},
	eprint = {0500581},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
	isbn = {2200000006},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	mendeley-groups = {Papers/Paper 1,Report/Features,Progress Report,Report},
	number = {1},
	pages = {1--127},
	pmid = {17348934},
	primaryClass = {submit},
	title = {{Learning Deep Architectures for AI}},
	volume = {2},
	year = {2009}
}
@article{Adebayo2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1810.03292v2},
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	eprint = {arXiv:1810.03292v2},
	file = {:E$\backslash$:/1810.03292.pdf:pdf},
	number = {Nips},
	title = {{Sanity Checks for Saliency Maps}},
	year = {2018}
}
@article{Socher2013a,
	abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Perelygin, Wu - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank.pdf:pdf},
	isbn = {9781937284978},
	journal = {Proceedings of the {\ldots}},
	pages = {1631--1642},
	title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
	url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
	year = {2013}
}
@inproceedings{Chen2014b,
	abstract = {Almost all current dependency parsers classify based on millions of sparse indi-cator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed signif-icantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based depen-dency parser. Because this classifier learns and uses just a small number of dense fea-tures, it can work very fast, while achiev-ing an about 2{\%} improvement in unla-beled and labeled attachment scores on both English and Chinese datasets. Con-cretely, our parser is able to parse more than 1000 sentences per second at 92.2{\%} unlabeled attachment score on the English Penn Treebank.},
	archivePrefix = {arXiv},
	arxivId = {1512.00567},
	author = {Chen, Danqi and Manning, Christopher},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	doi = {10.3115/v1/D14-1082},
	eprint = {1512.00567},
	isbn = {9781937284961},
	issn = {9781937284961},
	mendeley-groups = {Report/Features},
	pages = {740--750},
	pmid = {8190083},
	title = {{A Fast and Accurate Dependency Parser using Neural Networks}},
	url = {http://aclweb.org/anthology/D14-1082},
	year = {2014}
}
@article{Number2016,
	author = {Number, N I and Code, Post and Code, Sort and Account, Bank and Name, Society and Branch, Society and Roll, Building Society and Contact, Emergency and Number, Student},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Number et al. - 2016 - ENGAGEMENT OF A POST GRADUATE STUDENT DEMONSTRATOR.pdf:pdf},
	number = {August},
	pages = {1--7},
	title = {{ENGAGEMENT OF A POST GRADUATE STUDENT DEMONSTRATOR}},
	year = {2016}
}
@article{Ganin2015a,
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	archivePrefix = {arXiv},
	arxivId = {1505.07818},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
	doi = {10.1088/1475-7516/2015/08/013},
	eprint = {1505.07818},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
	issn = {1475-7516},
	mendeley-groups = {Progress Report},
	pages = {1--35},
	title = {{Domain-Adversarial Training of Neural Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	volume = {17},
	year = {2015}
}
@article{Zhang,
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
	file = {:E$\backslash$:/PhD/Papedrs/Zhang{\_}StackGAN{\_}Text{\_}to{\_}ICCV{\_}2017{\_}paper.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted,11Thesis/Interpretability/GAN's and VAE},
	pages = {5907--5915},
	title = {{StackGAN : Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}}
}
@article{Zhang2016a,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1611.06792v3},
	author = {Zhang, Ye and Rahman, Mustafizur and Braylan, Alex and Dang, Brandon and Chang, Heng-lu and Kim, Henna and Mcnamara, Quinten and Angert, Aaron and Banner, Edward and Khetan, Vivek and Mcdonnell, Tyler and Nguyen, An Thanh and Xu, Dan and Wallace, Byron C and Lease, Matthew},
	eprint = {arXiv:1611.06792v3},
	file = {:E$\backslash$:/1611.06792.pdf:pdf},
	keywords = {cnn,convolutional neural network,deep learning,distributed representations,neural net-,nn,recurrent neural network,rnn,search engine,word embedding,word2vec,work},
	title = {{Neural Information Retrieval: A Literature Review}},
	year = {2016}
}
@article{Volume,
	author = {Volume, Overlord},
	file = {:E$\backslash$:/Downloads/Work/Overlord Volume 1 - The Undead King .pdf:pdf},
	title = {{No Title}}
}
@article{NorouziM2009,
	abstract = {In this thesis, we present a method for learning problem-specific hierarchical features specialized for vision applications. Recently, a greedy layerwise learning mechanism has been proposed for tuning parameters of fully connected hierarchical networks. This approach views layers of a network as Restricted Boltzmann Machines (RBM), and trains them separately from the bottom layer upwards. We develop Convolutional RBM (CRBM), an extension of the RBM model in which connections are local and weights are shared to respect the spatial structure of images. We switch between the CRBM and down-sampling layers and stack them on top of each other to build a multilayer hierarchy of alternating filtering and pooling. This framework learns generic features such as oriented edges at the bottom levels and features specific to an object class such as object parts in the top layers. Afterward, we feed the extracted features into a discriminative classifier for recognition. It is experimentally demonstrated that the features automatically learned by our algorithm are effective for object detection, by using them to obtain performance comparable to the state-of-the-art on handwritten digit classification and pedestrian detection.},
	author = {{Norouzi M}},
	doi = {10.1109/CVPR.2009.5206577},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi M - 2009 - Convolutional restricted Boltzmann machines for feature learning.pdf:pdf},
	isbn = {9781424439911},
	journal = {School of Computing Science-Simon Fraser University},
	mendeley-groups = {Papers/Paper 1,Progress Report,Report},
	pages = {2735--2742},
	title = {{Convolutional restricted Boltzmann machines for feature learning}},
	year = {2009}
}
@article{Anderson,
	author = {Anderson, T W and Wiley, John},
	file = {:E$\backslash$:/Downloads/Work/194866033.pdf:pdf},
	title = {{An Introduction to Multivariate Statistical Analysis}}
}
@article{Zhang2007a,
	abstract = {Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms. ?? 2007 Pattern Recognition Society.},
	author = {Zhang, Min Ling and Zhou, Zhi Hua},
	doi = {10.1016/j.patcog.2006.12.019},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhou - 2007 - ML-KNN A lazy learning approach to multi-label learning.pdf:pdf},
	isbn = {0031-3203},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Functional genomics,K-nearest neighbor,Lazy learning,Machine learning,Multi-label learning,Natural scene classification,Text categorization},
	number = {7},
	pages = {2038--2048},
	title = {{ML-KNN: A lazy learning approach to multi-label learning}},
	volume = {40},
	year = {2007}
}
@article{Ager2012g,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:E$\backslash$:/Downloads/Work/CoNLL{\_}2018{\_}{\_}{\_}Tom (11).pdf:pdf},
	title = {{Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces}},
	year = {2012}
}
@article{Chang2009,
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
	doi = {10.1.1.100.1089},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Models.pdf:pdf},
	isbn = {9781615679119},
	issn = {1098-6596},
	journal = {Advances in Neural Information Processing Systems 22},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,11Thesis/Interpretability,11Thesis/Interpretability/Classifers},
	pages = {288----296},
	pmid = {25246403},
	title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
	url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
	year = {2009}
}
@article{Martensa,
	author = {Martens, David and Provost, Foster},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens, Provost - Unknown - Explaining Data-Driven Document Classifications.pdf:pdf},
	keywords = {comprehensibility,document classification,instance level explanation,text mining},
	mendeley-groups = {Annotated/Explanations,!Paper 3/task/newsgroups},
	title = {{Explaining Data-Driven Document Classifications *}}
}
@article{Sutskever2014,
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	archivePrefix = {arXiv},
	arxivId = {1409.3215},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc},
	eprint = {1409.3215},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
	isbn = {1409.3215},
	journal = {Advances in Neural Information Processing Systems},
	pages = {1--9},
	title = {{Sequence to Sequence Learning with Neural Networks}},
	year = {2014}
}
@article{Kulkarni2015a,
	abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model's efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.},
	archivePrefix = {arXiv},
	arxivId = {1503.03167},
	author = {Kulkarni, Td and Whitney, W},
	doi = {10.1063/1.4914407},
	eprint = {1503.03167},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni, Whitney - 2015 - Deep Convolutional Inverse Graphics Network.pdf:pdf},
	issn = {10897550},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Progress Report},
	pages = {2539--2547},
	title = {{Deep Convolutional Inverse Graphics Network}},
	url = {http://arxiv.org/abs/1503.03167},
	year = {2015}
}
@article{Goyal2017,
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\~{}}90{\%} scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.},
	archivePrefix = {arXiv},
	arxivId = {1706.02677},
	author = {Goyal, Priya and Doll{\'{a}}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	doi = {10.1561/2400000003},
	eprint = {1706.02677},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet in 1 Hour.pdf:pdf},
	isbn = {9781601987167},
	issn = {2167-3888},
	mendeley-groups = {!Paper 3/Training LSTMs},
	title = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
	url = {http://arxiv.org/abs/1706.02677},
	year = {2017}
}
@article{Martens2007,
	abstract = {In recent years, support vector machines (SVMs) were successfully applied to a wide range of applications. However, since the classifier is described as a complex mathematical function, it is rather incomprehensible for humans. This opacity property prevents them from being used in many real-life applications where both accuracy and comprehensibility are required, such as medical diagnosis and credit risk evaluation. To overcome this limitation, rules can be extracted from the trained SVM that are interpretable by humans and keep as much of the accuracy of the SVM as possible. In this paper, we will provide an overview of the recently proposed rule extraction techniques for SVMs and introduce two others taken from the artificial neural networks domain, being Trepan and G-REX. The described techniques are compared using publicly available datasets, such as Ripley's synthetic dataset and the multi-class iris dataset. We will also look at medical diagnosis and credit scoring where comprehensibility is a key requirement and even a regulatory recommendation. Our experiments show that the SVM rule extraction techniques lose only a small percentage in performance compared to SVMs and therefore rank at the top of comprehensible classification techniques. ?? 2006 Elsevier B.V. All rights reserved.},
	author = {Martens, David and Baesens, Bart and {Van Gestel}, Tony and Vanthienen, Jan},
	doi = {10.1016/j.ejor.2006.04.051},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martens et al. - 2007 - Comprehensible credit scoring models using rule extraction from support vector machines.pdf:pdf},
	isbn = {0377-2217},
	issn = {03772217},
	journal = {European Journal of Operational Research},
	keywords = {Classification,Credit scoring,Rule extraction,Support vector machine},
	mendeley-groups = {Annotated/Applications/Credit scoring},
	number = {3},
	pages = {1466--1476},
	title = {{Comprehensible credit scoring models using rule extraction from support vector machines}},
	volume = {183},
	year = {2007}
}
@article{Bakarov,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1801.09536v1},
	author = {Bakarov, Amir},
	eprint = {arXiv:1801.09536v1},
	file = {::},
	mendeley-groups = {Thesis/Word Vectors},
	title = {{A Survey of Word Embeddings Evaluation Methods}}
}
@article{Mrk,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1603.00892v1},
	author = {Mrkˇ, Nikola and Rojas-barahona, Lina and Su, Pei-hao and Vandyke, David and Wen, Tsung-hsien and Young, Steve},
	eprint = {arXiv:1603.00892v1},
	file = {::},
	mendeley-groups = {Thesis/Retrofitting Word Vectors},
	title = {{Counter-fitting Word Vectors to Linguistic Constraints ´}}
}
@article{Bolukbasi2016,
	author = {Bolukbasi, Tolga and Chang, Kai-wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	file = {::},
	mendeley-groups = {Thesis/Retrofitting Word Vectors},
	number = {Nips},
	pages = {1--9},
	title = {{Man is to Computer Programmer as Woman is to Homemaker ? Debiasing Word Embeddings}},
	year = {2016}
}

@article{Evy2007,
abstract = {In a previous article, we presented a systematic computational study of the extraction of semantic representations from the word-word co-occurrence statistics of large text corpora. The conclusion was that semantic vectors of pointwise mutual information values from very small co-occurrence windows, together with a cosine distance measure, consistently resulted in the best representations across a range of psychologically relevant semantic tasks. This article extends that study by investigating the use of three further factors-namely, the application of stop-lists, word stemming, and dimensionality reduction using singular value decomposition (SVD)-that have been used to provide improved performance elsewhere. It also introduces an additional semantic task and explores the advantages of using a much larger corpus. This leads to the discovery and analysis of improved SVD-based methods for generating semantic representations (that provide new state-of-the-art performance on a standard TOEFL task) and the identification and discussion of problems and misleading results that can arise without a full systematic study. {\textcopyright} 2012 Psychonomic Society, Inc.},
author = {Bullinaria, John A. and Levy, Joseph P.},
doi = {10.3758/s13428-011-0183-8},
file = {::},
issn = {1554351X},
journal = {Behavior Research Methods},
keywords = {Corpus statistics,SVD,Semantic representation},
mendeley-groups = {Thesis/Word Vectors},
number = {3},
pages = {890--907},
title = {{Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming, and SVD}},
volume = {44},
year = {2012}
}

@book{Hillier,
	author = {Hillier, Frederick S},
	file = {::},
	isbn = {9781461407683},
	mendeley-groups = {Thesis/Sparse reps},
	title = {{International Series in Operations Research {\&} Management Science}}
}
@article{Yin,
	author = {Yin, Bangjie and Tran, Luan and Li, Haoxiang and Shen, Xiaohui and Liu, Xiaoming},
	file = {::},
	mendeley-groups = {Thesis/Latent variables},
	pages = {9348--9357},
	title = {{Towards Interpretable Face Recognition}}
}
@article{Silva2004,
	author = {Silva, Vin De and Tenenbaum, Joshua B},
	file = {::},
	keywords = {embedding,feature discovery,online algorithms,unsupervised learn-,visualization},
	mendeley-groups = {Thesis/Sparse reps},
	pages = {1--41},
	title = {{Sparse multidimensional scaling using landmark points}},
	year = {2004}
}
@article{Wu,
	author = {Wu, Tianfu and Song, Xi},
	file = {::},
	mendeley-groups = {Thesis/Latent variables},
	pages = {6033--6043},
	title = {{Towards Interpretable Object Detection by Unfolding Latent Structures}}
}
@article{Aharon2006,
	author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
	file = {::},
	mendeley-groups = {Thesis/Sparse reps},
	number = {11},
	pages = {4311--4322},
	title = {{K -SVD : An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation}},
	volume = {54},
	year = {2006}
}
@article{Xu2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1905.11975v2},
	author = {Xu, Peng and Cao, Yanshuai and Chi, Jackie and Cheung, Kit},
	eprint = {arXiv:1905.11975v2},
	file = {::},
	mendeley-groups = {Thesis/Latent variables},
	pages = {1--16},
	title = {{On Variational Learning of Controllable Representations for Text without Supervision arXiv : 1905 . 11975v2 [ cs . CL ] 12 Oct 2019}},
	year = {2018}
}
@article{Zhao,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1906.06719v1},
	author = {Zhao, Shenjian},
	eprint = {arXiv:1906.06719v1},
	file = {::},
	mendeley-groups = {Thesis/Latent variables,Thesis/Sparse reps},
	title = {{Fixing Gaussian Mixture VAEs for Interpretable Text Generation}}
}
@article{Li2017,
	author = {Li, Yunzhu and Ermon, Stefano},
	file = {::},
	mendeley-groups = {Thesis/Latent variables},
	number = {Nips},
	title = {{InfoGAIL : Interpretable Imitation Learning from Visual Demonstrations}},
	year = {2017}
}
@article{Jaworska2009,
	author = {Jaworska, Natalia and Anastasova, Angelina Chupetlovska},
	file = {::},
	mendeley-groups = {Thesis/Representations},
	number = {1},
	title = {{A Review of Multidimensional Scaling ( MDS ) and its Utility in Various Psychological Domains}},
	volume = {5},
	year = {2009}
}

@article{Gimenez,
abstract = {In this article, we introduce a procedure for selecting variables in principal components analysis. It is developed to identify a small subset of the original variables that best explain the principal components through nonparametric relationships. There are usually some noisy uninformative variables in a dataset, and some variables that are strongly related to one another because of their general dependence. The procedure is designed to be used following the satisfactory initial principal components analysis with all variables, and its aim is to help to interpret the underlying structures. We analyze the asymptotic behavior of the method and provide some examples.},
archivePrefix = {arXiv},
arxivId = {1308.6626},
author = {Gimenez, Yanina and Giussani, Guido},
doi = {10.1214/17-BJPS361},
eprint = {1308.6626},
file = {::},
issn = {01030752},
journal = {Brazilian Journal of Probability and Statistics},
keywords = {Informative variables,Multivariate analysis,Principal components,Selection of variables},
mendeley-groups = {Thesis/Representations},
number = {4},
pages = {730--754},
title = {{Searching for the core variables in principal components analysis}},
volume = {32},
year = {2018}
}

@article{Zou2006a,
	abstract = {Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results. {\textcopyright}2006 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
	author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
	doi = {10.1198/106186006X113430},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/sparsepc.pdf:pdf},
	issn = {10618600},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Arrays,Gene expression,Lasso/elastic net,Multivariate analysis,Singular value decomposition,Thresholding},
	mendeley-groups = {11Thesis},
	number = {2},
	pages = {265--286},
	title = {{Sparse principal component analysis}},
	volume = {15},
	year = {2006}
}


@article{Sebastiani,
	archivePrefix = {arXiv},
	arxivId = {arXiv:cs/0110053v1},
	author = {Sebastiani, Fabrizio},
	eprint = {0110053v1},
	file = {::},
	mendeley-groups = {Thesis/Fundamental / bg},
	primaryClass = {arXiv:cs},
	title = {{Machine Learning in Automated Text Categorization}}
}
@article{Kim,
  title = 	 {Learning to Discover Cross-Domain Relations with Generative Adversarial Networks},
  author = 	 {Taeksoo Kim and Moonsu Cha and Hyunsoo Kim and Jung Kwon Lee and Jiwon Kim},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1857--1865},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/kim17a/kim17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/kim17a.html},
  abstract = 	 {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on a generative adversarial network that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.}
}


@article{Zhao2007,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1804.08069v1},
	author = {Zhao, Tiancheng and Lee, Kyusong and Eskenazi, Maxine},
	eprint = {arXiv:1804.08069v1},
	file = {::},
	mendeley-groups = {Thesis/Domains},
	title = {{Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation}},
	year = {2007}
}
@article{Kiela2015,
	author = {Kiela, Douwe and Hill, Felix and Clark, Stephen},
	file = {::},
	mendeley-groups = {Thesis/Retrofitting Word Vectors},
	number = {September},
	pages = {2044--2048},
	title = {{Specializing Word Embeddings for Similarity or Relatedness}},
	year = {2015}
}
@article{Liu2015,
	author = {Liu, Quan and Jiang, Hui and Wei, Si and Ling, Zhen-hua and Hu, Yu},
	file = {::},
	mendeley-groups = {Thesis/Retrofitting Word Vectors},
	pages = {1501--1511},
	title = {{Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints}},
	year = {2015}
}
@article{Hu,
	author = {Hu, Qiyang and Szab, Attila and Paolo, Portenier and Zwicker, Matthias},
	file = {::},
	mendeley-groups = {Thesis/New Folder},
	title = {{Disentangling Factors of Variation by Mixing Them}}
}

@article{Le2014a,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: They lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of- words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc and Mikolov, Tomas},
eprint = {1405.4053},
file = {::},
isbn = {9781634393973},
journal = {31st International Conference on Machine Learning, ICML 2014},
mendeley-groups = {Thesis/Representations},
pages = {2931--2939},
title = {{Distributed representations of sentences and documents}},
volume = {4},
year = {2014}
}

@article{Derrac2015,
	abstract = {Commonsense reasoning patterns such as interpolation and a fortiori inference have proven useful for dealing with gaps in structured knowledge bases. An important difficulty in applying these reasoning patterns in practice is that they rely on fine-grained knowledge of how different concepts and entities are semantically related. In this paper, we show how the required semantic relations can be learned from a large collection of text documents. To this end, we first induce a conceptual space from the text documents, using multi-dimensional scaling. We then rely on the key insight that the required semantic relations correspond to qualitative spatial relations in this conceptual space. Among others, in an entirely unsupervised way, we identify salient directions in the conceptual space which correspond to interpretable relative properties such as 'more fruity than' (in a space of wines), resulting in a symbolic and interpretable representation of the conceptual space. To evaluate the quality of our semantic relations, we show how they can be exploited by a number of commonsense reasoning based classifiers. We experimentally show that these classifiers can outperform standard approaches, while being able to provide intuitive explanations of classification decisions. A number of crowdsourcing experiments provide further insights into the nature of the extracted semantic relations.},
	author = {Derrac, Joaqu??n and Schockaert, Steven},
	doi = {10.1016/j.artint.2015.07.002},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derrac, Schockaert - 2015 - Inducing semantic relations from conceptual spaces A data-driven approach to plausible reasoning.pdf:pdf},
	isbn = {0004-3702},
	issn = {00043702},
	journal = {Artificial Intelligence},
	keywords = {Commonsense reasoning,Conceptual spaces,Dimensionality reduction,Qualitative spatial relations},
	mendeley-groups = {Annotated/Past work,Papers/Paper 1,Categories/Commonsense Reasoning,Report/Features,Progress Report,Report},
	pages = {66--94},
	publisher = {Elsevier B.V.},
	title = {{Inducing semantic relations from conceptual spaces: A data-driven approach to plausible reasoning}},
	url = {http://dx.doi.org/10.1016/j.artint.2015.07.002},
	volume = {228},
	year = {2015}
}
@article{Gilpin,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.00069v2},
	author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	eprint = {arXiv:1806.00069v2},
	file = {:E$\backslash$:/PhD/Papedrs/4da8391a737273be4868613468b61d80d466.pdf:pdf},
	mendeley-groups = {11Thesis/Unsorted},
	title = {{Explaining Explanations : An Approach to Evaluating Interpretability of Machine Learning}}
}
@article{Carvalho2019,
	author = {Carvalho, Diogo V and Pereira, Eduardo M and Cardoso, Jaime S},
	doi = {10.3390/electronics8080832},
	file = {:E$\backslash$:/Downloads/Work/electronics-08-00832.pdf:pdf},
	keywords = {explainability,interpretability,machine learning,xai},
	pages = {1--34},
	title = {{Machine Learning Interpretability : A Survey on Methods and Metrics}},
	year = {2019}
}
@article{Valizadegan2009,
	abstract = {Learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels. The ranking algorithms are often evaluated using information retrieval measures, such as Normalized Discounted Cumulative Gain (NDCG) [1] and Mean Average Precision (MAP) [2]. Until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures. The main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function. We propose a probabilistic framework that addresses this challenge by optimizing the expectation of NDCG over all the possible permutations of documents. A relaxation strategy is used to approximate the average of NDCG over the space of permutation, and a bound optimization approach is proposed to make the computation efficient. Extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets. 1},
	author = {Valizadegan, Hamed and Jin, R},
	doi = {10.1561/1500000016},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Valizadegan, Jin - 2009 - Learning to rank by optimizing ndcg measure.pdf:pdf},
	isbn = {9781615679119},
	issn = {1554-0669},
	journal = {Advances in neural {\ldots}},
	mendeley-groups = {Annotated/Ranking},
	pages = {1--9},
	title = {{Learning to rank by optimizing ndcg measure}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2009{\_}0344.pdf},
	year = {2009}
}
@article{Arthur,
	author = {Arthur, David and Vassilvitskii, Sergei},
	file = {:E$\backslash$:/Downloads/Work/2006-13.pdf:pdf},
	mendeley-groups = {11Thesis},
	pages = {1--11},
	title = {{k-means ++ : The Advantages of Careful Seeding}},
	volume = {8}
}
@article{Paige2016,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1804.02086v4},
	author = {Paige, Brooks and Brooks, Dana H and Dy, Jennifer},
	eprint = {arXiv:1804.02086v4},
	file = {:E$\backslash$:/1804.02086.pdf:pdf},
	mendeley-groups = {11Thesis/Disentanglement/Text},
	title = {{Structured Disentangled Representations}},
	year = {2016}
}
@article{Larsson2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1712.10066v1},
	author = {Larsson, Maria and Nilsson, Amanda and K{\aa}geb{\"{a}}ck, Mikael},
	eprint = {arXiv:1712.10066v1},
	file = {:E$\backslash$:/1712.10066.pdf:pdf},
	mendeley-groups = {11Thesis/Disentanglement/Text},
	title = {{Disentangled Representations for Manipulation of Sentiment in Text}},
	year = {2017}
}


@article{Hu2017,
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	archivePrefix = {arXiv},
	arxivId = {1703.00955},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	eprint = {1703.00955},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2017 - Toward Controlled Generation of Text.pdf:pdf},
	mendeley-groups = {11Thesis/Interpretability/GAN's and VAE,11Thesis/Disentanglement/Text},
	title = {{Toward Controlled Generation of Text}},
	url = {http://arxiv.org/abs/1703.00955},
	year = {2017}
}
@article{John2019,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1808.04339v2},
	author = {John, Vineet and Vechtomova, Olga},
	eprint = {arXiv:1808.04339v2},
	file = {:E$\backslash$:/1808.04339.pdf:pdf},
	mendeley-groups = {11Thesis/Disentanglement/Text},
	title = {{Disentangled Representation Learning for Non-Parallel Text Style Transfer}},
	year = {2019}
}
@article{Taddy2015,
	abstract = {There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.},
	archivePrefix = {arXiv},
	arxivId = {1504.07295},
	author = {Taddy, Matt},
	eprint = {1504.07295},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taddy - 2015 - Document Classification by Inversion of Distributed Language Representations.pdf:pdf},
	isbn = {9781941643730},
	journal = {Proceedings of the 53rd meeting of the Association for Computational Linquistics (ACL'15)},
	mendeley-groups = {Annotated/Word Vectors},
	pages = {45--49},
	title = {{Document Classification by Inversion of Distributed Language Representations}},
	url = {http://arxiv.org/abs/1504.07295},
	year = {2015}
}
@article{Shi2017,
	abstract = {Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such "two-step" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.},
	archivePrefix = {arXiv},
	arxivId = {1706.07276},
	author = {Shi, Bei and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Lai, Kwun Ping},
	doi = {10.1145/3077136.3080806},
	eprint = {1706.07276},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2017 - Jointly Learning Word Embeddings and Latent Topics.pdf:pdf},
	isbn = {9781450350228},
	keywords = {china,document modeling,e work described in,grant council of the,hong kong special administrative,project code,region,supported by grants from,the research,this paper is substantially,topic model,word embedding},
	mendeley-groups = {Annotated/Topic models,Annotated/Topic models/Unsupervised Topic Models,!Paper 3/task/newsgroups},
	title = {{Jointly Learning Word Embeddings and Latent Topics}},
	url = {http://arxiv.org/abs/1706.07276{\%}0Ahttp://dx.doi.org/10.1145/3077136.3080806},
	year = {2017}
}
@article{Bengio2012,
	archivePrefix = {arXiv},
	arxivId = {1206.5538},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	doi = {10.1109/TPAMI.2013.50},
	eprint = {1206.5538},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2012 - Representation Learning A Review and New Perspectives.pdf:pdf},
	isbn = {0162-8828 VO - 35},
	issn = {1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	mendeley-groups = {Annotated/Representation Learning,Report/Features,11Thesis},
	number = {8},
	pages = {1798--1828},
	pmid = {23787338},
	title = {{Representation Learning: A Review and New Perspectives}},
	volume = {35},
	year = {2012}
}
@article{Dalessandro2014,
	author = {Dalessandro, C Perlich B and Stitelman, T Raeder O and Provost, F},
	doi = {10.1007/s10994-013-5375-2},
	file = {:E$\backslash$:/Perlich2014{\_}Article{\_}MachineLearningForTargetedDisp.pdf:pdf},
	keywords = {18th st,37 e,b,c,dalessandro,display advertising,editors,f,kiri wagstaff and cynthia,m6d research,new york,ny,o,perlich,predictive modeling,provost,raeder,rudin,stitelman,t,transfer learning,usa},
	mendeley-groups = {11Thesis/Applications},
	pages = {103--127},
	title = {{Machine learning for targeted display advertising : transfer learning in action}},
	year = {2014}
}

@article{Grgic-Hlaca2017,
	abstract = {Consider a binary decision making process where a single machine learning classifier replaces a multitude of humans. We raise questions about the resulting loss of diversity in the decision making process. We study the potential benefits of using random classifier ensembles instead of a single classifier in the context of fairness-aware learning and demonstrate various attractive properties: (i) an ensemble of fair classifiers is guaranteed to be fair, for several different measures of fairness, (ii) an ensemble of unfair classifiers can still achieve fair outcomes, and (iii) an ensemble of classifiers can achieve better accuracy-fairness trade-offs than a single classifier. Finally, we introduce notions of distributional fairness to characterize further potential benefits of random classifier ensembles.},
	archivePrefix = {arXiv},
	arxivId = {1706.10208},
	author = {Grgi{\'{c}}-Hla{\v{c}}a, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	eprint = {1706.10208},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grgi{\'{c}}-Hla{\v{c}}a et al. - 2017 - On Fairness, Diversity and Randomness in Algorithmic Decision Making.pdf:pdf},
	mendeley-groups = {Annotated/Fairness,11Thesis/Interpretability/Discrimination},
	title = {{On Fairness, Diversity and Randomness in Algorithmic Decision Making}},
	url = {http://arxiv.org/abs/1706.10208},
	year = {2017}
}
@article{Nam2014a,
	abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
	archivePrefix = {arXiv},
	arxivId = {1312.5419},
	author = {Nam, Jinseok and Kim, Jungi and {Loza Menc??a}, Eneldo and Gurevych, Iryna and F??rnkranz, Johannes},
	doi = {10.1007/978-3-662-44851-9_28},
	eprint = {1312.5419},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam et al. - 2014 - Large-scale multi-label text classification - Revisiting neural networks.pdf:pdf},
	isbn = {9783662448502},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Literature Review,Progress Report},
	number = {PART 2},
	pages = {437--452},
	title = {{Large-scale multi-label text classification - Revisiting neural networks}},
	volume = {8725 LNAI},
	year = {2014}
}
@article{Nam2014,
	abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
	archivePrefix = {arXiv},
	arxivId = {1312.5419},
	author = {Nam, Jinseok and Kim, Jungi and {Loza Menc{\'{i}}a}, Eneldo and Gurevych, Iryna and F{\"{u}}rnkranz, Johannes},
	doi = {10.1007/978-3-662-44851-9_28},
	eprint = {1312.5419},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam et al. - 2014 - Large-scale multi-label text classification - Revisiting neural networks(2).pdf:pdf},
	isbn = {9783662448502},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	mendeley-groups = {Report/Multi-label,11Thesis/Neural network multi-l;abe},
	number = {PART 2},
	pages = {437--452},
	title = {{Large-scale multi-label text classification - Revisiting neural networks}},
	volume = {8725 LNAI},
	year = {2014}
}
@article{Ager,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders.pdf:pdf},
	title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Agerb,
	author = {Ager, Thomas and Schockaert, Steven},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ager, Schockaert - Unknown - Inducing Symbolic Rules from Entity Embeddings using Auto-encoders(2).pdf:pdf},
	mendeley-groups = {Annotated/Past work,Progress Report},
	title = {{Inducing Symbolic Rules from Entity Embeddings using Auto-encoders}}
}
@article{Hu2017,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1703.00955v4},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
	eprint = {arXiv:1703.00955v4},
	file = {::},
	mendeley-groups = {Thesis/Latent variables},
	title = {{Toward Controlled Generation of Text}},
	year = {2017}
}
@article{Hsu2017,
	author = {Hsu, Wei-ning and Zhang, Yu and Glass, James},
	file = {::},
	mendeley-groups = {Thesis/Latent variables},
	number = {Nips},
	title = {{Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data}},
	year = {2017}
}
@article{,
	file = {::},
	mendeley-groups = {Thesis/Latent variables},
	title = {{Title of dissertation :}}
}
@article{Tsvetkov2014,
	author = {Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah A},
	file = {::},
	mendeley-groups = {Thesis/Sparse reps},
	title = {{Sparse Overcomplete Word Vector Representations}},
	year = {2014}
}
@article{Subramanian,
	author = {Subramanian, Anant and Pruthi, Danish and Jhamtani, Harsh and Berg-kirkpatrick, Taylor and Hovy, Eduard},
	file = {::},
	keywords = {Natural Language Processing and Knowledge Represen},
	mendeley-groups = {Thesis/Sparse reps},
	pages = {4921--4928},
	title = {{SPINE : SParse Interpretable Neural Embeddings}}
}
@article{Faruqui2015a,
	abstract = {Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substan- tial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms.},
	archivePrefix = {arXiv},
	arxivId = {1411.4166},
	author = {Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K and Dyer, Chris and Hovy, Eduard and Smith, Noah A},
	doi = {10.3115/v1/N15-1184},
	eprint = {1411.4166},
	file = {:C$\backslash$:/Users/Workk/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui et al. - 2015 - Retrofitting Word Vectors to Semantic Lexicons.pdf:pdf},
	isbn = {9781941643495},
	journal = {Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL},
	mendeley-groups = {!Paper 3/task/Sentiment treebank,Thesis/Retrofitting Word Vectors},
	number = {i},
	pages = {1606--1615},
	title = {{Retrofitting Word Vectors to Semantic Lexicons}},
	year = {2015}
}
@article{Trifonov,
  title={Learning and Evaluating Sparse Interpretable Sentence Embeddings},
  author={Valentin Trifonov and Octavian-Eugen Ganea and Anna Potapenko and Thomas Hofmann},
  booktitle={BlackboxNLP EMNLP},
  year={2018}
}

@article{Mescheder2018,
abstract = {GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss reg- ularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero- centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high- resolution generative image models for a variety of datasets with little hyperparameter tuning.},
archivePrefix = {arXiv},
arxivId = {1801.04406},
author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
eprint = {1801.04406},
file = {::},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
mendeley-groups = {Thesis/Latent variables},
pages = {5589--5626},
title = {{Which training methods for GANs do actually converge?}},
volume = {8},
year = {2018}
}

@article{Cohen1995,
	author = {Cohen, William},
	year = {1995},
	pages = {},
	title = {Fast Effective Rule Induction},
	journal = {Twelfth International Conference on Machine Learning: 1995},
	doi = {10.1016/B978-1-55860-377-6.50023-2}
}
@article{Mehrabi2019,
abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
archivePrefix = {arXiv},
arxivId = {1908.09635},
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
eprint = {1908.09635},
file = {:E$\backslash$:/1908.09635.pdf:pdf},
title = {{A Survey on Bias and Fairness in Machine Learning}},
url = {http://arxiv.org/abs/1908.09635},
year = {2019}
}


@article{Ribeiro2016a,
abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
archivePrefix = {arXiv},
arxivId = {1606.05386},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1606.05386},
file = {:E$\backslash$:/1606.05386.pdf:pdf},
title = {{Model-Agnostic Interpretability of Machine Learning}},
url = {http://arxiv.org/abs/1606.05386},
year = {2016}
}

@book{James2013,
	added-at = {2019-10-12T20:03:56.000+0200},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	biburl = {https://www.bibsonomy.org/bibtex/2444186c86d18bddb4433c12fa126f6be/lopusz_kdd},
	interhash = {b3febabdc45a8629023cee7323dfbd86},
	intrahash = {444186c86d18bddb4433c12fa126f6be},
	keywords = {general_machine_learning},
	publisher = {Springer},
	timestamp = {2019-10-12T23:45:37.000+0200},
	title = {An Introduction to Statistical Learning: with Applications in R },
	url = {https://faculty.marshall.usc.edu/gareth-james/ISL/},
	year = 2013
}

@article{Manning2002,
	author = {Manning, Christopher D. and Sch{\"{u}}tze, Hinrich and Weikurn, Gerhard},
	doi = {10.1145/601858.601867},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/Manning{\_}Schuetze{\_}StatisticalNLP.pdf:pdf},
	issn = {01635808},
	journal = {SIGMOD Record},
	mendeley-groups = {11Thesis},
	number = {3},
	pages = {37--38},
	title = {{Foundations of Statistical Natural Language Processing}},
	volume = {31},
	year = {2002}
}



@article{Klabunde2002,
	author = {Klabunde, Ralf},
	doi = {10.1515/zfsw.2002.21.1.134},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/NLP book recommendation.pdf:pdf},
	issn = {16133706},
	journal = {Zeitschrift fur Sprachwissenschaft},
	mendeley-groups = {11Thesis},
	number = {1},
	pages = {134--135},
	title = {{Daniel Jurafsky/James H. Martin: Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition}},
	volume = {21},
	year = {2002}
}


@article{Church1989,
	abstract = {The term word association is used in a very particular sense in the$\backslash$npsycholinguistic literature. {\{}(Generally{\}} speaking, subjects respond$\backslash$nquicker than normal to the word nurse if it follows a highly associated$\backslash$nword such as doctor. ) We will extend the term to provide the basis$\backslash$nfor a statistical description of a variety of interesting linguistic$\backslash$nphenomena, ranging from semantic relations of the doctor/nurse type$\backslash$n(content word/content word) to lexico-syntactic co-occurrence constraints$\backslash$nbetween verbs and prepositions (content word/function word). This$\backslash$npaper will propose an objective measure based on the information$\backslash$ntheoretic notion of mutual information, for estimating word association$\backslash$nnorms from computer readable corpora. {\{}(The{\}} standard method of obtaining$\backslash$nword association norms, testing a few thousand subjects on a few$\backslash$nhundred words, is both costly and unreliable.) The proposed measure,$\backslash$nthe association ratio, estimates word association norms directly$\backslash$nfrom computer readable corpora, making it possible to estimate norms$\backslash$nfor tens of thousands of words.},
	author = {Church, Kenneth Ward and Hanks, Patrick},
	doi = {10.3115/981623.981633},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/P89-1010.pdf:pdf},
	pages = {76--83},
	title = {{Word association norms, mutual information, and lexicography}},
	year = {1989}
}

@inbook{Jones1972,
	author = {Sparck Jones, Karen},
	title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
	year = {1988},
	isbn = {0947568212},
	publisher = {Taylor Graham Publishing},
	address = {GBR},
	booktitle = {Document Retrieval Systems},
	pages = {132–142},
	numpages = {11}
}

@article{Sorower2010,
	abstract = {Multi-label Learning is a form of supervised learning where the classification al- gorithm is required to learn from a set of instances, each instance can belong to multiple classes and so after be able to predict a set of class labels for a new in- stance. This is a generalized version of most popular multi-class problems where each instances is restricted to have only one class label. There exists a wide range of applications for multi-labelled predictions, such as text categorization, seman- tic image labeling, gene functionality classification etc. and the scope and interest is increasing with modern applications. This survey paper introduces the task of multi-label prediction (classification), presents the sparse literature in this area in an organized manner, discusses different evaluation metrics and performs a com- parative analysis of the existing algorithms. This paper also relates multi-label problems with similar but different problems that are often reduced to multi-label problems to have access to wide range of multi-label algorithms.},
	author = {Sorower, Ms},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/A{\_}Literature{\_}Survey{\_}on{\_}Algorithms{\_}for{\_}Multi-label{\_}.pdf:pdf},
	journal = {Oregon State University, Corvallis},
	mendeley-groups = {11Thesis},
	number = {March},
	pages = {1--25},
	title = {{A literature survey on algorithms for multi-label learning}},
	url = {http://people.oregonstate.edu/{~}sorowerm/pdf/Qual-Multilabel-Shahed-CompleteVersion.pdf},
	year = {2010}
}
@misc{Geman1992,
	abstract = {Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.},
	author = {Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'{e}}},
	booktitle = {Neural Computation},
	doi = {10.1162/neco.1992.4.1.1},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/VarbiasBiasGeman.pdf:pdf},
	issn = {0899-7667},
	number = {1},
	pages = {1--58},
	title = {{Neural Networks and the Bias/Variance Dilemma}},
	volume = {4},
	year = {1992}
}
@article{zou2018selective,
	title={A selective overview of sparse principal component analysis},
	author={Zou, Hui and Xue, Lingzhou},
	journal={Proceedings of the IEEE},
	volume={106},
	number={8},
	pages={1311--1320},
	year={2018},
	publisher={IEEE}
}

@article{turk1991eigenfaces,
	title={Eigenfaces for recognition},
	author={Turk, Matthew and Pentland, Alex},
	journal={Journal of cognitive neuroscience},
	volume={3},
	number={1},
	pages={71--86},
	year={1991},
	publisher={MIT Press}
}


@article{Larsen2008,
	author = {Larsen, Kai R and Bailey, Christopher N},
	file = {:E$\backslash$:/New PhD Thesis with revisions/Resources/10.1.1.1030.6339.pdf:pdf},
	title = {{Analyzing unstructured text data : using latent categorization to identify intellectual communities in information systems}},
	year = {2008}
}

@article{larsen2008analyzing,
	title={Analyzing unstructured text data: Using latent categorization to identify intellectual communities in information systems},
	author={Larsen, Kai R and Monarchi, David E and Hovorka, Dirk S and Bailey, Christopher N},
	journal={Decision Support Systems},
	volume={45},
	number={4},
	pages={884--896},
	year={2008},
	publisher={Elsevier}
}

@article{jolliffe1995rotation,
	title={Rotation of principal components: choice of normalization constraints},
	author={Jolliffe, Ian T},
	journal={Journal of Applied Statistics},
	volume={22},
	number={1},
	pages={29--35},
	year={1995},
	publisher={Taylor \& Francis}
}

@article{deerwester1990indexing,
	title={Indexing by latent semantic analysis},
	author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
	journal={Journal of the American society for information science},
	volume={41},
	number={6},
	pages={391--407},
	year={1990},
	publisher={Wiley Online Library}
}

@inproceedings{pauca2004text,
	title={Text mining using non-negative matrix factorizations},
	author={Pauca, V Paul and Shahnaz, Farial and Berry, Michael W and Plemmons, Robert J},
	booktitle={Proceedings of the 2004 SIAM International Conference on Data Mining},
	pages={452--456},
	year={2004},
	organization={SIAM}
}

@article{lee1999learning,
	title={Learning the parts of objects by non-negative matrix factorization},
	author={Lee, Daniel D and Seung, H Sebastian},
	journal={Nature},
	volume={401},
	number={6755},
	pages={788--791},
	year={1999},
	publisher={Nature Publishing Group}
}

@article{gefen2017guide,
	title={A guide to text analysis with latent semantic analysis in R with annotated code: Studying online reviews and the stack exchange community},
	author={Gefen, David and Endicott, James E and Fresneda, Jorge E and Miller, Jacob and Larsen, Kai R},
	journal={Communications of the Association for Information Systems},
	volume={41},
	number={1},
	pages={21},
	year={2017}
}

@article{shlens2014tutorial,
	title={A tutorial on principal component analysis},
	author={Shlens, Jonathon},
	journal={arXiv preprint arXiv:1404.1100},
	year={2014}
}

@article{Salton,
	author = {Salton, G. and Wong, A. and Yang, C. S.},
	title = {A Vector Space Model for Automatic Indexing},
	year = {1975},
	issue_date = {Nov. 1975},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {18},
	number = {11},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/361219.361220},
	doi = {10.1145/361219.361220},
	abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
	journal = {Commun. ACM},
	month = nov,
	pages = {613–620},
	numpages = {8},
	keywords = {content analysis, document space, automatic information retrieval, automatic indexing}
}
