\chapter{Introduction}


%Introduction+conclusion are shorter chapters

%Introduction
%Hypothesis
%research questions
%context
%Summary of the contributions
%Good to introduce gently but research questions can be fine
%Doesn't have to be accessible to the man on the street, need computer science background, vector spaces are included in that, so are decision trees and neural networks, if it's borderline reference background

%%Conclusion revisits research questions, provides the answer not just a summary

\section{Introduction}

%%%%% INTRODUCTION

% There is a lot of data, and a lot of good applications of machine-learning using that data

Applications that enable user-generated content e.g.\ Wikipedia, Social Media sites (Facebook, Twitter), Product and movie review sites (IMDB, Rotten Tomatoes, Amazon) and content-aggregation sites (Reddit, Tumblr) have resulted in widely available unstructured text data. This increased availability of text data has resulted in machine-learning models achieving state-of-the-art results on  a variety of problems e.g.\ machine translation \cite{Wu}, question answering \cite{Fisch2016}, or text classification. Text classification tasks are broadly applied, e.g.\  using text data to identify if social media posts or product reviews have a positive or negative sentiment about a product \cite{Burel2018},  identifying social media posts  that are useful to crisis responders during crises \cite{Burel2018}, or  predicting depression in social media users \cite{Aldarwish2017}. 

% Machine-learning models are essentially trying to get features that are meaningful in the domain and can so predict something specific

Historically in order to achieve strong results in Natural Language Processing tasks, expert knowledge was used, for instance in the form of a knowledge base \cite{Lewis1993}. However, as the volume of available text data has risen,  machine-learning techniques that leverage that data have eventually  become state-of-the-art in many fields\footnote{https://github.com/sebastianruder/NLP-progress}. However, to achieve such results, a critical question is how the raw unstructured input text is processed. For instance, when implementing a text classifier, we need to decide what features to present to this classifier. One simple and common method to obtain features is to use the  frequency statistics of words. However, frequency statistics features  leave out information that may benefit a text classifier. For example, a classifier of the sentiment of an Amazon product review would perform better if  information about the context of words was included in its features, so it could distinguish sarcasm.  % resulted in a variety of ways to take advantage of it, 

% Representations are a fundamental part of good machine-learning. 

%Similar to how it would not be possible for a human to solve a problem without a good understanding of the subject area, the first step of solving a problem with  machine-learning  is to obtain a suitable  representation of the data. If this  representation is not good, then no matter what steps are taken to try and solve the problem then they will not yield good results. Representations are typically composed of features, where each feature is some specific property of the domain that can be used to represent an entity. For example, when classifying if a person should be given a loan, people entities are represented by features of how much they earn, if they are a small business owner, and if they have a family. 

% But what is a good representation. How do you get a good representation. What are the advantages/disadvantages? Improtance of unsupervised reprsentations

%One way to obtain a representation of the data that results in strong performance is feature engineering [cite], integrating encoded domain knowledge [cite], or using experts to validate the features [cite].  Manually hand-crafting or improving features takes time and knowledge that is not available for many domains, and so methods have been developed to learn features from data without the need of hand-labelling or encoding expert knowledge [cite].

% What are vector spaces? Why are they used? Why are they important and what are they a part of? What are their disadvantages?

Ideally, features  represent complex domain knowledge while utilising a large amount of data, and are flexible in the information they can represent. This is typically done by inducing a low-dimensional vector space representation. These vector spaces, or 'semantic spaces' represent the semantic relationships between entities (such as words or documents)  spatially. For example, word-vectors \cite{Pennington2014} \cite{Mikolov2013} learned from a large corpora like Wikipedia encode the meaning of words spatially by leveraging their context across millions of documents, resulting in e.g. spatial analogical relationships, where vec(man) corresponds to the vector space representation of the word "man", vec(man) - vec(king) $\approx$ vec(woman) - vec(queen). Similarly, vector space representations of text documents can be learned from unstructured text data and enable strong results on many datasets. However, the problem with these semantic spaces, especially those learned from neural networks, is that despite the fact that these representations clearly capture meaning in a non-trivial way, it is not clear how meaningful features can be extracted from such representations. In particular, the dimensions of these vector space do not typically have any particular meaning, which make it difficult to interpret these representations or to integrate them with domain knowledge.



% What is disentanglement? How does it relate to machine-learning feature Interpretability?  Why is it important?

These limitations of vector representations are addressed in the theory of conceptual spaces, which aims to provide a bridge between vector spaces and symbolic knowledge representation. Conceptual spaces  represent entities (e.g. in a conceptual space of fruit, entities are "orange" "apple" "watermelon") as points in a vector spaces  where the dimensions  correspond to primitive features of the domain (e.g.\ color, shape, taste in a conceptual space of fruit). % and overlapping regions occur that correspond to properties of the domain (e.g. "tasty", "acidic", "bitter", "exotic"). Below, an example is shown of such a vector space.




Related to conceptual spaces is the idea of disentanglement in neural representation learning \cite{Bengio2012}. A disentangled representation is one where the 'factors of variation' have been spatially separated from each other, for example, separating style from content  \cite{Chen2016}  \cite{John2019},  identifying key factors of interest in the medical domain \cite{Banner}. Disentanglement has many benefits, such as  enabling transformation of a factor e.g. sentiment  while leaving the content intact \cite{Larsson2017}, as well as potentially leading to better generalization, increasing the potential for transfer learning and resulting in more efficient learning without a supervised signal \cite{Banner}  \cite{Paige2016}. 

A particularly prominent advantage of disentangled representations is how they benefit learning, in particular by providing an inductive bias in machine-learning models. Disentangled representations and are also linked to the objective of interpretability. As machine-learning has extended into  real-world domains like medicine and policing,  the legality and risk of implementing systems that do not have easily understood features (e.g. vector spaces) has resulted in concerns of safety (providing the wrong decision in a high-risk domain), fairness (using properties like  the race of a person to e.g. deny a loan), and transparency (being able to know what the model is doing and improve it) have not been accommodated. The EU have introduced a legal "Right to explanation", requiring that machine-learning models must be able to explain why they have made a decision about a person. 

%Such publica- tions constitute the evidence drawn upon to sup- port evidence-based medicine (EBM), in which one formulates precise clinical questions with re- spect to the Populations, Interventions, Compara- tors and Outcomes (PICO elements) of interest (Sackett et al., 1996).1


%There are multiple advantages to advancing this area of research. Neural networks have been used to disentangle style and content in a latent space to transfer style \cite{John2019}, disentangle key aspects of clinical questions in a medical text domain (Populations, Interventions, Comparators and Outcomes)  for the sake of efficient model transfer and interpretability \cite{Banner}, transform the sentiment of text while leaving the content intact \cite{Larsson2017}, 

%Variational auto-encoders by enforcing independence \cite{Hu2017}  disentangle key aspects in multiple  domains (images and text) by enforcing statistical independence \cite{Paige2016}.

%Firstly, there are not many approaches that can induce disentangled representations in the domain of text, as 

%"Thus far in NLP, learned distributed represen-
%tations have, with few exceptions (Ruder et al., 2016; He et al., 2017; Zhang et al., 2017), been en- tangled: they indiscriminately encode all aspects of texts. Rather than representing text via a mono- lithic vector, we propose to estimate multiple em- beddings that capture complementary aspects of texts, drawing inspiration from the ML in vision community (Whitney, 2016; Veit et al., 2017a)" \cite{Banner}

%Benefits of disentanglement: Knowing if the model will generalize, transfer learning, more effiicent training when supervised objectivers are not aviavlalb,e  

 %Different from other vector space models, however, a conceptual space is typically composed of several vector spaces, each of which intuitively models a single facet from the given domain. For example, a conceptual space of fruit could be composed of vector spaces modelling color, shape, taste, price, size and weight.

This thesis combines the ideas from conceptual spaces and disentangled representations. It follows work by Derrac \cite{Derrac2015} that followed the assumption that modern vector space methods act as conceptual spaces with regions of properties, and  identified a method to induce features that correspond to properties of entities from the spatial relationships of vector space embeddings (e.g. in a domain of IMDB movie reviews a feature of "Comedy" would be a property of movies).  These features are rankings of entities, derived from vector directions in the representation that go from entities that least have a property (e.g. movies that are the least "Funny") to those that  have it the most. 

Previously, disentanglement has required particular  neural network architectures and learning methods, typically by learning a representation with a requirement that features must be independent from each other \cite{Banner}  \cite{Paige2016}.  Essentially, this thesis investigates the use of the method introduced by Derrac \cite{Derrac2015} as an unsupervised post-processing step to obtain a disentangled representation from existing vector space embeddings, by using the spatial structures of the vector space as features.

For this method, disentangled features are labelled, and e.g. in a domain of movie review documents where movies are represented by a concatenation of all their reviews, words like "Scary" or "Comedy" would be properties that movies are ranked on. However, this thesis does not attempt to validate the interpretability of these associated words or clusters of words. Rather, the focus is on validating that these features are indeed disentangled by testing them on tasks where dense but disentangled features (where the features must correspond to an important domain property and also  encode a large amount of information) would perform well. 

%Fundamentally, both of these views seek to find the essential components that determine why all entities vary in the domain, and use them as features. In the case of text processing which we investigate in this work, the factors of variation found correspond to clusters of words that represent properties in the domain. The representation is considered disentangled if the features obtained are interpretable and  predictive when used in key domain tasks.





 %This, among others, has catapulted machine-learning into the limelight of interpretability, and the result has been swathes of research in both explaining black-box machine-learning models and making machine-learning models interpretable, for good reason.

% What are methods to obtain interpretable features? What are methods to disentangle features?

%Methods that obtain interpretable representations include topic modelling with e.g. Latent Dirchlet Allocation (LDA), Negative Matrix Factorization (NMF), among others. 

%What is the specific case that we want disentangled features for that justify the method?  Explain exactly what the disentangled feature representation is

% What does this thesis do to address that specific case?

% that by separating the key properties of the domain into features,
%The advantage of disentangled representations in this regard is that if the properties of disentangled representations are indeed distinct important concepts in the domain, then that is a first step towards a representation where each feature can be understood by humans. From that representation,  effective machine-learning models can be learned that can explain themselves using these features. However, 

%This thesis offers a new approach for obtaining good disentangled representations, crucially being unsupervised, applied in the text domain, and used as a post-processing step on existing vector space representations. From these features, standard requirements like clustering, classification using simple classifiers, and understanding domains can be achieved, while using the information stored in a representation learned using a large volume of data from e.g. a complicated neural network architecture. 

%ssssssssssssssHowever, these methods lack the flexibility and broad usage of vector spaces, which are applied in a variety of tasks and are a primary component of neural networks, a method that achieves state-of-the-art in many tasks using text data and in other domains.  This thesis follows work by \cite{Derrac2015} who introduced a method to re-organize text document representations that encode semantic relationships such that these spatial relationships are used as features. Essentially, this work investigates unsupervised methods of post-processing vector spaces to re-organize them into disentangled feature representations.

% How does this thesis braek down? How does it address it specifically in terms of chapters?


%In Chapter \ref{ch3} the method introduced by Derrac \cite{Derrac2015} is tested extensively on five different text domains and multiple vector space embeddings in application to text document  classification, finding that they perform well even when using simple classifiers. In Chapter \ref{ch4} the method is used in application to qualitatively investigating neural networks, and in Chapter \ref{ch5} a method to improve these disentangled features is introduced.





% Topic models, NMF


%\begin{itemize}
%	\item Safety
%	\item Troubleshooting, bug fixing, model improvement
%	\item Knowledge learning
%	\item EU's "Right to explanation"
%	\item Discrimination
%\end{itemize}






%Interpretability, etc


\section{Hypothesis}

%Research hypothesis is the start-  don't give it all away - we dont know what we are doing
%(The previous work no clear rhyme or reason to what works)
%Use "disentangled" not "interpretable" because we are not concerned with labels
%In particular, claiming they are "interpretable" doesn't make sense 
%Provide a useful inductive bias to classifiers, across a range of domains and a range of different vector spaces
%High bias, less noise more robust beause it can only use those features
%Linear classifiers are robust because they can only find things that are linear
%Question 1 - What is the best way to use linear models to  obtain a disentangled representation semantic features by using words and a linear model
%Question 2: Useful qualitative insights into the characteristics of the layers of neural network models
%Question 3: How can the quality of the features be improved by fine-tuning the vector space

Vector spaces of text documents encode semantic relationships spatially, e.g. in a domain where documents are amazon product reviews, a vector space that is successful at sentiment analysis will be organized such that documents that are negative (i.e. a one-star review) about the product are distant from those that are positive (i.e. a five-star review), and there will be reviews inbetween (two, three or four star reviews). Vector spaces can be re-organized so that these semantic relationships are used as features, resulting in a disentangled feature representation. This disentangled feature representation will provide a useful inductive bias to classifiers and as it disentangles these properties, it can be used to qualitatively investigate the hidden layers of neural networks to gain valuable insights into how they represent documents. 

%Vector space models of text documents can be re-organized into interpretable feature representations. These interpretable feature representations are useful when used in simple interpretable classifiers of key domain tasks, as their features correspond to important properties in the domain. They are effective in multiple domains and can be derived from many types of vector-space. These interpretable feature representations can be made more accurate to domain knowledge and more interpretable with simple unsupervised procedures that ensure they more closely match a bag-of-words.

\section{Research Questions}

\textbf{Question 1:} How can directions that rank documents on words in a vector space e.g. In a vector space constructed from the raw text of Amazon Product Reviews where documents are reviews, ranking documents on how "good" they are where the most negative review of the product is the lowest ranked and the most positive review is the highest rank, be used to achieve good disentangled representations of documents across a range of domains and from a variety of vector space embeddings?

\textbf{Question 2:} How can these directions that rank documents on words be used to gain qualitative insights into the characteristics of different neural networks?

\textbf{Question 3:} How can these directions that rank documents on words be improved in an unsupervised way?

\section{Contributions}


In Chapter \ref{ch3}, the method to re-organize vector space embeddings into disentangled feature representations introduced by Derrac \cite{Derrac2015} is subject to an extensive qualitative and quantitative analysis in five different domains and four different unsupervised methods for obtaining vector space embeddings from raw text data. The disentangled feature representations are validated quantitatively using Document Classification on key domain tasks, e.g. classifying the genres of movies based on the raw text data of their reviews. The classifier used is a low-depth Decision Tree, limited to a depth of one, two or three. This simple classifier is chosen as it means that predictive performance corresponds to disentanglement, as if the classifier can perform well at a key domain task with e.g. a single feature in a one-depth decision tree, then that feature must have disentangled the key domain concept that the class represents  e.g. when classifying if a movie belongs to the Horror genre there must be a feature that models if a movie is a Horror movie. Similarly, in a depth-2 limited tree the features must correspond to aspects of that class, e.g. when classifying if a movie belongs to the Horror genre there must be features that correspond to if the movie has properties like "Scary" or "Bloody". The method is found to be robust across the domains and variants of the method are introduced for better performance.

In Chapter \ref{ch4} the method described in Chapter \ref{ch3} is used to qualitatively investigate the hidden layers of neural networks, specifically feed-forward networks and auto-encoders. In particular, feed-forward networks are trained on key domain tasks, and the hidden layers of those trained models are re-organized into disentangled feature representations. These disentangled feature representations are quantiatively tested using depth-3 decision trees and the predictive performance of these decision trees is compared to the neural networks. It is found that in some domains, these trees match or even out-perform the neural networks that they came from. A qualitative investigation of the disentangled features from feed-forward networks is conducted and they are found to be meaningful. Finally, auto-encoders are used to obtain a sequence of increasingly small vector space embeddings with the understanding that each one will model more abstract concepts, and these layers are qualitatively investigated using directions. The ability to find relationships between these layers using the properties is investigated qualitatively by attempting to build a domain theory that goes from specific properties in the earlier layers to more abstract ones in the later layers. This work was published in NeSy'16, the Eleventh International Workshop on Neural-Symbolic Learning and Reasoning.

In Chapter \ref{ch5} the conclusion is made from the results of the previous two chapters that the similarity centred objective used to build the vector space embedding, although resulting in meaningful directions can sometimes be counterproductive. Following this, a method is introduced to improve the directions in the vector space embedding at the expense of modelling similarity: First, Positive Pointwise Mutual Information (PPMI) scores for the words that label the interpretable features are obtained. Then, a target ranking for each feature is found by using isotonic regression to obtain values inbetween the PPMI scores and the rankings of the entities. This target ranking is used to train a single layer neural network with a non-linear activation function that attempts to match the rankings of entities to the target ranking. The intention is not to achieve 100\% accuracy, but instead rearrange the rankings so that similarity based information is de-prioritized over more meaningful directions. This results in a performance increase in some low-depth decision trees,  and a qualitatively investigation shows that the entity rankings become more specific and meaningful for the features. This work was published in The SIGNLL Conference on Computational Natural Language Learning (CoNLL) 2018.

%In particular, we use this method of building a representation of entities as a way to convert a vector space into an interpretable representation, for use in an interpretable classifier. The reason that we chose this representation to expand on is because by representing each entity $e$ with a vector $v$ that corresponds to a ranking $r$, the meaning of each dimension is distinct, and we are able to find labels composed of clusters of words for these dimensions. Here, we make the distinction between a property and a word, a property is a natural property of the space that exists in terms of a ranking of entities, and words are the labels we use to describe this property.

%%% HYPOTHESIS


% Introduction to the internet, data, basic text representations
% Introduction to machine learning, benefits of data for machine learning, machine learning representations
% Problems with machine learning, interpretability, lack of interpretability in machine learningg
% We introduce a series of methods for transforming uninterpretable machine learning representations into interpretable ones
% Finding directions in vector spaces and using those to produce interpretable representations
% Fine-tuning these directions to get a better result
% Interpreting and investigating neural networks with these directions


%This brings up an essential point: When using a semantic space, are we taking advantage of relationships that are discriminative or incorrect? The danger of relying on these spaces and the models that use them has greatly affected their adoption in critical application areas like medicine, %Citation needed
%and has raised legal concerns about their application in e.g. determining if someone is suitable for a loan. 


\section{Thesis Structure}

\begin{itemize}
	\item \textbf{Chapter \ref{ch2}} gives an overview of processing unstructured text data to obtain representations, some standard machine-learning classifiers, and provides a background on interpretable representations.
	\item \textbf{Chapter \ref{ch2.5}} introduces and explains the datasets used in this thesis, as well as giving an introduction to the hyper-parameters  used for the machine-learning models in this thesis.
	\item \textbf{Chapter \ref{ch3}} quantitatively and qualitatively investigates how the method introduced by \cite{Derrac2015} can produce disentangled representations of unstructured text documents.
	\item \textbf{Chapter \ref{ch4}} qualitatively investigates the use and application of this method to neural networks, in particular feed-forward networks and auto-encoders.
	\item \textbf{Chapter \ref{ch5}} introduces the method to improve the interpretable feature representation by prioritizing these features over similarity information in the vector space.
	\item \textbf{Chapter \ref{ch6}} provides conclusions on the contributions of this thesis.
\end{itemize}


\section{Summary}

This thesis experimentally validates and improves on a methodology for obtaining disentangled feature representations for text documents. It is validated using document classification tasks, and disentangled features from neural networks are  qualitatively  analysed. The method is unsupervised, acts as a post-processing step on vector space representations, and  disentangles important semantic relationships in the space.  

% Our work is...

%\section{Relationships}

%Vector spaces are representations that reduce the dimensionality of sparse representations like bag-of-words into dense spaces where semantic relationships e.g. two movies being similar to one another, are represented spatially. However, upon reducing this dimensionality the features are no longer interpretable.  One way to interpret what these vector spaces mean follows Conceptual Spaces \ref{????}, where entities in the domain e.g. movies in a domain of movie reviews are represented as points, and properties in the domain are represented as convex regions. The work in Chapter \ref{ch3} details a process where the vector space is transformed so that these properties are used as features, creating an interpretable but dense representation. The introduction goes into further detail about these properties.
% Conceptual spaces

% properties

% using properties as features

%\section{Contributions}\label{ch1:contributions}

% chapter 3 does this
%etc

%\section{Representations}




%\section{Motivation}
%What is text? How is it motivating?

%What are the desiradata of a good representation?
% Unsupervised


%One task of Natural Language Processing is to obtain this semantic understanding from text by obtaining a machine-readable representation that contains domain knowledge. A basic approach to obtain a representation of this text is to represent entities (e.g. reviews, text-posts) by the frequency of their words, see \ref{Bag-of-words-example}.

%\begin{figure}[t]
%	\includegraphics[width=\textwidth]{images/bowbowbow.png}
%	\centering
%	\caption{Bag-of-words  }\label{Bag-of-words-example}
%\end{figure}


 %Below, we show a review with its associated properties labelled.

%\begin{figure}[t]
%	\includegraphics[width=\textwidth]{images/leg_length.png}
%	\centering
%	\caption{Example properties  }\label{IntroDecisionTree}
%\end{figure}

%\subsection{Machine Learning}
% what is machine learning? why do i care?


% Granular
%We can understand these properties to have a degree to which they apply, for example the size of the clothing might be "XXL", "XL", "L", "M" or "S", or the quality may be "Very good", "Good", "Ok", "Bad" or "Very bad". For the former, we may rely on the metadata available from the site itself, but for the latter the way to obtain this information is less clear. Although we may infer that the rating has some indication of these properties, it does not describe the properties or the degree to which the review refers to them. %This kind of information is valuable for making sense of the world of unstructured text, and has broad applications, e.g. The most immediate example is perhaps that they allow for a natural way to implement critique-based recommendation systems, where users can specify how their desired result should relate to a given set of suggestions \cite{viappiani2006preference}. For instance, \cite{Vig:2012:TGE:2362394.2362395} propose a movie recommendation system in which the user can specify that they want to see suggestions for movies that are ``similar to this one, but scarier''. If the property of being scary is adequately modelled as a direction in a semantic space of movies, such critiques can be addressed in a straightforward way. Similarly, in \cite{kovashka2012whittlesearch} a system was developed that can find ``shoes like these but shinier'', based on a semantic space representation that was derived from visual features. Semantic search systems can use such directions to interpret queries involving gradual and possibly ill-defined features, such as ``\emph{popular} holiday destinations in Europe'' \cite{DBLP:conf/sigir/JameelBS17}. While features such as popularity are typically not encoded in traditional knowledge bases, they can often be represented as semantic space directions.  %Copied from CONLL

%\subsection{Directions}\label{intro:directions}


%However, manually labelling these properties and the degrees to which entities (e.g. reviews, text-posts) have them is extremely time-consuming. 

%A potentially ideal system would be as follows: We collect large amounts of unstructured text data, separated into domains, and obtain the properties of each domain from this data, and rank entities on the degree to which they have these properties. In this way, properties would be understood on a scale built from the domain directly, so that each domain has its own meanings for words according to their own idiosyncrasies. As the process does not require any manual labelling the quality of these properties could be improved simply by obtaining more data. Further, as we are learning from unstructured data, not only would this allow us to understand the data in terms of what we know, but it would also introduce us to new ideas that we may not have previously understood. This kind of representation also has value in application to Machine Learning tasks. If we can separate the semantics of the space linearly into properties, we are able to learn simple linear classifiers that perform well. 

%Simple linear classifiers built from a representation composed of rankings on properties have an additional benefit of being more understandable.


% Natural clustering
% Semantically distinct
% Interpretable
% Curse of dimensionality
% Generalizability ("shared factors across many tasks" \cite{Bengio2012})

%What is machine learning? What are its advantages? How is it motivating?

%What are the problems with machine learning? How is it motivating?


%What is domain-specific? What is domain knowledge? %On the web there is a large volume of raw text data, e.g. Reviews of products, movies, anime, books, music, social posts by individuals, self-descriptive text about a website or product, and so on. These can be categorized into domains; each domain has its own quirks, knowledge, and method of being brought about. Although a movie review may sound similar to a book review, they typically differ hugely in the distribution of words used.

%

%\section{Interpretability}\label{ch1:interpret}

%What is interpretability? How is the value of interpretability measured in the real world?

%How can we meet the needs of the real world?  Is it transparancy, the system having easy to understnad components, etc... what  are the different views on what an interpretabile system is?

%What specific interpretability task are we trying to solve? How do we define interpretability? Why is it valuable, where is it used? What was the hypothesis/research question?
%%What are distributional models?
%Most successful approaches in recent times, like vector-spaces, word-vectors, and others, rely on the distributional model of semantics. This model relies on encoding unstructured text e.g. of a movie review, as a vector, where each dimension corresponds to how frequent each word is, we are able to calculate how similar the entities are, e.g. we know that if two movies have a similar distribution of words in their reviews, like frequent use of the word 'scary', or 'horror', then they would have a higher similarity value. These models, also known as 'semantic spaces' encode this similarity information spatially.

%Semantic relationships can be obtained from semantic spaces. 

%applications/need for good interpretability:

%What is a conceptual space? What are entities?  What are properties? What is commonsense reasoning?
%properties of an interpretable classifier:
%\begin{itemize}
%	\item Complexity: 'the magic number is seven plus or minus two' \cite{Saaty2003} also has many positive effects for its users, like lower response times \cite{Narayanan2018, Huysmans2011}, better question answering and confidence for logical problem questions \cite{Huysmans2011} and higher satisfaction \cite{Narayanan2018}.
%	\item Transparancy: 
%	\item Explainability: 
%	\item Generalizability:
%\end{itemize}


%X%X%What is a symbolic approach?  %One approach to making sense of these domains is to produce rules from expert knowledge. An expert in movies would tell you that if the review talks about it being a "cannibal horror film", we can understand that it is likely a scary movie and is related to the original 'Cannibal Holocaust' movie. Encoding this kind of knowledge is difficult, time-consuming, and hard to automate reliably.

%Properties, entities, the benefits and application of a representation formed of these

%Basic introduction to directions, explanation of the utility and application of our approach
%\section{Thesis Overview / Contributions}

%What were our objectives starting out? 
%What are our intentions with how the work in the thesis will be used?
%What are our contributions?
%%What are our aims for this chapter? What do we overall want to do? (Already kind-of said in Chapter 1, but worth repeating I guess in some form)
%In \ref{Chapter3}, we introduce a pipeline that starts with unstructured text, and ends with an interpretable representation of entities represented by properties labelled by clusters of words. Further, we demonstrate the applicability of these representations in a simple Decision Tree that uses just a few of these properties to classify entities. In Figure \ref{ExamplesWithTree}, we show some example movie entities, their associated properties, and a Decision Tree classifying whether or not they are a Horror movie. 

