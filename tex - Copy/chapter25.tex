\chapter{Datasets and Semantic Spaces}\label{ch2.5}


\section{Introduction}\label{chapter3:datasets}

% The domains are text-based
% The domains are specific
% As we are providing qualitative results in this thesis, understanding what the words mean matters as they mean different things for different domains
% We also explain the general rules that we process the data with, and why we do it that way

For the experiments in this thesis, we will use  five different domains, each with their own particular vocabulary and meaning of words in their vocabulary. %This Chapter is intended to give insight into the semantics of the domain, so that results and examples are easier to understand, and to also give technical insight into the methods used to preprocess these datasets.
 This Chapter begins with a section to give insight into the datasets with explanations of each domain, accompanying examples, and their classes. This is followed by technical descriptions of preprocessing methods for the datasets. Finally, we introduce the bag-of-words and semantic space representations built from these preprocessed datasets that will be used in the remainder of the thesis.

\section{Datasets}\label{data:datasets}

First, we go through the history and class names of the datasets to give context, and provide examples of unprocessed text from three domains in Table \ref{ch3:TextExamples}. 

\begin{table}[] 
	\scriptsize
	\begin{tabular}{lp{6.75cm}p{6.75cm}}
		Domain  & Unprocessed                                                                                                                                                                                                                                                                                                                                                                               & Processed       \\
		\midrule[\heavyrulewidth]
		Newsgroups & morgan and guzman will have era's 1 run higher than last year, and  the cubs will be idiots and not pitch harkey as much as hibbard.  castillo won't be good (i think he's a stud pitcher)                                                                                                                                                                                                & morgan guzman eras run higher last year cubs idiots pitch harkey much hibbard castillo wont good think hes stud pitcher                            \\
		Sentiment  & All the world's a stage and its people actors in it--or something like that. Who the hell said that theatre stopped at the orchestra pit--or even at the theatre door? 
		Why is not the audience participants in the theatrical experience, including the story itself?<br /><br />This film was a grand experiment that said: "Hey! the story is you and it 
		needs more than your attention, it needs your active participation"". ""Sometimes we bring the story to you, sometimes you have to go to the story.""<br /><br />Alas no one listened, 
		but that does not mean it should not have been said." & worlds stage people actors something like hell said theatre stopped orchestra pit even theatre door audience participants
		theatrical experience including  story film grand experiment said hey story needs attention needs active participation sometimes bring story sometimes go story alas one listened mean
		said \\
		Reuters    & U.K. MONEY MARKET SHORTAGE FORECAST REVISED DOWN The Bank of England said it had revised its forecast of the shortage in the money market down to 450 mln stg before taking account of its morning operations. At noon the bank had estimated the shortfall at 500 mln stg.                                                                                                               & uk money market shortage forecast revised bank england said revised forecast shortage money market 450 mln stg taking account morning operations noon bank estimated shortfall 500 mln stg     \\
	\end{tabular}
	\caption{Text examples from three domains. For the Movies and Place-Type domains, the original text was not available}\label{ch3:TextExamples}
\end{table}



\textbf{IMDB Sentiment} Where documents are exclusively highly polar IMDB movie reviews, either rated <= 4 out of 10 or >= 7 out of 10. Reviews were collected such that it was limited to include at most 30 reviews from any movie in the collection, as some movies contained many more reviews than others. The corpus is split half and half between positive and negative reviews, with the task being to identify the sentiment of the review. This is a binary classification task.

\textbf{20 Newsgroups\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}} Originating from online news discussion groups from 1995 called Newsgroups, where group email-type discussions are made by users about particular topics within 20 different groups. In this dataset, each document is composed of a topic, where user posts are concatenated together. The groups that topics are categorized by are Atheism, Computer Graphics, Microsoft Windows, IBM PC Hardware, Mac Hardware, X-Window (GUI Software), Automobiles, Motorcycles, Baseball, Hockey, Cryptography, Electronics, Medicine, Space, Christianity, Guns, The Middle East, General Politics and General Religion, which also act as the classes for this dataset when being evaluated. This is a multi-class classification task. %Generally, it can be quite easy to identify if a document belongs to a particular group if it uses a keyword unique to that group, e.g.\ the word "chastity" will almost always mean that the document belongs to the "Christianity" class.

\textbf{Reuters-21578, Distribution 1.0}  Text from the Reuters financial news service in 1987, composed of a headline and body text. The classes were chosen with assistance from personnel at Reuters\footnote{For more detail on the history of the dataset: \url{https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection}}, as a result of which they often can contain jargon. For that reason, explanations are provided with the original names in brackets. The classes are Trade, Grain, Natural Gas (nat-gas), Crude Oil (crude), Sugar, Corn, Vegetable Oil (veg-oil), Ship, Coffee, Wheat, Gold, Acquisitions (acq), Interest, Money/Foreign Exchange (money-fx), Soybean, Oilseed, Earnings and Earnings Forecasts (earn), BOP, Gross National Product (gnp), Dollar (dlr) and Money-Supply.   This task is multi-label. 

\textbf{Place-Types} Taken from work by \hmark{Derrac and Schockaert} \cite{Derrac2015}. Originating from the photo-sharing website flickr, where photos are tagged (i.e.\ words describing the photos like "sepia" or "mountain") by users. 22,816,139 photos were considered, and tags that occurred in place-type taxonomies (Geonames, a taxonomy of man-made and natural features, Foursquare a mostly flat taxonomy of urban man-made places like bars and shops, and the site category for the common-sense knowledge base taxonomy OpenCYC) with more than 1,000 occurrences were chosen as documents. Each document, named after a flickr tag, is composed of all flickr tags where that tag occurred. There are three tasks, generated from the three different place type taxonomies. The Foursquare taxonomy, classifying the 9 top-level categories from Foursquare in September 2013, Arts and Entertainment, College and University, Food, Professional and Other Places, Nightlife Spot, Parks And Outdoors, Shops and Service, Travel and Transport and Residence. the GeoNames taxonomy limited to 7 classes, Stream/Lake, Parks/Area, Road/Railroad, Spot/Building/Farm, Mountain/Hill/Rock, Undersea, and Forest/Heath, and the OpenCYC Taxonomy, which we limited to 25 classes, Aqueduct, Border, Building, Dam, Facility, Foreground, Historical Site, Holy Site, Landmark, Medical Facility, Medical School, Military Place, Monsoon Forest, National Monument, Outdoor Location, Rock Formation, and Room. Naturally as these tasks were derived from taxonomies they are multi-label.\label{datasets:Place-Types}

\textbf{Movies} Taken from work by \hmark{Derrac and Schockaert} \cite{Derrac2015}. The top 50,000 most voted-on movies were chosen for this dataset initially, and reviews were collected from four different sources (Rotten Tomatoes, IMDB, SNAP project's Amazon Reviews \footnote{\url{https:/ /snap.stanford.edu/data/web-Amazon.html}} and the IMDB Sentiment dataset. Then, the top 15,000 movies with the highest number of words were chosen as documents, where each document is composed of all of that movies reviews concatenated together. Three tasks are used to evaluate this dataset: 23 movie genres, specifically Action, Adventure, Animation, Biography, Comedy, Crime, Documentary, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Short, Sport, Thriller, War, Western. 100 of the most common IMDB plot keywords and Age Ratings from the UK and US, USA-G, UK-12-12A, UK-15, UK-18, UK-PG, USA-PG-PG13, USA-R. All of these tasks are multi-label. Although it may be expected that the age-ratings task is multi-class, it is actually multi-label, as a movie can be both UK-12-12A and USA-PG-PG13. \label{datasets:movies} 


\section{Technical Details}\label{ch2.5:technical}

In this section, we describe the vocabulary and document characteristics of each domain. Each domain is preprocessed by converting all words to lower-case,  non-alphanumeric characters are removed and whitespace is stripped such that words are separated by a single space. Words were removed from a standard list of English stop words from the NLTK library \cite{Bird} and we filter out terms that do not occur in at least two documents. Additionally, any words that are not in the top 100,000 most frequent are removed, \hmarkn{this is a standard pre-processing method to remove terms that are not informative.}

\hmark{Each task in the domain has a different number of labelled documents. For each task in each domain, only the labelled data are used and that data is split into 2/3 training data, 1/3 test data. Additionally, 20\% of the training data is removed and used as development data for the hyper-parameters. }


\textbf{IMDB Sentiment}\footnote{Obtained by: \url{https://keras.io/datasets/}, Originally from \url{https://ai.stanford.edu/~amaas/data/sentiment/} \cite{Maas2011a}} When the original corpus was produced, the 50 most frequent terms were removed. It contains 50,000 documents with a vocabulary size of 78,588. After removing terms that did not occur in at least two documents, the vocabulary size was reduced to 55384.  the number of positive instances in the classes is 25,000.


\textbf{20 Newsgroups\footnote{\url{http://qwone.com/~jason/20Newsgroups/}}} Obtained from scikit-learn. \footnote{\url{https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch\textunderscore20newsgroups.html\#sklearn.datasets.\\fetch\textunderscore20newsgroups}} Originally containing 18,846 documents, in this work it is preprocessed using sklearn to remove headers, footers and quotes. Then, empty and duplicate documents are removed, resulting in 18302 documents. \hmark{The size of the original vocabulary is 141,321 and after filtering out terms that did not occur in at least two documents, the end result is a vocabulary of size 51,064. This is a larger change after filtering the vocabulary than for the Sentiment dataset, despite beginning with a larger vocabulary, \hmarkn{this shows that many words in this dataset are only appear in one Newsgroups document}. } The data is not shuffled. The number of positive instances averaged across all classes is 942, exactly 5\%.

\textbf{Reuters-21578, Distribution 1.0} Obtained from NLTK\footnote{\url{https://www.nltk.org/book/ch02.html}} originally containing 10788 documents. After removing empty and duplicate documents the result is 10655 documents. Originally contained 90 classes, but as they were extremely unbalanced all classes that did not have at least 100 positive instances were removed, resulting in 21 classes. The original vocabulary size is 51,001 and as in each other domain all words that did not occur in at least two documents were removed, resulting in a vocabulary size of 22,542. The number of positive instances averaged across all classes is 541, around 5\%. 

\textbf{Place-Types} It originally has a vocabulary size of 746,527 and 1383 documents. This is a very large vocabulary size to document ratio. The end vocabulary for this space was of size 100,000 due to the hard limit. This is roughly equivalent to removing all words that are not in at least 6 documents. As most classes in this domain are extremely sparse (less than 100 positive instances). OpenCYC classes are removed that do not have positive instances for at least 30 documents, leaving us with 17. For the Geonames taxonomy, the same rule resulted in only 7 of 9 categories being used.

\textbf{Movies} Another large dataset with a vocabulary size of 551,080 and a document size of 15,000. However, after investigating the data made available by the authors, it was found that there were a number of duplicate documents. After removing these duplicate documents, we ended up with 13978 documents. In the same way as the Place-Types, the vocabulary hit the hard limit of size 100,000. 



\section{Representations}\label{ch25:reps}

We use the bag-of-words representation of the documents as a baseline. In this case, terms are additionally filtered out that do not occur in at least 0.001\% of documents, as to scale with the number of documents in each domain. From this filtered vocabulary, a bag-of-words is obtained by creating a matrix of documents and words, with  the values of that matrix corresponding to how frequent each word was for each document. However, \hmark{frequency bag-of-words are not able to distinguish between terms that are frequent for a document and terms that are frequent overall. The former are less meaningful for distinguishing the difference between documents than the latter. To accommodate this, words are weighed such that words which occur frequently in a small number of documents are given a higher value than those that occur  frequently overall. Specifically,} Positive Pointwise Mutual Information (PPMI) scores are used, following success in similar work by \hmark{Derrac and Schockaert} \cite{Derrac2015}. See Section \ref{bg:ppmi} for more detail.

For the work in the following chapters, we wanted a variety of different vector space models to test the generality of the proposed methods. Below the choices for the vector space models that are formally described in Section \ref{ch2:vectorspaces} are explained:

\textbf{Multi-Dimensional Scaling (MDS) (See Section \ref{ch2:MDS})}:  Multi-Dimensional Scaling (MDS) is used for comparison, as it was the only space used in the work by \hmark{Derrac and Schockaert} that introduced this method  \cite{Derrac2015}. In this case, the input is a matrix of dissimilarity values between the PPMI vectors of documents of size  $n X n$, where $n$ is the number of documents.  %A non-linear transformation that is used to evaluate the quality of representations when built from a standard BOW-PPMI. Chosen as it performed well in the work introducing this method.

\textbf{Principal Component Analysis (PCA) (See Section \ref{ch2:PCA})}: We use PCA as a linear transformation of the PPMI weighted BoW vectors, as it is a standard dimensionality reduction technique used historically and prevalently today to serve as a baseline reference.

\textbf{Doc2Vec (D2V) (See Section \ref{ch2:Doc2Vec})}: Doc2Vec is inspired by the Skipgram model \cite{DBLP:conf/icml/LeM14}.  It is distributional in the sense that the context of words and documents is used during its learning process. It is used here as a it is of a recent class of neural embedding models, which has been reported in the literature to perform well in document classification tasks. For the Doc2Vec space, the following hyper-parameters are tuned: 

\begin{itemize}
	\item The ${window size} (5, 10, 15)$ referring to the  window of the words that are used as context during training 
	\item The ${min count} (1, 5, 10)$ referring to the minimum frequency of words  
	\item The ${epochs} (50, 100, 200)$ of the network for each size space. 
\end{itemize}



\textbf{Average Word Vectors (AWV)}: Finally, we also learn a document embedding by averaging word vectors, using a pre-trained GloVe word embeddings (See Section \ref{bg:WordVectors}) that was trained on the Wikipedia 2014 + Gigaword 5 corpus\footnote{\url{https://nlp.stanford.edu/projects/glove/}}. While simply averaging word vectors may seem naive, this was found to be a competitive approach for unsupervised representations in several applications \cite{DBLP:conf/naacl/HillCK16}. We simply average the vector representations of the words that appear at least twice in the BoW representation. Strangely, we found that this performed better than weighing the words on frequency or PPMI.

To determine the best Doc2Vec model for each task in each domain, and to investigate the quality of these representations, a linear SVM is used that takes the document representations as input. This SVM is also hyper-parameter tuned to find the best C values    $ C (1.0, 0.01, 0.001, 0.000)$, and if the  weights should be balanced such that positive instances are weighted in proportion to how rare they are ${balanced} (0, 1)$ \hmark{(See Section \ref{ch3:weightsbalance}).}

Unfortunately, Doc2Vec representations could not be obtained for the Movies or Place-Types domains as the original full text was not available, only the bag-of-words.
%%%%%%%%%%%%%%%%%%%%%%%%%




